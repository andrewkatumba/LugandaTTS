{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sne-MkYhbuE8",
        "outputId": "ba6ed631-7b21-4561-b109-d923073e3291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-855eab8c-682b-c136-d62d-2ae33bf0063f)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHlVv-96cAxu",
        "outputId": "d3aac716-7448-4f68-e211-a26c4f08e05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'TTS'...\n",
            "remote: Enumerating objects: 22479, done.\u001b[K\n",
            "remote: Counting objects: 100% (3116/3116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (974/974), done.\u001b[K\n",
            "remote: Total 22479 (delta 2205), reused 2765 (delta 2071), pack-reused 19363\u001b[K\n",
            "Receiving objects: 100% (22479/22479), 127.74 MiB | 27.48 MiB/s, done.\n",
            "Resolving deltas: 100% (16200/16200), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/coqui-ai/TTS/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl6C7YZXca41",
        "outputId": "d9afa472-f6d8-48f9-8aa8-eb1328ba27a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/TTS\n"
          ]
        }
      ],
      "source": [
        "%cd TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvbK6fjXceej",
        "outputId": "f6dab35f-2aea-4342-c564-e42d3156d42a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/TTS\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (4.62.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (3.2.2)\n",
            "Collecting pyworld\n",
            "  Downloading pyworld-0.3.0.tar.gz (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 9.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 58.8 MB/s \n",
            "\u001b[?25hCollecting pysbd\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (1.1.4)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (2.1.0)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (1.19.5)\n",
            "Collecting numba==0.53\n",
            "  Downloading numba-0.53.0-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 68.9 MB/s \n",
            "\u001b[?25hCollecting pypinyin\n",
            "  Downloading pypinyin-0.44.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (3.13)\n",
            "Collecting mecab-python3==1.0.3\n",
            "  Downloading mecab_python3-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (487 kB)\n",
            "\u001b[K     |████████████████████████████████| 487 kB 70.5 MB/s \n",
            "\u001b[?25hCollecting coqpit\n",
            "  Downloading coqpit-0.0.14-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (1.10.0+cu111)\n",
            "Collecting umap-learn==0.5.1\n",
            "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (3.6.4)\n",
            "Collecting librosa==0.8.0\n",
            "  Downloading librosa-0.8.0.tar.gz (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 65.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (0.29.24)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (0.42.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (1.1.5)\n",
            "Collecting unidic-lite==1.0.8\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4 MB 41.4 MB/s \n",
            "\u001b[?25hCollecting gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0\n",
            "  Downloading gruut-2.0.4.tar.gz (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 50.2 MB/s \n",
            "\u001b[?25hCollecting fsspec>=2021.04.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (1.4.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from TTS==0.4.2) (0.10.3.post1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.4.2) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.4.2) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.4.2) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.4.2) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.4.2) (1.5.2)\n",
            "Collecting llvmlite<0.37,>=0.36.0rc1\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 75.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.53->TTS==0.4.2) (57.4.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.5.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS==0.4.2) (2.9.1)\n",
            "Collecting dateparser~=1.0.0\n",
            "  Downloading dateparser-1.0.0-py2.py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 45.5 MB/s \n",
            "\u001b[?25hCollecting gruut-ipa~=0.10.0\n",
            "  Downloading gruut-ipa-0.10.1.tar.gz (38 kB)\n",
            "Collecting jsonlines~=1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: networkx<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS==0.4.2) (2.6.3)\n",
            "Collecting num2words<1.0.0,>=0.5.10\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.4 MB/s \n",
            "\u001b[?25hCollecting python-crfsuite~=0.9.7\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 54.1 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_de~=2.0.0\n",
            "  Downloading gruut_lang_de-2.0.0.tar.gz (18.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.1 MB 40.6 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_it~=2.0.0\n",
            "  Downloading gruut_lang_it-2.0.0.tar.gz (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 54.7 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_nl~=2.0.0\n",
            "  Downloading gruut_lang_nl-2.0.0.tar.gz (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 35.6 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_fr~=2.0.0\n",
            "  Downloading gruut_lang_fr-2.0.0.tar.gz (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 59.0 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_ru~=2.0.0\n",
            "  Downloading gruut_lang_ru-2.0.0.tar.gz (35.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 35.0 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_sv~=2.0.0\n",
            "  Downloading gruut_lang_sv-2.0.0.tar.gz (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 31.2 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_pt~=2.0.0\n",
            "  Downloading gruut_lang_pt-2.0.0.tar.gz (5.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.0 MB 31.9 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_es~=2.0.0\n",
            "  Downloading gruut_lang_es-2.0.0.tar.gz (31.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.4 MB 81.2 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_cs~=2.0.0\n",
            "  Downloading gruut_lang_cs-2.0.0.tar.gz (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from Babel<3.0.0,>=2.8.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS==0.4.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser~=1.0.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS==0.4.2) (2.8.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser~=1.0.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS==0.4.2) (1.5.1)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.7/dist-packages (from dateparser~=1.0.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS==0.4.2) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from jsonlines~=1.2.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS==0.4.2) (1.15.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words<1.0.0,>=0.5.10->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS==0.4.2) (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS==0.4.2) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS==0.4.2) (21.3)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS==0.4.2) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.0->TTS==0.4.2) (3.0.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->TTS==0.4.2) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->TTS==0.4.2) (2.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->TTS==0.4.2) (3.10.0.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->TTS==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->TTS==0.4.2) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->TTS==0.4.2) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->TTS==0.4.2) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->TTS==0.4.2) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS==0.4.2) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS==0.4.2) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS==0.4.2) (1.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS==0.4.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS==0.4.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS==0.4.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS==0.4.2) (2.10)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->TTS==0.4.2) (3.17.3)\n",
            "Building wheels for collected packages: librosa, umap-learn, unidic-lite, gruut, gruut-ipa, gruut-lang-cs, gruut-lang-de, gruut-lang-es, gruut-lang-fr, gruut-lang-it, gruut-lang-nl, gruut-lang-pt, gruut-lang-ru, gruut-lang-sv, pynndescent, pyworld\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.8.0-py3-none-any.whl size=201395 sha256=4b39e0669d795da78d323fdd0bd4301201b1952a066eea87447917a4d8f7eb4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/1e/aa/d91797ae7e1ce11853ee100bee9d1781ae9d750e7458c95afb\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76564 sha256=8cb433c3068b5c50f9ef40a6bd5ee83c321cd10cabc7a6e3756d36a37b45fa21\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e7/bb/347dc0e510803d7116a13d592b10cc68262da56a8eec4dd72f\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658836 sha256=541774078be6386f3ee9dc12c1f189a16d62a46938d8b270fcd451ced17cdf36\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/69/b1/112140b599f2b13f609d485a99e357ba68df194d2079c5b1a2\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.0.4-py3-none-any.whl size=15366560 sha256=eeab49b65f0c2a47cd354a0fd852de7f10c399251d5a34da529ad3fa3193bcc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/8e/69/9ffee642f8977cb5cc7e6a7774be47acf9bd8925ac18c7b5b5\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.10.1-py3-none-any.whl size=42197 sha256=ec9d00369c86c67281674542638333c3c637d5cbdbc79409f6cbe683c2aa0ffd\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/0e/63/7b64d29a1ff4ec113fccc262d3ea8494e8fb20797649d04936\n",
            "  Building wheel for gruut-lang-cs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-cs: filename=gruut_lang_cs-2.0.0-py3-none-any.whl size=7046387 sha256=673650551efabd3204b0d8110bf94d70f39998d09a4933254f3d665413d90fdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/6b/75/354899a9146b15b09c58c9d41ba734217ce6fc3564b23429da\n",
            "  Building wheel for gruut-lang-de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-de: filename=gruut_lang_de-2.0.0-py3-none-any.whl size=18498200 sha256=573f28269006791fe06890fd141c7658185768d6c175eb70b95312a11227ff99\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/e2/2f/fda506cfd22e4bd7435fb38ecf7760f9be1f2d10ab69933b4c\n",
            "  Building wheel for gruut-lang-es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-es: filename=gruut_lang_es-2.0.0-py3-none-any.whl size=32173815 sha256=50a0bf06ace16d52843c9bb448628cf0c787c427d9aca1d6eb6aebe4bb70eba6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/0c/5d/a5dacfeeb7482db19f954fe99e46879629711efb0e5263f1d1\n",
            "  Building wheel for gruut-lang-fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-fr: filename=gruut_lang_fr-2.0.0-py3-none-any.whl size=10987714 sha256=c2ce0019590656a2d8d555fc9564171ac44771ea9493a942396f4769adda87a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/88/5b/2c1e0c55a3fcc7ca30f6f4bb6e809539adfabd2a37477c05d1\n",
            "  Building wheel for gruut-lang-it (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-it: filename=gruut_lang_it-2.0.0-py3-none-any.whl size=2961848 sha256=1b2f7b8b3cf4c51671093ab667fb1b58332084e5480deeb67e3eaae8bb13561b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/4f/83/d46fec44ed3ea7cea8941549972daa30dd55213110b34aa168\n",
            "  Building wheel for gruut-lang-nl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-nl: filename=gruut_lang_nl-2.0.0-py3-none-any.whl size=8580839 sha256=5208f65fdc1afe933af40a48c07443227f94cbaff7a105d305941a7b15fca78a\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/31/d5/16a6e078caa7a3aee14d70f829c01981617ac7a681d7b1b7b9\n",
            "  Building wheel for gruut-lang-pt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-pt: filename=gruut_lang_pt-2.0.0-py3-none-any.whl size=5033211 sha256=91b491b2dd53dfe2689d359fdbfa6f5c9c6ed3c43a31590fa9ac64cbcf1c855d\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/6f/a7/65977a42f8bcb4b86ab078622c0b5e0a6c51f16717386435a7\n",
            "  Building wheel for gruut-lang-ru (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-ru: filename=gruut_lang_ru-2.0.0-py3-none-any.whl size=35301043 sha256=980d120bb470af677486a77f155a9e25b901be1329577d4a4ee53ef8cabf5b20\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/a2/f7/2c70e405a17ffd53a2773b6a10c8376e0bda1b615ddaa1a40b\n",
            "  Building wheel for gruut-lang-sv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-sv: filename=gruut_lang_sv-2.0.0-py3-none-any.whl size=2851739 sha256=fe310a39d2581d2057be2e2a803ef905b1140e5369e41dfebf43faa493b0889f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/97/85/80f8b114b45c1642b535ac2c4d9dd9f0c94529916ac20b6d24\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52603 sha256=4d71c6bbdea54276ecd58533fcad1406e3477dab4a3c64f4a3ebe5bde8c657f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/e9/33/04db1436df0757c42fda8ea6796d7a8586e23c85fac355f476\n",
            "  Building wheel for pyworld (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyworld: filename=pyworld-0.3.0-cp37-cp37m-linux_x86_64.whl size=609476 sha256=a941317bad21003a942786f763e858985fd4c00ed6df5f15fbbfd75892395bc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/7c/11/c775fffa0e1e7b05a6604b4323408a77f80fb4ab304d96b5c6\n",
            "Successfully built librosa umap-learn unidic-lite gruut gruut-ipa gruut-lang-cs gruut-lang-de gruut-lang-es gruut-lang-fr gruut-lang-it gruut-lang-nl gruut-lang-pt gruut-lang-ru gruut-lang-sv pynndescent pyworld\n",
            "Installing collected packages: llvmlite, python-crfsuite, numba, num2words, jsonlines, gruut-ipa, dateparser, pynndescent, gruut-lang-sv, gruut-lang-ru, gruut-lang-pt, gruut-lang-nl, gruut-lang-it, gruut-lang-fr, gruut-lang-es, gruut-lang-de, gruut-lang-cs, gruut, unidic-lite, umap-learn, tensorboardX, pyworld, pysbd, pypinyin, mecab-python3, librosa, fsspec, coqpit, anyascii, TTS\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.1\n",
            "    Uninstalling librosa-0.8.1:\n",
            "      Successfully uninstalled librosa-0.8.1\n",
            "  Running setup.py develop for TTS\n",
            "Successfully installed TTS anyascii-0.3.0 coqpit-0.0.14 dateparser-1.0.0 fsspec-2021.11.1 gruut-2.0.4 gruut-ipa-0.10.1 gruut-lang-cs-2.0.0 gruut-lang-de-2.0.0 gruut-lang-es-2.0.0 gruut-lang-fr-2.0.0 gruut-lang-it-2.0.0 gruut-lang-nl-2.0.0 gruut-lang-pt-2.0.0 gruut-lang-ru-2.0.0 gruut-lang-sv-2.0.0 jsonlines-1.2.0 librosa-0.8.0 llvmlite-0.36.0 mecab-python3-1.0.3 num2words-0.5.10 numba-0.53.0 pynndescent-0.5.5 pypinyin-0.44.0 pysbd-0.3.4 python-crfsuite-0.9.7 pyworld-0.3.0 tensorboardX-2.4.1 umap-learn-0.5.1 unidic-lite-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZD0R_2jPZ02"
      },
      "outputs": [],
      "source": [
        "!mkdir ../data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRg2_HBkPRad",
        "outputId": "5846a1f4-33f4-40e4-f561-6beeff9769d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: /content/data/wavs/22dfd2930027f3e70c73e8e38f2d9e0d.wav  \n",
            "  inflating: /content/data/wavs/2536622ef1418e536894b077b0e8b0bf.wav  \n",
            "  inflating: /content/data/wavs/6fbb4e7aba5be3a3d02394c31228b287.wav  \n",
            "  inflating: /content/data/wavs/cef05c9626616483f192dc32f15920f1.wav  \n",
            "  inflating: /content/data/wavs/dc6ed2919751165fb61272a4b47d600d.wav  \n",
            "  inflating: /content/data/wavs/bb33b96e61669200fc4ff1a6157e6c48.wav  \n",
            "  inflating: /content/data/wavs/c5c9d479a71929896abc81295cc598be.wav  \n",
            "  inflating: /content/data/wavs/dbd662e7529de07814d5af6a640760d8.wav  \n",
            "  inflating: /content/data/wavs/4d909535b2828d0027fa8db8a869521b.wav  \n",
            "  inflating: /content/data/wavs/730e3fb21c1d6eb8a27bf13b8f2d10b0.wav  \n",
            "  inflating: /content/data/wavs/858753ba7550bf670853d773bb47973a.wav  \n",
            "  inflating: /content/data/wavs/a62579709e8353a366f0ee30b1cf9206.wav  \n",
            "  inflating: /content/data/wavs/10757e5ed419fc6bf8cdbf7b1145e89f.wav  \n",
            "  inflating: /content/data/wavs/7c04a1706f7d20324f0fa0315a788a90.wav  \n",
            "  inflating: /content/data/wavs/3fb0997a3bc4e42f7763e156f509fe62.wav  \n",
            "  inflating: /content/data/wavs/013299934be0292525a69d698db981ce.wav  \n",
            "  inflating: /content/data/wavs/89e446267602a3277dda0b6e41edb3a2.wav  \n",
            "  inflating: /content/data/wavs/d776b3d0a412fc16119c2c642c0aa135.wav  \n",
            "  inflating: /content/data/wavs/a71ee5e9067fb93cb728f75ab595881f.wav  \n",
            "  inflating: /content/data/wavs/45b1b056931bfb1873f9222840f8ac64.wav  \n",
            "  inflating: /content/data/wavs/50f1ce039df9f74b87eb812b383fc024.wav  \n",
            "  inflating: /content/data/wavs/c5141a7a020db7724e7ec73250212945.wav  \n",
            "  inflating: /content/data/wavs/613c86d0902dc0942c42b80b276684e5.wav  \n",
            "  inflating: /content/data/wavs/98da0501261cb32070d304bb52071293.wav  \n",
            "  inflating: /content/data/wavs/b1f13ace9458b3b75923dcff7c0142d4.wav  \n",
            "  inflating: /content/data/wavs/9e3cec4978e62e30b3c72d873eb0be68.wav  \n",
            "  inflating: /content/data/wavs/960dd52823b07e4cc5e4a42a5fc43994.wav  \n",
            "  inflating: /content/data/wavs/a765208501f74b0a397b9ed4865425df.wav  \n",
            "  inflating: /content/data/wavs/4aebc6ee9b314c941557a5acf105c914.wav  \n",
            "  inflating: /content/data/wavs/3e9aac6f76da6be2755ced6b92ac2c0b.wav  \n",
            "  inflating: /content/data/wavs/ed6c505dcd9a06033a72172981e75c63.wav  \n",
            "  inflating: /content/data/wavs/817978924aea3e788c781008b406070f.wav  \n",
            "  inflating: /content/data/wavs/50eff194eaf0132306623820bbbf0b1b.wav  \n",
            "  inflating: /content/data/wavs/117955860dc2b59c357721cdc91b65ca.wav  \n",
            "  inflating: /content/data/wavs/80676270a87db704e0e7f3769c0f766d.wav  \n",
            "  inflating: /content/data/wavs/e6e5270934264fa45479c317eea2b9eb.wav  \n",
            "  inflating: /content/data/wavs/2a91ba1c304617f65d33a1b77f378b76.wav  \n",
            "  inflating: /content/data/wavs/b179129daa5d0782e356ce060f6194a8.wav  \n",
            "  inflating: /content/data/wavs/9560612385b3d52f8784b6def292e01a.wav  \n",
            "  inflating: /content/data/wavs/5e8fc0cca8880e71f0ea913eecf00a13.wav  \n",
            "  inflating: /content/data/wavs/c68d0dd0d0a7b400c926ec1d26f9aaf7.wav  \n",
            "  inflating: /content/data/wavs/0ebd00a1909a430765ed31da9353c025.wav  \n",
            "  inflating: /content/data/wavs/ae63602f7f7dc7d8af7f2f67f0037952.wav  \n",
            "  inflating: /content/data/wavs/70572993a844fda74592f3e000e56cdb.wav  \n",
            "  inflating: /content/data/wavs/7fe09984755c2a619ab1c45eaf481f64.wav  \n",
            "  inflating: /content/data/wavs/285f2a9d4925977a917a6071da6ae63c.wav  \n",
            "  inflating: /content/data/wavs/a40edc2daba1aa2403f691d0ec776497.wav  \n",
            "  inflating: /content/data/wavs/5b64a55b0b34f53e22b60ee9d57f7ad7.wav  \n",
            "  inflating: /content/data/wavs/08d393d30a2c82e0e69644b2c27cb2ef.wav  \n",
            "  inflating: /content/data/wavs/66cbc21fdf4aecda2f74481a1cc58a0c.wav  \n",
            "  inflating: /content/data/wavs/4b09ab0c13b0a3fa25d4f72a7459d139.wav  \n",
            "  inflating: /content/data/wavs/3f32ebc1aa6e6dcc7eaf63d0d5e6b0bb.wav  \n",
            "  inflating: /content/data/wavs/9fa1a8dcad9648e9ecda5a75c258909b.wav  \n",
            "  inflating: /content/data/wavs/4983f4e1d5ec0e87c2852787864cdec9.wav  \n",
            "  inflating: /content/data/wavs/feeba20d2e94c44a628543d08b2a3290.wav  \n",
            "  inflating: /content/data/wavs/4e6539d829d83af172451441305b2500.wav  \n",
            "  inflating: /content/data/wavs/ad133c50232c6ea9a1bcb1280aaa76ac.wav  \n",
            "  inflating: /content/data/wavs/8df92e77a255c5ef6eea7fd6d4994c80.wav  \n",
            "  inflating: /content/data/wavs/df13d58daa50d8901fb6e749e133184b.wav  \n",
            "  inflating: /content/data/wavs/080fcbb50174b22be20e5df03cb4e02b.wav  \n",
            "  inflating: /content/data/wavs/a1c44cf886429fd11c85d641a0dbc7f9.wav  \n",
            "  inflating: /content/data/wavs/7d575dc5cf379ef505ae1323b2a71cbc.wav  \n",
            "  inflating: /content/data/wavs/315884422385c34ec2272af1bf6a023d.wav  \n",
            "  inflating: /content/data/wavs/f0bf0d47fa3370ef603e066179296f2c.wav  \n",
            "  inflating: /content/data/wavs/4f14e68ffbbef75c73d78f5c511897cd.wav  \n",
            "  inflating: /content/data/wavs/afa98fe430f13b480104d311c325cea7.wav  \n",
            "  inflating: /content/data/wavs/1e8daab0ce96b1a02b7d96b11da4a8f3.wav  \n",
            "  inflating: /content/data/wavs/f33a8586f3ce3bedf3317d5f7cc49b4c.wav  \n",
            "  inflating: /content/data/wavs/d19a60011cd4b5dc1346950be9fdc46f.wav  \n",
            "  inflating: /content/data/wavs/7d6c750ed247547ccb71e976756c894b.wav  \n",
            "  inflating: /content/data/wavs/cc517048b4a13a5690a193b79877227d.wav  \n",
            "  inflating: /content/data/wavs/c6d873c4dbc164d5a10077c7dae685b4.wav  \n",
            "  inflating: /content/data/wavs/c9cfa091d2763629bedd11f812fd7f6b.wav  \n",
            "  inflating: /content/data/wavs/48178088b2556cc338b6aa8c010cf009.wav  \n",
            "  inflating: /content/data/wavs/1dd8121dddf6cd45be5b08eb59edd5e1.wav  \n",
            "  inflating: /content/data/wavs/545de0e140093931ae31671cb21e0dd7.wav  \n",
            "  inflating: /content/data/wavs/66809dd035036eba59b7999b810d91bf.wav  \n",
            "  inflating: /content/data/wavs/df8be97f69c55f1ce62334118dea02db.wav  \n",
            "  inflating: /content/data/wavs/fa578e79adb341fb6b712b437d3a78aa.wav  \n",
            "  inflating: /content/data/wavs/ce6cf0b6e81b6d49eecc76308881d1af.wav  \n",
            "  inflating: /content/data/wavs/2e8ca1dcc2c764efc2692b30d14fb276.wav  \n",
            "  inflating: /content/data/wavs/783cf4fbd6f4cb670c1377904d8deefa.wav  \n",
            "  inflating: /content/data/wavs/5ef42f7b52ebb7fa8cb7abff973fcbf2.wav  \n",
            "  inflating: /content/data/wavs/5d342003c1cc44e07076ee8762294d14.wav  \n",
            "  inflating: /content/data/wavs/0925682318796f8d2933d824280b7fe5.wav  \n",
            "  inflating: /content/data/wavs/1bc9b21a87f36aa4c3eb9afcccb79fcf.wav  \n",
            "  inflating: /content/data/wavs/bb3dc5ab9938d26e1b90ffa734cde739.wav  \n",
            "  inflating: /content/data/wavs/a67f96dcb3f608328d346dccb24c7a2d.wav  \n",
            "  inflating: /content/data/wavs/001422373f0fd0fb26b1542720d3f0c0.wav  \n",
            "  inflating: /content/data/wavs/a9222d19fb35fa5a10ae51c12f0ed7bb.wav  \n",
            "  inflating: /content/data/wavs/a17e6122226c53774b120740571d3e97.wav  \n",
            "  inflating: /content/data/wavs/29d671840ecf0c893bddbe2048185ea5.wav  \n",
            "  inflating: /content/data/wavs/3a052b3e68abfcc0fade0f58f967828f.wav  \n",
            "  inflating: /content/data/wavs/f77809b18d635501969dbe9f246fd3da.wav  \n",
            "  inflating: /content/data/wavs/1276b17b5ad8a781d849d32845f7a3b2.wav  \n",
            "  inflating: /content/data/wavs/a9dc748936d661c03a380afca9949891.wav  \n",
            "  inflating: /content/data/wavs/b3c84202b9c220cc1627a08f9eceaf79.wav  \n",
            "  inflating: /content/data/wavs/2894f7d234baf6beec12508218a55b8a.wav  \n",
            "  inflating: /content/data/wavs/e3e219c9eb079063d26a28cb2d4ad54a.wav  \n",
            "  inflating: /content/data/wavs/4fd70d0f3fac41c9614b356a1be14e11.wav  \n",
            "  inflating: /content/data/wavs/e8dbc1a5442582d4fc5e3ea77427cdb8.wav  \n",
            "  inflating: /content/data/wavs/3ab8014e5140fed9f128347992999c41.wav  \n",
            "  inflating: /content/data/wavs/0c88736ccde75737df73aa31da5e9e1a.wav  \n",
            "  inflating: /content/data/wavs/35d45ad3cfc947e6ed472ab581bda197.wav  \n",
            "  inflating: /content/data/wavs/eef30fbd770ca3b223d7f28fa63919e3.wav  \n",
            "  inflating: /content/data/wavs/b54028ad0c013caae61c07b7d2e0ca8e.wav  \n",
            "  inflating: /content/data/wavs/74221d653f979c9974f932b180016e3a.wav  \n",
            "  inflating: /content/data/wavs/3657ab0bf0264696647898b4eac8812e.wav  \n",
            "  inflating: /content/data/wavs/2db9883d47561dc08be8bc5ae58e04c3.wav  \n",
            "  inflating: /content/data/wavs/838a20553835034b5da4e5aa1df9b1ce.wav  \n",
            "  inflating: /content/data/wavs/3c25b43c21715fafb119e302ef18a1f4.wav  \n",
            "  inflating: /content/data/wavs/287567a24f4d65176dfdbfa95abd29bd.wav  \n",
            "  inflating: /content/data/wavs/8974713f54e258bf6a5c120320ae3d07.wav  \n",
            "  inflating: /content/data/wavs/fd9467fc949c83c06a5cef5dc930447d.wav  \n",
            "  inflating: /content/data/wavs/b22b019f7acf08fed6602442612dd9f2.wav  \n",
            "  inflating: /content/data/wavs/cd66474c827fe306d2dfbffbf1dea924.wav  \n",
            "  inflating: /content/data/wavs/4987f36fa35e97fd8add168c05f3a4ad.wav  \n",
            "  inflating: /content/data/wavs/f983331662af26cbd1f8723ad4c5d4b9.wav  \n",
            "  inflating: /content/data/wavs/d4b7a637878b8e2b0cbf6e0b629e1a03.wav  \n",
            "  inflating: /content/data/wavs/605637c5c371c295235702e6f3558920.wav  \n",
            "  inflating: /content/data/wavs/c9914054a0b8a9683027441244057584.wav  \n",
            "  inflating: /content/data/wavs/cfb156822374d6dea8efb223541858fc.wav  \n",
            "  inflating: /content/data/wavs/dc748e564564c055f4cf6ead33bfe78e.wav  \n",
            "  inflating: /content/data/wavs/1ad94a15ff9cbbfe58b924ef40c37327.wav  \n",
            "  inflating: /content/data/wavs/f456d0668862f617197d904240b5cd80.wav  \n",
            "  inflating: /content/data/wavs/3bb7bc6718df174ef1d513325e5b3c45.wav  \n",
            "  inflating: /content/data/wavs/546376a6b4cd20c53b1f674ea089099a.wav  \n",
            "  inflating: /content/data/wavs/b1db6598e1956299c0e72fe0beb1adf0.wav  \n",
            "  inflating: /content/data/wavs/cc67fc7727f24c61a3d80326835ef472.wav  \n",
            "  inflating: /content/data/wavs/3e7d1158669509c5bd115ff56e037a2a.wav  \n",
            "  inflating: /content/data/wavs/4fcdf7d68e842b4b94fb50eccd4c4f2d.wav  \n",
            "  inflating: /content/data/wavs/673bd1d8ac46b7ac362d2f0c149cfe95.wav  \n",
            "  inflating: /content/data/wavs/50a76024e6ec746f955dc384ead64d77.wav  \n",
            "  inflating: /content/data/wavs/5f6c590fd07a9b07b7c39bc71a3e9d8e.wav  \n",
            "  inflating: /content/data/wavs/a44d9823a10c9c27fa1dcc9d67d04c9d.wav  \n",
            "  inflating: /content/data/wavs/4440b673f0847052e187aea46a1660fb.wav  \n",
            "  inflating: /content/data/wavs/898fa9a097974434b37d08db9502e682.wav  \n",
            "  inflating: /content/data/wavs/a11d43e2a7079cbbdd7051b358b42cc2.wav  \n",
            "  inflating: /content/data/wavs/684946cbedd803c72626cb61c0f2c3e3.wav  \n",
            "  inflating: /content/data/wavs/b2150be708b661176528cca7f1bbcfe5.wav  \n",
            "  inflating: /content/data/wavs/3dca850d1aa25bdfacdd3c63c059fd94.wav  \n",
            "  inflating: /content/data/wavs/e9afa70b5aead9aa34f759e31e0e0bd4.wav  \n",
            "  inflating: /content/data/wavs/b9b7af6e6d320e45af521365e0fc6f36.wav  \n",
            "  inflating: /content/data/wavs/57317961567157f2da0907e9ceac82e1.wav  \n",
            "  inflating: /content/data/wavs/5fc7a1ef58fb49aa5df466e9394e4246.wav  \n",
            "  inflating: /content/data/wavs/02cabf6d1859de267611ae7f4eff5f00.wav  \n",
            "  inflating: /content/data/wavs/89b6dd9fc28b4fbda0bd33f2806a1680.wav  \n",
            "  inflating: /content/data/wavs/24d70600254e63b62a4e17febf4473da.wav  \n",
            "  inflating: /content/data/wavs/bd9683636017997b0ebac2c0b6bce3c8.wav  \n",
            "  inflating: /content/data/wavs/3566f8643fec537171fa3932fbdaa41b.wav  \n",
            "  inflating: /content/data/wavs/85df4e1f6e78e4285f2881bf79f9d302.wav  \n",
            "  inflating: /content/data/wavs/004b4caf6b131ceb19bd04453d6fd734.wav  \n",
            "  inflating: /content/data/wavs/595c8db41c6e4c11c2f5c6bc259fac11.wav  \n",
            "  inflating: /content/data/wavs/71654a3ed9eadfa11edf22473590f437.wav  \n",
            "  inflating: /content/data/wavs/6e436ba17c83e5896d25d031d92e63fa.wav  \n",
            "  inflating: /content/data/wavs/408c28a2d5560ca74e94f1fd728228cf.wav  \n",
            "  inflating: /content/data/wavs/4aa972ee6e710d33ac45c5b51e8cd180.wav  \n",
            "  inflating: /content/data/wavs/44836a5a3a883d4ae73b950cb5486e46.wav  \n",
            "  inflating: /content/data/wavs/063bdd6b57ad1394a289aa4986c405f8.wav  \n",
            "  inflating: /content/data/wavs/9587a45294b261578e7ff9e0d4e2d984.wav  \n",
            "  inflating: /content/data/wavs/2a7238e3b0ef381718e62fb60ea15877.wav  \n",
            "  inflating: /content/data/wavs/42bd5a83a6fe512ee8d62abdd6e654a5.wav  \n",
            "  inflating: /content/data/wavs/69719eebf6895c2c33107472d1d58d18.wav  \n",
            "  inflating: /content/data/wavs/d558d205d59841fa8dacece2cbc733a6.wav  \n",
            "  inflating: /content/data/wavs/9fe8f6d72e86a04e0011d23a58133626.wav  \n",
            "  inflating: /content/data/wavs/af31c5744f021934adc88703ac551e39.wav  \n",
            "  inflating: /content/data/wavs/8320e1c62633fe367cf9bdf3c60adea9.wav  \n",
            "  inflating: /content/data/wavs/102afdcdd338d13398eee14643009dce.wav  \n",
            "  inflating: /content/data/wavs/b641d7ee20277c671ff0e2cf7ee62360.wav  \n",
            "  inflating: /content/data/wavs/357df8f83535ca71f7f6bab582972524.wav  \n",
            "  inflating: /content/data/wavs/1b82ad06b16b1d44a23fd9e4f843bec4.wav  \n",
            "  inflating: /content/data/wavs/a0811c794a4f165a0a334450e713c712.wav  \n",
            "  inflating: /content/data/wavs/ad97b454ba669c145ab6d219f34ff23f.wav  \n",
            "  inflating: /content/data/wavs/574cd0c519eda33fd21f4c5eeb076aae.wav  \n",
            "  inflating: /content/data/wavs/09a421a3cff24269ad0440b4b2be84d9.wav  \n",
            "  inflating: /content/data/wavs/29cfd72e6f980e7d259138f3cc139931.wav  \n",
            "  inflating: /content/data/wavs/9c23e9eca3b41cc63f07667d4cdfc0ed.wav  \n",
            "  inflating: /content/data/wavs/eb2c89da0eb02d6efe37aaef07ceece7.wav  \n",
            "  inflating: /content/data/wavs/bda1d7181b2456d10290379712a1e432.wav  \n",
            "  inflating: /content/data/wavs/c7dfc1cff823755f81c671cd70575e33.wav  \n",
            "  inflating: /content/data/wavs/eca843316332e3f2ee5caaa67cd3a95c.wav  \n",
            "  inflating: /content/data/wavs/2beaa04dc0e8fb0c98ebb34ba8b40e5f.wav  \n",
            "  inflating: /content/data/wavs/2650385704d04523bf0d3570ed09a72d.wav  \n",
            "  inflating: /content/data/wavs/a6a7cf63664636fad89485a817e75c0d.wav  \n",
            "  inflating: /content/data/wavs/b225343070af177fdfaa39bd1a6b72e1.wav  \n",
            "  inflating: /content/data/wavs/05027cc38ee6b584208956acb7b15bee.wav  \n",
            "  inflating: /content/data/wavs/bd09febe05529f5e10079794d7e41af2.wav  \n",
            "  inflating: /content/data/wavs/ab67d2969fef85dabdbbd3ddce34d26d.wav  \n",
            "  inflating: /content/data/wavs/e02424b9d13e6c54866b86497c268190.wav  \n",
            "  inflating: /content/data/wavs/603806ef08ff0929ab2f0c984be75087.wav  \n",
            "  inflating: /content/data/wavs/ec8d940adddd9b419ed0c3560a99cb45.wav  \n",
            "  inflating: /content/data/wavs/30fd047a248208ca46df8d16d427cb94.wav  \n",
            "  inflating: /content/data/wavs/14e6ef9b0b1e09070e5911e2092c6bdb.wav  \n",
            "  inflating: /content/data/wavs/9adb47394d181a95033e10f76ae7a0f9.wav  \n",
            "  inflating: /content/data/wavs/6fbe11ece04b7faafdcb50f077605bc0.wav  \n",
            "  inflating: /content/data/wavs/02473d3a6ce7173236ddb9e21273c8e6.wav  \n",
            "  inflating: /content/data/wavs/cf5c0e38fec3ec9f2de4ea3d965f6e7b.wav  \n",
            "  inflating: /content/data/wavs/f13b0c1d8a0be4f0e7ba56e812b4a24e.wav  \n",
            "  inflating: /content/data/wavs/c29a973522c7cb5910160308dd160514.wav  \n",
            "  inflating: /content/data/wavs/205eefccefb44b585dc5d39f7dc5fc6a.wav  \n",
            "  inflating: /content/data/wavs/9b8e7063603afd6f77688333c3358f0f.wav  \n",
            "  inflating: /content/data/wavs/b745d9f71f437c75abb2eb68480317a6.wav  \n",
            "  inflating: /content/data/wavs/eb9415987a7039b42968c879515fb444.wav  \n",
            "  inflating: /content/data/wavs/7e7b0f00d1a03100253cf85650e08157.wav  \n",
            "  inflating: /content/data/wavs/ec47fc5adc58cfcd3c5bb0babaa765fc.wav  \n",
            "  inflating: /content/data/wavs/c7651cc6ebd92ee69438018ffdf6781a.wav  \n",
            "  inflating: /content/data/wavs/acd01abcfaaaa936023f7e903fb4cbab.wav  \n",
            "  inflating: /content/data/wavs/7cfcafe56bb7146a83edcbd1b9864a45.wav  \n",
            "  inflating: /content/data/wavs/b7f1660345e5882daf5a73aa2cdd4dc1.wav  \n",
            "  inflating: /content/data/wavs/26b07f8e7230cfa7270b96ea2b03ee59.wav  \n",
            "  inflating: /content/data/wavs/cc458c7874ef9074c667dd27e7411165.wav  \n",
            "  inflating: /content/data/wavs/c60eecf02823a4827cba525bd4805ffc.wav  \n",
            "  inflating: /content/data/wavs/ef91fcbabbbafa82442617ff2c51bb35.wav  \n",
            "  inflating: /content/data/wavs/4fdd71e588d16b907bcd0d111c32050f.wav  \n",
            "  inflating: /content/data/wavs/ec69a03c0ab3d4ae764ed85ba7fef154.wav  \n",
            "  inflating: /content/data/wavs/4dd91c9f7adc0c21405607e2f030d39b.wav  \n",
            "  inflating: /content/data/wavs/9e203e3949212009e2918e4ca6a56243.wav  \n",
            "  inflating: /content/data/wavs/fa477d0dbb976a58beb01d63267d064f.wav  \n",
            "  inflating: /content/data/wavs/bc653364939225740529c15951aeadd7.wav  \n",
            "  inflating: /content/data/wavs/1ba3769234111fcb0ef6c2f47ccbabcf.wav  \n",
            "  inflating: /content/data/wavs/a0320c54473457352ab2245d0301fe67.wav  \n",
            "  inflating: /content/data/wavs/838408d1dc13afd4c53b40f33d84d358.wav  \n",
            "  inflating: /content/data/wavs/3aa70c828012aa5f00629fa7649d257b.wav  \n",
            "  inflating: /content/data/wavs/91e693e2ef11a6b7d9dfefab2eb72042.wav  \n",
            "  inflating: /content/data/wavs/85715d0969c627cbbfcfaebf55c7f2ec.wav  \n",
            "  inflating: /content/data/wavs/06d95313eecc2067b1c28212931aba29.wav  \n",
            "  inflating: /content/data/wavs/b39f7bbf271340514607b79a660f442a.wav  \n",
            "  inflating: /content/data/wavs/dc8cad0e5aa5008e3232ef66bd28a5db.wav  \n",
            "  inflating: /content/data/wavs/bc779973d7f8157096d7a0a02843fba2.wav  \n",
            "  inflating: /content/data/wavs/54b239af15f30e909bfc6c577f96d872.wav  \n",
            "  inflating: /content/data/wavs/047ae66d291e4cfd6cde412144e58e21.wav  \n",
            "  inflating: /content/data/wavs/f65f4457319dcdebda1495ed5f976f8b.wav  \n",
            "  inflating: /content/data/wavs/5209b00d6fc4a371b803825b0d8e7eda.wav  \n",
            "  inflating: /content/data/wavs/00d6c20dd62a3db139054c97f84ec010.wav  \n",
            "  inflating: /content/data/wavs/3d9a18e6344a727cc0a78dd3f4198148.wav  \n",
            "  inflating: /content/data/wavs/c4a5ab4305e951f950941e680d9ec4c1.wav  \n",
            "  inflating: /content/data/wavs/d9499c5fb6add74e42b36cad6bf048c5.wav  \n",
            "  inflating: /content/data/wavs/d84e385d57736e652c5fe5440e8fc94f.wav  \n",
            "  inflating: /content/data/wavs/923b00d2b0a94c94c58bb333633f837b.wav  \n",
            "  inflating: /content/data/wavs/72b69059465728f7fef847456759b41c.wav  \n",
            "  inflating: /content/data/wavs/11463b6a8e2e3f782827d74eb4aeb2a3.wav  \n",
            "  inflating: /content/data/wavs/a1fe0f4a80bd45c4eab70bab4ae6b667.wav  \n",
            "  inflating: /content/data/wavs/0c17e69b6030b1a5b4505bad7a3e7619.wav  \n",
            "  inflating: /content/data/wavs/a0a91ac391b55286d5b0013824cd6286.wav  \n",
            "  inflating: /content/data/wavs/558643446b7307e8da106f246515a836.wav  \n",
            "  inflating: /content/data/wavs/ebdbce8fa5f110656360eab13af64154.wav  \n",
            "  inflating: /content/data/wavs/d90a3873e22c6fe083ca1f5e642a4fef.wav  \n",
            "  inflating: /content/data/wavs/cf9a28151196e4f529d6f48512a3d66c.wav  \n",
            "  inflating: /content/data/wavs/44303ad5178e2d78a34e221576eeccb4.wav  \n",
            "  inflating: /content/data/wavs/d2b7fedb61d0576e35e4c3698e5bb1a3.wav  \n",
            "  inflating: /content/data/wavs/f9979f520ae2dbc23bfdbeaa6cafd12a.wav  \n",
            "  inflating: /content/data/wavs/4295c541ac0075d683c635f82be7a534.wav  \n",
            "  inflating: /content/data/wavs/81bb2fc2fa0f2bc4cdeb4030cda86711.wav  \n",
            "  inflating: /content/data/wavs/04e338988c4ef9a004a69a5a929c43fa.wav  \n",
            "  inflating: /content/data/wavs/21b6c7f69579dbafe753c0de649ff534.wav  \n",
            "  inflating: /content/data/wavs/b2ed966cbbd50f08aa6c214a69ded271.wav  \n",
            "  inflating: /content/data/wavs/3b88846ddd7f3900153a24f9b5a6ea32.wav  \n",
            "  inflating: /content/data/wavs/8ce8da3bdf911cbbfffc86860a980d10.wav  \n",
            "  inflating: /content/data/wavs/c30fa71c16446aedd976f86b04bfd578.wav  \n",
            "  inflating: /content/data/wavs/8d4e9a1108f77b014d3b732a7ee298db.wav  \n",
            "  inflating: /content/data/wavs/17fe3430d5aac6d7dccc3e4c48d34c8d.wav  \n",
            "  inflating: /content/data/wavs/e3a11fa721a28d801d090e9d1aa539e3.wav  \n",
            "  inflating: /content/data/wavs/e7b2fae4049d7d1e6f5918862e38f68e.wav  \n",
            "  inflating: /content/data/wavs/2a27293b33fb64ca4a845d55122b8bc6.wav  \n",
            "  inflating: /content/data/wavs/ae5445e316b4c6f740b4436dfdbbf8f5.wav  \n",
            "  inflating: /content/data/wavs/ceeb0c71c7a65e862cac669b9c8326f2.wav  \n",
            "  inflating: /content/data/wavs/f496e44e3d5f0216441090e352e5d1e9.wav  \n",
            "  inflating: /content/data/wavs/31092451e7929b7ecd2d4803915d7273.wav  \n",
            "  inflating: /content/data/wavs/b145b496c2ff1ac87a2c13128b2792c4.wav  \n",
            "  inflating: /content/data/wavs/e7fbd499cb966c6a555c810e1ec4cf3a.wav  \n",
            "  inflating: /content/data/wavs/4c84d8d683d603ba468e6ff675a2eb8a.wav  \n",
            "  inflating: /content/data/wavs/3be065ba04f4320894fc66b5c65858bf.wav  \n",
            "  inflating: /content/data/wavs/d78b283b7309204f7600e385f27aabee.wav  \n",
            "  inflating: /content/data/wavs/1e74ff812baf1201758039c04096934e.wav  \n",
            "  inflating: /content/data/wavs/c645f5a70ae79f9c92405b5bb62c158f.wav  \n",
            "  inflating: /content/data/wavs/07f4cb7a0e84b73c59286976e1811e7d.wav  \n",
            "  inflating: /content/data/wavs/e85c45509c313712f172b83845f90033.wav  \n",
            "  inflating: /content/data/wavs/6f3e096d061a7521c03414188cd235f5.wav  \n",
            "  inflating: /content/data/wavs/328c05f77cd978d3bf83f53d8920b7af.wav  \n",
            "  inflating: /content/data/wavs/3a23ae6b6f22ed8579a3908bb4b526fa.wav  \n",
            "  inflating: /content/data/wavs/fd8ef221379f00146c7f9e666854430d.wav  \n",
            "  inflating: /content/data/wavs/45ac3b74054ecd0efb57103536b9898c.wav  \n",
            "  inflating: /content/data/wavs/23ee5b1c2e1d39aa07d02447504acf28.wav  \n",
            "  inflating: /content/data/wavs/922b89804f34b0500153491f5321896f.wav  \n",
            "  inflating: /content/data/wavs/cb9d521aeafc8e164a9d41ccf987e2bb.wav  \n",
            "  inflating: /content/data/wavs/7d9ba1e67ced347a3d80522b976ef8e1.wav  \n",
            "  inflating: /content/data/wavs/50e073f775aaa73a9559750233f41ea0.wav  \n",
            "  inflating: /content/data/wavs/bda7cc574d87297de1c059083440fad6.wav  \n",
            "  inflating: /content/data/wavs/bdecd9ec6f629dc3f959996b66fac167.wav  \n",
            "  inflating: /content/data/wavs/f95e27f3a81dbabb5fb94d23c12c4aea.wav  \n",
            "  inflating: /content/data/wavs/dd190f56824d11094422fad3943c212c.wav  \n",
            "  inflating: /content/data/wavs/13a3ad14eeaa3048d3ca5517be7fb68d.wav  \n",
            "  inflating: /content/data/wavs/fc840bfbde0c38384979f7f8eba0995f.wav  \n",
            "  inflating: /content/data/wavs/5860dcccdbcee2a3a69236b93c1413dd.wav  \n",
            "  inflating: /content/data/wavs/383ccad10b8ad86595b9ac075fd58bce.wav  \n",
            "  inflating: /content/data/wavs/b550e85f1e95500ac0194d44eab56d06.wav  \n",
            "  inflating: /content/data/wavs/355bbc307c01ced46c626a8be8cbcbf1.wav  \n",
            "  inflating: /content/data/wavs/7722fc553438d026dc2fd1181c2154b6.wav  \n",
            "  inflating: /content/data/wavs/b7e85314fcd46fee7150ad65657ee8d5.wav  \n",
            "  inflating: /content/data/wavs/556ae4b3f7ee523c5a8e5a1246966bcc.wav  \n",
            "  inflating: /content/data/wavs/a6d1b055a8c13feaedaf20704174d1d5.wav  \n",
            "  inflating: /content/data/wavs/097716642fa8fbcf56d196c12afd1253.wav  \n",
            "  inflating: /content/data/wavs/7517b2e883a11fb3a5deaf3cb311b6aa.wav  \n",
            "  inflating: /content/data/wavs/2f89a2eb7dd53f38641286717395eae1.wav  \n",
            "  inflating: /content/data/wavs/0113cee8a78bec5a9ab3fbbd6700e754.wav  \n",
            "  inflating: /content/data/wavs/2357158c737520232ce85bf547988b64.wav  \n",
            "  inflating: /content/data/wavs/145ca6800634fc87a458a0c69f072968.wav  \n",
            "  inflating: /content/data/wavs/2abae4c46ef65295c2f2e0576eec07dc.wav  \n",
            "  inflating: /content/data/wavs/07baa627255ff071e1650ba36de25cc4.wav  \n",
            "  inflating: /content/data/wavs/97627ea613eab8510a28c2b2dca11e23.wav  \n",
            "  inflating: /content/data/wavs/5a1a69e85a483186a3c3e7c788cfeea5.wav  \n",
            "  inflating: /content/data/wavs/4d8e172ebe5d59bfb17565596eda9cb3.wav  \n",
            "  inflating: /content/data/wavs/41e65625466456ed6c0c07798a3e96ce.wav  \n",
            "  inflating: /content/data/wavs/16246f7de23df489a1c7f71770e5b856.wav  \n",
            "  inflating: /content/data/wavs/8e67ae9aca24c3e1321c62bd6707dd80.wav  \n",
            "  inflating: /content/data/wavs/76243d537fdf0ef5a0b8364e2eaa2a2b.wav  \n",
            "  inflating: /content/data/wavs/c2bf954264489c70841e82c8b7ef809a.wav  \n",
            "  inflating: /content/data/wavs/1c1adb38fc9b1225e664066ef1a08154.wav  \n",
            "  inflating: /content/data/wavs/231c137be2f3141015bbaa7b67502a1f.wav  \n",
            "  inflating: /content/data/wavs/5caaae41bf2e6e0acb372b129409f976.wav  \n",
            "  inflating: /content/data/wavs/f18155174980c4370e97c50b78fe600a.wav  \n",
            "  inflating: /content/data/wavs/10835bb1c3ccdc3ebe75103c71aeceea.wav  \n",
            "  inflating: /content/data/wavs/197b1bb7ee4d70cf71fc4b7cde5d5df2.wav  \n",
            "  inflating: /content/data/wavs/6ef95daf2b831a773a0f16858fd766fb.wav  \n",
            "  inflating: /content/data/wavs/201b62472ddb851dfcda2a524f7c47af.wav  \n",
            "  inflating: /content/data/wavs/b6441d836f2f4f1eea994fca12e3bb3e.wav  \n",
            "  inflating: /content/data/wavs/b86d33b2de4217f1e134ea93a4d8db8d.wav  \n",
            "  inflating: /content/data/wavs/f919957e422b613b29a43eb62e871dc5.wav  \n",
            "  inflating: /content/data/wavs/e9b6377f35b3a4da452d503f8b318948.wav  \n",
            "  inflating: /content/data/wavs/ec33a6d5090e62bcd068d9c70a4b862a.wav  \n",
            "  inflating: /content/data/wavs/9dc95ed4ef87db7fe5f4131b32ceb1a3.wav  \n",
            "  inflating: /content/data/wavs/50f8e54f959d0fcddb2470f62884eaa7.wav  \n",
            "  inflating: /content/data/wavs/774517b34a4304d6a755145fa54de9ec.wav  \n",
            "  inflating: /content/data/wavs/ce07a870c6de0744a2c7e722898a7232.wav  \n",
            "  inflating: /content/data/wavs/2b3547fafb0bb882ad5170c945821fb8.wav  \n",
            "  inflating: /content/data/wavs/1251e4d3d2e67e5354db9b01b05d7129.wav  \n",
            "  inflating: /content/data/wavs/7c1302a8f513eb9a7710613f39a0bd3e.wav  \n",
            "  inflating: /content/data/wavs/c202ff557390c6e1b3e38ede9078793b.wav  \n",
            "  inflating: /content/data/wavs/37fd4bee6f5bbcfaa284543946057d2d.wav  \n",
            "  inflating: /content/data/wavs/405c862e71f57beed4bd3f5d854e282b.wav  \n",
            "  inflating: /content/data/wavs/983e97817abaa63ab9cebf2d0ae0be8c.wav  \n",
            "  inflating: /content/data/wavs/97db76031521bfa214c779c03f84eb8d.wav  \n",
            "  inflating: /content/data/wavs/30a9981b8595953a5bd150fc681aa5fd.wav  \n",
            "  inflating: /content/data/wavs/c375f6b012d1c4aed8bbf0579ed2a2b1.wav  \n",
            "  inflating: /content/data/wavs/cf76d33d08363b64e3fba889248eefbb.wav  \n",
            "  inflating: /content/data/wavs/95e540710ad451721d515458d9a22d68.wav  \n",
            "  inflating: /content/data/wavs/6d80332f7a9fe84bd8730b324df1e145.wav  \n",
            "  inflating: /content/data/wavs/5f0d99720cd25c9c9dcc9f98c2d4c76a.wav  \n",
            "  inflating: /content/data/wavs/aa846a09593912ffea222bddb9168496.wav  \n",
            "  inflating: /content/data/wavs/4c7ac79a2e3c450d4f012870f7d94d7d.wav  \n",
            "  inflating: /content/data/wavs/eeb61c167638e7f220a90a6499bec051.wav  \n",
            "  inflating: /content/data/wavs/4aecfce062a9382588a2ab7bba4a5926.wav  \n",
            "  inflating: /content/data/wavs/ea958c6bdc468d5b689b10b9b82efda8.wav  \n",
            "  inflating: /content/data/wavs/11df43de96e56e9b2af18e664cbb6b8e.wav  \n",
            "  inflating: /content/data/wavs/8df5184961dd3a09dfef100746748364.wav  \n",
            "  inflating: /content/data/wavs/57f6ad8c55411640b27228a56e9e139d.wav  \n",
            "  inflating: /content/data/wavs/4d13d90c43cc0d8d11e3874b3b55ef34.wav  \n",
            "  inflating: /content/data/wavs/0b1e814f23968fd0cf2b0c517a32a379.wav  \n",
            "  inflating: /content/data/wavs/51aa41b8d9a7c3279b87fe3c1313a1e8.wav  \n",
            "  inflating: /content/data/wavs/798c5a7ea0620803dabec98f3620e38d.wav  \n",
            "  inflating: /content/data/wavs/f861a68d74550b24d057c9094ff0064f.wav  \n",
            "  inflating: /content/data/wavs/f37b797099da54be3980100573f67c30.wav  \n",
            "  inflating: /content/data/wavs/013f5412aa820787aa0808d4dfbc73c7.wav  \n",
            "  inflating: /content/data/wavs/ecd37a4a26e6586250c3d25852f19cb3.wav  \n",
            "  inflating: /content/data/wavs/ec1e5dbe15735dc9461d347491a960dc.wav  \n",
            "  inflating: /content/data/wavs/d539840eb4f7c9b9e519871d38e2c4ca.wav  \n",
            "  inflating: /content/data/wavs/0f84fcdd7a9aaf5a04e8dd10a63aba42.wav  \n",
            "  inflating: /content/data/wavs/608b51950af886496b2f4e5c65c33801.wav  \n",
            "  inflating: /content/data/wavs/9cc9abf714123b3eab737c4ee570b41d.wav  \n",
            "  inflating: /content/data/wavs/12b031464b06ea6d04b590c497c37369.wav  \n",
            "  inflating: /content/data/wavs/a1af1c2f9b3b611b4aae36787c8d0a26.wav  \n",
            "  inflating: /content/data/wavs/67ecb3907f4fcfbf7f393beef4fff764.wav  \n",
            "  inflating: /content/data/wavs/b6ecf76da0163ba0ff70fc614e770419.wav  \n",
            "  inflating: /content/data/wavs/94c8f9713aa45234586198b894598d21.wav  \n",
            "  inflating: /content/data/wavs/6f47c79e7740c66c2bedc8543582b94f.wav  \n",
            "  inflating: /content/data/wavs/fecabdf93b26d1d21e9cebdb91cb67a9.wav  \n",
            "  inflating: /content/data/wavs/c50a47a2dc9f1ececf23b9eae712b0d8.wav  \n",
            "  inflating: /content/data/wavs/75e1059249bd7afa5c274ca56328b6ff.wav  \n",
            "  inflating: /content/data/wavs/568aa9ce15c2f12273794875b11c3e9f.wav  \n",
            "  inflating: /content/data/wavs/36f26433c014ac59204c9cb1c20031d0.wav  \n",
            "  inflating: /content/data/wavs/358671fe47e586bbd77485734a61bcb3.wav  \n",
            "  inflating: /content/data/wavs/84c2c4909630d3f30139b887acac9e33.wav  \n",
            "  inflating: /content/data/wavs/f8bca5c17c54edcf76c326562d55096d.wav  \n",
            "  inflating: /content/data/wavs/1694a10b467e32451a128a83ad4863c8.wav  \n",
            "  inflating: /content/data/wavs/abdaf8a0fba14b00c8ad0184274799a2.wav  \n",
            "  inflating: /content/data/wavs/2dd0de0950c92517bd0e28d874f7f445.wav  \n",
            "  inflating: /content/data/wavs/e07824d282135ee97a78a0481f45f40d.wav  \n",
            "  inflating: /content/data/wavs/bdd1c0a0d86ed99ef64d96d668db064f.wav  \n",
            "  inflating: /content/data/wavs/bc66d470f9bb7678fa1327fcb4c44729.wav  \n",
            "  inflating: /content/data/wavs/9d346ea352f4709fb7e87674a5245be9.wav  \n",
            "  inflating: /content/data/wavs/f663b344b1057c170f72dc529b5666c5.wav  \n",
            "  inflating: /content/data/wavs/6d9f4d015789953ad7acef73705fa3e3.wav  \n",
            "  inflating: /content/data/wavs/7c399c54b29532dcfa13e43e26e2b892.wav  \n",
            "  inflating: /content/data/wavs/1ae31f6fb7057c24093d7ed1880fefbf.wav  \n",
            "  inflating: /content/data/wavs/f0fb45d09d0340698c8b668d3c0b891e.wav  \n",
            "  inflating: /content/data/wavs/4b099126ba1dbef966444e3173471d0e.wav  \n",
            "  inflating: /content/data/wavs/e5763a2059323e9e865f4db1879e1c14.wav  \n",
            "  inflating: /content/data/wavs/974161924e733543c62cf758f35265e6.wav  \n",
            "  inflating: /content/data/wavs/619f6017c09421f1e80a0908e7d38e1f.wav  \n",
            "  inflating: /content/data/wavs/e382bf56d646f9cc93546352058b965d.wav  \n",
            "  inflating: /content/data/wavs/ddb91e7f5020a187fe5b57904c72f847.wav  \n",
            "  inflating: /content/data/wavs/d4c01450be93196e26bd10fec0c374d9.wav  \n",
            "  inflating: /content/data/wavs/03ff866e1e351be18a4fbd6ea7d70780.wav  \n",
            "  inflating: /content/data/wavs/62c6039f5d4364524e74cc07dab94479.wav  \n",
            "  inflating: /content/data/wavs/25808f4e715dc0af337a5830b25197b2.wav  \n",
            "  inflating: /content/data/wavs/f438d7df701f73207f6994a29a74be3e.wav  \n",
            "  inflating: /content/data/wavs/c281ca9209c4d5a063b0ced3c5317c02.wav  \n",
            "  inflating: /content/data/wavs/d079ebdc267fe8b69c2eb392a93f0ca9.wav  \n",
            "  inflating: /content/data/wavs/02500bb8c04cf7d380186883fab95555.wav  \n",
            "  inflating: /content/data/wavs/c7b8630e7642f676628745a609b61000.wav  \n",
            "  inflating: /content/data/wavs/d464a585e7976260dd53589a0774920d.wav  \n",
            "  inflating: /content/data/wavs/49ec9ca474a007b4ef3f9aada2806ab5.wav  \n",
            "  inflating: /content/data/wavs/800d1329164ecf12fc2a1ab4b2af26c2.wav  \n",
            "  inflating: /content/data/wavs/cace15197a929b89fea3a37b525e7b0c.wav  \n",
            "  inflating: /content/data/wavs/147ed04b56c26e9943bd5c6be7e57638.wav  \n",
            "  inflating: /content/data/wavs/e5c5e08d0d13b369cf11595792d3b0d8.wav  \n",
            "  inflating: /content/data/wavs/dbaf55f6a7ab0a5169fb91b374a5d4fa.wav  \n",
            "  inflating: /content/data/wavs/54dc82a4d1167482d49ecb789aad0d0b.wav  \n",
            "  inflating: /content/data/wavs/38d6c10fe9603d5e3568715780261573.wav  \n",
            "  inflating: /content/data/wavs/c410de58aed80196023134b011462a25.wav  \n",
            "  inflating: /content/data/wavs/34a9eb75017355eb3864014c41581de1.wav  \n",
            "  inflating: /content/data/wavs/80c761671c17cc7ab9f309c2fec59c99.wav  \n",
            "  inflating: /content/data/wavs/c1898ef157f6836b2660fde78ffae5a6.wav  \n",
            "  inflating: /content/data/wavs/3ccfc4f36d8a7896f330f6dcecd27893.wav  \n",
            "  inflating: /content/data/wavs/4b2148b8bc0b8378b004470443a37c5a.wav  \n",
            "  inflating: /content/data/wavs/fb146cb66c5a995d95dd57ce9e619faa.wav  \n",
            "  inflating: /content/data/wavs/15aafaeab19de001150d0e05ff631a0a.wav  \n",
            "  inflating: /content/data/wavs/8939d40cbc4936a3c1f261d7ad12d6c4.wav  \n",
            "  inflating: /content/data/wavs/d63584ed3729d7a4e511f1ceb12f5d55.wav  \n",
            "  inflating: /content/data/wavs/92127f199fdaeb3d2da8e2b70e628837.wav  \n",
            "  inflating: /content/data/wavs/74172bce3a2857485dda3459d4e0048c.wav  \n",
            "  inflating: /content/data/wavs/a9be159be37bc6463c2914a4b51e5bfb.wav  \n",
            "  inflating: /content/data/wavs/e02495c9844a4659a71a0bfdd44cc46a.wav  \n",
            "  inflating: /content/data/wavs/175243e497d08be181df72099bb04d55.wav  \n",
            "  inflating: /content/data/wavs/041e1b446ed97837a962439cf7ef04bc.wav  \n",
            "  inflating: /content/data/wavs/aca4919426bf704aa045368409fb8d91.wav  \n",
            "  inflating: /content/data/wavs/4600f649224b3cb6d2aed92c07ffb3c9.wav  \n",
            "  inflating: /content/data/wavs/fabfccb637a7e09049e7ffafaddcb1e4.wav  \n",
            "  inflating: /content/data/wavs/d79e418f19f85d7fa00e561db66e2c3e.wav  \n",
            "  inflating: /content/data/wavs/428c76e2c1cd203a8a07ca35ec157397.wav  \n",
            "  inflating: /content/data/wavs/e876269ee5469502e04d84c06ee707c1.wav  \n",
            "  inflating: /content/data/wavs/7e38c91a5e6881d58dabb8413a1b15ae.wav  \n",
            "  inflating: /content/data/wavs/b3afd1559fec0e29937ba74d74e6bf83.wav  \n",
            "  inflating: /content/data/wavs/233484c649d5c53b7697dca3d143dde4.wav  \n",
            "  inflating: /content/data/wavs/ad4f5c7bbb3c8a2448d438d4efa4bca4.wav  \n",
            "  inflating: /content/data/wavs/585b3cf6ca6c15e5f73d29335181cd0f.wav  \n",
            "  inflating: /content/data/wavs/32eeeec1559e82e0ba12c061008e817f.wav  \n",
            "  inflating: /content/data/wavs/aa46d113e38147bb6b9b807e905c03c1.wav  \n",
            "  inflating: /content/data/wavs/c9eee84352927cdde36a9a59b1b84d1d.wav  \n",
            "  inflating: /content/data/wavs/000348cbb7edcde099aedeade840d696.wav  \n",
            "  inflating: /content/data/wavs/5971e93268a12c0a94d59bf81c45adf0.wav  \n",
            "  inflating: /content/data/wavs/fad9e57296c82c3daf4aeb97e4ef6505.wav  \n",
            "  inflating: /content/data/wavs/e1f6fadc1e4466eb8abe718cd1be6cff.wav  \n",
            "  inflating: /content/data/wavs/fabcb64ed088657069c9596aba7eb629.wav  \n",
            "  inflating: /content/data/wavs/176f09f15e4869741c684a5cfce7d7da.wav  \n",
            "  inflating: /content/data/wavs/7e0e5de3986274737dd3db091fedc00e.wav  \n",
            "  inflating: /content/data/wavs/524543c43715eb990c7b24a9d00d3b8e.wav  \n",
            "  inflating: /content/data/wavs/2e2ef2c1e49851e8903dcb1040c3535f.wav  \n",
            "  inflating: /content/data/wavs/9d07c09175aa62a6d786ec5abb05866f.wav  \n",
            "  inflating: /content/data/wavs/7d5f0198ec9810d8c7f09d3d379b08af.wav  \n",
            "  inflating: /content/data/wavs/fce663c78183bf8d497822b198d2fc3c.wav  \n",
            "  inflating: /content/data/wavs/1aeac557bdc97389f1dc1a83b6beda2e.wav  \n",
            "  inflating: /content/data/wavs/dd13a35bd0dbbf3dc29360b38f5cad94.wav  \n",
            "  inflating: /content/data/wavs/c35e64b021a7e1c67d5c082b683b4344.wav  \n",
            "  inflating: /content/data/wavs/238ad7e60c87e7b94aaed29488cc17c8.wav  \n",
            "  inflating: /content/data/wavs/f3244e1b3ef03f6160e266d34fc5a91e.wav  \n",
            "  inflating: /content/data/wavs/1fd9425f60c155b55ee258b08db6bd73.wav  \n",
            "  inflating: /content/data/wavs/a8d675ab82b4f96fd3635d69841f1356.wav  \n",
            "  inflating: /content/data/wavs/f39d314f0c4141a2086422a3a46d29d2.wav  \n",
            "  inflating: /content/data/wavs/f84892cc0be097f43c3a87f4eae61fc0.wav  \n",
            "  inflating: /content/data/wavs/17f9819e18c6438c6f43d403f9ccae5c.wav  \n",
            "  inflating: /content/data/wavs/f7b4828e74048b8ff9640121f1b6c5f7.wav  \n",
            "  inflating: /content/data/wavs/1e6f36c7cce618369628ffa332ef9c3e.wav  \n",
            "  inflating: /content/data/wavs/321f104f639cc72e659075dde8c356e5.wav  \n",
            "  inflating: /content/data/wavs/edb27d44769f566bc996495b086a781f.wav  \n",
            "  inflating: /content/data/wavs/c577c5f2aa66476a9c12c9a2a95d8e92.wav  \n",
            "  inflating: /content/data/wavs/89485512683b30a2a0b4368886878cb4.wav  \n",
            "  inflating: /content/data/wavs/1ad2ec83dfb52ce915c467132286873a.wav  \n",
            "  inflating: /content/data/wavs/daffe0259a81fc11e0929e649598f2df.wav  \n",
            "  inflating: /content/data/wavs/5caa79ba36492a838deb556e60b2d375.wav  \n",
            "  inflating: /content/data/wavs/69b94fec1ddeb8b8acc873985a416db0.wav  \n",
            "  inflating: /content/data/wavs/e3278b21242a8933a9146b2587f09ddd.wav  \n",
            "  inflating: /content/data/wavs/ee3c7f9cc927163cf05367629ec2f6d8.wav  \n",
            "  inflating: /content/data/wavs/9626989239f3df0a2cfd6ce6d316c196.wav  \n",
            "  inflating: /content/data/wavs/6bedbd7f305bc4fa2482bed7d7e8a84a.wav  \n",
            "  inflating: /content/data/wavs/f5bc9a38a639fab89017d7ac0f4ca625.wav  \n",
            "  inflating: /content/data/wavs/b0038235dda80b4c7ce6fcf5943b9cb4.wav  \n",
            "  inflating: /content/data/wavs/ea835b236b7d81c96505d35d0b61ff41.wav  \n",
            "  inflating: /content/data/wavs/c81deb512fa1ee73ed01b956e96eddeb.wav  \n",
            "  inflating: /content/data/wavs/b547af4e598dafbd0c21ec880bbe02bd.wav  \n",
            "  inflating: /content/data/wavs/8bddc346860ebe7fd542e50cdca64f25.wav  \n",
            "  inflating: /content/data/wavs/da9c99cc7e7eb5c32cf46f24c5b9683a.wav  \n",
            "  inflating: /content/data/wavs/fd1018da94bd50391a2281e92142e219.wav  \n",
            "  inflating: /content/data/wavs/58259612c5eb670f485e8f07219134e5.wav  \n",
            "  inflating: /content/data/wavs/11fbe58d8a05240662fc5194b6556774.wav  \n",
            "  inflating: /content/data/wavs/ad9208e26902f4725870eff7b53b9368.wav  \n",
            "  inflating: /content/data/wavs/3a6d9cbd212d331ddf771d9d6976a4cb.wav  \n",
            "  inflating: /content/data/wavs/8dd1cb6c7e08b2462304aeb7cec29f8c.wav  \n",
            "  inflating: /content/data/wavs/082b902713f7d5c341df4310782121d9.wav  \n",
            "  inflating: /content/data/wavs/273fbf241a94c2a9612572e868edda8b.wav  \n",
            "  inflating: /content/data/wavs/4b6dd8d3b899019597f3a903a7a8d56e.wav  \n",
            "  inflating: /content/data/wavs/84631b8abe5251de7b3740c0a76a769b.wav  \n",
            "  inflating: /content/data/wavs/e6574681f975646fe1bfbca5e63aa5d8.wav  \n",
            "  inflating: /content/data/wavs/03d226ff247e46440097672c1b53cb09.wav  \n",
            "  inflating: /content/data/wavs/b3dd9b98d86896231dfed34497e9be24.wav  \n",
            "  inflating: /content/data/wavs/95eac96e56a909742ee0e3228de47258.wav  \n",
            "  inflating: /content/data/wavs/626ce9ec061f57d5494964c4e48c8066.wav  \n",
            "  inflating: /content/data/wavs/35f8c82230e7b3a1ed3ebcb6420f978b.wav  \n",
            "  inflating: /content/data/wavs/8329db08efc438bfff5f265d8472b2b9.wav  \n",
            "  inflating: /content/data/wavs/c208244674087ca22983cdd1544a429d.wav  \n",
            "  inflating: /content/data/wavs/286140e9d6bf4e3b3d543f4cf56aaec1.wav  \n",
            "  inflating: /content/data/wavs/9bb3e8a09914783f26bfac505aba604d.wav  \n",
            "  inflating: /content/data/wavs/c651f99378b863186adf9487ee0d77a5.wav  \n",
            "  inflating: /content/data/wavs/481e99a29c68efc5ddd5eb84143a0162.wav  \n",
            "  inflating: /content/data/wavs/97f91b07483b17b8659e387ad40212a9.wav  \n",
            "  inflating: /content/data/wavs/b6f425115cf8bc20a0073cf8cce39dc8.wav  \n",
            "  inflating: /content/data/wavs/41e05d1ff490d483fbf8f1286082d51b.wav  \n",
            "  inflating: /content/data/wavs/893fbb4875ee19c18096e2812c63753d.wav  \n",
            "  inflating: /content/data/wavs/5d2f94734d0477c822d6d608cf3b7639.wav  \n",
            "  inflating: /content/data/wavs/755a0d734035b49c78278a273e69700c.wav  \n",
            "  inflating: /content/data/wavs/85d4df19458c51ab9c7d062e3b88c83a.wav  \n",
            "  inflating: /content/data/wavs/9cc25bed8e35956b0653a66e974b143f.wav  \n",
            "  inflating: /content/data/wavs/bc413ab671f3d69f3d1d14331476b7af.wav  \n",
            "  inflating: /content/data/wavs/b2facedce1a24a2e184d39975b3bf12d.wav  \n",
            "  inflating: /content/data/wavs/863b095311cac6d27eb1c710ddda7d1a.wav  \n",
            "  inflating: /content/data/wavs/db629de4b356c655fa4a6bb7f2c0eaaa.wav  \n",
            "  inflating: /content/data/wavs/391e00f210c8ea416060d2fabc3f3906.wav  \n",
            "  inflating: /content/data/wavs/4226ac6bc587493352e2eaab6876fa8b.wav  \n",
            "  inflating: /content/data/wavs/44c56582da33270a4dfa2e762bec1eac.wav  \n",
            "  inflating: /content/data/wavs/552bb34e8b181cf89a95c3779c54c252.wav  \n",
            "  inflating: /content/data/wavs/dfe1cc6ada3c1189421d93c69f363a42.wav  \n",
            "  inflating: /content/data/wavs/d0bbbce0fd9b7ad4c18266dcc08f8710.wav  \n",
            "  inflating: /content/data/wavs/b65b4bf4da4c6a1ec2619f027fd6774d.wav  \n",
            "  inflating: /content/data/wavs/7ee75ea540a3488bfdfcd730f514e5f6.wav  \n",
            "  inflating: /content/data/wavs/784cdaa2444546f0cdbc2863d6bc84e9.wav  \n",
            "  inflating: /content/data/wavs/d5f1bc9d8c467822e2ee0d7e4dceebb5.wav  \n",
            "  inflating: /content/data/wavs/66ea1f06c58b69461d082efa91315e7d.wav  \n",
            "  inflating: /content/data/wavs/145e4093314d65d128c146b8d0330168.wav  \n",
            "  inflating: /content/data/wavs/8bbaf28fcec7593c48854d0503b179d7.wav  \n",
            "  inflating: /content/data/wavs/f1a73889f8972f6f163b494509bf36d7.wav  \n",
            "  inflating: /content/data/wavs/e9407818d53ed29f2ea28f2af85e5f96.wav  \n",
            "  inflating: /content/data/wavs/3fd6f22a126cd5cb64bd0ce79f1b0453.wav  \n",
            "  inflating: /content/data/wavs/3df362e14a909c502d96bc6d136a0edb.wav  \n",
            "  inflating: /content/data/wavs/ea135e9d2cc2f6f7e06b7f5f5578e6a5.wav  \n",
            "  inflating: /content/data/wavs/d948693e12f1ce5401e0b67f99d11f5a.wav  \n",
            "  inflating: /content/data/wavs/0e03615e438bf90e11922a4cdad9dd9d.wav  \n",
            "  inflating: /content/data/wavs/1a72883125dfb4a8bfc40ec308ab9f50.wav  \n",
            "  inflating: /content/data/wavs/9b5ed8b82e2099a652d598f8b928e657.wav  \n",
            "  inflating: /content/data/wavs/b4a70de2f5c7194b60d14b53f642532a.wav  \n",
            "  inflating: /content/data/wavs/16357bc3ffe43219ab32ac2af3bc00e5.wav  \n",
            "  inflating: /content/data/wavs/ab45f4ba0123f31e04578b1675e66860.wav  \n",
            "  inflating: /content/data/wavs/9d05ba0bf86035cde4c90cc43a88435d.wav  \n",
            "  inflating: /content/data/wavs/a1a4ead88f97f0fe8f925428fc766af7.wav  \n",
            "  inflating: /content/data/wavs/406350a97c7a26014481f68a04b80259.wav  \n",
            "  inflating: /content/data/wavs/ff14a3bfde1e48d66423f0d08da7a4d2.wav  \n",
            "  inflating: /content/data/wavs/ed6d97779adcce25cb1c0141f8f2f383.wav  \n",
            "  inflating: /content/data/wavs/f82c3935124f6cfa241e313133e6733b.wav  \n",
            "  inflating: /content/data/wavs/a1025d44feaa2d5ae3d20ba61e572d01.wav  \n",
            "  inflating: /content/data/wavs/1fbb344fcdc9e76eb21020295c3fb083.wav  \n",
            "  inflating: /content/data/wavs/175ed9d26d1fa767fa69ead9c2090a35.wav  \n",
            "  inflating: /content/data/wavs/de15e63f940e9d0b82ff7d57808f724d.wav  \n",
            "  inflating: /content/data/wavs/c58524e33990f319e6d8696be00a10df.wav  \n",
            "  inflating: /content/data/wavs/01d44429e599a4dc5f34711420936577.wav  \n",
            "  inflating: /content/data/wavs/810afd0e2f3cf0a35edf50f16b10fea3.wav  \n",
            "  inflating: /content/data/wavs/26c26e898eb50a4c70e42447c77ebfd2.wav  \n",
            "  inflating: /content/data/wavs/bca41d39894da0cb3f4f41c846e9e4e8.wav  \n",
            "  inflating: /content/data/wavs/13d383a6f7d0161353bda09d8450ca0c.wav  \n",
            "  inflating: /content/data/wavs/cea6721aee93bb873b7e7ea119025e2f.wav  \n",
            "  inflating: /content/data/wavs/2c3bf4f2a9a2ff8239cd0880ea8bde62.wav  \n",
            "  inflating: /content/data/wavs/e6f9a36111b1b000de797cc4bce1f946.wav  \n",
            "  inflating: /content/data/wavs/852faabb8a3914e97afe25cfca0d5bba.wav  \n",
            "  inflating: /content/data/wavs/d7144e040ac7dba6dfbfb65f409b965b.wav  \n",
            "  inflating: /content/data/wavs/ebf3d94c0fcd7f62c8f5d472f7c293d5.wav  \n",
            "  inflating: /content/data/wavs/843fa4671f92757ef5e55f1a3a17f253.wav  \n",
            "  inflating: /content/data/wavs/1174f217000a5ac3ba68d420cd2eafc2.wav  \n",
            "  inflating: /content/data/wavs/da2fc89730adc1427d42052ca103579e.wav  \n",
            "  inflating: /content/data/wavs/171fd1adc55432b848f3121ea048bdf5.wav  \n",
            "  inflating: /content/data/wavs/1762afde75b18e8eb70fd393d0e30cd9.wav  \n",
            "  inflating: /content/data/wavs/6109c14b907da6e72decfc3cbfe4eacc.wav  \n",
            "  inflating: /content/data/wavs/c0e8eb83a3bbe11e215aa77e1a7df215.wav  \n",
            "  inflating: /content/data/wavs/b487f4e3866b7a695b1e85767a3ab0c0.wav  \n",
            "  inflating: /content/data/wavs/dde8eebafa7db8db80ef5aaa7776e1b1.wav  \n",
            "  inflating: /content/data/wavs/578a5be2093f00fd0b5b652bf77aeafa.wav  \n",
            "  inflating: /content/data/wavs/ce847b7bb029f383ff1ed5dbaf87de18.wav  \n",
            "  inflating: /content/data/wavs/d9e7a4ca2bf3e932034a100e155fc781.wav  \n",
            "  inflating: /content/data/wavs/172ea11df66697345e53c56c874aaa51.wav  \n",
            "  inflating: /content/data/wavs/7cd0a07449dcd3a64ecc812f9086be95.wav  \n",
            "  inflating: /content/data/wavs/279e15b94bf3edade476d15f48dd4061.wav  \n",
            "  inflating: /content/data/wavs/81c3b9a14872178d5a72788523c209a3.wav  \n",
            "  inflating: /content/data/wavs/de89cceff63e2618edb5d8bbfef820f6.wav  \n",
            "  inflating: /content/data/wavs/85625ade93a53a07064a63132d0e23b5.wav  \n",
            "  inflating: /content/data/wavs/7e854da0d61f5c0a6e373720cad9a990.wav  \n",
            "  inflating: /content/data/wavs/fda2f4a0478ba3b75be7d531e7160d70.wav  \n",
            "  inflating: /content/data/wavs/88d04c7eb833b5abf9c311bcc7278eb9.wav  \n",
            "  inflating: /content/data/wavs/b0fe5bc3164711e3dbea3d69e86b9310.wav  \n",
            "  inflating: /content/data/wavs/9140328351ed07cf2e526b867c9d0ec5.wav  \n",
            "  inflating: /content/data/wavs/e1a2d0664cfd538d0ab83684d26627f6.wav  \n",
            "  inflating: /content/data/wavs/e3d88df0d819214fba0f08077aea96c8.wav  \n",
            "  inflating: /content/data/wavs/36b78b768259393a8b12939a43769b1e.wav  \n",
            "  inflating: /content/data/wavs/5fc774e8d2954a3becf69daf02095e6a.wav  \n",
            "  inflating: /content/data/wavs/de0decff31bbdbf1750c35800d507add.wav  \n",
            "  inflating: /content/data/wavs/42e3a07a77f16240bfa482484cbf0b21.wav  \n",
            "  inflating: /content/data/wavs/7bd75f35dedccddcb36a059dde354a4f.wav  \n",
            "  inflating: /content/data/wavs/2002aecd5ed78e0740df5ee29461943f.wav  \n",
            "  inflating: /content/data/wavs/bf73366647a89dd875a5860d768df35e.wav  \n",
            "  inflating: /content/data/wavs/a244b8055caa519d33d4c0d56dc4fb7e.wav  \n",
            "  inflating: /content/data/wavs/fe5c24b97323a8a7186c31989f72a71d.wav  \n",
            "  inflating: /content/data/wavs/bdde808ee9f6f5a49aa5a9d8d6bcacea.wav  \n",
            "  inflating: /content/data/wavs/70b74dda08010a42b72c7e151b3c62f3.wav  \n",
            "  inflating: /content/data/wavs/90e78cabd686246dd1d68d030905338c.wav  \n",
            "  inflating: /content/data/wavs/adbb8de85f962581fbf8bafc29c45288.wav  \n",
            "  inflating: /content/data/wavs/2e2f83b9e7b08f499499de393c130217.wav  \n",
            "  inflating: /content/data/wavs/c44f4735ec9291277f37a74f9a8ab94a.wav  \n",
            "  inflating: /content/data/wavs/06710ffac4d524062f35f5935b723d35.wav  \n",
            "  inflating: /content/data/wavs/1a780e4a41245b8bec903a0a1e3f7e4f.wav  \n",
            "  inflating: /content/data/wavs/cda6bb976690d175f0878ce59fd8afc6.wav  \n",
            "  inflating: /content/data/wavs/98ad78342836a93184077d786b4b0788.wav  \n",
            "  inflating: /content/data/wavs/db91e4c7b7b6bdb4e144a7ccd58c9b28.wav  \n",
            "  inflating: /content/data/wavs/84593a4dbaf798453d49c68175344364.wav  \n",
            "  inflating: /content/data/wavs/924eea2c2d0aa8f89f775c6e908654ec.wav  \n",
            "  inflating: /content/data/wavs/c4b78dd2817bb1794943d10a32f20915.wav  \n",
            "  inflating: /content/data/wavs/4c8b28f4386a7e0766a0cc60d1f14536.wav  \n",
            "  inflating: /content/data/wavs/e241afa7526a4fbe1b78c7373997bab9.wav  \n",
            "  inflating: /content/data/wavs/e7b881c9207391ce2bd18601322c6482.wav  \n",
            "  inflating: /content/data/wavs/ac6238a62d3ecd4c731b4057a2b1ddf8.wav  \n",
            "  inflating: /content/data/wavs/41797aac9088c2fa1a01f0da68884fa9.wav  \n",
            "  inflating: /content/data/wavs/70b6ffbf289d96436a864741abcc9cd1.wav  \n",
            "  inflating: /content/data/wavs/a7fcce6bf8d0f4f66da9633f9d03811d.wav  \n",
            "  inflating: /content/data/wavs/1817db91097b646a83bece3e1af1fa1b.wav  \n",
            "  inflating: /content/data/wavs/f10e8a8f6ab0fd83849e982d900eaaab.wav  \n",
            "  inflating: /content/data/wavs/4b8b0de7a6afcf0be0ce03e03907e7b0.wav  \n",
            "  inflating: /content/data/wavs/356fc8a8532a08a6983ef6df39ff2a42.wav  \n",
            "  inflating: /content/data/wavs/4b0cccefb59eb0e841cc9aed3038278f.wav  \n",
            "  inflating: /content/data/wavs/73c4aada97997296a2cad6ac6929b27b.wav  \n",
            "  inflating: /content/data/wavs/469c2dc2d7c4c3efdc020b9ba04ef6f5.wav  \n",
            "  inflating: /content/data/wavs/e48923308932868df5102ebf45f250bd.wav  \n",
            "  inflating: /content/data/wavs/ce2256f365e02a6c4a02cb0cbf73b584.wav  \n",
            "  inflating: /content/data/wavs/b053e0b40624d0ff8ae0b8f79f90a4fd.wav  \n",
            "  inflating: /content/data/wavs/9f062a27cf6aae517d7bd7be25f2d410.wav  \n",
            "  inflating: /content/data/wavs/53e6d327d51d51ab35f0f6f2911da638.wav  \n",
            "  inflating: /content/data/wavs/a73185a39cfedf503fe87bf58a593d8c.wav  \n",
            "  inflating: /content/data/wavs/a697e116434a492594a538e22653a67a.wav  \n",
            "  inflating: /content/data/wavs/2b44c7127b287be6e7a1adac0f4298ad.wav  \n",
            "  inflating: /content/data/wavs/5ebb56c19a5865644fa38dbde720e2a7.wav  \n",
            "  inflating: /content/data/wavs/a72111eb7a93c7a73488d1dcd7e94945.wav  \n",
            "  inflating: /content/data/wavs/a7951c2bb35b0f1a60858ca7e392ac8e.wav  \n",
            "  inflating: /content/data/wavs/25ab3ce7bfa54af1389cc3a319b0a7db.wav  \n",
            "  inflating: /content/data/wavs/b605bc7ba25c4367ae3ffc7c04b3304d.wav  \n",
            "  inflating: /content/data/wavs/fca5ac32860dfa7106415e9c1a000541.wav  \n",
            "  inflating: /content/data/wavs/83ec3b95a595879743b3ac05b317f8d0.wav  \n",
            "  inflating: /content/data/wavs/f460a195de1e5cbdb201b3bdeb6f7750.wav  \n",
            "  inflating: /content/data/wavs/9a821e47a6a24552b71064edce28f57d.wav  \n",
            "  inflating: /content/data/wavs/7cce441c47f306e2df2275b913909a34.wav  \n",
            "  inflating: /content/data/wavs/1a8b41c12c0f47770016ad78869c301c.wav  \n",
            "  inflating: /content/data/wavs/9f922e2da870f47ecd488a43ca7964d0.wav  \n",
            "  inflating: /content/data/wavs/9900d59516f378c4489c64b52ca9881f.wav  \n",
            "  inflating: /content/data/wavs/bfa7c9af92f78727f53d07ce84bd425f.wav  \n",
            "  inflating: /content/data/wavs/a4ab49f9c273a2ff7cc3170dcf593057.wav  \n",
            "  inflating: /content/data/wavs/02775dae70214b2c3e88d23533b24b7a.wav  \n",
            "  inflating: /content/data/wavs/eaa110842de607aa2c2ab28b14e8ba04.wav  \n",
            "  inflating: /content/data/wavs/8497fcab42fe8e6134c47bf0a43acfd1.wav  \n",
            "  inflating: /content/data/wavs/f9ffc71896b2480a5ac4fbed504a16cc.wav  \n",
            "  inflating: /content/data/wavs/dab0baf6309cc10d272d3b486a1e3568.wav  \n",
            "  inflating: /content/data/wavs/65fb2c97cc023ac3b82919eb156dffdd.wav  \n",
            "  inflating: /content/data/wavs/ceb54526ba31ca6efe724faf235bb0e1.wav  \n",
            "  inflating: /content/data/wavs/4be9fab9908d261638d64889657af1ea.wav  \n",
            "  inflating: /content/data/wavs/6be4f9df49c67da8ad55d48631edba69.wav  \n",
            "  inflating: /content/data/wavs/9f46f621247380f2368026a58a5f5fcb.wav  \n",
            "  inflating: /content/data/wavs/5e7ef763cc1b70c5d3f330be193bd17f.wav  \n",
            "  inflating: /content/data/wavs/36f09bb40dd793c00e8340a5be73e8f4.wav  \n",
            "  inflating: /content/data/wavs/8f18eece602c337c942fe8f693fe9968.wav  \n",
            "  inflating: /content/data/wavs/752d2b91f93dbd91fbc4f65e3dc0c2f5.wav  \n",
            "  inflating: /content/data/wavs/7c1e34c14e90b333b6aa631f5aa2f7c5.wav  \n",
            "  inflating: /content/data/wavs/c5c4121a1e3d4d524b3265d87b7ffaae.wav  \n",
            "  inflating: /content/data/wavs/c6794bd8c25bdf73aa61a36e33d2a0e6.wav  \n",
            "  inflating: /content/data/wavs/bad18f8cef36625eb4e3b4957262747f.wav  \n",
            "  inflating: /content/data/wavs/640c1192701c2293be526b7067c66570.wav  \n",
            "  inflating: /content/data/wavs/7a476a7b8bcf0f0f0ff17547126654ff.wav  \n",
            "  inflating: /content/data/wavs/13f732ee954701d6b20cc7b1b18e77a0.wav  \n",
            "  inflating: /content/data/wavs/aeb8a709b34f93fdab49e61e81fcba0a.wav  \n",
            "  inflating: /content/data/wavs/e181b2c52fb00b983db55089b5ee0609.wav  \n",
            "  inflating: /content/data/wavs/4ca89bbd8d4faa628bf33a1fcdcc4fc4.wav  \n",
            "  inflating: /content/data/wavs/f4028f126ab56ba108139949b877c449.wav  \n",
            "  inflating: /content/data/wavs/5e6dc2af824cd7d0b72f462d9c217ad2.wav  \n",
            "  inflating: /content/data/wavs/d22f013fd71fd9bf74ca55360ff1a316.wav  \n",
            "  inflating: /content/data/wavs/70a623b1a4d0255624bcbcd8f1ac20b5.wav  \n",
            "  inflating: /content/data/wavs/4d088a565fb7ebaa34b243637d7c3e1d.wav  \n",
            "  inflating: /content/data/wavs/696d52c027fd6f7910ca2f3b9e126498.wav  \n",
            "  inflating: /content/data/wavs/eb86c1ce81dae6702c4d0b5f8d5a0841.wav  \n",
            "  inflating: /content/data/wavs/2605a57f6e8c4eeaee186480a024fa62.wav  \n",
            "  inflating: /content/data/wavs/a556f27478f0fb33ec9472524800bfa0.wav  \n",
            "  inflating: /content/data/wavs/3dd4c9de59de68b7488b444b58778636.wav  \n",
            "  inflating: /content/data/wavs/4f012b6a1e1174367fc479a53f044447.wav  \n",
            "  inflating: /content/data/wavs/2993bf47c20f86fca69a7526cad07ede.wav  \n",
            "  inflating: /content/data/wavs/dc7b8898e890eadc70f75a8b7e30fafb.wav  \n",
            "  inflating: /content/data/wavs/3e272d8117922b6607401427cf5855ac.wav  \n",
            "  inflating: /content/data/wavs/8fbaa435b6d66b51e4205fa1c36743ae.wav  \n",
            "  inflating: /content/data/wavs/96c9ac78ec1a8808e477ed69da33ad84.wav  \n",
            "  inflating: /content/data/wavs/b260fbf9bb5eea62a47fb80b8e2e15df.wav  \n",
            "  inflating: /content/data/wavs/cdfbab801bad1c2a3518572bab7db5bf.wav  \n",
            "  inflating: /content/data/wavs/9261117b124820fc9a8bec4611aae7d5.wav  \n",
            "  inflating: /content/data/wavs/aa5b7c2f67a18fb5040dbc993e745edf.wav  \n",
            "  inflating: /content/data/wavs/191c6a00a45098c68932a0f4375db955.wav  \n",
            "  inflating: /content/data/wavs/662c6b4118892cb8bb94442401b67cd8.wav  \n",
            "  inflating: /content/data/wavs/1bf74709aa9dac98fc9f0e21db7f4b0f.wav  \n",
            "  inflating: /content/data/wavs/b1252d27068fba0024dfee55638bdde7.wav  \n",
            "  inflating: /content/data/wavs/7b35c57d3785328f014c33c21269c050.wav  \n",
            "  inflating: /content/data/wavs/316cf01d4a896e27fa02ff1fad8309d5.wav  \n",
            "  inflating: /content/data/wavs/b35cfa1391e7ff9545284406bc103492.wav  \n",
            "  inflating: /content/data/wavs/66501761bc21f32f7e68dce55d9c6bab.wav  \n",
            "  inflating: /content/data/wavs/609761d88e8faa99ed04aaa0aca12f8a.wav  \n",
            "  inflating: /content/data/wavs/45dc2e2567a7b970e85fe73217597c7a.wav  \n",
            "  inflating: /content/data/wavs/527146867d8771eff62a098f80955900.wav  \n",
            "  inflating: /content/data/wavs/b287b462cfa89c1cb9b258a61d46df42.wav  \n",
            "  inflating: /content/data/wavs/492feeb9b396ddf5c09d8223edb46db6.wav  \n",
            "  inflating: /content/data/wavs/51eb5ff472d75df4e7bfbd471fb19cf8.wav  \n",
            "  inflating: /content/data/wavs/a2852db1294c1591d1c8a4bf04d7d3ca.wav  \n",
            "  inflating: /content/data/wavs/3ba264697f0f7bde612b075a9cffa568.wav  \n",
            "  inflating: /content/data/wavs/8b69ce890a47e42b1e9ad42475738747.wav  \n",
            "  inflating: /content/data/wavs/4b9e2c7de3e09234238e7d1d700716f5.wav  \n",
            "  inflating: /content/data/wavs/2c631f94beeb05b6e40289488f6e225a.wav  \n",
            "  inflating: /content/data/wavs/90eaf229960dc0225dc402f5c6ee5bc9.wav  \n",
            "  inflating: /content/data/wavs/a596dce7ba493263aa3ddc092044e379.wav  \n",
            "  inflating: /content/data/wavs/5735461d549ea2d5b6e737aa4b612e4d.wav  \n",
            "  inflating: /content/data/wavs/b9d9a70cee3a99eded8eb2a079a29465.wav  \n",
            "  inflating: /content/data/wavs/d997c4503ee1404219f21f5e66c15330.wav  \n",
            "  inflating: /content/data/wavs/af43f2051a23325767f0e8d2c85c9c94.wav  \n",
            "  inflating: /content/data/wavs/78a487fde4867a79b9c8bc9e3c119b07.wav  \n",
            "  inflating: /content/data/wavs/47dd2920b1a278f432e79f3caf692810.wav  \n",
            "  inflating: /content/data/wavs/3a28a1ccdeb365470212b6c5c19277fa.wav  \n",
            "  inflating: /content/data/wavs/2bca16b94c394b28a4f94c7a2392b46c.wav  \n",
            "  inflating: /content/data/wavs/c4abb65c3ff1585cb5dfc023cfac750c.wav  \n",
            "  inflating: /content/data/wavs/588085e2a6beabe41759989f50b56928.wav  \n",
            "  inflating: /content/data/wavs/89b1051461f6d6ca98aa6a372b13b7f5.wav  \n",
            "  inflating: /content/data/wavs/af79f53400de868fdadf211ad83000c0.wav  \n",
            "  inflating: /content/data/wavs/59d4b23c4de018479893b327174be3a8.wav  \n",
            "  inflating: /content/data/wavs/0d81ecc48e53c431e58797da3f999c13.wav  \n",
            "  inflating: /content/data/wavs/d7ee11f46710fe2c086c3265ea8d9b64.wav  \n",
            "  inflating: /content/data/wavs/9c79c60e600979e1f4655abdf0e7b04f.wav  \n",
            "  inflating: /content/data/wavs/69c9b8abe789e1257e6904677fc2479a.wav  \n",
            "  inflating: /content/data/wavs/ebad5c662cf470b08e5d998796edee8e.wav  \n",
            "  inflating: /content/data/wavs/1033c972deadcc2b1cee69f367139a0c.wav  \n",
            "  inflating: /content/data/wavs/8f9cebb67971d3578042c8757c61337c.wav  \n",
            "  inflating: /content/data/wavs/ee8d178de2986531eec1eabee202ce09.wav  \n",
            "  inflating: /content/data/wavs/253b843aaf63e3031a9a4668c1d5a3c3.wav  \n",
            "  inflating: /content/data/wavs/e740cfbf20054cc49a434db9cbc6d502.wav  \n",
            "  inflating: /content/data/wavs/255bc18c17244717ee706145a4cf9873.wav  \n",
            "  inflating: /content/data/wavs/df438cb003b377f56b28bfef900056c8.wav  \n",
            "  inflating: /content/data/wavs/f56078d8f364e865920e4a6db90a8dd9.wav  \n",
            "  inflating: /content/data/wavs/cbfcb8a33ea3d42762f6b310dc8c5fa1.wav  \n",
            "  inflating: /content/data/wavs/37451595f3712230678116add31fb690.wav  \n",
            "  inflating: /content/data/wavs/b1d1c26a48a04ce49e190192ac237fe9.wav  \n",
            "  inflating: /content/data/wavs/fa5861d073723d4d7a457f1aff3f90fd.wav  \n",
            "  inflating: /content/data/wavs/2ba254ff914bc102c507eaeabd3c2423.wav  \n",
            "  inflating: /content/data/wavs/a6abddded8b89d6d446f10d9ee0dee1e.wav  \n",
            "  inflating: /content/data/wavs/1a9a4a185d9d00c37a832462c5f0948c.wav  \n",
            "  inflating: /content/data/wavs/44b1871ea2f528e234dee31489f03842.wav  \n",
            "  inflating: /content/data/wavs/47917de299ff5616749791c084eed95f.wav  \n",
            "  inflating: /content/data/wavs/6d1c3f8da8e361f75cb87e51da48b618.wav  \n",
            "  inflating: /content/data/wavs/56a38d6f25ba8220e09b03d6456ef635.wav  \n",
            "  inflating: /content/data/wavs/4ae1a1dd97ccc750fb511a66749f5366.wav  \n",
            "  inflating: /content/data/wavs/8a43a5e1b7a8defc0838d8ff43703a6d.wav  \n",
            "  inflating: /content/data/wavs/e2b4521bc71edc772b188f4e91b55c16.wav  \n",
            "  inflating: /content/data/wavs/34154f35af100764760849bd43c5982e.wav  \n",
            "  inflating: /content/data/wavs/3ef7c05dc709d359ce4482740a1a42b0.wav  \n",
            "  inflating: /content/data/wavs/1a875eee5b09d5b28572ded6df7185a5.wav  \n",
            "  inflating: /content/data/wavs/0f0c2a249ba74e197371a1a8ed6b5b99.wav  \n",
            "  inflating: /content/data/wavs/fabb526d7542d08c9a16de2b60d8ac6c.wav  \n",
            "  inflating: /content/data/wavs/49901bbafced0bbb5381fa48a1036683.wav  \n",
            "  inflating: /content/data/wavs/6d18e69f773c2e163c648ece01956052.wav  \n",
            "  inflating: /content/data/wavs/927b4fbbf2f02cec2b62d5b614182f56.wav  \n",
            "  inflating: /content/data/wavs/342d0c15b597ea6a81bd11b8ae013850.wav  \n",
            "  inflating: /content/data/wavs/de9a02e9abc5b3dfb63ffb8cd5afffe3.wav  \n",
            "  inflating: /content/data/wavs/1073473ed20af010302d8fe972b2f654.wav  \n",
            "  inflating: /content/data/wavs/e892c4ebb35baa036328cb2ffc0d0c00.wav  \n",
            "  inflating: /content/data/wavs/5cd07738c8267491aced343c5bb5bca1.wav  \n",
            "  inflating: /content/data/wavs/141c16d342ee262b9f034d78fdbafcfb.wav  \n",
            "  inflating: /content/data/wavs/b3c24f457f3f155cf070afa722e25aae.wav  \n",
            "  inflating: /content/data/wavs/a28cae7a50bcab286c16cd2160d6d16f.wav  \n",
            "  inflating: /content/data/wavs/a5e6eb51415c96afae7f3b63017ba0ce.wav  \n",
            "  inflating: /content/data/wavs/ec453bb71e26c36fbe13c31a0ebcc6f8.wav  \n",
            "  inflating: /content/data/wavs/d19134f026b2ecd880ca794311d399bd.wav  \n",
            "  inflating: /content/data/wavs/dbafb9b11372d6bc257705204f6c19d3.wav  \n",
            "  inflating: /content/data/wavs/73c40b7fe9f8ed04fe3e20cb14364308.wav  \n",
            "  inflating: /content/data/wavs/7a00fd864669ed822e2155f67de301fb.wav  \n",
            "  inflating: /content/data/wavs/3a793d8b5c1ca03876e02bfc668edcaa.wav  \n",
            "  inflating: /content/data/wavs/6e0294394dd4c67d38ed57e562dba0a8.wav  \n",
            "  inflating: /content/data/wavs/e46901721bdcbf9ebb6835df6b027e28.wav  \n",
            "  inflating: /content/data/wavs/f1bbb2f1df8d976b9013db0218b4e269.wav  \n",
            "  inflating: /content/data/wavs/ff7be996657798fa18a6a4ea9987edde.wav  \n",
            "  inflating: /content/data/wavs/9b2b8614109fe6f6f501308d282aadd6.wav  \n",
            "  inflating: /content/data/wavs/cf8d84bceddab6ce28297bdad0e75160.wav  \n",
            "  inflating: /content/data/wavs/352f8b2329a0031a9e2eb68d685f5209.wav  \n",
            "  inflating: /content/data/wavs/bf8a6d646720743de9202b581c8f9b1f.wav  \n",
            "  inflating: /content/data/wavs/f929ea6b7359d2de5692d8600cfc79b8.wav  \n",
            "  inflating: /content/data/wavs/8b588c71f099a4dccfe41cab4cdc4d72.wav  \n",
            "  inflating: /content/data/wavs/0e29ab4594bd37974d32c1388081477a.wav  \n",
            "  inflating: /content/data/wavs/8de4cf4086b828489928627b06b0a9d2.wav  \n",
            "  inflating: /content/data/wavs/45774177d6f23d2c46bf18743ac6be3c.wav  \n",
            "  inflating: /content/data/wavs/7bd3eb6b652caf2ad2bc9880dda5b567.wav  \n",
            "  inflating: /content/data/wavs/87e4a0a9cf42c0da4ce713e002596d90.wav  \n",
            "  inflating: /content/data/wavs/305b7f5071edf7a9bb88ca668b544289.wav  \n",
            "  inflating: /content/data/wavs/48d91283b9e47642c55569721e241170.wav  \n",
            "  inflating: /content/data/wavs/26395453cd2130e189167599cae12f7c.wav  \n",
            "  inflating: /content/data/wavs/e613486f9b131bc05bb707925179642e.wav  \n",
            "  inflating: /content/data/wavs/96e4856a3fd97c347239348b15725b9e.wav  \n",
            "  inflating: /content/data/wavs/30c64aa8330aeca8b0d729eb8baa7673.wav  \n",
            "  inflating: /content/data/wavs/13d1b1d4b98b7543ba07341175b0a2bf.wav  \n",
            "  inflating: /content/data/wavs/f7b86e864827de37bba5c4462462aafb.wav  \n",
            "  inflating: /content/data/wavs/03b2935616d4e1cdbb3cb9e9ca9f241c.wav  \n",
            "  inflating: /content/data/wavs/1d96268735483f21427147bca9465488.wav  \n",
            "  inflating: /content/data/wavs/d9bcb95eaac5227e3f7301220832f54b.wav  \n",
            "  inflating: /content/data/wavs/a5765119e5592580bb9801c4f1dd5ad4.wav  \n",
            "  inflating: /content/data/wavs/68ee5e56e4f9ceae9f467ee846d96a2c.wav  \n",
            "  inflating: /content/data/wavs/dea72f411f8bd155cc7cc2c76cdc382d.wav  \n",
            "  inflating: /content/data/wavs/7548a53396c63ed6ea7d98da48ed991c.wav  \n",
            "  inflating: /content/data/wavs/12ac172ad433cf3654625bbcb98cf43d.wav  \n",
            "  inflating: /content/data/wavs/4648dc1cfe29930db2dd872a117b15e8.wav  \n",
            "  inflating: /content/data/wavs/443e606e16283f4dc1ad8cd155377507.wav  \n",
            "  inflating: /content/data/wavs/ca7e15fd8ca11273a321577b95102120.wav  \n",
            "  inflating: /content/data/wavs/fcc9e7b98253b71f988e199804d5b455.wav  \n",
            "  inflating: /content/data/wavs/87536e7938249a3d282c9d97208d1db0.wav  \n",
            "  inflating: /content/data/wavs/b47f4babecc9e4da03157dab527ca7f6.wav  \n",
            "  inflating: /content/data/wavs/3dc60783fe3d442cca88f8ba15422de9.wav  \n",
            "  inflating: /content/data/wavs/aa83c7515066e165cb383d1faf2530c8.wav  \n",
            "  inflating: /content/data/wavs/41b7ca62377585cf4423728627add303.wav  \n",
            "  inflating: /content/data/wavs/c9cc59130d6fa8eaf33a303e2101d3e9.wav  \n",
            "  inflating: /content/data/wavs/acb479871b7cc3948d521a871f2a4b67.wav  \n",
            "  inflating: /content/data/wavs/89e918eb4631abcb98896adfb312038c.wav  \n",
            "  inflating: /content/data/wavs/83bf7f98a0e9f84687fe525f6742d867.wav  \n",
            "  inflating: /content/data/wavs/5a209a628e01d14c8c959fd25af21957.wav  \n",
            "  inflating: /content/data/wavs/36b624aa41494df1b549fc26695c813f.wav  \n",
            "  inflating: /content/data/wavs/a6199cf1a7b55a9d42d54d7747414076.wav  \n",
            "  inflating: /content/data/wavs/0bdf79e4a692bed90d893f49f6958404.wav  \n",
            "  inflating: /content/data/wavs/6294509592c09a43d4a28c6dfc35f115.wav  \n",
            "  inflating: /content/data/wavs/e01c9aacfbc10b40d02b511cd44cb913.wav  \n",
            "  inflating: /content/data/wavs/714984169f7bf60042c0410ef4fd377f.wav  \n",
            "  inflating: /content/data/wavs/731369f3dc88598b25f4ed0ae5541c93.wav  \n",
            "  inflating: /content/data/wavs/45155d9bda83addddf621918d6eab749.wav  \n",
            "  inflating: /content/data/wavs/f6f82e268575f82839465650308e20f2.wav  \n",
            "  inflating: /content/data/wavs/56e951a248052246c46584259e446153.wav  \n",
            "  inflating: /content/data/wavs/09c282bc764d2a2d53aed9e233e7bea4.wav  \n",
            "  inflating: /content/data/wavs/eeadfc5f28755bac74beede5fcd675c0.wav  \n",
            "  inflating: /content/data/wavs/564c5ab9ed8ddd01d05194a9ffb8d514.wav  \n",
            "  inflating: /content/data/wavs/4f0bc221c42aa7b3c199892dd173c5f1.wav  \n",
            "  inflating: /content/data/wavs/6cd82b7f5745cdda483b9f29f21d1942.wav  \n",
            "  inflating: /content/data/wavs/d00ed7fa48d7e1b442526d5132f0ff84.wav  \n",
            "  inflating: /content/data/wavs/f70d4b9dd247fb031ac36d948a26c2cf.wav  \n",
            "  inflating: /content/data/wavs/ccb3f671ab927034369f62e2d55fc236.wav  \n",
            "  inflating: /content/data/wavs/c77c6c7cb67fa221ec2ebea9ba1e021a.wav  \n",
            "  inflating: /content/data/wavs/e77807221b6ce9ad6cde4de2fc9565db.wav  \n",
            "  inflating: /content/data/wavs/9f58158ba3536ba75940d8794c954f64.wav  \n",
            "  inflating: /content/data/wavs/97219954775f2f0417722ace58cd7678.wav  \n",
            "  inflating: /content/data/wavs/736d68b4ce300388491877ea93ad9602.wav  \n",
            "  inflating: /content/data/wavs/98ac23ffcd491993246ee41ae9f2e1a2.wav  \n",
            "  inflating: /content/data/wavs/0f3d088a19fe08e4fc700efc9020a76f.wav  \n",
            "  inflating: /content/data/wavs/6dbd7a8b190de8ea45ec566015e75d9a.wav  \n",
            "  inflating: /content/data/wavs/945405bf3251647b17f830baa64b56e9.wav  \n",
            "  inflating: /content/data/wavs/9af25a67182b533f2f73dea3ffadb308.wav  \n",
            "  inflating: /content/data/wavs/9d6c2a9cea59af60bf16446610ffb921.wav  \n",
            "  inflating: /content/data/wavs/09fabe465f14bb42304c1f327c5b5aee.wav  \n",
            "  inflating: /content/data/wavs/7d3545ddf34e476a6199f6184871cfef.wav  \n",
            "  inflating: /content/data/wavs/dbedbedd7d0354d9ce3875f281995472.wav  \n",
            "  inflating: /content/data/wavs/c8469bab1e92be58cfc6533e3f5df8d7.wav  \n",
            "  inflating: /content/data/wavs/2464a96dc9f0008034bfe016cc9f1396.wav  \n",
            "  inflating: /content/data/wavs/938699bfc19db1edd592d6130ced2c45.wav  \n",
            "  inflating: /content/data/wavs/4812c40d1c3bfcaf0aadc38a91a4764e.wav  \n",
            "  inflating: /content/data/wavs/266b87e37b38262114dd12bdfb87dfeb.wav  \n",
            "  inflating: /content/data/wavs/6e7fd0f94796a70768efa489c7c383b2.wav  \n",
            "  inflating: /content/data/wavs/33b0932949f17b85457a71a8b7512105.wav  \n",
            "  inflating: /content/data/wavs/e0cddc2204460af84673078529b45767.wav  \n",
            "  inflating: /content/data/wavs/034f1015f8c8f4f229e5b980a871db81.wav  \n",
            "  inflating: /content/data/wavs/b57d468974264b19924dad550671fc71.wav  \n",
            "  inflating: /content/data/wavs/0127bb3829d50d2fd19e75371d7fc9bc.wav  \n",
            "  inflating: /content/data/wavs/2d76d0c9ffd4f5c54236998a0f317414.wav  \n",
            "  inflating: /content/data/wavs/f46b7444ae1e54d51675f035e6bc1676.wav  \n",
            "  inflating: /content/data/wavs/98948e253ba81ae2195ad5ba91fee8b5.wav  \n",
            "  inflating: /content/data/wavs/a10c4cd744863b39b346f176ced51242.wav  \n",
            "  inflating: /content/data/wavs/ef6f3769657be1fb2a08849128abd41b.wav  \n",
            "  inflating: /content/data/wavs/0ad92bcc09d6e0a6773cf591c6e977b2.wav  \n",
            "  inflating: /content/data/wavs/ef40fc7d3bdff73fc8f000cc6049572d.wav  \n",
            "  inflating: /content/data/wavs/39ead42c11b631f6ec4f11d447423ce5.wav  \n",
            "  inflating: /content/data/wavs/f40daebbe9696a2c7e7a8f08f06f1a59.wav  \n",
            "  inflating: /content/data/wavs/f59dbb2e0e97af65511bbf794f2d0021.wav  \n",
            "  inflating: /content/data/wavs/7f57af4f8b174c60ddb825a184fab37e.wav  \n",
            "  inflating: /content/data/wavs/0b72dbee9dadca5ccbef6cd9a3891662.wav  \n",
            "  inflating: /content/data/wavs/48690277604e4752f3bcb18b9a7bd493.wav  \n",
            "  inflating: /content/data/wavs/0ae077b4837b5c0d46d2bdb98fc49d53.wav  \n",
            "  inflating: /content/data/wavs/5609900b573047a946f5f97517befda7.wav  \n",
            "  inflating: /content/data/wavs/51c48636c7977ef8d9e982c08c3c53fb.wav  \n",
            "  inflating: /content/data/wavs/355158b79b06cff527cef12022409098.wav  \n",
            "  inflating: /content/data/wavs/729112fb6ca3540ed5b71eaf9e41ee08.wav  \n",
            "  inflating: /content/data/wavs/bc83f09f538d3946325294866f1c1e42.wav  \n",
            "  inflating: /content/data/wavs/c7977b282c64e1e1b722907913d8bfca.wav  \n",
            "  inflating: /content/data/wavs/db7f47d3632897a1db2f4950a43603d5.wav  \n",
            "  inflating: /content/data/wavs/b9b3fbde0ba793f7771adc00d63d7097.wav  \n",
            "  inflating: /content/data/wavs/eb419a4fb5373f79054a15c1ce9c5432.wav  \n",
            "  inflating: /content/data/wavs/fc22071d101b30fb051d2f0da98a9edc.wav  \n",
            "  inflating: /content/data/wavs/5c7d3dfb9b59b4b62f8eebd86fb740b3.wav  \n",
            "  inflating: /content/data/wavs/1257ce4136b3de9f3e2ef95d7d9cc51f.wav  \n",
            "  inflating: /content/data/wavs/5d7d7fd67a63dd5faa8fa485d583e7f8.wav  \n",
            "  inflating: /content/data/wavs/3b4b00f136487475038a5eb871f43e28.wav  \n",
            "  inflating: /content/data/wavs/bea058c8ab855258a01651db583fecb8.wav  \n",
            "  inflating: /content/data/wavs/948f37136f87b36e555d0a420106d240.wav  \n",
            "  inflating: /content/data/wavs/59bce34911926786b282a327db20b008.wav  \n",
            "  inflating: /content/data/wavs/20962385cbd6da6a6b880aa60bd1e50d.wav  \n",
            "  inflating: /content/data/wavs/e066e21679e38b01f6bad6dc784bea18.wav  \n",
            "  inflating: /content/data/wavs/8004a3bd262548f40b770b4c501b4acd.wav  \n",
            "  inflating: /content/data/wavs/09d6223c54391280c2e629d79d586432.wav  \n",
            "  inflating: /content/data/wavs/a8c652a2a7d550c5bf4f7da0ce2944b2.wav  \n",
            "  inflating: /content/data/wavs/dc19664d84041185ee35fe8be59d8cc8.wav  \n",
            "  inflating: /content/data/wavs/91667d8b22782fb29ee3d86a87a882d1.wav  \n",
            "  inflating: /content/data/wavs/2e1232467dcd6ea86d0cc248b3d37aa5.wav  \n",
            "  inflating: /content/data/wavs/187ea47c47c0e7be88d97b85c958eaf4.wav  \n",
            "  inflating: /content/data/wavs/9366136772df2ebde9840f353a2df0de.wav  \n",
            "  inflating: /content/data/wavs/d431bd380dca7658cd189ded3ba96f9a.wav  \n",
            "  inflating: /content/data/wavs/b1483ee8508b798f10ba31c4bab78622.wav  \n",
            "  inflating: /content/data/wavs/5860ec7b2a7da0ee1a6b4e71a0e5d5f8.wav  \n",
            "  inflating: /content/data/wavs/f3745154c35d44560c373b78494111b7.wav  \n",
            "  inflating: /content/data/wavs/0c8f209b433200108a7a938c19ce5d51.wav  \n",
            "  inflating: /content/data/wavs/dea4fc688d1cbde0b18472b0566bbe09.wav  \n",
            "  inflating: /content/data/wavs/fa621ac3359a24ae52a0ae0bd6b20efc.wav  \n",
            "  inflating: /content/data/wavs/f054b08a2ff491b265c7638c9dae0c58.wav  \n",
            "  inflating: /content/data/wavs/ffafdcae794fbac36b7e98055412e2aa.wav  \n",
            "  inflating: /content/data/wavs/cf80b5a622d98f6a79c86d9d92f1550e.wav  \n",
            "  inflating: /content/data/wavs/585c4bb3cde65449589f74afb2058a1f.wav  \n",
            "  inflating: /content/data/wavs/7676f80831cc44bdd90b74a76472cf7c.wav  \n",
            "  inflating: /content/data/wavs/554ff2822d5129ff895091f2e2e0571b.wav  \n",
            "  inflating: /content/data/wavs/f7d243e690f180e029ebfa3051ac3dc6.wav  \n",
            "  inflating: /content/data/wavs/7153364be1cd37dd7a903aa0a82b0caa.wav  \n",
            "  inflating: /content/data/wavs/4e90143cbb6ecdcdaf249164acba3b62.wav  \n",
            "  inflating: /content/data/wavs/6dc47999e809b855da64bb70551c8fd7.wav  \n",
            "  inflating: /content/data/wavs/901dd4e148bbef7e1727db9e7e4c555b.wav  \n",
            "  inflating: /content/data/wavs/8fb7070de3408b2f2952bfbd6434d51e.wav  \n",
            "  inflating: /content/data/wavs/0d02ba57444a989348a889a3a1769cc5.wav  \n",
            "  inflating: /content/data/wavs/bc610ea23b8eedca82f17ce98fbede50.wav  \n",
            "  inflating: /content/data/wavs/f7187703263e30c01847b1d7449eaa36.wav  \n",
            "  inflating: /content/data/wavs/a93e540c791ae1c4ebe16892d48242a9.wav  \n",
            "  inflating: /content/data/wavs/12eda26e065b4c6e75b1c00c104641da.wav  \n",
            "  inflating: /content/data/wavs/bb32fe2b856b865d852acc612b913116.wav  \n",
            "  inflating: /content/data/wavs/dd93000e6a28a32d8bb74487bc7f3e51.wav  \n",
            "  inflating: /content/data/wavs/63413316980e5cec19532e5df2ea86e1.wav  \n",
            "  inflating: /content/data/wavs/829de6a0704691bdcf69fa3f681519fb.wav  \n",
            "  inflating: /content/data/wavs/a24905e5c2d28e7e23a750d404277879.wav  \n",
            "  inflating: /content/data/wavs/c92b114c03559f32e154b26d2df40b06.wav  \n",
            "  inflating: /content/data/wavs/4a4a4da73c82dd3c9b0d2511669933e9.wav  \n",
            "  inflating: /content/data/wavs/f4acfd3767c4497ec950b1cd7ab86a85.wav  \n",
            "  inflating: /content/data/wavs/eddbcc65884dd1d486f64fec9796ad09.wav  \n",
            "  inflating: /content/data/wavs/d8b139adcc7bb838d6da1009665d47ef.wav  \n",
            "  inflating: /content/data/wavs/e75a9b32344d0ac17e4c83c824003431.wav  \n",
            "  inflating: /content/data/wavs/d6b0b2921a503e9ca769b2c98fd0066a.wav  \n",
            "  inflating: /content/data/wavs/1bd0ac969d3609a2833015f26d0dea73.wav  \n",
            "  inflating: /content/data/wavs/3452e41f6afaa235a3772aa305106c6a.wav  \n",
            "  inflating: /content/data/wavs/0c9cf6a115f48e2d19d087f52a6f4b71.wav  \n",
            "  inflating: /content/data/wavs/b595bcf717924e4f3c4605a6146f1f76.wav  \n",
            "  inflating: /content/data/wavs/71f0bc37078143c093afd77e03fc7dc1.wav  \n",
            "  inflating: /content/data/wavs/0f9c99a20519c7a7f5fef89b3d9a4313.wav  \n",
            "  inflating: /content/data/wavs/8df3f8d5b9e2ec868b1a73c552338540.wav  \n",
            "  inflating: /content/data/wavs/77f9a3448fcc9bc3bcb84b48899de576.wav  \n",
            "  inflating: /content/data/wavs/37ef34adde12a683bfc2a111421338c7.wav  \n",
            "  inflating: /content/data/wavs/8bf640987feb217b2c48786fa577cc3e.wav  \n",
            "  inflating: /content/data/wavs/596dbee61170b9f8088c97b61d284fa1.wav  \n",
            "  inflating: /content/data/wavs/e790c578dc96f886f2ee2a7e502ad2aa.wav  \n",
            "  inflating: /content/data/wavs/9e7cc1c42716e8f5cedbaf0924debc52.wav  \n",
            "  inflating: /content/data/wavs/56236f6580403e7f39bd7258961be5c7.wav  \n",
            "  inflating: /content/data/wavs/8e701372676f2d16b75cf6ab0e91919f.wav  \n",
            "  inflating: /content/data/wavs/079d3e91e855be324cd414b8a81af491.wav  \n",
            "  inflating: /content/data/wavs/4f444472bff645de1beab4dc7dd87277.wav  \n",
            "  inflating: /content/data/wavs/95267a5685e1879b4cefd9412f6b0a4f.wav  \n",
            "  inflating: /content/data/wavs/17f3430efd676b5b91c4605262084d1d.wav  \n",
            "  inflating: /content/data/wavs/6e7013e2b07f8f34acf0cfdde3f0ffaa.wav  \n",
            "  inflating: /content/data/wavs/1d9c355f5eedcaa808e69fa463e02baf.wav  \n",
            "  inflating: /content/data/wavs/d23dffd2e5bed59964b104cf7913cc7c.wav  \n",
            "  inflating: /content/data/wavs/4d47b5d3b1544fef2f73474e15d63a73.wav  \n",
            "  inflating: /content/data/wavs/7e020b9396aab276245076cf846a5a97.wav  \n",
            "  inflating: /content/data/wavs/5b0ce62bd26ffbb3e1903b2c369a675f.wav  \n",
            "  inflating: /content/data/wavs/7a92d7ee4894cfe1836dc3fba47b8027.wav  \n",
            "  inflating: /content/data/wavs/c647b57690fa8525cb074b1fabf2fde1.wav  \n",
            "  inflating: /content/data/wavs/93a24cd795a508ca97d4d1f20023d155.wav  \n",
            "  inflating: /content/data/wavs/a7c0a2877d9e47f308844b0f6c11763c.wav  \n",
            "  inflating: /content/data/wavs/e6cbcf14b5bb22352793236165ffcf33.wav  \n",
            "  inflating: /content/data/wavs/170a98704cb768d9e95ea23f64fa2e4d.wav  \n",
            "  inflating: /content/data/wavs/79aff47c9035e6fa7a091faf2a919cc6.wav  \n",
            "  inflating: /content/data/wavs/6a2e14c3704b26a04ab0bb1a7a922a06.wav  \n",
            "  inflating: /content/data/wavs/488976bafd68c2fd6c8233542ed0161a.wav  \n",
            "  inflating: /content/data/wavs/2fd8860648be0bab9a57112d1ff6d7a5.wav  \n",
            "  inflating: /content/data/wavs/8e883b903687b09c1b67e7506c3cb02e.wav  \n",
            "  inflating: /content/data/wavs/abccf600df134eca052450c98def28b8.wav  \n",
            "  inflating: /content/data/wavs/8555b2addb4fc5eac87de85f9b68c38b.wav  \n",
            "  inflating: /content/data/wavs/7cdd58aed97c502cf201152cef499021.wav  \n",
            "  inflating: /content/data/wavs/4b965be889a196800f053b792fcd7283.wav  \n",
            "  inflating: /content/data/wavs/007bba3d829d34da154fe55c8a7b479e.wav  \n",
            "  inflating: /content/data/wavs/a3a823265ca456cdbea937c932c4fa0c.wav  \n",
            "  inflating: /content/data/wavs/e69982eeb53919ada0818d15122e99c8.wav  \n",
            "  inflating: /content/data/wavs/676f9455d0867f64b19f2b46199d3a5b.wav  \n",
            "  inflating: /content/data/wavs/d4f5cd09f702703645a03a79733227ba.wav  \n",
            "  inflating: /content/data/wavs/9e61f714e15df4bd8da42c9af88270ed.wav  \n",
            "  inflating: /content/data/wavs/bfe6de56c7e457ae2ae0199a7926529d.wav  \n",
            "  inflating: /content/data/wavs/3270899095185768233f3e3a6368a5a3.wav  \n",
            "  inflating: /content/data/wavs/84e110c96b441c4f1b920e251de91ae5.wav  \n",
            "  inflating: /content/data/wavs/dd53dd1cd4b4451b4608c41498bf4631.wav  \n",
            "  inflating: /content/data/wavs/b6753653092a5277e5f88309f404f58f.wav  \n",
            "  inflating: /content/data/wavs/f10e670b36ae778355ebb08ee6cfa195.wav  \n",
            "  inflating: /content/data/wavs/e1270790a4923f110d2140e4969c4a1d.wav  \n",
            "  inflating: /content/data/wavs/685b815eacab8d94573ebc4f2228ac03.wav  \n",
            "  inflating: /content/data/wavs/7a9d4541e1b99b9b8582e37d2a00a8c0.wav  \n",
            "  inflating: /content/data/wavs/6a8b71f9b758bfd6d2a366fe863d9765.wav  \n",
            "  inflating: /content/data/wavs/9b531629d67df843a4530bd0ee359df0.wav  \n",
            "  inflating: /content/data/wavs/b3c27c83950cf25191ae80b1fff80805.wav  \n",
            "  inflating: /content/data/wavs/840d61d2f56e920342c6e7f7296cd2bf.wav  \n",
            "  inflating: /content/data/wavs/88216ea61f8b603d9894c663e69fa95e.wav  \n",
            "  inflating: /content/data/wavs/c74c3d1754771d30396ac23840ddfb86.wav  \n",
            "  inflating: /content/data/wavs/27c8573d8dc6595300fe16377c8b50a2.wav  \n",
            "  inflating: /content/data/wavs/a6a82c40b9210fd2d7c8cf7aa472f3e4.wav  \n",
            "  inflating: /content/data/wavs/74724e93320b74cf4e5826284db3d992.wav  \n",
            "  inflating: /content/data/wavs/6b498592a38d057327cdef16587d793d.wav  \n",
            "  inflating: /content/data/wavs/03a66c843495c96399f9096d60788a75.wav  \n",
            "  inflating: /content/data/wavs/752a327ac461446e11b24b7d53c46c48.wav  \n",
            "  inflating: /content/data/wavs/6de10fcc7154cbc43bed87c71cd409a3.wav  \n",
            "  inflating: /content/data/wavs/4cdedb2291ccf28ce347853410902fed.wav  \n",
            "  inflating: /content/data/wavs/83e6be41a64f1585b82cd66edea94168.wav  \n",
            "  inflating: /content/data/wavs/cd6e34f0e6c3c283ee97f66c6314e8d0.wav  \n",
            "  inflating: /content/data/wavs/aec29847ad3f38e0ad20b21348e7e16f.wav  \n",
            "  inflating: /content/data/wavs/4844598a9604b49fab0ec1a6c46b4004.wav  \n",
            "  inflating: /content/data/wavs/316dac49b12e0f58bcdc9bbaba05a986.wav  \n",
            "  inflating: /content/data/wavs/8ea9fce410605accd54252f9a3b3a349.wav  \n",
            "  inflating: /content/data/wavs/c6ec6da379a1dff9ea004c53d1b7033b.wav  \n",
            "  inflating: /content/data/wavs/b3b501731ea987a548b5fcd2503b9ca6.wav  \n",
            "  inflating: /content/data/wavs/838d62b614cbbd7bb438699f8bd47aa0.wav  \n",
            "  inflating: /content/data/wavs/c577d5ee9623590d013b43187555eaae.wav  \n",
            "  inflating: /content/data/wavs/bcb198b44edab82dd92c495a3ec31379.wav  \n",
            "  inflating: /content/data/wavs/7ac6f93f4838e1288c140f8f43389ca7.wav  \n",
            "  inflating: /content/data/wavs/700fe9ffecf6971b19746404448451c8.wav  \n",
            "  inflating: /content/data/wavs/92e636ce95d8994351603024b3db7419.wav  \n",
            "  inflating: /content/data/wavs/a38d94e21c0b1e331e3c9f0205dd2e40.wav  \n",
            "  inflating: /content/data/wavs/779fd4a3cfd8e3edfd67d92bb2fc3afe.wav  \n",
            "  inflating: /content/data/wavs/bc2284357dd0a513d63a99825c6d3410.wav  \n",
            "  inflating: /content/data/wavs/2321a3c3f73bb3de9e8d27934af9697e.wav  \n",
            "  inflating: /content/data/wavs/b94470f32406ce47ea6832733c78ed5d.wav  \n",
            "  inflating: /content/data/wavs/eefba80aa5c1a47bc7d713ed8d6be5d6.wav  \n",
            "  inflating: /content/data/wavs/50ad19e68c98e8b6db95ddd1adb2de51.wav  \n",
            "  inflating: /content/data/wavs/ad8ead5df0d2930034c6389563481f0d.wav  \n",
            "  inflating: /content/data/wavs/3edd663eca450b9bd8273edb53e2300d.wav  \n",
            "  inflating: /content/data/wavs/be0c240caf5f2cdd00ab6143ffdcd5b9.wav  \n",
            "  inflating: /content/data/wavs/fa4ccb32ea71d7c3f87e3a32410a20b1.wav  \n",
            "  inflating: /content/data/wavs/829b66186ff1683cd6b4a3023f3d5d57.wav  \n",
            "  inflating: /content/data/wavs/6f9b8db2d095bdf3c8e4da3b39fb5820.wav  \n",
            "  inflating: /content/data/wavs/7a59891a422ecb6cf2c827ff679590d2.wav  \n",
            "  inflating: /content/data/wavs/0d8cd10e844e1a14e5025f453a650656.wav  \n",
            "  inflating: /content/data/wavs/b059e1993c92163ff0b5f1e4dc58c46f.wav  \n",
            "  inflating: /content/data/wavs/3b9ac7c2a9c24859cad0da158c14c031.wav  \n",
            "  inflating: /content/data/wavs/10af9d53cd4f2c953a8b352f4ddded91.wav  \n",
            "  inflating: /content/data/wavs/1218d97755a5aa24df3c99ee3b0dd929.wav  \n",
            "  inflating: /content/data/wavs/fe067009505527d0d32d8a18cb0b933e.wav  \n",
            "  inflating: /content/data/wavs/2546b396bcf5682f82150c05fd9b198e.wav  \n",
            "  inflating: /content/data/wavs/340079b7df55bdd18bd37e4d57fa0275.wav  \n",
            "  inflating: /content/data/wavs/2ccc704d9d314387c76277e6c50339ec.wav  \n",
            "  inflating: /content/data/wavs/a6b6744d90c490b70ff172dfe129235a.wav  \n",
            "  inflating: /content/data/wavs/e9ce7827c9dbf776731dd7339c08cba6.wav  \n",
            "  inflating: /content/data/wavs/3089e02fb25a5e0bbeaa3c1e0ba0985f.wav  \n",
            "  inflating: /content/data/wavs/6b9e596850abeca00e945f47c50aea8b.wav  \n",
            "  inflating: /content/data/wavs/aad5e8825ef87e5487f06be53b49eac4.wav  \n",
            "  inflating: /content/data/wavs/bbb247ca598d8a17543aff95c63e1d5a.wav  \n",
            "  inflating: /content/data/wavs/3b2035f2b92d71cc751acf1e74182d7e.wav  \n",
            "  inflating: /content/data/wavs/83fac6537c3a95ffdfce2190db42237a.wav  \n",
            "  inflating: /content/data/wavs/0466f759a2c8aebae4d8957784940d91.wav  \n",
            "  inflating: /content/data/wavs/6d78314aa120b7b177667b6728ce1127.wav  \n",
            "  inflating: /content/data/wavs/6586c2575f7aceb1a2998178b8ac34d0.wav  \n",
            "  inflating: /content/data/wavs/6d4b1bcc0938fa8c7e51b498b81979de.wav  \n",
            "  inflating: /content/data/wavs/bd72f36c6675e5265677c88a004f59cc.wav  \n",
            "  inflating: /content/data/wavs/830f51d84e40144fff55b293bb2b725d.wav  \n",
            "  inflating: /content/data/wavs/46f79aee21acd4eb2fccb1e526b97303.wav  \n",
            "  inflating: /content/data/wavs/a9be04607d5a689a0adb4fe9c04695c7.wav  \n",
            "  inflating: /content/data/wavs/8904df264affea8d30f25675d897a276.wav  \n",
            "  inflating: /content/data/wavs/7ebd673138f7fb4dbabca566444ef30e.wav  \n",
            "  inflating: /content/data/wavs/ee35e65a424b483f9f23c18856c80c8a.wav  \n",
            "  inflating: /content/data/wavs/3cd6ba267ac816c84195df007adc9ece.wav  \n",
            "  inflating: /content/data/wavs/0d30a880955474b4e21177d5e1c15f6b.wav  \n",
            "  inflating: /content/data/wavs/9dda2159732fb4dded47c5b4f49f57d2.wav  \n",
            "  inflating: /content/data/wavs/1dc76ada1b7b3ad00e564b2634b6519b.wav  \n",
            "  inflating: /content/data/wavs/3d05b622d8254a5cf64f38dd2e38758f.wav  \n",
            "  inflating: /content/data/wavs/3bcbbe337aa8c54c088c170f844af458.wav  \n",
            "  inflating: /content/data/wavs/422401e3c73016c9ebce3dec44055d5a.wav  \n",
            "  inflating: /content/data/wavs/bd5aa2fb351e74d3bd33f7136e794ff9.wav  \n",
            "  inflating: /content/data/wavs/188dfe5c01df92928d1e9062be3901b9.wav  \n",
            "  inflating: /content/data/wavs/07bcad3174a97b081802bd31a61a1188.wav  \n",
            "  inflating: /content/data/wavs/4d46f76e8ecaf594e71ff895936c6636.wav  \n",
            "  inflating: /content/data/wavs/1490920ed4ac97beacfe80a734e498b7.wav  \n",
            "  inflating: /content/data/wavs/52437116708939d85af1c4a4f2cc9699.wav  \n",
            "  inflating: /content/data/wavs/2cbfa2bd05584473b073b14b4d5bdf8d.wav  \n",
            "  inflating: /content/data/wavs/0d9181b65208ca67231322d53e03763e.wav  \n",
            "  inflating: /content/data/wavs/9e5a7c12581ca73d1cd8417f9a932a70.wav  \n",
            "  inflating: /content/data/wavs/f39f1480d1ae77c2b620fae10c38ad7a.wav  \n",
            "  inflating: /content/data/wavs/cf3b6c05e37e8e91af84ae9a7eb6a20b.wav  \n",
            "  inflating: /content/data/wavs/d8f29f8ba892bdaebbe864b152099e86.wav  \n",
            "  inflating: /content/data/wavs/9587de973eb55abc95db18e7fdfc75be.wav  \n",
            "  inflating: /content/data/wavs/ca55e8ef97a52cab482054b87f157cd3.wav  \n",
            "  inflating: /content/data/wavs/df7346018b410e45c5fa7837b199cde1.wav  \n",
            "  inflating: /content/data/wavs/4fcb0a93a6c6d226d9f7d6a462c28293.wav  \n",
            "  inflating: /content/data/wavs/ae9832099debe6b1c3b9d215184292dc.wav  \n",
            "  inflating: /content/data/wavs/8010823d0a1b3a179998faab260e7205.wav  \n",
            "  inflating: /content/data/wavs/31952343cc51d3651b40f5afcff340f5.wav  \n",
            "  inflating: /content/data/wavs/db43ac481ffee135a457d8e725de4503.wav  \n",
            "  inflating: /content/data/wavs/0b176e67725cd82e44103d52843af90d.wav  \n",
            "  inflating: /content/data/wavs/bd80e6276213af051165e5674529b2d8.wav  \n",
            "  inflating: /content/data/wavs/0bbdea34f72d2ffc175085ca0c16d84b.wav  \n",
            "  inflating: /content/data/wavs/48d0da40d7bef9708ac0cc801c336f30.wav  \n",
            "  inflating: /content/data/wavs/aa58779aa30a76cde24b18efb517fc9f.wav  \n",
            "  inflating: /content/data/wavs/d11384329bc6dc01e681138db5299c0e.wav  \n",
            "  inflating: /content/data/wavs/0772f191b0ed1523b9a30be87591ce69.wav  \n",
            "  inflating: /content/data/wavs/d80ef8ee2c9313494adfdf42c003e3ee.wav  \n",
            "  inflating: /content/data/wavs/f69722bbf6cccd5e5b250e327971f80c.wav  \n",
            "  inflating: /content/data/wavs/3483c666bedd370bdd108a373b204a30.wav  \n",
            "  inflating: /content/data/wavs/ebd89b07db9886c95a3d9541b9e74d20.wav  \n",
            "  inflating: /content/data/wavs/cdf9c4071eeb8cedf22a6a24089d1905.wav  \n",
            "  inflating: /content/data/wavs/fb8f72f1b9f8b87299140d9473cd5848.wav  \n",
            "  inflating: /content/data/wavs/13f9cfbeb1d3ce031af22569bd3cfacc.wav  \n",
            "  inflating: /content/data/wavs/ff36ffe5aeb22d32ca6d1579e9e63702.wav  \n",
            "  inflating: /content/data/wavs/041f0eea373ddc302e1816a19bd40bd0.wav  \n",
            "  inflating: /content/data/wavs/9fd6d26ca3ae785275dc7ece4b166443.wav  \n",
            "  inflating: /content/data/wavs/2c237c36bebeea248ffc33076a35f3d4.wav  \n",
            "  inflating: /content/data/wavs/f271ed6be3591c9bd1c784a0a04d1ccd.wav  \n",
            "  inflating: /content/data/wavs/1169457c6c735ce181d9c2d3eeca7ad5.wav  \n",
            "  inflating: /content/data/wavs/ebcbd613ffdf94181830de264ed04ae8.wav  \n",
            "  inflating: /content/data/wavs/4d4b1c2c2da5878c92cda9ff5c325780.wav  \n",
            "  inflating: /content/data/wavs/a4f1e574be1e14f17ab205982dbd9c63.wav  \n",
            "  inflating: /content/data/wavs/c5545e252376a9f25a52878135471a16.wav  \n",
            "  inflating: /content/data/wavs/b72872e5fea20e925897c3434e44ed4d.wav  \n",
            "  inflating: /content/data/wavs/1f23245695aca07d8e93ea76963ccfbc.wav  \n",
            "  inflating: /content/data/wavs/ccf17e20d812f5c520c09cc9ae36e41d.wav  \n",
            "  inflating: /content/data/wavs/87cc950a20ed98c73718cf8b5a44e8c8.wav  \n",
            "  inflating: /content/data/wavs/15c4c04fdcaae2161bfa6edb4cd4333d.wav  \n",
            "  inflating: /content/data/wavs/16402ede067cef68f923ea4415ed0266.wav  \n",
            "  inflating: /content/data/wavs/11e6dc2ccdc068c0e04e32685ba292fa.wav  \n",
            "  inflating: /content/data/wavs/65a710cb4437d6f1f7679451a23b8a79.wav  \n",
            "  inflating: /content/data/wavs/f44a3210a4f0f67c77e8a96ac514eab9.wav  \n",
            "  inflating: /content/data/wavs/af5acca6152d5acc9e6058020887cb99.wav  \n",
            "  inflating: /content/data/wavs/ff5ae9d1d4a5d4d7819b5bd256b0a749.wav  \n",
            "  inflating: /content/data/wavs/8587de0d2ad20e206918fecaf93f15c5.wav  \n",
            "  inflating: /content/data/wavs/238c7501a8cd29f548ddfa9f027c626a.wav  \n",
            "  inflating: /content/data/wavs/efa6c059be484d30a5fb83dc88f4f5e5.wav  \n",
            "  inflating: /content/data/wavs/ea0a679ff50967e199de7492a576c48b.wav  \n",
            "  inflating: /content/data/wavs/c51878dda8055233dafa7e6b5d20e9e6.wav  \n",
            "  inflating: /content/data/wavs/65eadfbd46615de8ee1acd926a39e639.wav  \n",
            "  inflating: /content/data/wavs/7b89547abea55e74cb40499409560336.wav  \n",
            "  inflating: /content/data/wavs/7198250fb54de38f6bc7c0f3aea053b9.wav  \n",
            "  inflating: /content/data/wavs/bcd8a7efd261edbe4c38d3297a96e05e.wav  \n",
            "  inflating: /content/data/wavs/b12214c804edd06022aee6a00986e121.wav  \n",
            "  inflating: /content/data/wavs/68f24579b178e8abf986464cc198fee9.wav  \n",
            "  inflating: /content/data/wavs/a5faaedfe634d6f682e3893983af31df.wav  \n",
            "  inflating: /content/data/wavs/89dc1e4c6c961c7f6d85365b138c1d1d.wav  \n",
            "  inflating: /content/data/wavs/3fbd7a006869f0a227654c6a4024da37.wav  \n",
            "  inflating: /content/data/wavs/f0b98b4f6cd437e2e77ed3299755d7ea.wav  \n",
            "  inflating: /content/data/wavs/b539d6ff0a855ee236a6bb5f7ba6c440.wav  \n",
            "  inflating: /content/data/wavs/6bd7ced2b55438a314c479ab84504fca.wav  \n",
            "  inflating: /content/data/wavs/3093d4a5228d85d3942991f451bec4ce.wav  \n",
            "  inflating: /content/data/wavs/4245cc0c203e0e4b2f9b615a38fcb6e1.wav  \n",
            "  inflating: /content/data/wavs/14233f9f252037ec9536eca47c8a898e.wav  \n",
            "  inflating: /content/data/wavs/8267b0e37717ce7e61e7218190dbc3d9.wav  \n",
            "  inflating: /content/data/wavs/3df4a1e003496ba446761697e2bb6f1c.wav  \n",
            "  inflating: /content/data/wavs/61522363447fb6d4d44d874db0600a4c.wav  \n",
            "  inflating: /content/data/wavs/5c57b7384a5ed443c46ea9df7f3dc3a0.wav  \n",
            "  inflating: /content/data/wavs/d14d0b08ace96363c2d8501d4035adf7.wav  \n",
            "  inflating: /content/data/wavs/d3a82bbd3ba4aaf863de9817dbb73496.wav  \n",
            "  inflating: /content/data/wavs/f790fc8e32db76e2641162b0268a8b55.wav  \n",
            "  inflating: /content/data/wavs/1ef8ac425cec4baebdeb744293c0a5ba.wav  \n",
            "  inflating: /content/data/wavs/f546f81b8464481c40c133411284b5a1.wav  \n",
            "  inflating: /content/data/wavs/af918e88dd7aad9db6a80608063613bd.wav  \n",
            "  inflating: /content/data/wavs/1added4928322b9d1506b1fbdf7f6438.wav  \n",
            "  inflating: /content/data/wavs/e3929028d79e213bc5f75497872bff1f.wav  \n",
            "  inflating: /content/data/wavs/8e603bcd432990993144575df764a858.wav  \n",
            "  inflating: /content/data/wavs/9a2ca624150a3a09ca9bf3d2c55645f6.wav  \n",
            "  inflating: /content/data/wavs/b82ad5d1c14e3599bb7828ba9d90bc19.wav  \n",
            "  inflating: /content/data/wavs/27a9a4ff684d508e40825361cdb9e2b2.wav  \n",
            "  inflating: /content/data/wavs/fb605b8b886affc88e6e866af6882339.wav  \n",
            "  inflating: /content/data/wavs/ee15679d2039324c8049f49f5e6a7c15.wav  \n",
            "  inflating: /content/data/wavs/92659f776894288c159c2831728d274d.wav  \n",
            "  inflating: /content/data/wavs/29961a2583ff8074ddef1e59f951245c.wav  \n",
            "  inflating: /content/data/wavs/e5f71f3852cb620dcc706b1d24e2aed2.wav  \n",
            "  inflating: /content/data/wavs/83d99d7715520edf59197fbd2ad8c2aa.wav  \n",
            "  inflating: /content/data/wavs/e2209e13897dd6925f689ef8eca22afa.wav  \n",
            "  inflating: /content/data/wavs/712b298bb779b1b0db45c76849b68273.wav  \n",
            "  inflating: /content/data/wavs/e1c2175872c920545d145d4ad0d0800b.wav  \n",
            "  inflating: /content/data/wavs/bf3d854a2551caac7757edaabb636efa.wav  \n",
            "  inflating: /content/data/wavs/3674bb3911fb92d817d2c24395c06749.wav  \n",
            "  inflating: /content/data/wavs/40470c4fc5edd4f40940d7ec3ca103fb.wav  \n",
            "  inflating: /content/data/wavs/c4d7f932638a2f0e397c1919dafb181c.wav  \n",
            "  inflating: /content/data/wavs/ada750a35d6d5026b9d0010c43c273c5.wav  \n",
            "  inflating: /content/data/wavs/c30fd3ebb949a87547b69bc3ded5472f.wav  \n",
            "  inflating: /content/data/wavs/08d8438567d3f1964ccea291326d0634.wav  \n",
            "  inflating: /content/data/wavs/c1864739e37f727e88e663d17553bf51.wav  \n",
            "  inflating: /content/data/wavs/1197c881b6365ce76f7e7c7829aa113d.wav  \n",
            "  inflating: /content/data/wavs/af11269990930bfb88c5075149df4def.wav  \n",
            "  inflating: /content/data/wavs/de23266f19aa1761185c5721ce47910e.wav  \n",
            "  inflating: /content/data/wavs/011c23ac473e47c31e94a4bfe22a2be6.wav  \n",
            "  inflating: /content/data/wavs/433c52600eeba00e31f65e1005312ac4.wav  \n",
            "  inflating: /content/data/wavs/c0c99fcae9e0504adbf4a1df3b0ba0b2.wav  \n",
            "  inflating: /content/data/wavs/7a856aef0fae801b56e559be2b49bff2.wav  \n",
            "  inflating: /content/data/wavs/a9387c535fd50fc01f62f3d14fcb512e.wav  \n",
            "  inflating: /content/data/wavs/a9431913279f495197039c9f1bfa1d51.wav  \n",
            "  inflating: /content/data/wavs/329bc50d0b4270531fbf7de864f2cd65.wav  \n",
            "  inflating: /content/data/wavs/1fcf0858d361bdd158ed67710e0a9eba.wav  \n",
            "  inflating: /content/data/wavs/177f87770d9a0969ec1ee2634c46d5bd.wav  \n",
            "  inflating: /content/data/wavs/7f99ab5e9ca58ef11a704e5b50e3a317.wav  \n",
            "  inflating: /content/data/wavs/c150018c6f1ebd33e5d5cd343a59ba7c.wav  \n",
            "  inflating: /content/data/wavs/60b79b9678610dbde4d76e9b3010c5e8.wav  \n",
            "  inflating: /content/data/wavs/aa82173480c4b11ebe4f0ec7a51c4c53.wav  \n",
            "  inflating: /content/data/wavs/b717c1bb99320df285999bf5f618dc49.wav  \n",
            "  inflating: /content/data/wavs/b470951ca30a1709125c4db118915d87.wav  \n",
            "  inflating: /content/data/wavs/e86c41b23c5b9f2641523f91dd3b5541.wav  \n",
            "  inflating: /content/data/wavs/fade3f14a64f38d2d02915183861a465.wav  \n",
            "  inflating: /content/data/wavs/9d01e5f602d107ebaee4849effa16a07.wav  \n",
            "  inflating: /content/data/wavs/c9b908ecaa4f4d16fa1b91f666ba35d3.wav  \n",
            "  inflating: /content/data/wavs/e903938fa1df6c281b07c2263185b99f.wav  \n",
            "  inflating: /content/data/wavs/a1804f76d622da583e6f35c7c5467149.wav  \n",
            "  inflating: /content/data/wavs/445ffc05dab80626ce783cb27efb1bd2.wav  \n",
            "  inflating: /content/data/wavs/7b0b12e8c33db5ede50c2d02169c2e61.wav  \n",
            "  inflating: /content/data/wavs/d39e816b112e78fdaf67d626a78e232c.wav  \n",
            "  inflating: /content/data/wavs/785bfd6f45d20127e0b08820ab456f27.wav  \n",
            "  inflating: /content/data/wavs/58099e3a10231e3dbb57d435d85561ba.wav  \n",
            "  inflating: /content/data/wavs/c417779c12c43fd4b2e988abeefff971.wav  \n",
            "  inflating: /content/data/wavs/568804b737d26263fceb38f3d74ab1b4.wav  \n",
            "  inflating: /content/data/wavs/4055f6d40154306da2e248b0efa42243.wav  \n",
            "  inflating: /content/data/wavs/8ab2af53eb325ec7cbc16eb59ee582d2.wav  \n",
            "  inflating: /content/data/wavs/0f55d257173474ec12dd339c9ba559c8.wav  \n",
            "  inflating: /content/data/wavs/ad5a14767e8fccd8e914553e6e27e404.wav  \n",
            "  inflating: /content/data/wavs/0b3d25845eaee29ee06a0cedc2e7d83a.wav  \n",
            "  inflating: /content/data/wavs/d9baef073e46cadd357f68cd1ff7233e.wav  \n",
            "  inflating: /content/data/wavs/d800d1045c029f530e03c2f681b2bb31.wav  \n",
            "  inflating: /content/data/wavs/0376e9bbe9e455c1250e817574b5544e.wav  \n",
            "  inflating: /content/data/wavs/dab3df410a399e9e0781529652c15ff5.wav  \n",
            "  inflating: /content/data/wavs/4f9257d24b045b257d759cd744c18861.wav  \n",
            "  inflating: /content/data/wavs/a241e1811faa0df027ec5ad0228aeddc.wav  \n",
            "  inflating: /content/data/wavs/c8f6b54cd6fd2e5ca8ae3a5e9cd5661b.wav  \n",
            "  inflating: /content/data/wavs/fbdf9a512cbbba438a734d484b7fb8f9.wav  \n",
            "  inflating: /content/data/wavs/410f6e568082cd12ecd0c7f3275e415b.wav  \n",
            "  inflating: /content/data/wavs/5a1698e8742e493f4e264b1ec961953a.wav  \n",
            "  inflating: /content/data/wavs/c28f1397219607c91652dd43d7e1823c.wav  \n",
            "  inflating: /content/data/wavs/c3e0f4f4d96aed8391ab10ed6e07668c.wav  \n",
            "  inflating: /content/data/wavs/b4cdbcc9e2c425fb7fcea85d27308d3b.wav  \n",
            "  inflating: /content/data/wavs/8e573edf09aae1eb49bfed83bfe02b75.wav  \n",
            "  inflating: /content/data/wavs/266611a9ae7c7c8a62852ce120d521c2.wav  \n",
            "  inflating: /content/data/wavs/9bd47c9891d28c0d9aaa9d669025306d.wav  \n",
            "  inflating: /content/data/wavs/9dd8649cdd17edf23f8c1a59031bedc9.wav  \n",
            "  inflating: /content/data/wavs/9b10db5f62853f6c74ab4039c917d68f.wav  \n",
            "  inflating: /content/data/wavs/1ab25ebc2a33870a3c8bb35d1f731077.wav  \n",
            "  inflating: /content/data/wavs/2c85bffd79ad0733cd889b16065efa13.wav  \n",
            "  inflating: /content/data/wavs/f8e6efc5bf730bc05100d7535b589cb9.wav  \n",
            "  inflating: /content/data/wavs/894a04cb944853aadf8913219613d2c1.wav  \n",
            "  inflating: /content/data/wavs/25314c17c7462b46976512830e075a77.wav  \n",
            "  inflating: /content/data/wavs/a99a6427e77751d87758d846081712b1.wav  \n",
            "  inflating: /content/data/wavs/44f25940087bf3f7985534c4b71a95f6.wav  \n",
            "  inflating: /content/data/wavs/d4a859a6733b727afe3f6743936dfae3.wav  \n",
            "  inflating: /content/data/wavs/ae44da66fbf938d88b5a3c7e177e6141.wav  \n",
            "  inflating: /content/data/wavs/0f4300b118c943146033acdf908202be.wav  \n",
            "  inflating: /content/data/wavs/156b934eaae92c15ab0a134738ce3df2.wav  \n",
            "  inflating: /content/data/wavs/9909ecf7ac30cc1c112a484cb7686977.wav  \n",
            "  inflating: /content/data/wavs/f14ac9680b611b8f6c78691700209de3.wav  \n",
            "  inflating: /content/data/wavs/2fa29f38e2142ea59ac53272de757005.wav  \n",
            "  inflating: /content/data/wavs/8f5986bd8796800ace089879975ecda9.wav  \n",
            "  inflating: /content/data/wavs/de7f1d4952ead8c33b3b80435862b4c1.wav  \n",
            "  inflating: /content/data/wavs/ffbd77a99746f142ad3311c435f8c790.wav  \n",
            "  inflating: /content/data/wavs/15bc8c78d9b80eb094469ba17d7a01a3.wav  \n",
            "  inflating: /content/data/wavs/8ec23c91c57add27f5882d537b78f1ae.wav  \n",
            "  inflating: /content/data/wavs/2a045d4ed9d3f7798b6da67b554eeeee.wav  \n",
            "  inflating: /content/data/wavs/e94e3646d9f50852976d9d115eb0f7b9.wav  \n",
            "  inflating: /content/data/wavs/6690d3c2ac6c733d2e5bdab7af54670d.wav  \n",
            "  inflating: /content/data/wavs/80aafdb5f459bcafec1b85af86dc5ea0.wav  \n",
            "  inflating: /content/data/wavs/7c250b30faf834ae50fdfeb3f0261220.wav  \n",
            "  inflating: /content/data/wavs/f0ac2118d8037bf7231f71cc31a59123.wav  \n",
            "  inflating: /content/data/wavs/a377c16423da7437146ce6fc5afd0669.wav  \n",
            "  inflating: /content/data/wavs/07f39b56d7d282c46765432b2d990fbc.wav  \n",
            "  inflating: /content/data/wavs/cab8800d4508d4c9de1995ced59f93d3.wav  \n",
            "  inflating: /content/data/wavs/bec34bd28d565050536468cd383913b3.wav  \n",
            "  inflating: /content/data/wavs/cd6e67fbe3ba938e28e7a58dbd38225e.wav  \n",
            "  inflating: /content/data/wavs/c27ca0035f855996d079b63b6bf36438.wav  \n",
            "  inflating: /content/data/wavs/a4a5d47d84dd9474f12c6450e264769b.wav  \n",
            "  inflating: /content/data/wavs/15442d99d653ae392a6d734b531e2258.wav  \n",
            "  inflating: /content/data/wavs/98a690ca4abd17a878a9a9549298df43.wav  \n",
            "  inflating: /content/data/wavs/b03f44fd0f3e3585a5be5709ee160bcb.wav  \n",
            "  inflating: /content/data/wavs/9acbe343e83dbaeec52f81077c5b8f1b.wav  \n",
            "  inflating: /content/data/wavs/d480ee5bfd1cf145bbfcbeb81e42af6e.wav  \n",
            "  inflating: /content/data/wavs/b10b75bdb027dbbb990d23669210f40e.wav  \n",
            "  inflating: /content/data/wavs/471a6d2f086ee461ffc8fbcb47bfbc2a.wav  \n",
            "  inflating: /content/data/wavs/fe80ea7d837e382b9d20b65f9ebe14ea.wav  \n",
            "  inflating: /content/data/wavs/bd7d59e0c615335f63bca46e094fb953.wav  \n",
            "  inflating: /content/data/wavs/b7c6e022d05faae97c010d161f0545aa.wav  \n",
            "  inflating: /content/data/wavs/12ef9a6dd1e2085c014e8660e07015a4.wav  \n",
            "  inflating: /content/data/wavs/c407a46b0dd8b969b6e5ae92ae9acd1b.wav  \n",
            "  inflating: /content/data/wavs/94cfc4c1b39091cb1ac708201a59151c.wav  \n",
            "  inflating: /content/data/wavs/39a246ae18a5bb1c5605ed5cdbe9d050.wav  \n",
            "  inflating: /content/data/wavs/a6eb03655f0f85733800b43b713e12dc.wav  \n",
            "  inflating: /content/data/wavs/3259ae06ca6dc41a17793e33d4de9dd0.wav  \n",
            "  inflating: /content/data/wavs/2d3496de70ed8ab671bc7db169a2179a.wav  \n",
            "  inflating: /content/data/wavs/75286480e688c745b3b4d6e667c715aa.wav  \n",
            "  inflating: /content/data/wavs/8637b68fc183e61cd84b7a2b5729c1b8.wav  \n",
            "  inflating: /content/data/wavs/570659551d6d0a21732b41801c5223dc.wav  \n",
            "  inflating: /content/data/wavs/43564bcaa14d8b24f82a388f840153d1.wav  \n",
            "  inflating: /content/data/wavs/799f41e295f476c1d2987ed863ec1c63.wav  \n",
            "  inflating: /content/data/wavs/4eb648c4dc28d0e65e257cbe2e8ac2fa.wav  \n",
            "  inflating: /content/data/wavs/482fad0a7d046538c74ec0b08c5b712e.wav  \n",
            "  inflating: /content/data/wavs/b891ec6cabeae65eb7630d40150477c5.wav  \n",
            "  inflating: /content/data/wavs/73fd17c8768ed70c12d6dc6a384d83f5.wav  \n",
            "  inflating: /content/data/wavs/c9d3fe68f4c56ed595c518041dd06112.wav  \n",
            "  inflating: /content/data/wavs/bb741cb4eb458daf933d40b85f812d5d.wav  \n",
            "  inflating: /content/data/wavs/09b54dddd88d1b5da2ca5672c56b241b.wav  \n",
            "  inflating: /content/data/wavs/ee646fde9558ab5b58e0cd428c495f3d.wav  \n",
            "  inflating: /content/data/wavs/c9e903995943fcc968052adc8210a8c3.wav  \n",
            "  inflating: /content/data/wavs/8e3ec6e074f1fb9b1a47d541fa268db3.wav  \n",
            "  inflating: /content/data/wavs/003dbdca79db78ed537060113789f73c.wav  \n",
            "  inflating: /content/data/wavs/45a195c971876c75d9f3792b28c3929f.wav  \n",
            "  inflating: /content/data/wavs/e37ea8c53ff0986c551e82bfc8254b51.wav  \n",
            "  inflating: /content/data/wavs/b718c9cab54c9bdaee7fd64ea0e92548.wav  \n",
            "  inflating: /content/data/wavs/2757aa1237dc1b43fe79112c2fe29444.wav  \n",
            "  inflating: /content/data/wavs/d2c5ff9a6beab4c208348a9432212671.wav  \n",
            "  inflating: /content/data/wavs/8e6ad64f011b94d7203d4828a92eec96.wav  \n",
            "  inflating: /content/data/wavs/614727d5eaa8f68d0dbbc3eb96ff8716.wav  \n",
            "  inflating: /content/data/wavs/74f345b689887df28f1342d1ee3a9c55.wav  \n",
            "  inflating: /content/data/wavs/0c5f06d6853e4a630505997ca169fc8f.wav  \n",
            "  inflating: /content/data/wavs/cb5278baaddf5f4a0e6601457693b3c5.wav  \n",
            "  inflating: /content/data/wavs/520349d3434f89a9d0958a414e1c2a80.wav  \n",
            "  inflating: /content/data/wavs/d1fa2f578d18737c88fed7a25407f3cc.wav  \n",
            "  inflating: /content/data/wavs/58aff616944399a88b4f38204ef52fbe.wav  \n",
            "  inflating: /content/data/wavs/debda77a90da1768ad4cfcca44f79545.wav  \n",
            "  inflating: /content/data/wavs/2d368cfa23b0d424602455e1cf739f7b.wav  \n",
            "  inflating: /content/data/wavs/a37aab9e1395307af5404335bfffdfdc.wav  \n",
            "  inflating: /content/data/wavs/242bbef1e867be122e4e6e68c7983ffc.wav  \n",
            "  inflating: /content/data/wavs/6a0147135324b959e28f3b742135d86e.wav  \n",
            "  inflating: /content/data/wavs/5db3a51ae38dc68caf5b8cf32dc5a169.wav  \n",
            "  inflating: /content/data/wavs/2f97de7c16794d60e51f542c049474c6.wav  \n",
            "  inflating: /content/data/wavs/15c7a6634fc525893bc2c91845bb987a.wav  \n",
            "  inflating: /content/data/wavs/ecd3c961c4369e662e74a007f288f6b4.wav  \n",
            "  inflating: /content/data/wavs/089c970540f17a718b1329af087e0d9f.wav  \n",
            "  inflating: /content/data/wavs/fd973919f2b70fbdcd7561125fea2b53.wav  \n",
            "  inflating: /content/data/wavs/ac7896388d050d0ea41b17d0c5fe80a9.wav  \n",
            "  inflating: /content/data/wavs/5a101f63c0ab82b3d3f00385b32d14c0.wav  \n",
            "  inflating: /content/data/wavs/38ed0f8583f0b854f0ef1550d65aed8e.wav  \n",
            "  inflating: /content/data/wavs/f8577664bd23dde8cb13567f0f12b4fd.wav  \n",
            "  inflating: /content/data/wavs/0dabdff7203accfc57a1dce61d4d795c.wav  \n",
            "  inflating: /content/data/wavs/385523aafe5efa148a92c8a95ef6b947.wav  \n",
            "  inflating: /content/data/wavs/739995229e9f8476adab65d4efc4acf7.wav  \n",
            "  inflating: /content/data/wavs/1581650f25b7ff8d64ac4ebbda44e865.wav  \n",
            "  inflating: /content/data/wavs/7cd4fdfcabaf954b7096b3479ac6e077.wav  \n",
            "  inflating: /content/data/wavs/d6d3660dbec277b3b654d985ba87c298.wav  \n",
            "  inflating: /content/data/wavs/005f6c5d8a670ef1df5db1f183e3f19f.wav  \n",
            "  inflating: /content/data/wavs/544a78ac5da4b9f32d389b3daac885a2.wav  \n",
            "  inflating: /content/data/wavs/bc14a875a383dc5e69125e60d62367ef.wav  \n",
            "  inflating: /content/data/wavs/7e039f47683b4d18ba417fe499f680f4.wav  \n",
            "  inflating: /content/data/wavs/c42b26da3668e189129cd49ab9fbc3cc.wav  \n",
            "  inflating: /content/data/wavs/20f1981f196f3d1624ee28a328892390.wav  \n",
            "  inflating: /content/data/wavs/dbe2d6838735ad6737fb6c9430a5997e.wav  \n",
            "  inflating: /content/data/wavs/32bb91e7a9a09559e7c86e34843ac8e5.wav  \n",
            "  inflating: /content/data/wavs/d32a9cd9a3a8be93f05095b88a061870.wav  \n",
            "  inflating: /content/data/wavs/ce8c12bb1669eee507eeadd2b59ce5bd.wav  \n",
            "  inflating: /content/data/wavs/bc59baed51678b21491486e7becb07a1.wav  \n",
            "  inflating: /content/data/wavs/35d49caf777bd8fdbebad469f0d69405.wav  \n",
            "  inflating: /content/data/wavs/f16f087b64725bd64e21f8838135604b.wav  \n",
            "  inflating: /content/data/wavs/8dd6c98a6699bbf977a8bd91788f02d4.wav  \n",
            "  inflating: /content/data/wavs/48312cd2d458adc744516efdb5b56a4b.wav  \n",
            "  inflating: /content/data/wavs/5f49ea7c8040c82cd4aa2e45442a3b77.wav  \n",
            "  inflating: /content/data/wavs/dc493af987f8893cc78aae27b6840d3a.wav  \n",
            "  inflating: /content/data/wavs/357b15d0b774af26c3094c69bdbf5b3c.wav  \n",
            "  inflating: /content/data/wavs/63b883067acd72e7683871dfff9f1aee.wav  \n",
            "  inflating: /content/data/wavs/c1fe7710d67cce8c86fa67dd43cd0307.wav  \n",
            "  inflating: /content/data/wavs/34f6ace0cd2d80f5de8f698babc5e1c4.wav  \n",
            "  inflating: /content/data/wavs/13fc7bf9256bbacf7803c70a4b8f0b55.wav  \n",
            "  inflating: /content/data/wavs/355ec9619c0e0430fad1f96801bebc28.wav  \n",
            "  inflating: /content/data/wavs/6a7f779b40be5e83b135a5636bee3dcb.wav  \n",
            "  inflating: /content/data/wavs/683cc3bf3b0d222e8fc5dcb4ae37288f.wav  \n",
            "  inflating: /content/data/wavs/b9e6fc6d973dc4114e891146495d6da5.wav  \n",
            "  inflating: /content/data/wavs/e4be756a6653da1ef1a89df517b4ef42.wav  \n",
            "  inflating: /content/data/wavs/2d08ea1ae4f2270b0a984cc11716b2d1.wav  \n",
            "  inflating: /content/data/wavs/412d622ef934312ecadb4f2a35d4e67b.wav  \n",
            "  inflating: /content/data/wavs/9eff5cb738a4f38d76d5e6f9bf61a637.wav  \n",
            "  inflating: /content/data/wavs/4d8cc30cb54bfb40667e505231304ef0.wav  \n",
            "  inflating: /content/data/wavs/6ba948bc59cbfbe5834db9c360a4d90c.wav  \n",
            "  inflating: /content/data/wavs/37d034918fd177bb3df0ccfbf38b3451.wav  \n",
            "  inflating: /content/data/wavs/00b092c22a64e3b88da1fc617d0f41ae.wav  \n",
            "  inflating: /content/data/wavs/a84cee78311ceb90c73a32af9e9beb09.wav  \n",
            "  inflating: /content/data/wavs/bfb165e36bfe98919633c281f023f919.wav  \n",
            "  inflating: /content/data/wavs/4ac0ca8f73f70d4ca205293a26887ecd.wav  \n",
            "  inflating: /content/data/wavs/556256428ddf8c6bc891fac7007469e8.wav  \n",
            "  inflating: /content/data/wavs/c539bbf1f625eb0671b76075a7687b29.wav  \n",
            "  inflating: /content/data/wavs/e1f3c15e9c6a633fe25b6621aea3f01e.wav  \n",
            "  inflating: /content/data/wavs/afc227090ef908cc93a130b6b9a69a28.wav  \n",
            "  inflating: /content/data/wavs/5163e75311836c060f214ac218bc232e.wav  \n",
            "  inflating: /content/data/wavs/1bfdcc8a35b6655ff00310e7cf820ddb.wav  \n",
            "  inflating: /content/data/wavs/ec9f31d7cb7006186ce6f95712a4f102.wav  \n",
            "  inflating: /content/data/wavs/f1af19cc46cea8071dd14e3c02713ead.wav  \n",
            "  inflating: /content/data/wavs/89046763cf5b4c592b83465a854b6e1b.wav  \n",
            "  inflating: /content/data/wavs/af10c00651a9535ef6197ef6795ed8bf.wav  \n",
            "  inflating: /content/data/wavs/9fe4f67836f6c75e74c52617121b0335.wav  \n",
            "  inflating: /content/data/wavs/c478296f7510caa6722e95d092aa5406.wav  \n",
            "  inflating: /content/data/wavs/8bc77d17f63e3e665bea40f4cee9236d.wav  \n",
            "  inflating: /content/data/wavs/07f4d71a4550fef6c26aa63d1b48a959.wav  \n",
            "  inflating: /content/data/wavs/2368721d8afa6c4a16fd6e35c3de91fd.wav  \n",
            "  inflating: /content/data/wavs/051f1e93fd588a96f17ec791db28b71e.wav  \n",
            "  inflating: /content/data/wavs/4e123b16be8cf2aef17fc940fe9a10cb.wav  \n",
            "  inflating: /content/data/wavs/a38bd1f3b75a5ad3550cde4ca511171e.wav  \n",
            "  inflating: /content/data/wavs/5ceb2dc3d557828ac86279f428689333.wav  \n",
            "  inflating: /content/data/wavs/944b098a0c67fd764103cadec24b1845.wav  \n",
            "  inflating: /content/data/wavs/51c607f709f65d1fbd61fd6d4b6481fd.wav  \n",
            "  inflating: /content/data/wavs/498126596702c873419b4814d333313d.wav  \n",
            "  inflating: /content/data/wavs/023314a8a840fd7b675572b4280c97c5.wav  \n",
            "  inflating: /content/data/wavs/fc0556702aac8dba4f639c7a8f09a1db.wav  \n",
            "  inflating: /content/data/wavs/8d5d241d2da988a5d1c16ac2f1e31300.wav  \n",
            "  inflating: /content/data/wavs/0160fa2b9e83d5a7f6503b8b02c7aa55.wav  \n",
            "  inflating: /content/data/wavs/3a97a316bcfae869bb03dde9d53c98c8.wav  \n",
            "  inflating: /content/data/wavs/173b71ec100b05675dcdb23334843755.wav  \n",
            "  inflating: /content/data/wavs/f148dba99059fb3161a797e798b0deb2.wav  \n",
            "  inflating: /content/data/wavs/61007041a28b4d2fa8813f3fa75ca960.wav  \n",
            "  inflating: /content/data/wavs/0cc1cedbdc7fc7eb617a781b7e90dc6f.wav  \n",
            "  inflating: /content/data/wavs/339d1e836693a36423ca34629e0f5e5d.wav  \n",
            "  inflating: /content/data/wavs/1b0d9af254f04fe43b485f963ecd65ff.wav  \n",
            "  inflating: /content/data/wavs/616cb50072da3ef60def2ca885146c1c.wav  \n",
            "  inflating: /content/data/wavs/ff398d0cd57e6a84679ae23e67daac99.wav  \n",
            "  inflating: /content/data/wavs/d3f495335e82d8de9418f1942b14625c.wav  \n",
            "  inflating: /content/data/wavs/ffc03c248ca79c8edb95db0e856f6e67.wav  \n",
            "  inflating: /content/data/wavs/7e6938d85c8b6ae26cfaedb58cac3d95.wav  \n",
            "  inflating: /content/data/wavs/c4775b9615e1ea12b1d246ae72a524b3.wav  \n",
            "  inflating: /content/data/wavs/c062e9bdcdae534dcb30d26c31af591b.wav  \n",
            "  inflating: /content/data/wavs/25339663ffaf6f57c28f8f3e05719e29.wav  \n",
            "  inflating: /content/data/wavs/dd08f90c3b3f80b7415bd82549b5c09b.wav  \n",
            "  inflating: /content/data/wavs/143c48c9fdc04b1ac8b9174341d3a253.wav  \n",
            "  inflating: /content/data/wavs/de957997cbec3f7be30b18daf9e8c554.wav  \n",
            "  inflating: /content/data/wavs/2756846c2964de6130dde11dcdbbe784.wav  \n",
            "  inflating: /content/data/wavs/bb072c52a873045d3efd0767594dbdc3.wav  \n",
            "  inflating: /content/data/wavs/33cb5c9071507b3d6b5cf6439554c089.wav  \n",
            "  inflating: /content/data/wavs/960967d42899937357e26f082beb6775.wav  \n",
            "  inflating: /content/data/wavs/7b352bf56fa0e39c3933358293a13015.wav  \n",
            "  inflating: /content/data/wavs/0da6bd48aacc66bea45851b24b0b07c4.wav  \n",
            "  inflating: /content/data/wavs/0eee0f5e01dc346bfebbd55a6b9d642f.wav  \n",
            "  inflating: /content/data/wavs/896c3763ccd865896f64d4b9e84ea4f0.wav  \n",
            "  inflating: /content/data/wavs/7a47e9fd35205487e8049d6b498bb09a.wav  \n",
            "  inflating: /content/data/wavs/2283fd719137f9745ab920a4016a98cc.wav  \n",
            "  inflating: /content/data/wavs/21ab8908ec6774273c21bf91ba0462ee.wav  \n",
            "  inflating: /content/data/wavs/6adcc51c698017bdcfcc7ebcab86ca5e.wav  \n",
            "  inflating: /content/data/wavs/16c9def30dceef86b0e11a842eb38e7a.wav  \n",
            "  inflating: /content/data/wavs/a681351c40cc198d31fe621657222555.wav  \n",
            "  inflating: /content/data/wavs/75aa1de8fca142ca782b5311b42a039c.wav  \n",
            "  inflating: /content/data/wavs/6ba5e5682f57d6472d16a67bab59ed3b.wav  \n",
            "  inflating: /content/data/wavs/a8ebf7927a5790a9a7fad24075788253.wav  \n",
            "  inflating: /content/data/wavs/087af1cb777afd6169486d0949e4cee7.wav  \n",
            "  inflating: /content/data/wavs/33ab6ebc7934791ec680b5e20c9c3a0f.wav  \n",
            "  inflating: /content/data/wavs/67dee1cb01269d8932b203ce3ba905e7.wav  \n",
            "  inflating: /content/data/wavs/608b064fb1e7b6b9ff834a80702bf7ef.wav  \n",
            "  inflating: /content/data/wavs/6e6721863c478dc8ca0e61573e721f28.wav  \n",
            "  inflating: /content/data/wavs/ab8d55636efe17cb1861787dcaef3c78.wav  \n",
            "  inflating: /content/data/wavs/3175628133b1256648918e20ac072680.wav  \n",
            "  inflating: /content/data/wavs/c1e0a1d5819a2a8dfa5718092b5e6528.wav  \n",
            "  inflating: /content/data/wavs/1a5e492314b27649b8e99f6b21b7b69f.wav  \n",
            "  inflating: /content/data/wavs/9ae26235546c7c92e898ae798d384902.wav  \n",
            "  inflating: /content/data/wavs/b6d3af41971f91618ec65324996837eb.wav  \n",
            "  inflating: /content/data/wavs/e07ad9a57df3fca0032c6bea8212fb2f.wav  \n",
            "  inflating: /content/data/wavs/bc8931903d7668154ba4264e11909055.wav  \n",
            "  inflating: /content/data/wavs/2680c7a898650ea26b9f5b15f309aac6.wav  \n",
            "  inflating: /content/data/wavs/d0cca878c3f8d1335c19d090dc707c94.wav  \n",
            "  inflating: /content/data/wavs/11f87e079354ba9b54a7ccff939f3077.wav  \n",
            "  inflating: /content/data/wavs/d9aba7282a219097e3252ee8b11dfe40.wav  \n",
            "  inflating: /content/data/wavs/8aba61ad39b9984eae9b95f385b29b9a.wav  \n",
            "  inflating: /content/data/wavs/73dc3e06f029169353d7de0abd5c7a44.wav  \n",
            "  inflating: /content/data/wavs/ab1beb648fe930603ee2def575c2d594.wav  \n",
            "  inflating: /content/data/wavs/7d17cb8379397abdc1a9cab3caff351d.wav  \n",
            "  inflating: /content/data/wavs/d8133c80414188975724f4635bf0a056.wav  \n",
            "  inflating: /content/data/wavs/b701ba6928d10c63169a35645fd467ff.wav  \n",
            "  inflating: /content/data/wavs/15e275396ec64221815d9b9a89a55e3a.wav  \n",
            "  inflating: /content/data/wavs/13ae6738aec0bd8eba142b6bbfecfe3f.wav  \n",
            "  inflating: /content/data/wavs/79f3bf5cf6ed1bb82e825e1703535849.wav  \n",
            "  inflating: /content/data/wavs/2e3e1416ce2803b32e6ecd8829601ba5.wav  \n",
            "  inflating: /content/data/wavs/e0408cf7e2653c376c33960aff49d0ab.wav  \n",
            "  inflating: /content/data/wavs/28f21dbc4e71e696d6cce37577e55dc7.wav  \n",
            "  inflating: /content/data/wavs/0486d6b8a9f8190f2db2efc313a8c767.wav  \n",
            "  inflating: /content/data/wavs/ab6e0bdf836de3b8e257c33d71949ecf.wav  \n",
            "  inflating: /content/data/wavs/a370e10ab42da7f8f1d8abf2efab2d6b.wav  \n",
            "  inflating: /content/data/wavs/2ed92a8d0f0b91952b59044f7d2fbf51.wav  \n",
            "  inflating: /content/data/wavs/f0a9eaab721c50f4073c174662602780.wav  \n",
            "  inflating: /content/data/wavs/03f96129df2999efde65567ec6d6c242.wav  \n",
            "  inflating: /content/data/wavs/debe7ec28c14a7af947027055174ae77.wav  \n",
            "  inflating: /content/data/wavs/441c62864b81e14a031f7f65f5bf719b.wav  \n",
            "  inflating: /content/data/wavs/697416063514710ae7ebb3719ff91329.wav  \n",
            "  inflating: /content/data/wavs/9912890c9e35e43cbb5596d61b41b94c.wav  \n",
            "  inflating: /content/data/wavs/598d052537f371b8c82a309baa90c2b9.wav  \n",
            "  inflating: /content/data/wavs/61cd1ecf672b4a2d3b4b8d27067dbf7d.wav  \n",
            "  inflating: /content/data/wavs/f68492ae9d33c5dedbb84dfc546d6e14.wav  \n",
            "  inflating: /content/data/wavs/ceacde1323a0367cade25ee1b9d20147.wav  \n",
            "  inflating: /content/data/wavs/d1b0a6beeb7a66bec211b8cfa2662ad8.wav  \n",
            "  inflating: /content/data/wavs/a22e583600c3ea12082b4843cc4c02ec.wav  \n",
            "  inflating: /content/data/wavs/f1caa0432d9f64bf3036bfdf3035448f.wav  \n",
            "  inflating: /content/data/wavs/5801c5c16dd572496fc8b7dac3343475.wav  \n",
            "  inflating: /content/data/wavs/6768080c42385722d93c31c3349147d1.wav  \n",
            "  inflating: /content/data/wavs/7ccd0e0cd6c8665c209db152f914c4e0.wav  \n",
            "  inflating: /content/data/wavs/69192ab7a3463419937026ebaf5503c0.wav  \n",
            "  inflating: /content/data/wavs/ac04901d17e0f2ac9bd343edd27467ab.wav  \n",
            "  inflating: /content/data/wavs/c1d0743ee9b4ad884afe209dcb49ff46.wav  \n",
            "  inflating: /content/data/wavs/7997ae324e8e0d017170cd9da060ab4b.wav  \n",
            "  inflating: /content/data/wavs/982144a260c2016d9c3dfede7e0f7b23.wav  \n",
            "  inflating: /content/data/wavs/ad5b5d473a9eccf8f4647c53c07dea93.wav  \n",
            "  inflating: /content/data/wavs/1b81cbceac35e35916918173ad7d61a7.wav  \n",
            "  inflating: /content/data/wavs/95886d424c71e6e053c83a2b6a117ff2.wav  \n",
            "  inflating: /content/data/wavs/0a76411f8cc5a1f0931f23850c9f58d5.wav  \n",
            "  inflating: /content/data/wavs/59851220eceeba83e53eb093b547184d.wav  \n",
            "  inflating: /content/data/wavs/3d250b9f8c6df9f800eef05c112ded17.wav  \n",
            "  inflating: /content/data/wavs/4be2fa44b1d51053a99516acd0acef9c.wav  \n",
            "  inflating: /content/data/wavs/85446782e89b36ddf1296891ccbbaf25.wav  \n",
            "  inflating: /content/data/wavs/6ac34077e6b44acf89c26723b23c98d9.wav  \n",
            "  inflating: /content/data/wavs/65648e1bcdd8e2c4dba6e14a36cc39bd.wav  \n",
            "  inflating: /content/data/wavs/1726fd4f4d4fb884afdbd6a090d63fd0.wav  \n",
            "  inflating: /content/data/wavs/f10a1ea262296273346ce73375013497.wav  \n",
            "  inflating: /content/data/wavs/c89868731d2c5397f3abf8c442a78418.wav  \n",
            "  inflating: /content/data/wavs/a361c1d70dd4b117e661d5e305f971db.wav  \n",
            "  inflating: /content/data/wavs/c61ea5a26bb6c44a07e4eba307f6d520.wav  \n",
            "  inflating: /content/data/wavs/f27c4183a7936a59285b1335fcd35772.wav  \n",
            "  inflating: /content/data/wavs/0c049759eeabad51e206bf8e31dec9d8.wav  \n",
            "  inflating: /content/data/wavs/02606cd33787f9bd86f02e6b4e78f611.wav  \n",
            "  inflating: /content/data/wavs/a47b467ac88e92816eba76b8346be5ac.wav  \n",
            "  inflating: /content/data/wavs/8592142e672d7cf11b92f511a743e779.wav  \n",
            "  inflating: /content/data/wavs/914465ad6625b8e51fa4a3ac2325261c.wav  \n",
            "  inflating: /content/data/wavs/9e3fabbdc6afcb7dadd4f36172208b62.wav  \n",
            "  inflating: /content/data/wavs/f631d52c8f4ded4c4c80e96b4980beab.wav  \n",
            "  inflating: /content/data/wavs/66bd07b3edca02a3844b1a8e5e411cfd.wav  \n",
            "  inflating: /content/data/wavs/25d8bb59883eb6abfbd690520db00fa3.wav  \n",
            "  inflating: /content/data/wavs/8b29bc62b768d01aec2b143aa7163bfa.wav  \n",
            "  inflating: /content/data/wavs/d3a6376a963e9c49350b0494dd15f70b.wav  \n",
            "  inflating: /content/data/wavs/cdc273d147c6272fc9a07d328b85f9df.wav  \n",
            "  inflating: /content/data/wavs/d56fca436f3a4c778e8030b714c3bec8.wav  \n",
            "  inflating: /content/data/wavs/298fd70c53510ade69dfd807cad45e5d.wav  \n",
            "  inflating: /content/data/wavs/291e4b1eba6952e910e684dd35871ddf.wav  \n",
            "  inflating: /content/data/wavs/d775c8f2080afce66a5b02aa229433ee.wav  \n",
            "  inflating: /content/data/wavs/e912193e564669d802663cf40e8ccb0e.wav  \n",
            "  inflating: /content/data/wavs/1d3bf800076d41406bd7405ea92f140c.wav  \n",
            "  inflating: /content/data/wavs/bf4c04d2f44b546ddf8f23d442f7c333.wav  \n",
            "  inflating: /content/data/wavs/80e2464c93e717ccf05ffd59d3379d2e.wav  \n",
            "  inflating: /content/data/wavs/bd07a6dd1c6454ee203edf76ce91ee3f.wav  \n",
            "  inflating: /content/data/wavs/56cd370868de59cce3f55351e7335096.wav  \n",
            "  inflating: /content/data/wavs/72aa41766156432cb019fbd5dc450f70.wav  \n",
            "  inflating: /content/data/wavs/2597dc5609dc4630ce6ea8e342e522f0.wav  \n",
            "  inflating: /content/data/wavs/0879ae48e894542ad0959d5b253e44a0.wav  \n",
            "  inflating: /content/data/wavs/36be9bf47e2323bcbc0a5063bbb270c1.wav  \n",
            "  inflating: /content/data/wavs/5420d497ccecae12db9f1fe70a438bd6.wav  \n",
            "  inflating: /content/data/wavs/08a8d72fd6c1ea0f38ec3e066f431054.wav  \n",
            "  inflating: /content/data/wavs/845c3faf0c6f0ce9cf5be3daeb0503d3.wav  \n",
            "  inflating: /content/data/wavs/7b3452ed50bf6cf25df765bdb642243c.wav  \n",
            "  inflating: /content/data/wavs/c70645dcea783f282d6cb3c78fc8f088.wav  \n",
            "  inflating: /content/data/wavs/45d2c1d2c0cea8fdb161a65dd0723a2c.wav  \n",
            "  inflating: /content/data/wavs/bc28673e409d948a2f1e89eb62f8dbe6.wav  \n",
            "  inflating: /content/data/wavs/2b8f891217551a4a5aaf98ce2b7cc85e.wav  \n",
            "  inflating: /content/data/wavs/d8bdfd28e652084d917125e53d07d612.wav  \n",
            "  inflating: /content/data/wavs/55eef7a4ae53513c5fbac2209e2cbda4.wav  \n",
            "  inflating: /content/data/wavs/eca256c8e7eb880088f4bb3f89a1a68f.wav  \n",
            "  inflating: /content/data/wavs/3b14b3f4cd10cdb34deefadc6abeb43c.wav  \n",
            "  inflating: /content/data/wavs/6c7d2ed158bef8a3a289ed998700158a.wav  \n",
            "  inflating: /content/data/wavs/4d664538736db0b6c85aa2297fdf2e35.wav  \n",
            "  inflating: /content/data/wavs/79140074fbee43887463b097de0107fd.wav  \n",
            "  inflating: /content/data/wavs/125ca0c2cd0a03a2adae33f1ee6c2a5e.wav  \n",
            "  inflating: /content/data/wavs/eb268e66c766e603bbd1cea66cd780a8.wav  \n",
            "  inflating: /content/data/wavs/284dd8505d41c84f1a59fabb8717eb2f.wav  \n",
            "  inflating: /content/data/wavs/81f1e605187d0545ae9c6428f0ad0041.wav  \n",
            "  inflating: /content/data/wavs/dff4ad7e6d2624798e7b7ecbce33342c.wav  \n",
            "  inflating: /content/data/wavs/cfaabb667b51b8c8be1c1922c0b46d3c.wav  \n",
            "  inflating: /content/data/wavs/f7c305d34b86d661462c6ba457582c6b.wav  \n",
            "  inflating: /content/data/wavs/d858185a449c818375d9cd41ff77d4b1.wav  \n",
            "  inflating: /content/data/wavs/b04143baeb5756e76558b880c36b385a.wav  \n",
            "  inflating: /content/data/wavs/c45d2409cfc40dca11f3d24324579212.wav  \n",
            "  inflating: /content/data/wavs/ecf57b10cfec6fb321c3438548200ea2.wav  \n",
            "  inflating: /content/data/wavs/9d761758eb496d813f75250de35d60d7.wav  \n",
            "  inflating: /content/data/wavs/c8c05501930ca995e27d370656ddb02c.wav  \n",
            "  inflating: /content/data/wavs/69669ee39174ee8cea74a744d532a331.wav  \n",
            "  inflating: /content/data/wavs/cda8b15e2143f8e25106b02038cdc16c.wav  \n",
            "  inflating: /content/data/wavs/5ca328075912520b24fb7c27aac363e9.wav  \n",
            "  inflating: /content/data/wavs/8c3b4bec3b07e4cd991a558a7086b33f.wav  \n",
            "  inflating: /content/data/wavs/ef3afed394be9522cd8f18189b380215.wav  \n",
            "  inflating: /content/data/wavs/d1ce3af3b30d9147261c1a4f560075de.wav  \n",
            "  inflating: /content/data/wavs/4be2ebd9c12dc50fd7ce1580b0775d0e.wav  \n",
            "  inflating: /content/data/wavs/330c03284d37efd7241a32857ade78fe.wav  \n",
            "  inflating: /content/data/wavs/06dd8613d7d2373883804147fb522560.wav  \n",
            "  inflating: /content/data/wavs/68c060bcca56ea382de384d01bddc623.wav  \n",
            "  inflating: /content/data/wavs/09002cfabf11c0f4e8846c1c1e7e95ea.wav  \n",
            "  inflating: /content/data/wavs/77cff1e60f665c191ceca2c124f80796.wav  \n",
            "  inflating: /content/data/wavs/d73e7f11502110b33286374d9c2a44a0.wav  \n",
            "  inflating: /content/data/wavs/a2743bd3f42170b3794bad7f062edc0d.wav  \n",
            "  inflating: /content/data/wavs/f305448ac8d5100aa78c942690ccf865.wav  \n",
            "  inflating: /content/data/wavs/9f9992d644ffabb169f589abf107d335.wav  \n",
            "  inflating: /content/data/wavs/eb8c168a34e2628a011d195ac9b94d36.wav  \n",
            "  inflating: /content/data/wavs/00d856081aea9a35436a07ff8d995ae6.wav  \n",
            "  inflating: /content/data/wavs/7f2f9cb36996d822db434a66e3ee4713.wav  \n",
            "  inflating: /content/data/wavs/ea643e7c76325841a7498e170c3f1cf2.wav  \n",
            "  inflating: /content/data/wavs/ee98035f9551342b122eb5bba52108c1.wav  \n",
            "  inflating: /content/data/wavs/20db5c2592c59c9698de9d6cad8ce210.wav  \n",
            "  inflating: /content/data/wavs/c6e71bc7f52aca003f292bcce9359258.wav  \n",
            "  inflating: /content/data/wavs/7217982262d634d6ab834a6c7befd73a.wav  \n",
            "  inflating: /content/data/wavs/0fefa1b2e0152ac40939c9843e0e0a0a.wav  \n",
            "  inflating: /content/data/wavs/4fd4e2e9906d320a0e3e0882d4bbec8f.wav  \n",
            "  inflating: /content/data/wavs/92f8cc99618b920319979a72b0de7f81.wav  \n",
            "  inflating: /content/data/wavs/70fa70b0e0146c61ef88a84ad8b126d8.wav  \n",
            "  inflating: /content/data/wavs/954d252f7207b9fb714e78e3d754c9e4.wav  \n",
            "  inflating: /content/data/wavs/b23e2d0d78c2ccef1ed9a18574bb62cb.wav  \n",
            "  inflating: /content/data/wavs/b0fb6bdca172587f3dc82bf931f6f895.wav  \n",
            "  inflating: /content/data/wavs/c434357bc2bc0bbc58e704bb2bb83832.wav  \n",
            "  inflating: /content/data/wavs/757e4cf63d4d51af71e30e3153c22e0c.wav  \n",
            "  inflating: /content/data/wavs/cd926c38e189d8f98d2660466696b598.wav  \n",
            "  inflating: /content/data/wavs/0a9e7c74e80044a2c09e4cb75871de42.wav  \n",
            "  inflating: /content/data/wavs/8433d5be1ac9c4f3eb3e2f97a366ef31.wav  \n",
            "  inflating: /content/data/wavs/a25194f17248e135708f2cbde1f7607d.wav  \n",
            "  inflating: /content/data/wavs/6bce10f183a86a1ff8050e113e60718a.wav  \n",
            "  inflating: /content/data/wavs/b8d3a99b35c366c00599c5035404d7d6.wav  \n",
            "  inflating: /content/data/wavs/dcae61b75612eae5736adf0bc3084863.wav  \n",
            "  inflating: /content/data/wavs/b440c7ca444b69dcb7a4d2e279520048.wav  \n",
            "  inflating: /content/data/wavs/1d5a7983e279cae21b69585d5495ef53.wav  \n",
            "  inflating: /content/data/wavs/ef22522192b113279591420d1f56ec77.wav  \n",
            "  inflating: /content/data/wavs/cae94515a82be8187e525d576786ef1b.wav  \n",
            "  inflating: /content/data/wavs/3e12d9ffa467199f6e077f447ae1fba1.wav  \n",
            "  inflating: /content/data/wavs/b7de842cd34c7fbb2bcf5fa30d3d5ba0.wav  \n",
            "  inflating: /content/data/wavs/75d49fbeb7c231c1e364e25073b551d6.wav  \n",
            "  inflating: /content/data/wavs/c7da745aa5e686acf6f22ef2ad2dc1d4.wav  \n",
            "  inflating: /content/data/wavs/7d1f3a1f9233b95b6e7277070e424960.wav  \n",
            "  inflating: /content/data/wavs/0d6b47a7b8ae7dfe7d20f4c2a034877a.wav  \n",
            "  inflating: /content/data/wavs/cbd93fcd1163e43544bf6e4440838b9e.wav  \n",
            "  inflating: /content/data/wavs/256cfc9db2c67aa81f8250f2408f5c1b.wav  \n",
            "  inflating: /content/data/wavs/1605dd42e3c3e218c22df844f08ff86c.wav  \n",
            "  inflating: /content/data/wavs/dbdb276f7253b9f941c128602e3ba686.wav  \n",
            "  inflating: /content/data/wavs/fc08590d22b2140b7acffb8cb3bc6d2d.wav  \n",
            "  inflating: /content/data/wavs/e20fde75f2638e0be5f36dabf4268567.wav  \n",
            "  inflating: /content/data/wavs/58db5368fc1dd1ea03249ee325a52334.wav  \n",
            "  inflating: /content/data/wavs/d777141be1aaae67d89e7fbb41a1b9d5.wav  \n",
            "  inflating: /content/data/wavs/33ec45691fd5d461c9ec2f2b745942ae.wav  \n",
            "  inflating: /content/data/wavs/72914ec4316a8c14ef026753b8032d6a.wav  \n",
            "  inflating: /content/data/wavs/62df76d1b7f457b25f512fb10b2f7539.wav  \n",
            "  inflating: /content/data/wavs/2971e5604e3d30102fb5fb3241849261.wav  \n",
            "  inflating: /content/data/wavs/cfb517b420cef469c4f167691d743ff5.wav  \n",
            "  inflating: /content/data/wavs/cc5a84942cd351c22f75f608f9c86888.wav  \n",
            "  inflating: /content/data/wavs/013e880661d9e7c18f70657185530f0d.wav  \n",
            "  inflating: /content/data/wavs/3fd10731544f2ccbf71a3af3d9bc5094.wav  \n",
            "  inflating: /content/data/wavs/6b6e6dc8e01c7097f4b9a3d0eb3220a6.wav  \n",
            "  inflating: /content/data/wavs/3bf75667eb891601677600067581d5e8.wav  \n",
            "  inflating: /content/data/wavs/9a90be4ee71aa547a65e8eaf4aa454b7.wav  \n",
            "  inflating: /content/data/wavs/630dc86818c241baa5736dc2d65d0715.wav  \n",
            "  inflating: /content/data/wavs/138c6e9707f8ae925f2f1cdaf50b40ee.wav  \n",
            "  inflating: /content/data/wavs/d2e050d819b23008f580bad12b4777eb.wav  \n",
            "  inflating: /content/data/wavs/33f074d99d8ff8c8c3d3c1864c79878b.wav  \n",
            "  inflating: /content/data/wavs/9b3063d4d3565c9c5a24292fa3bfba05.wav  \n",
            "  inflating: /content/data/wavs/1f4ac50e010897e608fed2d1d754fe8b.wav  \n",
            "  inflating: /content/data/wavs/0dc822cca93e301a25100bfb3772ff1f.wav  \n",
            "  inflating: /content/data/wavs/4ebcb4846d84f49d937674fa49fb6ce0.wav  \n",
            "  inflating: /content/data/wavs/dfc8a00ba481caa5de1c60835f3322f8.wav  \n",
            "  inflating: /content/data/wavs/e23ffca4ac044de2633c7770cf73a46b.wav  \n",
            "  inflating: /content/data/wavs/1cd9dadcbb339915bbf0fab5612ee406.wav  \n",
            "  inflating: /content/data/wavs/3f3ab5c995d6305757faf2389c3dd4f2.wav  \n",
            "  inflating: /content/data/wavs/26e1c8f98e69f627acdc44709ebfaaf9.wav  \n",
            "  inflating: /content/data/wavs/7746120d3db2843ae5ac936049b5ffee.wav  \n",
            "  inflating: /content/data/wavs/cde4e6590e10ff644e09061c6d9f8c64.wav  \n",
            "  inflating: /content/data/wavs/c690f52fa8ce658676aab86a6134748f.wav  \n",
            "  inflating: /content/data/wavs/c5b1b55f3a5cf0df08c6fcb475344975.wav  \n",
            "  inflating: /content/data/wavs/8f39fde0a1f44ac547093da98166662e.wav  \n",
            "  inflating: /content/data/wavs/b3909af8fb93723d4fc2e59b1eef2fd4.wav  \n",
            "  inflating: /content/data/wavs/a2c5e2633e6d0cb13504a7bdde41c185.wav  \n",
            "  inflating: /content/data/wavs/34abdf9e0fe74b5d53e0d4597cc8afe4.wav  \n",
            "  inflating: /content/data/wavs/b94b4aaf10cb12ef361c2b3bde40c9a1.wav  \n",
            "  inflating: /content/data/wavs/22c0535c9d6cae7976b33bbb0b9c5af3.wav  \n",
            "  inflating: /content/data/wavs/9bb1312667b665ae69b5384a80ee340d.wav  \n",
            "  inflating: /content/data/wavs/8404a2431bb6811dc9535bcd4f9e2d75.wav  \n",
            "  inflating: /content/data/wavs/b33f4909e3749753bd0a574b9104f970.wav  \n",
            "  inflating: /content/data/wavs/3f96ad5f082800d9b8a51d2716598e67.wav  \n",
            "  inflating: /content/data/wavs/315fed5d290d4c4c7b95850db6028672.wav  \n",
            "  inflating: /content/data/wavs/b9abba8b66bd3f1cd47eaf6df28c17c9.wav  \n",
            "  inflating: /content/data/wavs/3f11701334012ed52a47917ca7bf1920.wav  \n",
            "  inflating: /content/data/wavs/ec9b8e1a0a845b42b2d96cfd06efb554.wav  \n",
            "  inflating: /content/data/wavs/ed46b60e53134cd151bde20b66167273.wav  \n",
            "  inflating: /content/data/wavs/4b097caec5427d91cdbd640cface1d7e.wav  \n",
            "  inflating: /content/data/wavs/a0dd73e5c453773a286bdaddf9024147.wav  \n",
            "  inflating: /content/data/wavs/30b4fae91ba3dcb3f69c0fa52d61d9b9.wav  \n",
            "  inflating: /content/data/wavs/794d61b6b90d020e33ac0cc985a2d8be.wav  \n",
            "  inflating: /content/data/wavs/34ddb5816f29e91ed9e62c6678a5a93d.wav  \n",
            "  inflating: /content/data/wavs/f234451a42b13fe385f5cb1ef50bb6de.wav  \n",
            "  inflating: /content/data/wavs/2ab42971035320bd7ff28a2386b9e221.wav  \n",
            "  inflating: /content/data/wavs/b8333484d60a53ed358b12298d115ea1.wav  \n",
            "  inflating: /content/data/wavs/236878a131f1b63d7abd71016f3ae155.wav  \n",
            "  inflating: /content/data/wavs/4adf79b29824ae191d8b8c7a008a9cc7.wav  \n",
            "  inflating: /content/data/wavs/5da246d3202c0f577d328365ab5ad269.wav  \n",
            "  inflating: /content/data/wavs/843c75504fae15f0002e7d3f36fda1db.wav  \n",
            "  inflating: /content/data/wavs/4659532c5fc7e099a805dec329776a13.wav  \n",
            "  inflating: /content/data/wavs/350aded258e1d19f34bd5f24a28439a7.wav  \n",
            "  inflating: /content/data/wavs/f5d624d9ee960d84a8f66cff5399df96.wav  \n",
            "  inflating: /content/data/wavs/ab8c202d6598cc52a7ec48748d52e5cc.wav  \n",
            "  inflating: /content/data/wavs/5faa059b0912b94283b27d82ab816879.wav  \n",
            "  inflating: /content/data/wavs/133890456162d7e484e814fbf4607793.wav  \n",
            "  inflating: /content/data/wavs/356ad1d6c9ef5fce731f7b0c1e4ba929.wav  \n",
            "  inflating: /content/data/wavs/8372480ff3cb5bb0293140cafacdbfeb.wav  \n",
            "  inflating: /content/data/wavs/c5c4299a9522f82446f7cd3a32b8ab7e.wav  \n",
            "  inflating: /content/data/wavs/564d96e742eee15eb84a014cea8740bc.wav  \n",
            "  inflating: /content/data/wavs/4c7c01be21007f4ebe374ca5defe3a65.wav  \n",
            "  inflating: /content/data/wavs/abafbecc897efb07fb841d8e6b4200e9.wav  \n",
            "  inflating: /content/data/wavs/9e3894d1744e3f56d457d63a4e00ee5b.wav  \n",
            "  inflating: /content/data/wavs/b065f2073de534153a645a52743bf493.wav  \n",
            "  inflating: /content/data/wavs/19ac46bef5eac1d4ff6bc0bb42f5db4c.wav  \n",
            "  inflating: /content/data/wavs/056bbc93488bf9f2111c35584c284dc7.wav  \n",
            "  inflating: /content/data/wavs/e42e6083daedf598e5ac65daa6824142.wav  \n",
            "  inflating: /content/data/wavs/3d6c0f6bffb5cdcc2545ee4e246f0796.wav  \n",
            "  inflating: /content/data/wavs/f55bf9f9de358881fce8465ace127a05.wav  \n",
            "  inflating: /content/data/wavs/16013cb81b478906d09ad8642a6fd840.wav  \n",
            "  inflating: /content/data/wavs/1693c612869fbc8664c15f4530891d19.wav  \n",
            "  inflating: /content/data/wavs/d9150262d16a1693918396fe927a3c59.wav  \n",
            "  inflating: /content/data/wavs/9c28beb1a8ea78b73acb14d4f29db712.wav  \n",
            "  inflating: /content/data/wavs/4972ebc4f175ebeab2c1e5a87a2c0c26.wav  \n",
            "  inflating: /content/data/wavs/fc0837558ecd4610814336cb662cc3d3.wav  \n",
            "  inflating: /content/data/wavs/690fa1e2654e8cf63a06fae8ed9f2914.wav  \n",
            "  inflating: /content/data/wavs/49c20f9358f03cafd6b9d95210fdcd07.wav  \n",
            "  inflating: /content/data/wavs/5ba12535d2e81533135669419b2e960a.wav  \n",
            "  inflating: /content/data/wavs/3830bcfbc5d57b7528811a83bb0051d6.wav  \n",
            "  inflating: /content/data/wavs/be5c3354e38f9af130b28449a081d3aa.wav  \n",
            "  inflating: /content/data/wavs/9fb86e4421ffe7a168364a50a2e054fa.wav  \n",
            "  inflating: /content/data/wavs/3598afd7ef6c933b336931002458a7e5.wav  \n",
            "  inflating: /content/data/wavs/418ff66b9c1ebe35aaff1c8e2dc0c6bc.wav  \n",
            "  inflating: /content/data/wavs/88690a9bed0354a1e535a51770ae8ebe.wav  \n",
            "  inflating: /content/data/wavs/5a2828adcb5727a6e0c5e30bd58e65ce.wav  \n",
            "  inflating: /content/data/wavs/f353957e188d19f75d96c48bd7ca7c1b.wav  \n",
            "  inflating: /content/data/wavs/9b4686b423b35fc322586756c59906c4.wav  \n",
            "  inflating: /content/data/wavs/0edbb32591493146346ca1b03a67b50e.wav  \n",
            "  inflating: /content/data/wavs/7109b1252288b208fdefa9aa6f92f0ed.wav  \n",
            "  inflating: /content/data/wavs/53663a397c6feab1b3ee2684bc539eab.wav  \n",
            "  inflating: /content/data/wavs/81e8624c49e8db0972f7dcc673494ac0.wav  \n",
            "  inflating: /content/data/wavs/865caf22036ee7d84e74b17ce8f101fc.wav  \n",
            "  inflating: /content/data/wavs/be3242f683365b1119fe3acbf22c4b61.wav  \n",
            "  inflating: /content/data/wavs/292229144d2bb7cda64681df844f0e96.wav  \n",
            "  inflating: /content/data/wavs/6eb4c170ea8a1e05bd2a749dd0a729bb.wav  \n",
            "  inflating: /content/data/wavs/848994467100e3cb1ba95c484cb4e8dc.wav  \n",
            "  inflating: /content/data/wavs/0a7694822f9eca400929f30bb2cfecf8.wav  \n",
            "  inflating: /content/data/wavs/d97290a4bc443e66073e5ab04fbeb981.wav  \n",
            "  inflating: /content/data/wavs/d5b67a0eb8b9fc6364e2a81c113efa9e.wav  \n",
            "  inflating: /content/data/wavs/b832e14bd82617c97a7109e7349aa442.wav  \n",
            "  inflating: /content/data/wavs/6544a461936afbb8c1bc86394106b1a7.wav  \n",
            "  inflating: /content/data/wavs/4cb26d0c6813236ad1487bf5b5e3098a.wav  \n",
            "  inflating: /content/data/wavs/402b80bd48546e2322661ffd1b888697.wav  \n",
            "  inflating: /content/data/wavs/7eb9d9805208b9b55c42865afd4871b4.wav  \n",
            "  inflating: /content/data/wavs/293c0085820c130ca9ad2e2c96940fed.wav  \n",
            "  inflating: /content/data/wavs/4378131f8df1e1576fbd1d0e0b87c113.wav  \n",
            "  inflating: /content/data/wavs/4359d2b129d48cc8937a41e9cba39b0c.wav  \n",
            "  inflating: /content/data/wavs/5a389f5ee51920281f44d1bc660b00f9.wav  \n",
            "  inflating: /content/data/wavs/30463ff821be6ba9a0d73911bbca5ae6.wav  \n",
            "  inflating: /content/data/wavs/f6b3ef228ff1689ae4e86ca0e1654f40.wav  \n",
            "  inflating: /content/data/wavs/6b099e116911fd695bbd3f5461866c52.wav  \n",
            "  inflating: /content/data/wavs/1257595fa9ccfc36650accce6209ebd9.wav  \n",
            "  inflating: /content/data/wavs/17eed9f867066eb7f051414c72c1474d.wav  \n",
            "  inflating: /content/data/wavs/dd873cc568bc22f084d54aac37205d3d.wav  \n",
            "  inflating: /content/data/wavs/a70c6da08117deab6bf5b746e1ee4326.wav  \n",
            "  inflating: /content/data/wavs/7ac8f60b84eb955e543f654c4315dc57.wav  \n",
            "  inflating: /content/data/wavs/ba3748754fe5f7776060da3f864a8a5d.wav  \n",
            "  inflating: /content/data/wavs/0f621841de38c93d090d31fe5535ecb5.wav  \n",
            "  inflating: /content/data/wavs/bdb39ef6023997c276e89df860b39046.wav  \n",
            "  inflating: /content/data/wavs/2e7290693a750885ed1a58784c2259e4.wav  \n",
            "  inflating: /content/data/wavs/b2c5a98ea1291b24af770400c2ed5752.wav  \n",
            "  inflating: /content/data/wavs/b4223fab2d363617f1e214b6c07dfbcf.wav  \n",
            "  inflating: /content/data/wavs/629d101e3c0a04552534fc4685bf4910.wav  \n",
            "  inflating: /content/data/wavs/fd13f7012845768f86b1b2945d4212fd.wav  \n",
            "  inflating: /content/data/wavs/a75653f1a96fed7c2d71323aef5d5bb5.wav  \n",
            "  inflating: /content/data/wavs/502005d34b4d7f7e61e811419ee657fb.wav  \n",
            "  inflating: /content/data/wavs/c7a317fc216c764038160d646b8fe655.wav  \n",
            "  inflating: /content/data/wavs/5ff75146e78a16f630c91c8fbc1d0e8f.wav  \n",
            "  inflating: /content/data/wavs/fefb14d0acf8b2b3f464d7a01980e683.wav  \n",
            "  inflating: /content/data/wavs/81ff675b1dfb7e2917ad7df4e6971f3d.wav  \n",
            "  inflating: /content/data/wavs/abce442eeabfc038b4e003b21e1c7a77.wav  \n",
            "  inflating: /content/data/wavs/1b7ba9217c8a3cb48f8ce8708829efa0.wav  \n",
            "  inflating: /content/data/wavs/50517f7718513fc09210fe06ba9f57fd.wav  \n",
            "  inflating: /content/data/wavs/8f4710c1df361e208170c950f005c745.wav  \n",
            "  inflating: /content/data/wavs/03c401fc4c66128cdcbb8c53633981d5.wav  \n",
            "  inflating: /content/data/wavs/7af9d1011c4950bbd86126029f849593.wav  \n",
            "  inflating: /content/data/wavs/07767053146425df64377477715539fb.wav  \n",
            "  inflating: /content/data/wavs/ce04df313d5ef3992d34d0d40c4d0c45.wav  \n",
            "  inflating: /content/data/wavs/02d857115285e1a27e89ad5c9552ef0f.wav  \n",
            "  inflating: /content/data/wavs/384ef6f486fbcf620e0ad394f9facdae.wav  \n",
            "  inflating: /content/data/wavs/5137904c54459a5b48097e9d60c5307d.wav  \n",
            "  inflating: /content/data/wavs/69450be75e2428568112435bc82839a6.wav  \n",
            "  inflating: /content/data/wavs/2ae576d3c150519ca44717600986b8ef.wav  \n",
            "  inflating: /content/data/wavs/0d1037f923b540e1b372d76b27cc75a4.wav  \n",
            "  inflating: /content/data/wavs/a415b1b3ef0d8b09999ca861c3dd8bc7.wav  \n",
            "  inflating: /content/data/wavs/34e30a7055e0935f260e11f2d935aac8.wav  \n",
            "  inflating: /content/data/wavs/5838aa18a8aaeb3280fe1818a3c62044.wav  \n",
            "  inflating: /content/data/wavs/e07be82a3e9be4a78211a469d69f685a.wav  \n",
            "  inflating: /content/data/wavs/efcbb9b8ebd4fd7a73986b73e86961e6.wav  \n",
            "  inflating: /content/data/wavs/65a285ccfee833eed4fad787c7b14412.wav  \n",
            "  inflating: /content/data/wavs/f371d8e232652ef184bc5f23aa31193e.wav  \n",
            "  inflating: /content/data/wavs/4d629a4b7dc787a802db530e47ccc666.wav  \n",
            "  inflating: /content/data/wavs/9c04e4112ca9659a087256f79a872814.wav  \n",
            "  inflating: /content/data/wavs/4239568365eca9a406f25562db21dbaa.wav  \n",
            "  inflating: /content/data/wavs/ee69cc376d3c240adaf2edd0b9ef454f.wav  \n",
            "  inflating: /content/data/wavs/74f1cb20ac1f428c9858102b656d841e.wav  \n",
            "  inflating: /content/data/wavs/63648dba2df816157370319e768407fe.wav  \n",
            "  inflating: /content/data/wavs/c2daf352daf495d05d22c1e09043189b.wav  \n",
            "  inflating: /content/data/wavs/b87fe39072b5b054db36410a0eafa3a7.wav  \n",
            "  inflating: /content/data/wavs/4a329ad5a7fdd04a8df03b8c16d45223.wav  \n",
            "  inflating: /content/data/wavs/2b9391be89418c7ec040710a6c5160ee.wav  \n",
            "  inflating: /content/data/wavs/cce73d84ff7ad6c5316dbb85228110b2.wav  \n",
            "  inflating: /content/data/wavs/7dcb85c8abe79f386500284faeac1f18.wav  \n",
            "  inflating: /content/data/wavs/64173dccc2d8dd9778cfbfce8d3a2e97.wav  \n",
            "  inflating: /content/data/wavs/e83ec0325f929557b90c40dd2dd993a6.wav  \n",
            "  inflating: /content/data/wavs/73066ef1ad72f767241293dd4523042f.wav  \n",
            "  inflating: /content/data/wavs/6d6cc97254fd041126ac2af5437ee7cb.wav  \n",
            "  inflating: /content/data/wavs/35c67f07d3d842679b5775bf9f50d04f.wav  \n",
            "  inflating: /content/data/wavs/8b44b188538c32bb1b461ab1bfc704e8.wav  \n",
            "  inflating: /content/data/wavs/8bd060edc52538e315ebf13400ce3cc4.wav  \n",
            "  inflating: /content/data/wavs/45cc28e249185ae52b1a1e6a4aa957f0.wav  \n",
            "  inflating: /content/data/wavs/0cf16a12fcb8bfebd89e30bb43dfd4d3.wav  \n",
            "  inflating: /content/data/wavs/757dc00a21dde9f315fc4e56a4f99e30.wav  \n",
            "  inflating: /content/data/wavs/7995fb83f80dcf0f49bdc36f4df2cc66.wav  \n",
            "  inflating: /content/data/wavs/939ee68671b783cdb991108b736f2040.wav  \n",
            "  inflating: /content/data/wavs/97dccc1b97039f675f1bbfa8758579ff.wav  \n",
            "  inflating: /content/data/wavs/46d25b78d21204bb6cca4bb2bab816ef.wav  \n",
            "  inflating: /content/data/wavs/684cae636a710df18b5833fe30610896.wav  \n",
            "  inflating: /content/data/wavs/ca14d0f5c5e04f4b01e30d9e7877b7f4.wav  \n",
            "  inflating: /content/data/wavs/0b9834730430207803b05ea503bc5682.wav  \n",
            "  inflating: /content/data/wavs/484612929dc0d920a0074c0f3df260da.wav  \n",
            "  inflating: /content/data/wavs/6cf1e2805b728f17a7a31ee2f2208897.wav  \n",
            "  inflating: /content/data/wavs/7f38550c35cc72efee91fc8a9186929c.wav  \n",
            "  inflating: /content/data/wavs/42498fff28bc17c01b77af07bb905e50.wav  \n",
            "  inflating: /content/data/wavs/bb99714a9fc2af11adb9ff03187b9591.wav  \n",
            "  inflating: /content/data/wavs/c79789bb50c106c2eabe367a3a8a23fd.wav  \n",
            "  inflating: /content/data/wavs/bd2588b9e27bc5c6843557017e322534.wav  \n",
            "  inflating: /content/data/wavs/f1731e3bf040178079906610dd22b26e.wav  \n",
            "  inflating: /content/data/wavs/2dd133fecfa15fe26cb6107cb6d1fcaa.wav  \n",
            "  inflating: /content/data/wavs/dec40f565fef46f1ee43efb829ee91b7.wav  \n",
            "  inflating: /content/data/wavs/775aff58e5d2ab1cc6959b42dc57ab9e.wav  \n",
            "  inflating: /content/data/wavs/dc02d2d157fb008627c1d60756d3cc4e.wav  \n",
            "  inflating: /content/data/wavs/b484809f8e7d651e45a9b54571c5cd05.wav  \n",
            "  inflating: /content/data/wavs/ad6054acfc76e1c7f688e52ea570f7a1.wav  \n",
            "  inflating: /content/data/wavs/b857b06b2d85c974bae95383419b31e9.wav  \n",
            "  inflating: /content/data/wavs/24069e289cdf5241485dea2610c970d9.wav  \n",
            "  inflating: /content/data/wavs/068b8e406923e70141b4b6c5721eaf67.wav  \n",
            "  inflating: /content/data/wavs/49e781b21dbd60348b9f6bccd6f17d06.wav  \n",
            "  inflating: /content/data/wavs/a3faa5b40eb7a939d07b8b8ed69e96a6.wav  \n",
            "  inflating: /content/data/wavs/858888b7570b88cb401d4480f5a6cac0.wav  \n",
            "  inflating: /content/data/wavs/897aad3f4dcb9377a9eeb39eaefb2afe.wav  \n",
            "  inflating: /content/data/wavs/e43dbff547df75a9155654bf9c57eff9.wav  \n",
            "  inflating: /content/data/wavs/50e479e5892da88d2429c074fad0eb79.wav  \n",
            "  inflating: /content/data/wavs/7d4294170f63b370e9b69fc08c5ef4ab.wav  \n",
            "  inflating: /content/data/wavs/967964a73699c88eb2a1a552b62854b7.wav  \n",
            "  inflating: /content/data/wavs/b72c764747bc0204bc24976be3ae1835.wav  \n",
            "  inflating: /content/data/wavs/8c70595be0c488ff902b9e9a511bf14e.wav  \n",
            "  inflating: /content/data/wavs/9203c74eaccee9a5101d7e53b5c96466.wav  \n",
            "  inflating: /content/data/wavs/8f86ee97393db1a2ac6b8a4dd432ace8.wav  \n",
            "  inflating: /content/data/wavs/99e0fb40012893cae1304dd8c4d548cd.wav  \n",
            "  inflating: /content/data/wavs/0ce30a5752d8250f5df88e3c1690e76b.wav  \n",
            "  inflating: /content/data/wavs/2cbec39d49d8d846b6e09c29ed20e6ce.wav  \n",
            "  inflating: /content/data/wavs/8b1531023039a3a0478e58e939355498.wav  \n",
            "  inflating: /content/data/wavs/1288997847cb473e5ddf29edbcbe494f.wav  \n",
            "  inflating: /content/data/wavs/4f95b684cbfe191afdf542fc3f4cbc99.wav  \n",
            "  inflating: /content/data/wavs/312128dc34303be186282fdf82fe5e5a.wav  \n",
            "  inflating: /content/data/wavs/73bbfb786a00e9e2cb03c54b30df145a.wav  \n",
            "  inflating: /content/data/wavs/f4c2854841af91ba7033087b0563ff57.wav  \n",
            "  inflating: /content/data/wavs/8b22266bf951fca99bf5aa2a9488b62a.wav  \n",
            "  inflating: /content/data/wavs/3d28d738f976f138b7ea1886c9b3b9fd.wav  \n",
            "  inflating: /content/data/wavs/934cacb7fc66e8d977c2099f2469f621.wav  \n",
            "  inflating: /content/data/wavs/0b40b1749c33b53f7569911a851472d2.wav  \n",
            "  inflating: /content/data/wavs/b4764e6c7e3b91a544df6a6d5af71027.wav  \n",
            "  inflating: /content/data/wavs/b98a18ca590f44113f435732d9334520.wav  \n",
            "  inflating: /content/data/wavs/93cfefbb3e83ecf505c8ef1bbf6d79af.wav  \n",
            "  inflating: /content/data/wavs/033e5cc92ac19de1f2c70039705baeef.wav  \n",
            "  inflating: /content/data/wavs/00cb3f722e8e4675bfbea57c846fe6db.wav  \n",
            "  inflating: /content/data/wavs/3bfbcd26eb790df60b377798a3655d66.wav  \n",
            "  inflating: /content/data/wavs/5f46e368794240b8c0f7295fc06e1633.wav  \n",
            "  inflating: /content/data/wavs/4ac764ec28c332d98bb60b1f5fb60f9b.wav  \n",
            "  inflating: /content/data/wavs/ca91669385e086e5a1684a6f9efb3d87.wav  \n",
            "  inflating: /content/data/wavs/dc400d35d5040682db3692239bec702d.wav  \n",
            "  inflating: /content/data/wavs/9d66d318608f61861481c2626cf45611.wav  \n",
            "  inflating: /content/data/wavs/3c1a08a594404aee289e3433227c8273.wav  \n",
            "  inflating: /content/data/wavs/d197d6c9c1cd7e7fdc78b2f29a6ff034.wav  \n",
            "  inflating: /content/data/wavs/2829179778c2471770ae4b9a97e53c3e.wav  \n",
            "  inflating: /content/data/wavs/65a2f2734d5454f276f8c3b7db32bb53.wav  \n",
            "  inflating: /content/data/wavs/75267615117159befffad715d13ade08.wav  \n",
            "  inflating: /content/data/wavs/cacadf47745857bd4f76bb1306ecfc97.wav  \n",
            "  inflating: /content/data/wavs/ccba86045f0a67768d37a9d72af4bbee.wav  \n",
            "  inflating: /content/data/wavs/dced761a48180a1a6b4c63feee3a6228.wav  \n",
            "  inflating: /content/data/wavs/cb0bec7ef99f2088b5d3c3adcaf73204.wav  \n",
            "  inflating: /content/data/wavs/a7380c649aa78e5cd68caf1ff29bd157.wav  \n",
            "  inflating: /content/data/wavs/ac0db9f0287596582a3e69a0887c1dd2.wav  \n",
            "  inflating: /content/data/wavs/2ac030961da00cedf919089a4ba270e9.wav  \n",
            "  inflating: /content/data/wavs/0d276df3a99be66a2b468002d20b9ef9.wav  \n",
            "  inflating: /content/data/wavs/26f363062ce77e5c1b0d90b9e3b00344.wav  \n",
            "  inflating: /content/data/wavs/e0fe328571f584a934d71c9a8c7e8888.wav  \n",
            "  inflating: /content/data/wavs/a4b2b27d313d5ba024e5e1117cb2cae2.wav  \n",
            "  inflating: /content/data/wavs/21bdc36da94a71361e69c3f41bf6d790.wav  \n",
            "  inflating: /content/data/wavs/3e07b41cd213df4d8fc78fcc1e28b1a3.wav  \n",
            "  inflating: /content/data/wavs/2619aec3212e07663a418127cc7f71e8.wav  \n",
            "  inflating: /content/data/wavs/c0b885e66f57d1541155fb654afc6951.wav  \n",
            "  inflating: /content/data/wavs/3e34be44f03921afd6f5d14bf7b5bfff.wav  \n",
            "  inflating: /content/data/wavs/5c1291fbc40e277caba6c0be2b0fc65a.wav  \n",
            "  inflating: /content/data/wavs/3bf2ce14ad135f2fb4cb7407d107a7c3.wav  \n",
            "  inflating: /content/data/wavs/b469f9b7daecf972159403ee70d44224.wav  \n",
            "  inflating: /content/data/wavs/7680e849a64af21a7be1e7a059dcc289.wav  \n",
            "  inflating: /content/data/wavs/cf28eae1b5350f5c4d64a80b752821ef.wav  \n",
            "  inflating: /content/data/wavs/c2a85c3bad1972a557bcc59e894ca113.wav  \n",
            "  inflating: /content/data/wavs/e351a7af4d38527121eed83454bc3ec9.wav  \n",
            "  inflating: /content/data/wavs/f26f27d4b345314fc40a42c3d5c669ea.wav  \n",
            "  inflating: /content/data/wavs/cdca93ad1505751a39aebefe4077460a.wav  \n",
            "  inflating: /content/data/wavs/b5f7dcaaec13c47a1ffab60bbdfb9607.wav  \n",
            "  inflating: /content/data/wavs/7b4f55f170fddca22f3a56e3d73e0dd8.wav  \n",
            "  inflating: /content/data/wavs/4f6997b51ae16d711de8009da2acfeee.wav  \n",
            "  inflating: /content/data/wavs/b5936905eb2ff75289f814c56bb6c9e4.wav  \n",
            "  inflating: /content/data/wavs/3a620a54305b1bee8350c7d08664b948.wav  \n",
            "  inflating: /content/data/wavs/4830c4d84ee6bc450af5ddfdd21b8582.wav  \n",
            "  inflating: /content/data/wavs/45bc18d9fc72a688b91164b7a83d6bbc.wav  \n",
            "  inflating: /content/data/wavs/08407a4f97d0b5f703417ab4674d00b7.wav  \n",
            "  inflating: /content/data/wavs/0a28fdfbac1ad4ccc4ff3d1ebd7fd7de.wav  \n",
            "  inflating: /content/data/wavs/85b3b7dd5041930176d62a839674ef49.wav  \n",
            "  inflating: /content/data/wavs/f386835c8a0a34b91cc49308d178572b.wav  \n",
            "  inflating: /content/data/wavs/3af8132b185b1ec091a63f3bef420009.wav  \n",
            "  inflating: /content/data/wavs/d7f4f06529f3e944baaa9b29c0723c65.wav  \n",
            "  inflating: /content/data/wavs/3a828d68711f42ca9a94eb2768112dd4.wav  \n",
            "  inflating: /content/data/wavs/0b6dfe4312c42952b9a670741921d01d.wav  \n",
            "  inflating: /content/data/wavs/7c9d1efb45d1ccdae94e1835943a6ac9.wav  \n",
            "  inflating: /content/data/wavs/b1886a9cf658322ef186a6226f964dbe.wav  \n",
            "  inflating: /content/data/wavs/70f4314809bd71cdc10bdcacbdbc70e7.wav  \n",
            "  inflating: /content/data/wavs/647ce0e308dba11ef7475302f945b5df.wav  \n",
            "  inflating: /content/data/wavs/4972ca2301fe631fc06d1524b21952c7.wav  \n",
            "  inflating: /content/data/wavs/caed3a9e58d872d725f8dff30e88c401.wav  \n",
            "  inflating: /content/data/wavs/0c0e5de3b1a8205a25d97664c8fa2a3c.wav  \n",
            "  inflating: /content/data/wavs/3216234995af51f77bc7bbde4d75614b.wav  \n",
            "  inflating: /content/data/wavs/aa424a0ba2816f5d5f8fe349b3021025.wav  \n",
            "  inflating: /content/data/wavs/3c41b2587c7c80230abb7bf1ba9ea372.wav  \n",
            "  inflating: /content/data/wavs/64386b38d4edc2ca18c79527be79da75.wav  \n",
            "  inflating: /content/data/wavs/9716003e2c995b6a2ad43b9231cfa55c.wav  \n",
            "  inflating: /content/data/wavs/e7e44c49f344b54494a6cfc26d0f222f.wav  \n",
            "  inflating: /content/data/wavs/97b2246a71ab58dcd75e591b3993051e.wav  \n",
            "  inflating: /content/data/wavs/61c8ccb9a7b8a6b0125f3497017525e8.wav  \n",
            "  inflating: /content/data/wavs/55fd8412c8a9c5b20c8730b9d5ccdfda.wav  \n",
            "  inflating: /content/data/wavs/06d3633705d80e5072797b24fff9a577.wav  \n",
            "  inflating: /content/data/wavs/6b5e8c18c5b8cc081f7601211347913e.wav  \n",
            "  inflating: /content/data/wavs/526a8f7744c790defec3b4233a2b2b98.wav  \n",
            "  inflating: /content/data/wavs/79e50a98170aaf65bb63bc00d16c6ff7.wav  \n",
            "  inflating: /content/data/wavs/9e0e719ee8f1ca3372000fb7e55a6de1.wav  \n",
            "  inflating: /content/data/wavs/c1efc8c1717b768d6389ff4293acfe1f.wav  \n",
            "  inflating: /content/data/wavs/13e2891f11805c65a984c121d3168b34.wav  \n",
            "  inflating: /content/data/wavs/051bea6adf41f05305a6764c61669ca4.wav  \n",
            "  inflating: /content/data/wavs/cb7d425fd51d27640b364cccc8444c6d.wav  \n",
            "  inflating: /content/data/wavs/cbcaa89dcce89e103cfb8734c61b36e4.wav  \n",
            "  inflating: /content/data/wavs/f81d73d3b2eb85f99bdaf9468786d59b.wav  \n",
            "  inflating: /content/data/wavs/b00f111eeb7c3d67c5bd370b48ea90ab.wav  \n",
            "  inflating: /content/data/wavs/2a69eaea8c88a36c37c9f85cc988c868.wav  \n",
            "  inflating: /content/data/wavs/95f6fd97c869cf785e9591ea93576a1b.wav  \n",
            "  inflating: /content/data/wavs/a1bd2fd4a10b93872de7114fc25333fe.wav  \n",
            "  inflating: /content/data/wavs/1cb635114a0400237e8194deed52eea8.wav  \n",
            "  inflating: /content/data/wavs/024fcc2eaa0698082d0b87fc5c6e67e3.wav  \n",
            "  inflating: /content/data/wavs/de308f82291d8957509b4167a686de27.wav  \n",
            "  inflating: /content/data/wavs/b8d61cf1f7f7fcefdfa637b897907263.wav  \n",
            "  inflating: /content/data/wavs/7f9432ac330c53e0c97260eddef1cdee.wav  \n",
            "  inflating: /content/data/wavs/26e1151d0392be2de4bf4a5fc4753dcf.wav  \n",
            "  inflating: /content/data/wavs/816cfd9ab70da98ee4ccc52c9900fe2c.wav  \n",
            "  inflating: /content/data/wavs/1fdb87c457bf1eca70ecefb96db12762.wav  \n",
            "  inflating: /content/data/wavs/7c85983b105602e3436b3b75f48b0343.wav  \n",
            "  inflating: /content/data/wavs/6ff30b8c18582c3cd5adab0577d96f04.wav  \n",
            "  inflating: /content/data/wavs/a29fa0e08bdf8811fda843b534aa8bb7.wav  \n",
            "  inflating: /content/data/wavs/7facf485637307cd392af1c309eb5329.wav  \n",
            "  inflating: /content/data/wavs/7dc111700db10e5eaff1ba4d1e2781ec.wav  \n",
            "  inflating: /content/data/wavs/6aaa409b3415270429e06627367b9820.wav  \n",
            "  inflating: /content/data/wavs/0b8c5a2a2c8081daba9604f1440dbce3.wav  \n",
            "  inflating: /content/data/wavs/acbb250acad3ca55728ec16a08dc82cf.wav  \n",
            "  inflating: /content/data/wavs/d5f91b87b16caf7eece3e9b4fed19dc9.wav  \n",
            "  inflating: /content/data/wavs/938e65e7bcbfeda7e0fd264b63ab326a.wav  \n",
            "  inflating: /content/data/wavs/4f2d15fc970c7a4a1ff0918bc12b9a0b.wav  \n",
            "  inflating: /content/data/wavs/c105961827f88673799a24c041b3de6d.wav  \n",
            "  inflating: /content/data/wavs/16b6b75634e673adf7b4981e39e97cf8.wav  \n",
            "  inflating: /content/data/wavs/c462795d81064532c9b343eb8363be75.wav  \n",
            "  inflating: /content/data/wavs/988434d5cc728351d2937a34c1c4241f.wav  \n",
            "  inflating: /content/data/wavs/4d606e3a44381619b730a1fd9cff9fef.wav  \n",
            "  inflating: /content/data/wavs/29bd1dec8c6a26c11727dcbd29b4d1ba.wav  \n",
            "  inflating: /content/data/wavs/1c617890fe62063224833f9a828b1d72.wav  \n",
            "  inflating: /content/data/wavs/18fdbaafe87c6c9b58cc9f84dac06e41.wav  \n",
            "  inflating: /content/data/wavs/808ebb0628aa284e6e3f73950134710b.wav  \n",
            "  inflating: /content/data/wavs/f913a5cc56a5c60530b3d80d7b0ee9a9.wav  \n",
            "  inflating: /content/data/wavs/c2c5a748499c88d6120b13ddaa78fb5e.wav  \n",
            "  inflating: /content/data/wavs/2564e82fcc77dd1ce89ae507050fdce2.wav  \n",
            "  inflating: /content/data/wavs/4e18e3e2b182839fb260d0cb08575604.wav  \n",
            "  inflating: /content/data/wavs/475f979ba808ec16815c5d264175b976.wav  \n",
            "  inflating: /content/data/wavs/c5d7f716d8bb4b08ff8e45d61dca8b5d.wav  \n",
            "  inflating: /content/data/wavs/e7d434da591f657689c8152504a84e8c.wav  \n",
            "  inflating: /content/data/wavs/7ea3add3199087b6e904925cbbe1ff49.wav  \n",
            "  inflating: /content/data/wavs/d3a18da65225da8af98b18496f9d167a.wav  \n",
            "  inflating: /content/data/wavs/27f5e88a4f0d5fcf9618594e3a651c53.wav  \n",
            "  inflating: /content/data/wavs/3e3131b77ebeb156ebb730e8d3a8bbed.wav  \n",
            "  inflating: /content/data/wavs/2dcedf0e6e0210afe705b835c6b89e1c.wav  \n",
            "  inflating: /content/data/wavs/ad6ffc09338ca877f637056f852a10a3.wav  \n",
            "  inflating: /content/data/wavs/43919891bdf5ad7490ce276b84389762.wav  \n",
            "  inflating: /content/data/wavs/50f0a1d690ae5232c8a3ce32cc260f63.wav  \n",
            "  inflating: /content/data/wavs/b718e591ed1ba199a8f0c6d7801ed9e8.wav  \n",
            "  inflating: /content/data/wavs/48b509572c5384f02a604b102fb8985c.wav  \n",
            "  inflating: /content/data/wavs/ce210a8e354f7c2f117ac83443533ec3.wav  \n",
            "  inflating: /content/data/wavs/91473e404815deb5245bce44111f19ce.wav  \n",
            "  inflating: /content/data/wavs/3ee24d42888eee83408fb00aa1926461.wav  \n",
            "  inflating: /content/data/wavs/edb0c0acb46b3e989579e68e7c724064.wav  \n",
            "  inflating: /content/data/wavs/e11733a11327ac4244930b7ddb6f9cd3.wav  \n",
            "  inflating: /content/data/wavs/c13810f4bbd0deb970b4e5d175ff3ced.wav  \n",
            "  inflating: /content/data/wavs/04373518f44040163f641a04999c7702.wav  \n",
            "  inflating: /content/data/wavs/48d6f90dcbe3edee98d1ccfce7b98e5e.wav  \n",
            "  inflating: /content/data/wavs/201b0ab00b4ea46e3467638f8284a2ce.wav  \n",
            "  inflating: /content/data/wavs/80c6b19061af1eef694db16bac233bf0.wav  \n",
            "  inflating: /content/data/wavs/19641e215630ebec804ea5e66fbaec8e.wav  \n",
            "  inflating: /content/data/wavs/a79f53a71a757ea913822297b9feaf83.wav  \n",
            "  inflating: /content/data/wavs/123d6aa837dba176e6bea0168e19f758.wav  \n",
            "  inflating: /content/data/wavs/51bc4068b037fa964184ae9e97acdb66.wav  \n",
            "  inflating: /content/data/wavs/c08fab215713724ed9e3cf2f220f2acc.wav  \n",
            "  inflating: /content/data/wavs/7111159d2fd413d29b7f2665f43e03aa.wav  \n",
            "  inflating: /content/data/wavs/c5a8557e9b9c1a86a9cc9d404348969b.wav  \n",
            "  inflating: /content/data/wavs/3cf127d50fc472081a9fbdc35a25d028.wav  \n",
            "  inflating: /content/data/wavs/7ec2d06c1caec2dfde503de7c135d03c.wav  \n",
            "  inflating: /content/data/wavs/d31545f70effd4d973fcfd92b0e0c80c.wav  \n",
            "  inflating: /content/data/wavs/927cc87d827fb23e831d7df2ffb161ce.wav  \n",
            "  inflating: /content/data/wavs/f7fea42705164477c36c4a285341d0d3.wav  \n",
            "  inflating: /content/data/wavs/23dc36754fa4b49ad349a12d9e148851.wav  \n",
            "  inflating: /content/data/wavs/aeb5dbe753f9bd013f7c9af5eb4c4996.wav  \n",
            "  inflating: /content/data/wavs/69b78766af854f085d8c830ed6f89eec.wav  \n",
            "  inflating: /content/data/wavs/4f58191ba40f9e0e218bedf7434a68b4.wav  \n",
            "  inflating: /content/data/wavs/feb73b911b7f3efb63d2ffe5efe21bbe.wav  \n",
            "  inflating: /content/data/wavs/3d6dd1417b996c95667419cbc6ff5f35.wav  \n",
            "  inflating: /content/data/wavs/a0542bc002b00aff2668ffd34a58dd23.wav  \n",
            "  inflating: /content/data/wavs/f8ec3d597268473e9fd4c88c3a883130.wav  \n",
            "  inflating: /content/data/wavs/df60a32974cf22409a3a306d1eb268bf.wav  \n",
            "  inflating: /content/data/wavs/55a8fdf472d5c11da6f66eaca86f1f4e.wav  \n",
            "  inflating: /content/data/wavs/11b48306b5174d838e893866676a7424.wav  \n",
            "  inflating: /content/data/wavs/5b3a751166f73ca88f341073452a5705.wav  \n",
            "  inflating: /content/data/wavs/53f5a1610877873c8368fb2c9a8311ce.wav  \n",
            "  inflating: /content/data/wavs/1447636344904d4b2a7d0a3a3e1959cc.wav  \n",
            "  inflating: /content/data/wavs/47889b19c1d4d30e841b0ed57977550f.wav  \n",
            "  inflating: /content/data/wavs/7655328e51002190769023f162dc20e0.wav  \n",
            "  inflating: /content/data/wavs/23aa41c5dcd93b361e5e1ce8a509e60b.wav  \n",
            "  inflating: /content/data/wavs/6fa0f7c52d0643113ece607873c6ce73.wav  \n",
            "  inflating: /content/data/wavs/078b96ad8c95797922d4856f960b1f85.wav  \n",
            "  inflating: /content/data/wavs/28f34358de587d700081e6cfeab4ca4f.wav  \n",
            "  inflating: /content/data/wavs/7402ff29b71523df7d85de28100e36ce.wav  \n",
            "  inflating: /content/data/wavs/ca0fde5c87ea943a384dc0001d3a7767.wav  \n",
            "  inflating: /content/data/wavs/956b31a3950c969ae502e2886a65119f.wav  \n",
            "  inflating: /content/data/wavs/ed16852f3e45d88a81bb9905b940f827.wav  \n",
            "  inflating: /content/data/wavs/8140a28f608fc175a900b244f3a1c28a.wav  \n",
            "  inflating: /content/data/wavs/2be9d8c58cdcdf1e54ee8f99054f31bd.wav  \n",
            "  inflating: /content/data/wavs/e995a1228d52706a0905ef8faab87553.wav  \n",
            "  inflating: /content/data/wavs/6fa520f61d20f45a9606fdc6dd73c5b1.wav  \n",
            "  inflating: /content/data/wavs/656966d12c3adce3300c928261fe631f.wav  \n",
            "  inflating: /content/data/wavs/63cc0cb7a91b54fbdedc3e48e1495d89.wav  \n",
            "  inflating: /content/data/wavs/3b721b664fb2e41b03fd265ab5dc6d30.wav  \n",
            "  inflating: /content/data/wavs/952baa412de01699774a477126492cb3.wav  \n",
            "  inflating: /content/data/wavs/9e2f9ebc4b678d82472afa19e4f828b2.wav  \n",
            "  inflating: /content/data/wavs/305888f9e04da9956afa10fcde44a024.wav  \n",
            "  inflating: /content/data/wavs/aaa865c584b9c898f50821c61e4b31ac.wav  \n",
            "  inflating: /content/data/wavs/12bcb40fd6372bd0aaf6a7822236dfb8.wav  \n",
            "  inflating: /content/data/wavs/a23eb67a69c3763a23fb8293e7565b77.wav  \n",
            "  inflating: /content/data/wavs/ea6f38e56fc09cb520bd34876490e587.wav  \n",
            "  inflating: /content/data/wavs/c54d37a395c2c9228f93e05dcff6ac34.wav  \n",
            "  inflating: /content/data/wavs/52b12e27436e0fbb24f80f46c79e5683.wav  \n",
            "  inflating: /content/data/wavs/54c3bbc9e662fd90fec82007aae18039.wav  \n",
            "  inflating: /content/data/wavs/cdaa9b07c2b6fe039003748374fe9fb0.wav  \n",
            "  inflating: /content/data/wavs/031879a92bea137977f573ed071794e8.wav  \n",
            "  inflating: /content/data/wavs/54eea1a9880b5c35a1b28b5f9595a884.wav  \n",
            "  inflating: /content/data/wavs/997493451392500a81979fe92f001c0c.wav  \n",
            "  inflating: /content/data/wavs/866c6fbbe62f282b83311f1475845f77.wav  \n",
            "  inflating: /content/data/wavs/b8a6d7b4727de8156fefe72680e2f46a.wav  \n",
            "  inflating: /content/data/wavs/35dcf4c24abf3b991c9195d739401d96.wav  \n",
            "  inflating: /content/data/wavs/557befecc074bfff633630cde2531cce.wav  \n",
            "  inflating: /content/data/wavs/913b2865b83af9e31deec1b20d6483f7.wav  \n",
            "  inflating: /content/data/wavs/2fed707071aefb4ae8e561f1cec9305e.wav  \n",
            "  inflating: /content/data/wavs/1eba8e464741f9f474f7f26a48290d51.wav  \n",
            "  inflating: /content/data/wavs/24eaf84cacadcd52363657e3d6d38edb.wav  \n",
            "  inflating: /content/data/wavs/68a25ca8b262255165f31206c57083d7.wav  \n",
            "  inflating: /content/data/wavs/72d0fd6fe8e7eaaecdb73cfaaea37dfa.wav  \n",
            "  inflating: /content/data/wavs/a787e317fd204b2e668292a853045f11.wav  \n",
            "  inflating: /content/data/wavs/91de4a302700265b92e0b48c60aa432c.wav  \n",
            "  inflating: /content/data/wavs/3b9bba0edb0332edb676da908b0ec75b.wav  \n",
            "  inflating: /content/data/wavs/bf9004b4aa0ea5991d911e48e3846b14.wav  \n",
            "  inflating: /content/data/wavs/b4de7bacebe567afccd01e8277352a3d.wav  \n",
            "  inflating: /content/data/wavs/1366a4cb5b85b4a69fa48efd324f6e96.wav  \n",
            "  inflating: /content/data/wavs/a5499afc8300a08e72437bf1dabf2dda.wav  \n",
            "  inflating: /content/data/wavs/4ba424cb8f95fcf36be020aeaea014e5.wav  \n",
            "  inflating: /content/data/wavs/34e293c1b3f226f8ac1d3cdded6d0651.wav  \n",
            "  inflating: /content/data/wavs/a93e9f134989c967b7bbb9da29e2126a.wav  \n",
            "  inflating: /content/data/wavs/4b3fa80926de9b42936cdad0a2867a1a.wav  \n",
            "  inflating: /content/data/wavs/14413b8efe8882368c44fbafb4ac6e7e.wav  \n",
            "  inflating: /content/data/wavs/1449ab8e01ec84dd4c22f5b9f197ac44.wav  \n",
            "  inflating: /content/data/wavs/3e611863c7f48000a593e96f8183bd0c.wav  \n",
            "  inflating: /content/data/wavs/e6a1ba0ac83300038771105bff215664.wav  \n",
            "  inflating: /content/data/wavs/22100989e5a78a26441862c381cbeca6.wav  \n",
            "  inflating: /content/data/wavs/542bd34c88ad57b9d571c13d85b829b5.wav  \n",
            "  inflating: /content/data/wavs/49777d76a1aebe8cf423f9ce48a7d7cf.wav  \n",
            "  inflating: /content/data/wavs/a87e42c21048f23a3b77642f120c9cd3.wav  \n",
            "  inflating: /content/data/wavs/d70afe8332fc1906b062f5112b942333.wav  \n",
            "  inflating: /content/data/wavs/6f9d3b03a65469c542f9275f9b1fca37.wav  \n",
            "  inflating: /content/data/wavs/e3ee56544e3e6dbcfb4bde57420df988.wav  \n",
            "  inflating: /content/data/wavs/125650a05739f154b1547abed22401a2.wav  \n",
            "  inflating: /content/data/wavs/9243d76ef00096f6f3206f34521a9b6d.wav  \n",
            "  inflating: /content/data/wavs/8a6ef0fad2f48093d2d2c2e8d8289caa.wav  \n",
            "  inflating: /content/data/wavs/0111f42c776e5df101d575dd0030d35c.wav  \n",
            "  inflating: /content/data/wavs/b0e3cf5c5adfe73850e07fb12a533647.wav  \n",
            "  inflating: /content/data/wavs/31811a672e95dcd6c6aecc4bc2e63aec.wav  \n",
            "  inflating: /content/data/wavs/13ad9bf2a5cc1b1ba29a9b66e48c2618.wav  \n",
            "  inflating: /content/data/wavs/bedf202f4e3e546dce7a8407cf58ed2d.wav  \n",
            "  inflating: /content/data/wavs/89bcc14d8938fd34689d298ec65c0028.wav  \n",
            "  inflating: /content/data/wavs/6c2628b0b9803cc3d514c1ef91239d44.wav  \n",
            "  inflating: /content/data/wavs/3252c7c190a5da2484082f13979327aa.wav  \n",
            "  inflating: /content/data/wavs/b4b46a30a9f3406f1c34f110f3eb0a73.wav  \n",
            "  inflating: /content/data/wavs/2cbcf363a0245c88df1744e480545ddf.wav  \n",
            "  inflating: /content/data/wavs/5a3bac850b58241ef7e78fdb66ac5438.wav  \n",
            "  inflating: /content/data/wavs/3dd38867967dd4d2f33209692672a97a.wav  \n",
            "  inflating: /content/data/wavs/1f3f295d58175d20d9ca9328b0089216.wav  \n",
            "  inflating: /content/data/wavs/3830a7fe8478cc6164cb92d7e30e79c5.wav  \n",
            "  inflating: /content/data/wavs/c31b9f86a67affef796b3c2670ba0322.wav  \n",
            "  inflating: /content/data/wavs/c6aea22a7c35b4309b17ecce8de44fa4.wav  \n",
            "  inflating: /content/data/wavs/cc1169e1a0a94c51ee86197cc05deede.wav  \n",
            "  inflating: /content/data/wavs/adca702fde2e94314d0c9e12b4426f5d.wav  \n",
            "  inflating: /content/data/wavs/c268c5fbd9150d88750ed00bc164db8c.wav  \n",
            "  inflating: /content/data/wavs/9db815d04004ca8f7522b94c4fb03672.wav  \n",
            "  inflating: /content/data/wavs/37e20bad3d4e8b661a6328d1b62bb1c7.wav  \n",
            "  inflating: /content/data/wavs/c82960d676a9cde2ede1f6469b8ee046.wav  \n",
            "  inflating: /content/data/wavs/78296914784fbb7e61e6dbdea31d416d.wav  \n",
            "  inflating: /content/data/wavs/f87d3b374e2a0a26840313b6b7af2749.wav  \n",
            "  inflating: /content/data/wavs/d0be1bfb18811159f42865050d65afb6.wav  \n",
            "  inflating: /content/data/wavs/a6c718b20adde38a9585e22e5c3b3d07.wav  \n",
            "  inflating: /content/data/wavs/40704970994fe31767208b48201bfb2e.wav  \n",
            "  inflating: /content/data/wavs/daedb0adbe64e1e07e33786c31461245.wav  \n",
            "  inflating: /content/data/wavs/59be89a3fd2ad7061937d769176144e0.wav  \n",
            "  inflating: /content/data/wavs/610ec3847d727d6962fac9ba1b720ece.wav  \n",
            "  inflating: /content/data/wavs/23e12e028c8b29942400d5b8f81dc5e3.wav  \n",
            "  inflating: /content/data/wavs/cac3447ee8896c728609ff5c2e7cb157.wav  \n",
            "  inflating: /content/data/wavs/8ae8e1e2a121edd0e125a265425466a7.wav  \n",
            "  inflating: /content/data/wavs/1c69e767595672b42ce6942befb4a118.wav  \n",
            "  inflating: /content/data/wavs/eb0ea7889e698306d59af9eda95891fb.wav  \n",
            "  inflating: /content/data/wavs/4abd52fff7b3584971f3c7c88779c8dc.wav  \n",
            "  inflating: /content/data/wavs/df1e8309ab9e3deb7c6316ce78e40ef5.wav  \n",
            "  inflating: /content/data/wavs/c09a8a8ae84ebc0cb7123f5bd6fcfc25.wav  \n",
            "  inflating: /content/data/wavs/94e6f2d8f2b28da8345bfb096ba661f2.wav  \n",
            "  inflating: /content/data/wavs/bf3002702f26492d48a0b86237a04c1d.wav  \n",
            "  inflating: /content/data/wavs/a181fc6bd1581da4623f1fca2ca4ef92.wav  \n",
            "  inflating: /content/data/wavs/49f228e6ae351d4799eb86d1f0c2e62c.wav  \n",
            "  inflating: /content/data/wavs/9f09420b65c4fa450eef2f9bf6ccef7e.wav  \n",
            "  inflating: /content/data/wavs/c438e08485246c351a099a49da7df1df.wav  \n",
            "  inflating: /content/data/wavs/0c2b235445cfeb2919a9fd7c8d6bd13b.wav  \n",
            "  inflating: /content/data/wavs/b31397c9e74b76b7508fce556f068d67.wav  \n",
            "  inflating: /content/data/wavs/64a063adb501160e2b637eb4a692ed1c.wav  \n",
            "  inflating: /content/data/wavs/be2f223f825bd4b15ab63151c48a4fc1.wav  \n",
            "  inflating: /content/data/wavs/e5aa9fbacc3ebbd3485f74d1dcb84677.wav  \n",
            "  inflating: /content/data/wavs/dc7807a61c3b4395ed0a9f35a67065c3.wav  \n",
            "  inflating: /content/data/wavs/cab81de7197b9855228fd95d944dd9f4.wav  \n",
            "  inflating: /content/data/wavs/39446ff76b4a7290276547a9a0d6f920.wav  \n",
            "  inflating: /content/data/wavs/7855b514363abef92dc976461f078967.wav  \n",
            "  inflating: /content/data/wavs/e642fdb9a89fde06e7941e7436b71165.wav  \n",
            "  inflating: /content/data/wavs/907f03e2e9229a6cd34c514a8bb70045.wav  \n",
            "  inflating: /content/data/wavs/6f173d92687c3ccd36bfc08fc9a04d26.wav  \n",
            "  inflating: /content/data/wavs/b47cfa0a19968176108cfe29d7e52f93.wav  \n",
            "  inflating: /content/data/wavs/99976de6d851e50b4003f48c00333f4a.wav  \n",
            "  inflating: /content/data/wavs/294a6579d717f8ebdde7ee7370278f37.wav  \n",
            "  inflating: /content/data/wavs/e7e5c5bc3b256d0006007a44fcfe6a35.wav  \n",
            "  inflating: /content/data/wavs/e6a2c8945fc767cbf0eace83dee78dc5.wav  \n",
            "  inflating: /content/data/wavs/a4301a6e73d737f582f94ed47d2fb450.wav  \n",
            "  inflating: /content/data/wavs/f3a02f97991acf5f509cb08c9fed8834.wav  \n",
            "  inflating: /content/data/wavs/02115510ef8326767d28efc8e4f73157.wav  \n",
            "  inflating: /content/data/wavs/f81d8b526b125611430191852f1c3fec.wav  \n",
            "  inflating: /content/data/wavs/67d6c975f3668dd762c01284759969e2.wav  \n",
            "  inflating: /content/data/wavs/f78523a9690bf933a1dca22f9941d073.wav  \n",
            "  inflating: /content/data/wavs/f7230280093ad209de0d130a456ed436.wav  \n",
            "  inflating: /content/data/wavs/a885863f0e12d21b5e04c1406c779e7d.wav  \n",
            "  inflating: /content/data/wavs/fc35f4fa1c72c3a9b82c23395df9dbf5.wav  \n",
            "  inflating: /content/data/wavs/0bb01a3a50c52ea4e196cfe3da2b8349.wav  \n",
            "  inflating: /content/data/wavs/775119f51d9554be0cd2755f15459788.wav  \n",
            "  inflating: /content/data/wavs/0e967f4e431eed1a036cd25061d62c48.wav  \n",
            "  inflating: /content/data/wavs/365a698a8214c47fd2e28102429e179b.wav  \n",
            "  inflating: /content/data/wavs/0314c9d3a9e2c852809b348a889adfb7.wav  \n",
            "  inflating: /content/data/wavs/afe95bbeab0687d97e41b220f186a474.wav  \n",
            "  inflating: /content/data/wavs/eff2a71b07d52e2a0fcd082edb6050f6.wav  \n",
            "  inflating: /content/data/wavs/2ad619a93476205a24830b666eceab85.wav  \n",
            "  inflating: /content/data/wavs/8ff513eb6a4b1e3d30b0ff625b88b9c4.wav  \n",
            "  inflating: /content/data/wavs/8ed13173e5164f7ea0421d43e28deb90.wav  \n",
            "  inflating: /content/data/wavs/a12924e9c019f038e5f9b358ee14a8d7.wav  \n",
            "  inflating: /content/data/wavs/db5d6b6038f636d327841b2f7647c35c.wav  \n",
            "  inflating: /content/data/wavs/470dfe86b43393348a8ba17162a7a9f3.wav  \n",
            "  inflating: /content/data/wavs/c9cfb3b5ffcd5fd2a3e4f4897166ad15.wav  \n",
            "  inflating: /content/data/wavs/8b8878107aefaa734f009b8e13afaa25.wav  \n",
            "  inflating: /content/data/wavs/821cf15fc60fb6eb22c4601d916cb585.wav  \n",
            "  inflating: /content/data/wavs/2682a6d28095274d17d42e45d9946718.wav  \n",
            "  inflating: /content/data/wavs/4ac3cd26dd0320992a548aec6f74a0b4.wav  \n",
            "  inflating: /content/data/wavs/d5b5651d932f3fc1082652d0e413e4d8.wav  \n",
            "  inflating: /content/data/wavs/64720511aa999b625178f8a1a72d623e.wav  \n",
            "  inflating: /content/data/wavs/76a0a5dd5272f317d46c1c03f5d47fc2.wav  \n",
            "  inflating: /content/data/wavs/139d7b85a26e388bf4427e281708a223.wav  \n",
            "  inflating: /content/data/wavs/5f14a3757694c2ae660d9e5c44214186.wav  \n",
            "  inflating: /content/data/wavs/df574fda1adc5c6d3df1cf10966e4129.wav  \n",
            "  inflating: /content/data/wavs/4bc8a2f9b7c125daf37e377861a44eea.wav  \n",
            "  inflating: /content/data/wavs/22bd1a8324f9eace0fda66b9770cdaf0.wav  \n",
            "  inflating: /content/data/wavs/81ca384b195b34481ec676424d509ff6.wav  \n",
            "  inflating: /content/data/wavs/a6a3359d32e9f6bd78ee2afcf8478594.wav  \n",
            "  inflating: /content/data/wavs/a13c85c0cf6028fa2869207a2ae61fa7.wav  \n",
            "  inflating: /content/data/wavs/b232e512fe9d93adec52f31a815420b8.wav  \n",
            "  inflating: /content/data/wavs/564233e21c2e41d67a7fb232057bc2f2.wav  \n",
            "  inflating: /content/data/wavs/d0b573f3691c78d2417339805d94c2ba.wav  \n",
            "  inflating: /content/data/wavs/fc3fcda02e44db09b30ef5a850f01cd8.wav  \n",
            "  inflating: /content/data/wavs/439ffbb240bfa4e1b9bf3fa0bcd4d10f.wav  \n",
            "  inflating: /content/data/wavs/b81ff82a8cd150d70af93c1b8ba50f53.wav  \n",
            "  inflating: /content/data/wavs/4195b3939805682921749c268fc9def8.wav  \n",
            "  inflating: /content/data/wavs/a97bb8b8d69fb095ea13c6d490462301.wav  \n",
            "  inflating: /content/data/wavs/d30b946d82973a2435add7d1835bec30.wav  \n",
            "  inflating: /content/data/wavs/f47d526712778a5444c680c6a716595e.wav  \n",
            "  inflating: /content/data/wavs/53acdfc03f9110499af4fbcd22d9171c.wav  \n",
            "  inflating: /content/data/wavs/f3512316ee3527d6d8be87a576e3b7b2.wav  \n",
            "  inflating: /content/data/wavs/c6f90e63810eb24cf50fa5fff98b547d.wav  \n",
            "  inflating: /content/data/wavs/01ac17e3f151a95c7f021d613b119f72.wav  \n",
            "  inflating: /content/data/wavs/0297c4c4c5ee5030d89dbfc8ef969557.wav  \n",
            "  inflating: /content/data/wavs/320fb9786b50f7428ec798fac8a8b713.wav  \n",
            "  inflating: /content/data/wavs/ef2313657691dc33737c3c00b5e653d2.wav  \n",
            "  inflating: /content/data/wavs/a25180c5889f225907883e11c7e6b43a.wav  \n",
            "  inflating: /content/data/wavs/f406e49284d8e69fced03d6994a46529.wav  \n",
            "  inflating: /content/data/wavs/ff342cb69c3b818ba61d7422c1a74794.wav  \n",
            "  inflating: /content/data/wavs/131d9ecf2a1f5a0adfac6c5f8a108518.wav  \n",
            "  inflating: /content/data/wavs/6cf2cb408104e7fa29605ead2524e5f6.wav  \n",
            "  inflating: /content/data/wavs/2c2f1f988c67660bd1b0b2d8cc1968c2.wav  \n",
            "  inflating: /content/data/wavs/07e3d77e55eef38938873095333b5e6d.wav  \n",
            "  inflating: /content/data/wavs/1feda2c6e9be80e5ceff5b9947960c8f.wav  \n",
            "  inflating: /content/data/wavs/7047ad80a4ee88f56751d91d82f6b638.wav  \n",
            "  inflating: /content/data/wavs/31f6b9ea41b65c01e9533871b6d5b726.wav  \n",
            "  inflating: /content/data/wavs/6135d094643f30e5744a516d6fde1c20.wav  \n",
            "  inflating: /content/data/wavs/19392739b142d954c3b1e5dd09f0284d.wav  \n",
            "  inflating: /content/data/wavs/0f61e0f4e758adfcb8004dab83b5fcfb.wav  \n",
            "  inflating: /content/data/wavs/4472eae16f189dcdbe0d6dd2c3bfbd3f.wav  \n",
            "  inflating: /content/data/wavs/b5cce70848034c6fd89f656839a86d91.wav  \n",
            "  inflating: /content/data/wavs/f3ccaf82717e9992ef2d0e0b3154d4fd.wav  \n",
            "  inflating: /content/data/wavs/c741fc863c8db23ceaa28eceff440575.wav  \n",
            "  inflating: /content/data/wavs/d73245aceddd642f83d888555ab6e9cf.wav  \n",
            "  inflating: /content/data/wavs/4b6d5e95ba18688453bf42242e1a8da8.wav  \n",
            "  inflating: /content/data/wavs/b620676d08443d8e2176c599392c4acf.wav  \n",
            "  inflating: /content/data/wavs/9483187a6084a5b0a3a1c8002ad9df16.wav  \n",
            "  inflating: /content/data/wavs/bd118ac8aeea6c0273680af44cf1570d.wav  \n",
            "  inflating: /content/data/wavs/5d8c63fdeeeef2f0b5ef8fa80c16a118.wav  \n",
            "  inflating: /content/data/wavs/e28e0488392e0b24058d99be81fb4df6.wav  \n",
            "  inflating: /content/data/wavs/787dba93da67129b822d6b0332df2a1e.wav  \n",
            "  inflating: /content/data/wavs/74cdf3a4f1ba0619fba8e11157b4d721.wav  \n",
            "  inflating: /content/data/wavs/1462401c952cd7c6a4c7d2bd144bb430.wav  \n",
            "  inflating: /content/data/wavs/d620cd0e48b31b2375bb7c752fb16035.wav  \n",
            "  inflating: /content/data/wavs/66e16496727a3863838cdd52a283d9de.wav  \n",
            "  inflating: /content/data/wavs/de2b8715cddbadf83f6f8f8065b3e18f.wav  \n",
            "  inflating: /content/data/wavs/029a7321ae39cb8cac7e2e1faafbb5e6.wav  \n",
            "  inflating: /content/data/wavs/2ca17d46fe0c9bdfda30dba81fbd549e.wav  \n",
            "  inflating: /content/data/wavs/13e96728f9984ea74eafb86373483537.wav  \n",
            "  inflating: /content/data/wavs/f35d124cebdea7e129e71cf1aae93272.wav  \n",
            "  inflating: /content/data/wavs/363c14aed25716aca7913e735f7695e3.wav  \n",
            "  inflating: /content/data/wavs/f43674304e38619bcc998beac6ee7ced.wav  \n",
            "  inflating: /content/data/wavs/001b157f075b441c9fc1bedf49a24c38.wav  \n",
            "  inflating: /content/data/wavs/402b2383aa1f86ce6a95ae4ebee72adc.wav  \n",
            "  inflating: /content/data/wavs/b8b3305bc6e273c528d6e85c4371d718.wav  \n",
            "  inflating: /content/data/wavs/10bca812c040c0b245a39b62439267e5.wav  \n",
            "  inflating: /content/data/wavs/aa5f123523eae28d707533fe3dc5c8bd.wav  \n",
            "  inflating: /content/data/wavs/db3a16e6b9a54b8389591418c244a2e3.wav  \n",
            "  inflating: /content/data/wavs/fd77e8792c56a1539e166f0d82b4f597.wav  \n",
            "  inflating: /content/data/wavs/c5b83da2f6f7b44b31840a9f31133b99.wav  \n",
            "  inflating: /content/data/wavs/8777638b4e5b6d124b77fecc44fc371a.wav  \n",
            "  inflating: /content/data/wavs/0e324499269fd5afeef7567f685da498.wav  \n",
            "  inflating: /content/data/wavs/e57df956dadf03c45c3b1c57e369227d.wav  \n",
            "  inflating: /content/data/wavs/a4091dfb90f03788697abfb51dc9e1c2.wav  \n",
            "  inflating: /content/data/wavs/6746c6e4c9de1d37aeee52624f887157.wav  \n",
            "  inflating: /content/data/wavs/ce99ddc3e6efeefce582c4f029e11b78.wav  \n",
            "  inflating: /content/data/wavs/1fb9e57e65014bbf386f74adcffa9c1a.wav  \n",
            "  inflating: /content/data/wavs/f102003ec8c1a48a86d5c346a7073ebf.wav  \n",
            "  inflating: /content/data/wavs/9293396da64fb1233997230af4be50ff.wav  \n",
            "  inflating: /content/data/wavs/bd353f84b79d7729e68388cfef49a82b.wav  \n",
            "  inflating: /content/data/wavs/4edb12753aa39ece219f20b47cbce2df.wav  \n",
            "  inflating: /content/data/wavs/df2c69f46350d68f0f800a8d1366f203.wav  \n",
            "  inflating: /content/data/wavs/5122ec334322ff08992fa59919362daf.wav  \n",
            "  inflating: /content/data/wavs/d091280af8cd82e82e0a2ddb8bd2f83b.wav  \n",
            "  inflating: /content/data/wavs/1193c70484c8b93a80aecfda5f46094d.wav  \n",
            "  inflating: /content/data/wavs/1205c4d1aca3e89afb268f2291f9f0b5.wav  \n",
            "  inflating: /content/data/wavs/5cbc9faf419e3f37df1e994a5f065790.wav  \n",
            "  inflating: /content/data/wavs/edc2766e89b6adf3224409a01d271d92.wav  \n",
            "  inflating: /content/data/wavs/1789f17d093c09dee17672f28533ee00.wav  \n",
            "  inflating: /content/data/wavs/09c619cb1fda072c2475671efffc03c5.wav  \n",
            "  inflating: /content/data/wavs/cbe39ca0c504857113373231c0451dbb.wav  \n",
            "  inflating: /content/data/wavs/cf42305570a2d69b9c0fcf8b2ef255cd.wav  \n",
            "  inflating: /content/data/wavs/7354d65d381c9234e27c808874482202.wav  \n",
            "  inflating: /content/data/wavs/33d0763e51f7b8c6e453e756e0949b4e.wav  \n",
            "  inflating: /content/data/wavs/f1fb4580f1e963db53f05cc853d8c49b.wav  \n",
            "  inflating: /content/data/wavs/ce951bf919fdc685e61fd45bec25d53d.wav  \n",
            "  inflating: /content/data/wavs/f081d248a63e7aee3fdc0c70fe2b29b3.wav  \n",
            "  inflating: /content/data/wavs/28ee8e7b8db57dc3070507559e622a4d.wav  \n",
            "  inflating: /content/data/wavs/a475afd1001b69e8bb8d1e1833f4377e.wav  \n",
            "  inflating: /content/data/wavs/220cf75e0a286868b8e340e64ea86aea.wav  \n",
            "  inflating: /content/data/wavs/65a5ea23c1f0706eb7c89c4813dc86b4.wav  \n",
            "  inflating: /content/data/wavs/32ab08c82f38e91c1c4b082eb035a339.wav  \n",
            "  inflating: /content/data/wavs/5d1cf00e0d373cf2eea72aecc58629d6.wav  \n",
            "  inflating: /content/data/wavs/8930bdc0e76271a12438188af88d9e74.wav  \n",
            "  inflating: /content/data/wavs/651cc986ed238d1daeb49cf90134bdcc.wav  \n",
            "  inflating: /content/data/wavs/4008bac12f73596439e3274e63b94c15.wav  \n",
            "  inflating: /content/data/wavs/fcc993b74d45af9ecbfabbdf61634d50.wav  \n",
            "  inflating: /content/data/wavs/70b3175faf7a78c407af55870c095d16.wav  \n",
            "  inflating: /content/data/wavs/471665253894f6d7edf936de5827862d.wav  \n",
            "  inflating: /content/data/wavs/a353eb6c6c0db45362e37ce2763058c2.wav  \n",
            "  inflating: /content/data/wavs/8a3816d9ab013b86588978ea0ee11c81.wav  \n",
            "  inflating: /content/data/wavs/69f4478c57810dd8c0ca7d6a73d322db.wav  \n",
            "  inflating: /content/data/wavs/03f082033b16d4af080015bdf09936f0.wav  \n",
            "  inflating: /content/data/wavs/a29adec63d6538ba7b45c4beaf3c3a3c.wav  \n",
            "  inflating: /content/data/wavs/d2fbc2b2ec93ef33ae8ff6f8904cb703.wav  \n",
            "  inflating: /content/data/wavs/4e2d097509f7c6a965456ec71452cadd.wav  \n",
            "  inflating: /content/data/wavs/06bfd8d5cec70d88329b31c1f4c03223.wav  \n",
            "  inflating: /content/data/wavs/4e44c30aa12e37828353a93b960cf060.wav  \n",
            "  inflating: /content/data/wavs/a88bc4e33fde2d225119689baf119a84.wav  \n",
            "  inflating: /content/data/wavs/10f2879770bb77239bf33de7eb10ad5d.wav  \n",
            "  inflating: /content/data/wavs/0421e3814168a39234323fd334932022.wav  \n",
            "  inflating: /content/data/wavs/111ac392fad9de090ddde893531383ec.wav  \n",
            "  inflating: /content/data/wavs/29df7fae17c4c80ffe54c53ebda0b59e.wav  \n",
            "  inflating: /content/data/wavs/2b8296d9fae04c5ca7b6679dd6aa6d8b.wav  \n",
            "  inflating: /content/data/wavs/cafb6a53a89018bd6023b2fa05228391.wav  \n",
            "  inflating: /content/data/wavs/222e98cb527d04e8fd6dbba86883f6e5.wav  \n",
            "  inflating: /content/data/wavs/6ac0bbe391efbf992340d2f173e33f91.wav  \n",
            "  inflating: /content/data/wavs/4acff955b9aa1fafdf40c97be2eb42a4.wav  \n",
            "  inflating: /content/data/wavs/1901e7c71785c7a2b87ae4e2a6db26a2.wav  \n",
            "  inflating: /content/data/wavs/b8244ca1d8a5f96eb4bdb21f6780365e.wav  \n",
            "  inflating: /content/data/wavs/09b9dd21f826261cf819645d704038b5.wav  \n",
            "  inflating: /content/data/wavs/24fb4f18ea9538f9f0e80407b389c303.wav  \n",
            "  inflating: /content/data/wavs/de1763b65656bd7ef3f28575a8e820a2.wav  \n",
            "  inflating: /content/data/wavs/13c3b72b251a8e0024bed1b897ff7ec2.wav  \n",
            "  inflating: /content/data/wavs/84ada25d5e50e4ddfee951b9e0954909.wav  \n",
            "  inflating: /content/data/wavs/66195933b1085490dc15b118f719456d.wav  \n",
            "  inflating: /content/data/wavs/abdf71c2d855249e762d049602035519.wav  \n",
            "  inflating: /content/data/wavs/d3101adc0b778a73391aadcc30415355.wav  \n",
            "  inflating: /content/data/wavs/21269ca3ab06222feff460c72c26caa3.wav  \n",
            "  inflating: /content/data/wavs/d002c9e07f03babe0de99d4a8c8ec2da.wav  \n",
            "  inflating: /content/data/wavs/dfa8cf35b3ecef5717ceac915d7f7e55.wav  \n",
            "  inflating: /content/data/wavs/eeefa2c295c8b6f8a0790e425fb317df.wav  \n",
            "  inflating: /content/data/wavs/cebf8205e6afa31771b2b7b4ad2854da.wav  \n",
            "  inflating: /content/data/wavs/a5d10a6a26403494116821afc2493dd1.wav  \n",
            "  inflating: /content/data/wavs/6f2a46c228ff50634f95658be1ffcae2.wav  \n",
            "  inflating: /content/data/wavs/dcc04320c58517ad9d1a58dc04ece21b.wav  \n",
            "  inflating: /content/data/wavs/4ab1d967697a2a652c1900f8d3d0653d.wav  \n",
            "  inflating: /content/data/wavs/a94e5f467b9f7a572501956b56aecfbb.wav  \n",
            "  inflating: /content/data/wavs/c8abce73179efc3fcf1e39274f6ecf1e.wav  \n",
            "  inflating: /content/data/wavs/30b0a4faf31f225b6dc64c301febf02b.wav  \n",
            "  inflating: /content/data/wavs/6c8747eb016a1faf335727de6e332810.wav  \n",
            "  inflating: /content/data/wavs/0dd6217252c8be1fc2c0005ae47060ef.wav  \n",
            "  inflating: /content/data/wavs/edb850b6beaa4bf37d8f5702ebca40b4.wav  \n",
            "  inflating: /content/data/wavs/7999f74fa8e44d5573748379319bbf83.wav  \n",
            "  inflating: /content/data/wavs/88cca99dabdba4391d323d26e6537717.wav  \n",
            "  inflating: /content/data/wavs/0864fdbcde9b300ed464bb34a872f92c.wav  \n",
            "  inflating: /content/data/wavs/f2b1fff1e002d967bde2d0b445a37229.wav  \n",
            "  inflating: /content/data/wavs/1555dcf3e32711880a22b2ad10f8dec1.wav  \n",
            "  inflating: /content/data/wavs/6848e3c3ba0259bd9ffe14e7d3143f7b.wav  \n",
            "  inflating: /content/data/wavs/4b2965ef0599231badab34d2eacea925.wav  \n",
            "  inflating: /content/data/wavs/920693999952d24f56ce6b78ebaea33a.wav  \n",
            "  inflating: /content/data/wavs/1bbbeece7e475d39ab0ccc44b4133ef9.wav  \n",
            "  inflating: /content/data/wavs/9497be901661e0915d4767627a0f2b5d.wav  \n",
            "  inflating: /content/data/wavs/c00f10d3e213e01385deb63c0b94280e.wav  \n",
            "  inflating: /content/data/wavs/7af3a5dc0a31e360ca54948177f6e276.wav  \n",
            "  inflating: /content/data/wavs/63224df77fbb513f13643192b80b2bd5.wav  \n",
            "  inflating: /content/data/wavs/4700fd9250b9eac9cb6dffa761241d34.wav  \n",
            "  inflating: /content/data/wavs/cdd01ba3a8f704e132f6065e7e021aa5.wav  \n",
            "  inflating: /content/data/wavs/67ef91725aab52289c64bba7ce00f443.wav  \n",
            "  inflating: /content/data/wavs/7eb25fb4feb2f90c942a724fbf6aa87a.wav  \n",
            "  inflating: /content/data/wavs/9cf0baf5780488743d10c8677ec4189f.wav  \n",
            "  inflating: /content/data/wavs/7c2fc89c150101d51c99305d679370ea.wav  \n",
            "  inflating: /content/data/wavs/3229f422235db1d90af51be01e3bcfde.wav  \n",
            "  inflating: /content/data/wavs/8c9329ce181d691d8e9092ea34775cc9.wav  \n",
            "  inflating: /content/data/wavs/631e959167e91ab584a77e2b30a7ae88.wav  \n",
            "  inflating: /content/data/wavs/1f0b5a773b353139356487ac2a50f38f.wav  \n",
            "  inflating: /content/data/wavs/d9012c3a3facebdbcac3046cb13977ca.wav  \n",
            "  inflating: /content/data/wavs/4947daab878cdcf000ebda8d7bc61af2.wav  \n",
            "  inflating: /content/data/wavs/f8bbc73f3ec5e3d9ae365a91f8171b15.wav  \n",
            "  inflating: /content/data/wavs/ce750145225725f5e4d4de99d40b3aad.wav  \n",
            "  inflating: /content/data/wavs/13d7b85e530149805922e1037690dcb8.wav  \n",
            "  inflating: /content/data/wavs/75672345e24b0617e6c6dd11fca2e023.wav  \n",
            "  inflating: /content/data/wavs/34e29dd3452424d3cf95fecb1ab32ffe.wav  \n",
            "  inflating: /content/data/wavs/0f08d2536bf4dce027d9f5252f2f14b6.wav  \n",
            "  inflating: /content/data/wavs/a6ac86c13c37a1cabddb569ea1989161.wav  \n",
            "  inflating: /content/data/wavs/2e6b415e466207859f43824fafd8b8f1.wav  \n",
            "  inflating: /content/data/wavs/01221e555f6b0b516d17c26c99c65bfd.wav  \n",
            "  inflating: /content/data/wavs/4f31a94116ef38437d5e67378a4f85ed.wav  \n",
            "  inflating: /content/data/wavs/24fe91d2afadbe1810ab6d9b8057ce58.wav  \n",
            "  inflating: /content/data/wavs/8641dabfa1ceec5ce1e0cec9fec92efe.wav  \n",
            "  inflating: /content/data/wavs/b8c0f453046ba3b2e45c4eb725369067.wav  \n",
            "  inflating: /content/data/wavs/ed0b6502e1649c72dc655e21a16e60b9.wav  \n",
            "  inflating: /content/data/wavs/5ee97e86460096dbfcb53ed62b0003a1.wav  \n",
            "  inflating: /content/data/wavs/31b46c0dba0f41b218fa709567cfac92.wav  \n",
            "  inflating: /content/data/wavs/578b5d61b915fbeb7110357078b9125e.wav  \n",
            "  inflating: /content/data/wavs/04d83c18cd16b0229e18d2dc75e41736.wav  \n",
            "  inflating: /content/data/wavs/a58d48caf7a21b2cfe10dd035ed814c6.wav  \n",
            "  inflating: /content/data/wavs/059ce8ad61f1fcfc222d14bde4185e24.wav  \n",
            "  inflating: /content/data/wavs/f4a992e70b3e91107a0f6341d1da4712.wav  \n",
            "  inflating: /content/data/wavs/b0540354e95b3aab4fa0715c6303bf6a.wav  \n",
            "  inflating: /content/data/wavs/e9bb26c1326c170964a0613c1a6542b2.wav  \n",
            "  inflating: /content/data/wavs/8f766eda536a49f2ab5e4cb6d928e5ac.wav  \n",
            "  inflating: /content/data/wavs/8c060d73e810d5c3b375084f2a28eb7c.wav  \n",
            "  inflating: /content/data/wavs/e50d89fecb6215490b601c270e8aac90.wav  \n",
            "  inflating: /content/data/wavs/159039fe201fdc9778c5b828fd787e10.wav  \n",
            "  inflating: /content/data/wavs/d3cfe967001d78c56344e8649da23dce.wav  \n",
            "  inflating: /content/data/wavs/8f827a0ea80e4878c2f2e03b5a2a183f.wav  \n",
            "  inflating: /content/data/wavs/45c57f6c1183f2ed71137ef2d8bcca4f.wav  \n",
            "  inflating: /content/data/wavs/5b6941a25a1da068e74ecb41c5944e79.wav  \n",
            "  inflating: /content/data/wavs/515de11d49a6f5293df2f8407bfc90bd.wav  \n",
            "  inflating: /content/data/wavs/cabab7b8b3d325793f1e7e6fc48b4eb3.wav  \n",
            "  inflating: /content/data/wavs/1a5345cd96690dc5cc1f23b89ee84ec9.wav  \n",
            "  inflating: /content/data/wavs/9a7eff0122689712cab37638503f2c3a.wav  \n",
            "  inflating: /content/data/wavs/576d7a265f91463d786b55bcd11ec1d0.wav  \n",
            "  inflating: /content/data/wavs/3c47d5e2ab9aacd26fddd940ac6e5405.wav  \n",
            "  inflating: /content/data/wavs/bb92a1c37bceaefd3e1b456bb7462980.wav  \n",
            "  inflating: /content/data/wavs/ee4034e7a136ddacebbc0658447753fa.wav  \n",
            "  inflating: /content/data/wavs/4e4146a05b6aeeb6e3aa3b2bd99cc1c6.wav  \n",
            "  inflating: /content/data/wavs/61ff056dca9b39cd04f336ba75574ce6.wav  \n",
            "  inflating: /content/data/wavs/1735fcaf50891a0d274d8eb8e4f8d640.wav  \n",
            "  inflating: /content/data/wavs/f14f939ba9f2b2aadeba3e03b0e984d5.wav  \n",
            "  inflating: /content/data/wavs/e33fb509e64de399ecd680a8c380cf20.wav  \n",
            "  inflating: /content/data/wavs/c6bbd54c733ef569e48a5fc9089b650f.wav  \n",
            "  inflating: /content/data/wavs/d7e62f48890a5fb42e355820d1e7f83b.wav  \n",
            "  inflating: /content/data/wavs/7e7243878721e9af341cb7dcd60a3edc.wav  \n",
            "  inflating: /content/data/wavs/426677174dc74626956a72fb45f4b547.wav  \n",
            "  inflating: /content/data/wavs/161c51c17d78bc034a86d2b797ca578b.wav  \n",
            "  inflating: /content/data/wavs/bd8b059385834472ec8a2ebedd646538.wav  \n",
            "  inflating: /content/data/wavs/ee9ac5989f18ce07ec1e151575138860.wav  \n",
            "  inflating: /content/data/wavs/d651b1f1055696df3ebd6d82dd8217ac.wav  \n",
            "  inflating: /content/data/wavs/2dd2a7e284611190ccd5209ab0439970.wav  \n",
            "  inflating: /content/data/wavs/0d3cf539061e309eacb25ee48f1ecb9f.wav  \n",
            "  inflating: /content/data/wavs/84ea43f3d27350938d835921038e2c7f.wav  \n",
            "  inflating: /content/data/wavs/49a5bad25c4642ae75d44866cfa8e7ff.wav  \n",
            "  inflating: /content/data/wavs/54ead2ca7d3e94775ba9a58f93cd89e3.wav  \n",
            "  inflating: /content/data/wavs/fef899386870c7583f79b7b41ec06aa9.wav  \n",
            "  inflating: /content/data/wavs/c3859056be4482f2d86e4cc1d86c85ee.wav  \n",
            "  inflating: /content/data/wavs/c457f2d6d2f1fcac3e5eeb4c0d8207fc.wav  \n",
            "  inflating: /content/data/wavs/7235d6d3790e8d0232f4225460b03e5b.wav  \n",
            "  inflating: /content/data/wavs/05fdeca23e88d0b2020fea8e2d8e3ad2.wav  \n",
            "  inflating: /content/data/wavs/e7638c0bfb09fb16e76dfbeb2e03770b.wav  \n",
            "  inflating: /content/data/wavs/fa7fd7965b2029fd40a32528569326a8.wav  \n",
            "  inflating: /content/data/wavs/5e784f8638d0c901609cd9df98995e87.wav  \n",
            "  inflating: /content/data/wavs/6bf439c8bf84175c3909360c27156850.wav  \n",
            "  inflating: /content/data/wavs/4aebc71096c17643177fca234a718433.wav  \n",
            "  inflating: /content/data/wavs/a36d0ef49b7c6b27a8a0ad8a43fd84d2.wav  \n",
            "  inflating: /content/data/wavs/fa1b478796d6b2bb07fb846b9c1895a7.wav  \n",
            "  inflating: /content/data/wavs/2a6e280dc78fec0ba34771185b842804.wav  \n",
            "  inflating: /content/data/wavs/aad9df28e4306d294544f16104d6dbd4.wav  \n",
            "  inflating: /content/data/wavs/a0b7ae763826a8e5c9e2371ae05cd0a3.wav  \n",
            "  inflating: /content/data/wavs/15cf9c771477a997da329951d7aeb42d.wav  \n",
            "  inflating: /content/data/wavs/b61a8e3e240001f90865aba974727b0b.wav  \n",
            "  inflating: /content/data/wavs/bad0c3236a1b1a049b4c466fdb117e9c.wav  \n",
            "  inflating: /content/data/wavs/605c5b25cbff074b2d428ef2ad24e683.wav  \n",
            "  inflating: /content/data/wavs/fed3237db931741d29cb420917b932f4.wav  \n",
            "  inflating: /content/data/wavs/e2b759a82606bee6aa83358ecfffbc05.wav  \n",
            "  inflating: /content/data/wavs/b31c6ab01e12f69b81c3570742295475.wav  \n",
            "  inflating: /content/data/wavs/1c1ad952d4102375f55bb5fc20184a55.wav  \n",
            "  inflating: /content/data/wavs/2ecd58e4b23fc81f8cfcb8c72a63da0a.wav  \n",
            "  inflating: /content/data/wavs/a2a01869bc6022d603dc6caca080e1ee.wav  \n",
            "  inflating: /content/data/wavs/8afbca92bf73fa2fed4cf7609e8baf84.wav  \n",
            "  inflating: /content/data/wavs/0e2397611a09f02ba373ed96a9b68758.wav  \n",
            "  inflating: /content/data/wavs/e51c217dee86c28e913a181de91dd048.wav  \n",
            "  inflating: /content/data/wavs/59aa08cb9afd758db5a6af6104b716f6.wav  \n",
            "  inflating: /content/data/wavs/e9967d65abf70f40bd88fa4e143560c2.wav  \n",
            "  inflating: /content/data/wavs/a63e48d674540fd967245ab771ac13f2.wav  \n",
            "  inflating: /content/data/wavs/c0a32d4a034d22beb0e637d72f9044e8.wav  \n",
            "  inflating: /content/data/wavs/22314fd204f36a85defd02985207515f.wav  \n",
            "  inflating: /content/data/wavs/0a5aa72a5c7da802a9107c45cf90dfd3.wav  \n",
            "  inflating: /content/data/wavs/a05c0d4e44738ec3afa7cfa98873f366.wav  \n",
            "  inflating: /content/data/wavs/ae876e778f5939993118b16fdf51c286.wav  \n",
            "  inflating: /content/data/wavs/1df7a80ce845db3b3b99d1464540c48f.wav  \n",
            "  inflating: /content/data/wavs/e17c84da9ec24c6c4f0f36f083edef5d.wav  \n",
            "  inflating: /content/data/wavs/aff28f81f31afdc16353e3ca433d1b00.wav  \n",
            "  inflating: /content/data/wavs/c1cafe068112841298838afce30b96a1.wav  \n",
            "  inflating: /content/data/wavs/6be8b9140297bae2b2c90adba10e84d0.wav  \n",
            "  inflating: /content/data/wavs/2ef1ac0afc0827fcde3bc4aa49df054b.wav  \n",
            "  inflating: /content/data/wavs/ac04e65aa05971b33b84de716c080b33.wav  \n",
            "  inflating: /content/data/wavs/9e6439d8c97d14d1d749bfa8ccab1f35.wav  \n",
            "  inflating: /content/data/wavs/7bfa24507ca4cb994c2810c12047565d.wav  \n",
            "  inflating: /content/data/wavs/b787ff2ba7f711a99dc8ea7c9a283002.wav  \n",
            "  inflating: /content/data/wavs/47cc513bf49fefdfa6fa07601b22d4c3.wav  \n",
            "  inflating: /content/data/wavs/a432cedea68a71e087bccacb96b88428.wav  \n",
            "  inflating: /content/data/wavs/d082a48506dd02a1f8a7123c686d8135.wav  \n",
            "  inflating: /content/data/wavs/20256b4c2262a9b31413b7b374f58f7a.wav  \n",
            "  inflating: /content/data/wavs/b3a1e761d56aab0dcd0e98de919fc542.wav  \n",
            "  inflating: /content/data/wavs/b2b6f2a6132698568d4dd4eb3c501dd5.wav  \n",
            "  inflating: /content/data/wavs/83ed07fc9e28051744ec9af00f4fc616.wav  \n",
            "  inflating: /content/data/wavs/af142312ca6d10755d1a5c564b0da23e.wav  \n",
            "  inflating: /content/data/wavs/a72efffea23281e3ae5793ebc66aa473.wav  \n",
            "  inflating: /content/data/wavs/39f120c66038425172ee295f23ca755c.wav  \n",
            "  inflating: /content/data/wavs/19c99deb1dcbf5e2f9dc7e456238c576.wav  \n",
            "  inflating: /content/data/wavs/f30ff0896d5d3d2a8cbe707dde9c8cfe.wav  \n",
            "  inflating: /content/data/wavs/a6f536e25092e6ab709c2e9f87ed6235.wav  \n",
            "  inflating: /content/data/wavs/f20c09712c21cf10006948949facf752.wav  \n",
            "  inflating: /content/data/wavs/e8f2fc863779cd1423afac939623db34.wav  \n",
            "  inflating: /content/data/wavs/88179ae59c6476d5b4fff01edf1457ca.wav  \n",
            "  inflating: /content/data/wavs/a7c5fd0d8bcd56666ed07163ff1b3245.wav  \n",
            "  inflating: /content/data/wavs/0befeee8dd6176d08595205556ff47bc.wav  \n",
            "  inflating: /content/data/wavs/dec1ffa646a067ed92f7fa7476896ee0.wav  \n",
            "  inflating: /content/data/wavs/22acb1e742699afa507e80ed59cf31d2.wav  \n",
            "  inflating: /content/data/wavs/3b0e8ed0f32a3eeba2e0b3e1340bef21.wav  \n",
            "  inflating: /content/data/wavs/b3bb6d5b6ea4fc5df4ea47beb60d5beb.wav  \n",
            "  inflating: /content/data/wavs/702751875745fde1b02974be15a0a64b.wav  \n",
            "  inflating: /content/data/wavs/0e824e11046c1bdb7412d0db87ac43bf.wav  \n",
            "  inflating: /content/data/wavs/9af25f3271201ec72207ff12ac2d485c.wav  \n",
            "  inflating: /content/data/wavs/01efe3a76632f3b0bbd84d4689a3f051.wav  \n",
            "  inflating: /content/data/wavs/f63df205d2b1de23bc879c35a5db3429.wav  \n",
            "  inflating: /content/data/wavs/5ae1e8acc323539bfcdccb931938b23e.wav  \n",
            "  inflating: /content/data/wavs/d91e2387607e02717c6e6cb83146839c.wav  \n",
            "  inflating: /content/data/wavs/353bd78a8fb4325338bcc8e0b16c9175.wav  \n",
            "  inflating: /content/data/wavs/84463c28e36154ac8661cd122420284b.wav  \n",
            "  inflating: /content/data/wavs/67b9fb4ffd1775fa8d64eb30544f6f52.wav  \n",
            "  inflating: /content/data/wavs/95d80aa0d6b3cc2b17b8ff00e971b4f5.wav  \n",
            "  inflating: /content/data/wavs/b61285cafa830b7234044ef624d6c6d3.wav  \n",
            "  inflating: /content/data/wavs/09d0c935d658b4d81b2edfa1fd7bf810.wav  \n",
            "  inflating: /content/data/wavs/f6adc9153e7064094bba64df37670f20.wav  \n",
            "  inflating: /content/data/wavs/36514d889d484081596a7d241b7dd41b.wav  \n",
            "  inflating: /content/data/wavs/c08b2051bc176c00cac84d94017f1247.wav  \n",
            "  inflating: /content/data/wavs/9d96a945617eb63a2c00ecd19167234c.wav  \n",
            "  inflating: /content/data/wavs/a6a08973bb1e1c1312e4f96c24f77c0b.wav  \n",
            "  inflating: /content/data/wavs/c7be2d4f34e8602188a6b3b18cbaec00.wav  \n",
            "  inflating: /content/data/wavs/13b0359dd54a00c5fed84b75b05b1db0.wav  \n",
            "  inflating: /content/data/wavs/31c0855f7b2b3d1b4c9bad9a0849888e.wav  \n",
            "  inflating: /content/data/wavs/b741465e484503844b4d2cce95211c9c.wav  \n",
            "  inflating: /content/data/wavs/3bb776e7edf68802d00ca9115826c771.wav  \n",
            "  inflating: /content/data/wavs/da25488ddad1d5184cad5633755474c9.wav  \n",
            "  inflating: /content/data/wavs/6b5d61a51bab3d58cea584ac1d82dbbd.wav  \n",
            "  inflating: /content/data/wavs/cc4f8b3d226484ee8aa2f2b6985fff4e.wav  \n",
            "  inflating: /content/data/wavs/748609ffacc948c9ccc28e562b09924a.wav  \n",
            "  inflating: /content/data/wavs/9ce74b3174335ceaa247068cb11719ce.wav  \n",
            "  inflating: /content/data/wavs/9a0130cc2c26413502fcbdd0e255ff8c.wav  \n",
            "  inflating: /content/data/wavs/b3ec1ea99ba80c1465d0faa77186063d.wav  \n",
            "  inflating: /content/data/wavs/63d546353d3491af492e9c3bccb67cc9.wav  \n",
            "  inflating: /content/data/wavs/f5e6bf03d27f5b71e39af6808851ebe2.wav  \n",
            "  inflating: /content/data/wavs/dd7f5f77bd33ddc46efa3c3be4d4ea1b.wav  \n",
            "  inflating: /content/data/wavs/c2447df84919367e84986847bae19bab.wav  \n",
            "  inflating: /content/data/wavs/a1311b29b5019938d680137ffc36937f.wav  \n",
            "  inflating: /content/data/wavs/4d5de25efc1db3b1d06a3f506c066706.wav  \n",
            "  inflating: /content/data/wavs/7544273c9459e15ff9b01ba19f22f134.wav  \n",
            "  inflating: /content/data/wavs/58a0c2e69a543a6868c22ff0f6469eb7.wav  \n",
            "  inflating: /content/data/wavs/96bc65cbb1f200ffd7147cf2f5566293.wav  \n",
            "  inflating: /content/data/wavs/ee851960e675b778e79fca04823162b4.wav  \n",
            "  inflating: /content/data/wavs/c7324ce55fb79d4627fa890e4f375296.wav  \n",
            "  inflating: /content/data/wavs/eb83056ddd4ed24b59421bf720532900.wav  \n",
            "  inflating: /content/data/wavs/45ffd8964c89cd9f951c440a229843e6.wav  \n",
            "  inflating: /content/data/wavs/9d1cff46f7eb7ff2af1dd581e4490fee.wav  \n",
            "  inflating: /content/data/wavs/2924f64c83331e7433d4892df3df13a0.wav  \n",
            "  inflating: /content/data/wavs/f82baad15e01b8c040ff6ccc0e90b3a2.wav  \n",
            "  inflating: /content/data/wavs/6630fec936b751798e6532338ed6f9cb.wav  \n",
            "  inflating: /content/data/wavs/36f084e5181ebe09ff27651bd300c2a4.wav  \n",
            "  inflating: /content/data/wavs/49cef60c6f6fabacc824e63b0f5d6e2b.wav  \n",
            "  inflating: /content/data/wavs/d9b813f54bd03475de15152fc68dbe5c.wav  \n",
            "  inflating: /content/data/wavs/2db1aefef4cdb0c2d2e7a4ed624a7989.wav  \n",
            "  inflating: /content/data/wavs/3b82948fcdf268bc5bf9db776a81e072.wav  \n",
            "  inflating: /content/data/wavs/b86236c841bcedb3d2c73d18465c41fd.wav  \n",
            "  inflating: /content/data/wavs/ecd49c54ea21367c168e32cce5275451.wav  \n",
            "  inflating: /content/data/wavs/905d7716ea96057535458494534bf47c.wav  \n",
            "  inflating: /content/data/wavs/0a0665f9f8ed2d8473e7edb96e5f331e.wav  \n",
            "  inflating: /content/data/wavs/8e628b40e8805b6f0f9eba6da0f261b9.wav  \n",
            "  inflating: /content/data/wavs/e9a3ddd4ed305b00ef824b1c90364e53.wav  \n",
            "  inflating: /content/data/wavs/f4a09485bab13a6acc14b6aca4427922.wav  \n",
            "  inflating: /content/data/wavs/d9e4872086d3b4d2eeb7a5c238babe37.wav  \n",
            "  inflating: /content/data/wavs/a010d0611220d991f946b794e42a146b.wav  \n",
            "  inflating: /content/data/wavs/ee9bc674b03570346cd0e996661f9fea.wav  \n",
            "  inflating: /content/data/wavs/a5d297ae45cabf33cae77eeb3f91b5d2.wav  \n",
            "  inflating: /content/data/wavs/4836b476a2b58a8fe6088da78dbc245a.wav  \n",
            "  inflating: /content/data/wavs/43a8eae75473f6966c057074fe75d0fc.wav  \n",
            "  inflating: /content/data/wavs/02a581b3e27f406069ad6c42b708c715.wav  \n",
            "  inflating: /content/data/wavs/37cc6f2ce40919398311e84d724d4bda.wav  \n",
            "  inflating: /content/data/wavs/4ab00eb18e19eaedeb8d876ef0a1c282.wav  \n",
            "  inflating: /content/data/wavs/a96aadc4a4f0aafc483114c7d051487a.wav  \n",
            "  inflating: /content/data/wavs/23a0f44fdfd90c62e1af1b26b33b20a9.wav  \n",
            "  inflating: /content/data/wavs/a212bf7dbede9d709a4792d18c23292d.wav  \n",
            "  inflating: /content/data/wavs/37ecdc0729dda84aa524ab45bbfc5e92.wav  \n",
            "  inflating: /content/data/wavs/785ea05c7bf8fae2801b8baf4e32b0ff.wav  \n",
            "  inflating: /content/data/wavs/0a52080536a23a6c52d3b0dcbf56852e.wav  \n",
            "  inflating: /content/data/wavs/d2d04a9ba977473d5062cb2de9bfe8c2.wav  \n",
            "  inflating: /content/data/wavs/240184fb1ef7f9fc296dd616369e70e6.wav  \n",
            "  inflating: /content/data/wavs/f59432c5917c515329d1b11b3bf942ab.wav  \n",
            "  inflating: /content/data/wavs/5cffd55a432e7a0696266de219ecaa54.wav  \n",
            "  inflating: /content/data/wavs/0d7dc715e8526c91c64674bc5eea8be4.wav  \n",
            "  inflating: /content/data/wavs/50f906a6c1e1efb6772929e4e7a60c40.wav  \n",
            "  inflating: /content/data/wavs/3ff7a4052f4686c8d625a8b303abd02c.wav  \n",
            "  inflating: /content/data/wavs/3b6f8d72df5418aae600e9f365015ba5.wav  \n",
            "  inflating: /content/data/wavs/e9ccfe88691baefc82760036bdcac627.wav  \n",
            "  inflating: /content/data/wavs/0f00cf51cb87353bdcc0afc78478795c.wav  \n",
            "  inflating: /content/data/wavs/b1c95936bdd095a4468f0e879309f3a7.wav  \n",
            "  inflating: /content/data/wavs/f2b2743084ad87e5e21c4833ccda46be.wav  \n",
            "  inflating: /content/data/wavs/01ad93164de1d01319797a5da40fec65.wav  \n",
            "  inflating: /content/data/wavs/7e28494e0c87a65e1a115b6596d0afb7.wav  \n",
            "  inflating: /content/data/wavs/64283efbc32b1f3e883f386b1e774e88.wav  \n",
            "  inflating: /content/data/wavs/71eadf1d82f5da147e2e5acbe4dec4ae.wav  \n",
            "  inflating: /content/data/wavs/84692b5d57c4de873eba45f442ff289b.wav  \n",
            "  inflating: /content/data/wavs/2f80b0bb6868cebcca600b03efa28c98.wav  \n",
            "  inflating: /content/data/wavs/6c0581ab17ffd6b86b10669e2b038b6e.wav  \n",
            "  inflating: /content/data/wavs/31b3e4c115cd02ec8f064154642ce41d.wav  \n",
            "  inflating: /content/data/wavs/2ebaeee680933f87eb7f640c9c4311ba.wav  \n",
            "  inflating: /content/data/wavs/7359531b3e29a240ea7c22592333e85c.wav  \n",
            "  inflating: /content/data/wavs/54a679121c5af014c3286824c6833156.wav  \n",
            "  inflating: /content/data/wavs/4e40f8581a3799cea0f38db660119215.wav  \n",
            "  inflating: /content/data/wavs/6efd4b73ab4d22f029024a9fee1f3fab.wav  \n",
            "  inflating: /content/data/wavs/8ee734f228c0ec6d9446b589c3112fba.wav  \n",
            "  inflating: /content/data/wavs/4c5d8848ee944ccbee2edf7cb98d89c2.wav  \n",
            "  inflating: /content/data/wavs/ad32c44f414e5dcc4f23f2dcb6c03fc0.wav  \n",
            "  inflating: /content/data/wavs/cd2ae4075d2739246bff7f8c4923db6b.wav  \n",
            "  inflating: /content/data/wavs/f4a489883054a2353bc6c2bda2a3d265.wav  \n",
            "  inflating: /content/data/wavs/c25bde39ada1e966f4599bcefc019f02.wav  \n",
            "  inflating: /content/data/wavs/7241f672f6b2ec557e25693dded64e58.wav  \n",
            "  inflating: /content/data/wavs/b1718195ca131a5e1d03a70d3571ece9.wav  \n",
            "  inflating: /content/data/wavs/c042c168c20f92afae6ab989a2443775.wav  \n",
            "  inflating: /content/data/wavs/4be1be096880cb3490a0d82bd5a2956a.wav  \n",
            "  inflating: /content/data/wavs/bb9b2a691a7ecbc1c45e4662d1be091f.wav  \n",
            "  inflating: /content/data/wavs/63633a720c38cd3406ac8360297eb83a.wav  \n",
            "  inflating: /content/data/wavs/d3a3cd0912a600f2217139d38958e2bc.wav  \n",
            "  inflating: /content/data/wavs/3da3a4582e08845c04e547bcd1f5471e.wav  \n",
            "  inflating: /content/data/wavs/908a3091d089c273e515d561913a57d5.wav  \n",
            "  inflating: /content/data/wavs/a0b6781e84e7031bf1038fd8c4a267cd.wav  \n",
            "  inflating: /content/data/wavs/2c85bdb6e9fced1de475205edf3c794c.wav  \n",
            "  inflating: /content/data/wavs/d380628cabf4e9f536277f174cc34163.wav  \n",
            "  inflating: /content/data/wavs/236e9ffca9d95a4409233f078cbc16b8.wav  \n",
            "  inflating: /content/data/wavs/8704af9b8cf92c3c1fce29a99f7564ee.wav  \n",
            "  inflating: /content/data/wavs/59087a44d0d5ce96baa5b4c512dc3ea8.wav  \n",
            "  inflating: /content/data/wavs/c5f4f41e9c1dd910e6c080d0790e2dd6.wav  \n",
            "  inflating: /content/data/wavs/7a9a16c1fc3fcfd636690f8d8ac4947a.wav  \n",
            "  inflating: /content/data/wavs/007be24014ba787ea10f539d80e6551d.wav  \n",
            "  inflating: /content/data/wavs/a08f026ecd375f2b282dc6d7bf4a8561.wav  \n",
            "  inflating: /content/data/wavs/ff26ba7f402f102418687a2415b39ebc.wav  \n",
            "  inflating: /content/data/wavs/5cdc1c05b4149a6281919bc9e6ed62fe.wav  \n",
            "  inflating: /content/data/wavs/d65a7442239c1952ecd98d339df630d5.wav  \n",
            "  inflating: /content/data/wavs/6f023b922e20d9966fa7d9f6db957761.wav  \n",
            "  inflating: /content/data/wavs/0e0045387f6d0884f1187e7111c07882.wav  \n",
            "  inflating: /content/data/wavs/8a8c5aa0d2c6fb54f5e3cf89519a2896.wav  \n",
            "  inflating: /content/data/wavs/61e5da79e29f9fcc47791450760be020.wav  \n",
            "  inflating: /content/data/wavs/c53e04bc9b54ab4ff91e0697d7373f65.wav  \n",
            "  inflating: /content/data/wavs/411305c65fa7a387dadd891af708e1ef.wav  \n",
            "  inflating: /content/data/wavs/88aa8cb8353372add5b58fd8fac340b4.wav  \n",
            "  inflating: /content/data/wavs/cc2aba4f607fb84a5192ca22a97fd78f.wav  \n",
            "  inflating: /content/data/wavs/1369bab6a96661039524269b38127b68.wav  \n",
            "  inflating: /content/data/wavs/d0440ff7119306eaeaed7fdd4c6ef9fb.wav  \n",
            "  inflating: /content/data/wavs/fcf5f051e54f2ea0eecbcb2f8a5f1f34.wav  \n",
            "  inflating: /content/data/wavs/b30f21aaacafa8a2109cbb5809f7b422.wav  \n",
            "  inflating: /content/data/wavs/634f527b5d79b64def12e47220742637.wav  \n",
            "  inflating: /content/data/wavs/7d4882d4b8cef9043a1e2fc3ceba42ea.wav  \n",
            "  inflating: /content/data/wavs/07c9d9d34245d1e56cd6bed38e64f26a.wav  \n",
            "  inflating: /content/data/wavs/1a43ff8d03f82fa4d45169662e94b536.wav  \n",
            "  inflating: /content/data/wavs/7a6fa8b76b0efbe826933dfaf934a658.wav  \n",
            "  inflating: /content/data/wavs/d91889c0f3e2a4e66da173f6ac83a94f.wav  \n",
            "  inflating: /content/data/wavs/79d0dd7dd61cca08585d0ec0f11a5c96.wav  \n",
            "  inflating: /content/data/wavs/a10f9add4c635033f00f9dedac7cba04.wav  \n",
            "  inflating: /content/data/wavs/ca777cd9077cb52a1df5c82f0ed687f5.wav  \n",
            "  inflating: /content/data/wavs/93f62939d627bb9a35d98bff2960f9e7.wav  \n",
            "  inflating: /content/data/wavs/9a9c4303a047141631f1cf7f97c9b409.wav  \n",
            "  inflating: /content/data/wavs/aa0c1f7b170227b0562032b8691f756e.wav  \n",
            "  inflating: /content/data/wavs/605f2b9b4f818f55c5cda65c2066dcb2.wav  \n",
            "  inflating: /content/data/wavs/017997983f8a84f46920ab956a231db5.wav  \n",
            "  inflating: /content/data/wavs/d2e7359a2275d9d6969f0c5adccb0e0d.wav  \n",
            "  inflating: /content/data/wavs/581e8c2efe3e146fcc51493da63486ea.wav  \n",
            "  inflating: /content/data/wavs/789ec18f8dfd8d73b0cffd55603fc9be.wav  \n",
            "  inflating: /content/data/wavs/4e9d117c7b39de312b02a5f6c60a12ad.wav  \n",
            "  inflating: /content/data/wavs/6329a2e6720f202f0f007bbe798948e0.wav  \n",
            "  inflating: /content/data/wavs/80ed6128dc999d773187cc5285ce1581.wav  \n",
            "  inflating: /content/data/wavs/cd9dcf4cbf1cc67fdb1ac9b496d1becc.wav  \n",
            "  inflating: /content/data/wavs/2674c603635342ca0ed0ad8558912d0a.wav  \n",
            "  inflating: /content/data/wavs/67f5a4bd825a3ff348ceb00f8a3cff29.wav  \n",
            "  inflating: /content/data/wavs/9f5c2cf8dc83118029fc2b17c81c4da4.wav  \n",
            "  inflating: /content/data/wavs/295830fc523c87b8c8b97224c45cac30.wav  \n",
            "  inflating: /content/data/wavs/5b7c946d75f72b3705bee8ae7ce09c6c.wav  \n",
            "  inflating: /content/data/wavs/8c6caf7f60e6152b751247c2c348848a.wav  \n",
            "  inflating: /content/data/wavs/32cae5d8846458f7c5068ea1ba1c2bb8.wav  \n",
            "  inflating: /content/data/wavs/c4f88705cea13d36244e3b6d0ac83308.wav  \n",
            "  inflating: /content/data/wavs/fc32b663d121d14287841a15bf882917.wav  \n",
            "  inflating: /content/data/wavs/0ec43bd6b1963b475778c980e80e8c14.wav  \n",
            "  inflating: /content/data/wavs/2d3094a8793f0359fdc0e457fb7f55d7.wav  \n",
            "  inflating: /content/data/wavs/e0af6277877bd2dac29d31483cbab2ca.wav  \n",
            "  inflating: /content/data/wavs/81a7cd773afc8f70242382e7487f6af0.wav  \n",
            "  inflating: /content/data/wavs/c6ada4785eeab90f361d06076b9c094e.wav  \n",
            "  inflating: /content/data/wavs/3a525d4f9f91ae2a11bc3836b679c006.wav  \n",
            "  inflating: /content/data/wavs/fd6794e833a551fb43d1a638a04d5045.wav  \n",
            "  inflating: /content/data/wavs/07e7abf6121be22c6cf09eabc3cb2394.wav  \n",
            "  inflating: /content/data/wavs/b02d8ac5cb1ace4be3cdbc95096e288a.wav  \n",
            "  inflating: /content/data/wavs/abe62d346f8da8044bb22dddf1807895.wav  \n",
            "  inflating: /content/data/wavs/d08a34e5566126a4d22c3da2e8600409.wav  \n",
            "  inflating: /content/data/wavs/297ee8ffc59acf33de32556985143a28.wav  \n",
            "  inflating: /content/data/wavs/ff4c1183c44d1bdf3be6aa908300dd2e.wav  \n",
            "  inflating: /content/data/wavs/c1b48c807583e7d35ce08bdfb948c27e.wav  \n",
            "  inflating: /content/data/wavs/0a667cf07f44c8964f6671ed6dcda2d5.wav  \n",
            "  inflating: /content/data/wavs/6565bd9e561add2c297ebd7e033a0586.wav  \n",
            "  inflating: /content/data/wavs/db587cebccf04b5b5aa618f16b07eaa8.wav  \n",
            "  inflating: /content/data/wavs/354b261e74d9f971a4f520e9a1a482e4.wav  \n",
            "  inflating: /content/data/wavs/94dcf90258f9cf0f35a34d30e22ef7fc.wav  \n",
            "  inflating: /content/data/wavs/4e34166f6e801e04d3106e02a0fb32c6.wav  \n",
            "  inflating: /content/data/wavs/3200519ef40e697a35d21f8a23efce2f.wav  \n",
            "  inflating: /content/data/wavs/796644e657f87072a61c294163e82e4f.wav  \n",
            "  inflating: /content/data/wavs/835a6e6cc89857eb95a27fb5add69bc1.wav  \n",
            "  inflating: /content/data/wavs/871d643e6235625d3ebfe48c18e02eab.wav  \n",
            "  inflating: /content/data/wavs/e329b87bdc32121b0664d0de42079323.wav  \n",
            "  inflating: /content/data/wavs/257c46c6a026405dba67cf92580a7221.wav  \n",
            "  inflating: /content/data/wavs/236c5173204b0a720c48f3c2b144cbda.wav  \n",
            "  inflating: /content/data/wavs/51688dc2f318d3eeed63be1cd4e97d48.wav  \n",
            "  inflating: /content/data/wavs/b8eab1b05f6c8b337b931d93f4c699aa.wav  \n",
            "  inflating: /content/data/wavs/f952437e274a5193bd0cee1c670389d4.wav  \n",
            "  inflating: /content/data/wavs/29aa9e14a913871ebd843fa6128071ea.wav  \n",
            "  inflating: /content/data/wavs/534be174f22ff0be19014915d4654a87.wav  \n",
            "  inflating: /content/data/wavs/22b45941f9853fc1dad2dd50e55c90b4.wav  \n",
            "  inflating: /content/data/wavs/40b880ff5b7d5596cb045c3b0721014b.wav  \n",
            "  inflating: /content/data/wavs/4421dd825090933912c7c049b0da2caf.wav  \n",
            "  inflating: /content/data/wavs/40f2b3bc9f713bec81813d148c2a7765.wav  \n",
            "  inflating: /content/data/wavs/12eb6f85c26a10a49e1c99ab17868d7d.wav  \n",
            "  inflating: /content/data/wavs/ee5bd32547c0d78f230fbe94ff7abf27.wav  \n",
            "  inflating: /content/data/wavs/cd66c37b964a173bd793936b97c42d35.wav  \n",
            "  inflating: /content/data/wavs/aa5731cc2fbcfab68edf629a3310137b.wav  \n",
            "  inflating: /content/data/wavs/c5c3802158228ee22cdeaf3b4ffe5767.wav  \n",
            "  inflating: /content/data/wavs/f4293f86240f2548f40abf02131c9271.wav  \n",
            "  inflating: /content/data/wavs/fedde9a93961c520a08647cf5bf4fff1.wav  \n",
            "  inflating: /content/data/wavs/39b4eb9b8bf5423afa70f5be45f82b17.wav  \n",
            "  inflating: /content/data/wavs/6bc737b380544acf68df89534391bda8.wav  \n",
            "  inflating: /content/data/wavs/2f64de75141ca2b7669a34235135fb86.wav  \n",
            "  inflating: /content/data/wavs/746698c49c2fbf6e093e69ec833d96ae.wav  \n",
            "  inflating: /content/data/wavs/0b2d3b0ad428f3269a8ecb9d7f46215e.wav  \n",
            "  inflating: /content/data/wavs/78201875ae7c23455ca328e868e3592f.wav  \n",
            "  inflating: /content/data/wavs/ece1d313f86070c8fe4179eaf0c00d5f.wav  \n",
            "  inflating: /content/data/wavs/00cb732d442eab6f5269caa503152627.wav  \n",
            "  inflating: /content/data/wavs/84f43a890f7ae92b441c17a4b5271ffb.wav  \n",
            "  inflating: /content/data/wavs/a9783aface0508c6df5f8675942fcbcc.wav  \n",
            "  inflating: /content/data/wavs/3e7497e04cd28ef646ccfd5adc431c3a.wav  \n",
            "  inflating: /content/data/wavs/304e1ea9c25e56df246aecd7a983a806.wav  \n",
            "  inflating: /content/data/wavs/fd9a2b2d8505d49346a60fdd875d47d4.wav  \n",
            "  inflating: /content/data/wavs/07207f563bb3bdf50373ce47967123f6.wav  \n",
            "  inflating: /content/data/wavs/16075981df7e2885b6e8e620c19627ba.wav  \n",
            "  inflating: /content/data/wavs/94c5fb61fe7a9e009d66fe877244ca7f.wav  \n",
            "  inflating: /content/data/wavs/2f745c9ccbb83ef7b895c0301d612eea.wav  \n",
            "  inflating: /content/data/wavs/389a5cf4041aa6a52cbab1980c2b1b90.wav  \n",
            "  inflating: /content/data/wavs/410538ed59578dc2bf1a130f95b2faf3.wav  \n",
            "  inflating: /content/data/wavs/a6ea0a2b15e93507533ef95363b083f5.wav  \n",
            "  inflating: /content/data/wavs/103e5ed4165cbe9cd69a06c6bc66ff5e.wav  \n",
            "  inflating: /content/data/wavs/997234ac7723d0cbe056ecaee7f8b8d0.wav  \n",
            "  inflating: /content/data/wavs/a42674e3c6f6c8e81aca29c8342d205b.wav  \n",
            "  inflating: /content/data/wavs/72758d50babb461b1c27b370aadf48ab.wav  \n",
            "  inflating: /content/data/wavs/86a28b75c3c3b808ca27485a56187da0.wav  \n",
            "  inflating: /content/data/wavs/9e2feb194b56ad9fa060f83331c5d106.wav  \n",
            "  inflating: /content/data/wavs/156d40fda1e8d511eeb42b04253d28fe.wav  \n",
            "  inflating: /content/data/wavs/50bfad1b0dafa49f16e63fadcc096b35.wav  \n",
            "  inflating: /content/data/wavs/dd35f56c419252cbb573ae02f3446936.wav  \n",
            "  inflating: /content/data/wavs/a6f354355851cec843c3049ee3f8fd13.wav  \n",
            "  inflating: /content/data/wavs/3b5fa40a73eb8f3bf89c823c1d63535f.wav  \n",
            "  inflating: /content/data/wavs/f570004af0b7cafe738a8525dfc4d679.wav  \n",
            "  inflating: /content/data/wavs/37dcb92b37e67e20785b4ab68f3cfb3a.wav  \n",
            "  inflating: /content/data/wavs/cb2eb2b41dbe8325894f5c2a4d3d13ac.wav  \n",
            "  inflating: /content/data/wavs/6640b722e655ace4368354459e446c93.wav  \n",
            "  inflating: /content/data/wavs/fa93d22502a47b68fb604a20ee812626.wav  \n",
            "  inflating: /content/data/wavs/30e3389dca3a1a70e942a49f754b9df6.wav  \n",
            "  inflating: /content/data/wavs/a3771322457a53b838e210d69c813e3e.wav  \n",
            "  inflating: /content/data/wavs/cdb510ce293a8339e0eaa5921e90413a.wav  \n",
            "  inflating: /content/data/wavs/bd7d915da1059b505a1f69f13a07ed16.wav  \n",
            "  inflating: /content/data/wavs/a2ff1f04f22ab340c0dcee7f00b436f6.wav  \n",
            "  inflating: /content/data/wavs/e666657528d2b4adc46ee7fb8326386c.wav  \n",
            "  inflating: /content/data/wavs/5adf805824200264740487fc2a476dd0.wav  \n",
            "  inflating: /content/data/wavs/2b8f478e03390eb53566c9de8f8a6bdf.wav  \n",
            "  inflating: /content/data/wavs/2fa324f8ad7acc19b20cda044ccb63b5.wav  \n",
            "  inflating: /content/data/wavs/d1e3c69fb2d05e68d7f952278d078f18.wav  \n",
            "  inflating: /content/data/wavs/d174c839463e949e23bcd9a3426372a8.wav  \n",
            "  inflating: /content/data/wavs/a755f894b79b47b85b33a4154fb8bc9e.wav  \n",
            "  inflating: /content/data/wavs/98a8088ab539f3a9681a57be3131e58f.wav  \n",
            "  inflating: /content/data/wavs/62c03731ce86f0e95e786e034fe05fe8.wav  \n",
            "  inflating: /content/data/wavs/613e931b313e6943b4b885693b9520ab.wav  \n",
            "  inflating: /content/data/wavs/5132f6b82bf3537931c7a467c68ac8c4.wav  \n",
            "  inflating: /content/data/wavs/b6ce4685eeb08d79f84dc738e502c5df.wav  \n",
            "  inflating: /content/data/wavs/f7e931538ffc97837dd901e5a8c7fa26.wav  \n",
            "  inflating: /content/data/wavs/71668516e11e98ed03cb7619174207f7.wav  \n",
            "  inflating: /content/data/wavs/cc15eb6a631822f780cb9a6c82be0fac.wav  \n",
            "  inflating: /content/data/wavs/030814df9098f5bb33c20b3c0e5a4837.wav  \n",
            "  inflating: /content/data/wavs/59e68e15ee8d21444a28ede7170d1628.wav  \n",
            "  inflating: /content/data/wavs/1be5f5b35a176ebbff991c6f3d735826.wav  \n",
            "  inflating: /content/data/wavs/a568d9a7373b7f462ba6c93e0d8b28a3.wav  \n",
            "  inflating: /content/data/wavs/e2143091330bef8646577ada11ed2067.wav  \n",
            "  inflating: /content/data/wavs/c608d47db99d63cc8c37ee02150fd5c3.wav  \n",
            "  inflating: /content/data/wavs/e197e0b3b5d3163998cec888a356af91.wav  \n",
            "  inflating: /content/data/wavs/6ac41ad96818ec6a27ddd6181cff6f9c.wav  \n",
            "  inflating: /content/data/wavs/e3fd9fd1a12a8c2f4a2343130d60e518.wav  \n",
            "  inflating: /content/data/wavs/bad6f155dff9b62010b35731d4692f3e.wav  \n",
            "  inflating: /content/data/wavs/654a7221ba2d24e5b7d56d2ea6fcc92e.wav  \n",
            "  inflating: /content/data/wavs/d26ac8c4c44f1a7debebcb8cb5488906.wav  \n",
            "  inflating: /content/data/wavs/5dd3c5adb669deadd70e80613429c8ea.wav  \n",
            "  inflating: /content/data/wavs/e9f0efd31a7288cdc0f75f8a603014e9.wav  \n",
            "  inflating: /content/data/wavs/b33d481ffd8860feeb24bac6331460da.wav  \n",
            "  inflating: /content/data/wavs/5e298930ee0bc153c126baf2accc4bc8.wav  \n",
            "  inflating: /content/data/wavs/1e46d9d5614b601b79764324ce33c759.wav  \n",
            "  inflating: /content/data/wavs/d5763abbdb07a772f12e81c776c41d05.wav  \n",
            "  inflating: /content/data/wavs/49c349512cf5e10f46a69a228f949c0e.wav  \n",
            "  inflating: /content/data/wavs/9a27dbb76a449bdf9741c406c99f1b83.wav  \n",
            "  inflating: /content/data/wavs/d6c619af2a33b57a6bb67a57532232cf.wav  \n",
            "  inflating: /content/data/wavs/c258b190d77a164d9ebccea6507332f2.wav  \n",
            "  inflating: /content/data/wavs/c74d6ceb78b766b7e0017f22c92eb597.wav  \n",
            "  inflating: /content/data/wavs/71266a6c15d9e6b6a1416b5e3a597c8f.wav  \n",
            "  inflating: /content/data/wavs/df8c6c73adc1f531b4a76ead055d57fd.wav  \n",
            "  inflating: /content/data/wavs/bc9408c70c900c0da449d5860430da26.wav  \n",
            "  inflating: /content/data/wavs/c9bdb91e05de7ae7d1ea844be07f2a60.wav  \n",
            "  inflating: /content/data/wavs/ed347140ec52c830e9edd7bf816c777c.wav  \n",
            "  inflating: /content/data/wavs/d3af93d1e2eb1c4bf1716f0f87c7cacc.wav  \n",
            "  inflating: /content/data/wavs/8a202bf9ac5edd9f65c330c8106ee4ab.wav  \n",
            "  inflating: /content/data/wavs/aeb65a7066f786f5e3a40b4caa33c8e9.wav  \n",
            "  inflating: /content/data/wavs/303540e5cccf18e678baf0d0d23632f6.wav  \n",
            "  inflating: /content/data/wavs/9ce4e957d3d3a5831e60bb9dbdfc5883.wav  \n",
            "  inflating: /content/data/wavs/a55fd45937efc7974ef2e4c206d8cb45.wav  \n",
            "  inflating: /content/data/wavs/2146735b52b28b1f4d46ea88a92a2ea5.wav  \n",
            "  inflating: /content/data/wavs/f2ff3d6594c66c98e160955f4c818921.wav  \n",
            "  inflating: /content/data/wavs/799ccae918bf12c7cabee435c7714c68.wav  \n",
            "  inflating: /content/data/wavs/ef55a1de56352add19c7db246d7c5c0a.wav  \n",
            "  inflating: /content/data/wavs/1f890d450f75cefea9e9bf40f77d6e31.wav  \n",
            "  inflating: /content/data/wavs/5324bd55f7a7a6fbfd99d1f1f323d590.wav  \n",
            "  inflating: /content/data/wavs/8e7ddf7ff3bfcf0b969b411f90801535.wav  \n",
            "  inflating: /content/data/wavs/c1afa3c82eea3cc77c2fff7766b2af78.wav  \n",
            "  inflating: /content/data/wavs/c242edb7cf1e4126f13a73cafe0e333f.wav  \n",
            "  inflating: /content/data/wavs/b3ef9a1758d34584d5dd9c5ca069bb8f.wav  \n",
            "  inflating: /content/data/wavs/114fb1322b7397631cd791adbd7272bd.wav  \n",
            "  inflating: /content/data/wavs/56d74d82a0155044ad9ed9b2eb650521.wav  \n",
            "  inflating: /content/data/wavs/8d7806e09d3990a7cb711f8ef8fcb8c3.wav  \n",
            "  inflating: /content/data/wavs/e9e5d8c6ab1f2ba9828fe03bbe2f2807.wav  \n",
            "  inflating: /content/data/wavs/9d5a2b68937c0147a704ee4c023f979c.wav  \n",
            "  inflating: /content/data/wavs/427d28966889d905551315192335c9eb.wav  \n",
            "  inflating: /content/data/wavs/6b921a6a7842854ed9db98cac06100af.wav  \n",
            "  inflating: /content/data/wavs/2c80e65d9f3e3ff4d0ea10ee155a9c71.wav  \n",
            "  inflating: /content/data/wavs/617fce8c92ab16e5ea99702047fa39ff.wav  \n",
            "  inflating: /content/data/wavs/d4ec5c9ca3bb8af4ffe1ed7778415a7a.wav  \n",
            "  inflating: /content/data/wavs/14f023552eb57daff7d80ac919cc852d.wav  \n",
            "  inflating: /content/data/wavs/7770985874ff9c0573230cfa7cc086b8.wav  \n",
            "  inflating: /content/data/wavs/4595727a67770b45b07b7f0c05763317.wav  \n",
            "  inflating: /content/data/wavs/a78fefa0194e48812108e088cd90e20a.wav  \n",
            "  inflating: /content/data/wavs/12e6689a4cbac2382d126d9288fcb779.wav  \n",
            "  inflating: /content/data/wavs/3ac631c1f4fc405e3c9e061728d3f663.wav  \n",
            "  inflating: /content/data/wavs/4dc2f77d77353910456a0efaf6bb98fd.wav  \n",
            "  inflating: /content/data/wavs/2ba5bf2b74903484f575c443e5d6d5f8.wav  \n",
            "  inflating: /content/data/wavs/c5c09fbc119370d57a57e1e9c2c178ad.wav  \n",
            "  inflating: /content/data/wavs/4ff9e32fd0a5566f52368879e9e24f5f.wav  \n",
            "  inflating: /content/data/wavs/139883889ba997708e489a089f5c76e7.wav  \n",
            "  inflating: /content/data/wavs/2d4f826435c08052c9978121bcc777a8.wav  \n",
            "  inflating: /content/data/wavs/e6a2f561df896bc74d7f2e3b6ddf1807.wav  \n",
            "  inflating: /content/data/wavs/e1203e9f4fdc5f4210f38956e10a512a.wav  \n",
            "  inflating: /content/data/wavs/4d981aa61b665026ecd15c74c9659baf.wav  \n",
            "  inflating: /content/data/wavs/12d87c648bba0f704402900cb84e41b5.wav  \n",
            "  inflating: /content/data/wavs/f16aa39af8972729ff1355643ee1abf4.wav  \n",
            "  inflating: /content/data/wavs/0a060bc14208bd11cc38a62ad2830883.wav  \n",
            "  inflating: /content/data/wavs/c27735cfb4c9de8032323e45f86bc13c.wav  \n",
            "  inflating: /content/data/wavs/f4bad50494ea578f638c04be3d0f8c49.wav  \n",
            "  inflating: /content/data/wavs/c2f8e288cfa12206e7ab0b4765b63d7a.wav  \n",
            "  inflating: /content/data/wavs/0f7cfffe99da792079d36dfd021d3b63.wav  \n",
            "  inflating: /content/data/wavs/c9972c1f5c6d9def26b99b8d119530f4.wav  \n",
            "  inflating: /content/data/wavs/3397b0d88854236312ba7ef55ff55293.wav  \n",
            "  inflating: /content/data/wavs/4768aa58ac21b44bcc8a08950e4e1fb7.wav  \n",
            "  inflating: /content/data/wavs/c0591c5fde73bbc73539d8b15723b316.wav  \n",
            "  inflating: /content/data/wavs/5725d9a68393381590809503b7519ddf.wav  \n",
            "  inflating: /content/data/wavs/b17c64dcecbfae0cac3f127124e150e4.wav  \n",
            "  inflating: /content/data/wavs/50db1a3afc6ea2a8744452460a394181.wav  \n",
            "  inflating: /content/data/wavs/ab0be9413729eda80db0e033cd2cde6c.wav  \n",
            "  inflating: /content/data/wavs/15dcde173011106d70f76a9e5c7a8c2c.wav  \n",
            "  inflating: /content/data/wavs/aaabd15134ad9eb330247e7295a33bea.wav  \n",
            "  inflating: /content/data/wavs/5b7b01f0e9c1d11cfc46255fbe230d39.wav  \n",
            "  inflating: /content/data/wavs/47f2ddf72480cdb758a5ecfb078d907f.wav  \n",
            "  inflating: /content/data/wavs/815dbb7f4282807e9919cf4655222507.wav  \n",
            "  inflating: /content/data/wavs/c6284b4ca005e20e31b587856ea337de.wav  \n",
            "  inflating: /content/data/wavs/cfda5271a51b874264b16d3129454109.wav  \n",
            "  inflating: /content/data/wavs/4f106584a17e07ca4b6c45615493864c.wav  \n",
            "  inflating: /content/data/wavs/f5044797a7c76f054ed8e1847f74d109.wav  \n",
            "  inflating: /content/data/wavs/3016751457414cbf431dfed9e42d66d0.wav  \n",
            "  inflating: /content/data/wavs/2fd34cf1e18316af66dbe2b9f92aedaf.wav  \n",
            "  inflating: /content/data/wavs/9df1e35b99d16db3d8ff72dbc7d8c37e.wav  \n",
            "  inflating: /content/data/wavs/174167ecdae04b5c0276b91c3c9b8ed8.wav  \n",
            "  inflating: /content/data/wavs/18442438d3854398ab4ee49a4d7f6ef9.wav  \n",
            "  inflating: /content/data/wavs/2761f7c24524c3a81e4047cc83839197.wav  \n",
            "  inflating: /content/data/wavs/8624aeeff6cbcc5cdec4fa209e367393.wav  \n",
            "  inflating: /content/data/wavs/ed04a7aa10d37ca589c6efa9165693b6.wav  \n",
            "  inflating: /content/data/wavs/32d50780944656b26f0db930775d61f2.wav  \n",
            "  inflating: /content/data/wavs/cc4e5ec6586eea97f9fa42e0f211d338.wav  \n",
            "  inflating: /content/data/wavs/22baba43ea5f8714aa506dbc3378c5eb.wav  \n",
            "  inflating: /content/data/wavs/194ce354daa157ac37de6c84f376df97.wav  \n",
            "  inflating: /content/data/wavs/7b1d73fad7c88809643fdf002c61ed5d.wav  \n",
            "  inflating: /content/data/wavs/126490518cbf7a577188b61d614a87ab.wav  \n",
            "  inflating: /content/data/wavs/1b5f1937926542ebee96409d94ca665d.wav  \n",
            "  inflating: /content/data/wavs/0e88c302ea9b21657dc589c04248b02f.wav  \n",
            "  inflating: /content/data/wavs/1703aff24d140ef00a92c459b80792f1.wav  \n",
            "  inflating: /content/data/wavs/551e95d0e5f1b8ecace7088c8a30d36c.wav  \n",
            "  inflating: /content/data/wavs/2385af655a82a52520d87b21f463ff75.wav  \n",
            "  inflating: /content/data/wavs/31d3b68f63cd94e600df914bf310e36e.wav  \n",
            "  inflating: /content/data/wavs/a223cd452e89d4c0824a263e16cfdd30.wav  \n",
            "  inflating: /content/data/wavs/0474adf098fb0d337f640b82159b6f1e.wav  \n",
            "  inflating: /content/data/wavs/a9c92d2e9d9f84c88a06b8936241eb74.wav  \n",
            "  inflating: /content/data/wavs/f311c06d7d4d5d501b5294eb220fc9f0.wav  \n",
            "  inflating: /content/data/wavs/4030ae4e85747d62007973af3bbb9c35.wav  \n",
            "  inflating: /content/data/wavs/bc29e7524cbd897514f2530ad49cf197.wav  \n",
            "  inflating: /content/data/wavs/652d022477e9126903de8369aad76f0f.wav  \n",
            "  inflating: /content/data/wavs/3ead2be0aa16345eb7cfc836775de192.wav  \n",
            "  inflating: /content/data/wavs/ca6a3391ccf979b032acc098d8e95db1.wav  \n",
            "  inflating: /content/data/wavs/d809d17c5b2e32143af885315454cad8.wav  \n",
            "  inflating: /content/data/wavs/6c4308c6aadf28a57fae577a0ebfef0f.wav  \n",
            "  inflating: /content/data/wavs/6708af11a1ba4a7565210bd2f9e8c6db.wav  \n",
            "  inflating: /content/data/wavs/fa43409b8d5c784943400d70545c7687.wav  \n",
            "  inflating: /content/data/wavs/67a5bf8e83ef051f89c70e5ea847012f.wav  \n",
            "  inflating: /content/data/wavs/31be98f66b240058d00d5dd8dbb49ae1.wav  \n",
            "  inflating: /content/data/wavs/947d5bdd868e32c52aa7218497734d57.wav  \n",
            "  inflating: /content/data/wavs/3c844c7e135c45b1605eb29a6d8b04a2.wav  \n",
            "  inflating: /content/data/wavs/4b627bc9dc27a6060b0a2de520953f6a.wav  \n",
            "  inflating: /content/data/wavs/e43b8a8b30eed643201c3f7288acc2d5.wav  \n",
            "  inflating: /content/data/wavs/35fe6072a5367c43eb8b7aabb9f221a1.wav  \n",
            "  inflating: /content/data/wavs/3c8dc6506bfcc748ced45ad067ce183d.wav  \n",
            "  inflating: /content/data/wavs/06c9cb4fcefd9e4d51eead860b1a684a.wav  \n",
            "  inflating: /content/data/wavs/b10f75f48bc00ea0c3dd0b2d94a3bfe2.wav  \n",
            "  inflating: /content/data/wavs/631409327f306561b0cb09eaa1268107.wav  \n",
            "  inflating: /content/data/wavs/1956beccfba46e6457502505fe635998.wav  \n",
            "  inflating: /content/data/wavs/51911e1988fafbd3c12cd94afc399a0a.wav  \n",
            "  inflating: /content/data/wavs/118e122662a48015454348a2276927e2.wav  \n",
            "  inflating: /content/data/wavs/8c21d1a9350729e63b20df7db880cda9.wav  \n",
            "  inflating: /content/data/wavs/6d4166ab1077d02acc16e6cba761bd41.wav  \n",
            "  inflating: /content/data/wavs/5d15c65926b1c199baa548e1e241366b.wav  \n",
            "  inflating: /content/data/wavs/f34779ee5348d82cdd2ac170ce8a44c2.wav  \n",
            "  inflating: /content/data/wavs/6fd0db6e0ea7445ef1f0c777ff1fa600.wav  \n",
            "  inflating: /content/data/wavs/5ddbd5ec5d7e6c9975a84593b5056c9b.wav  \n",
            "  inflating: /content/data/wavs/d210a45740b1f5164c77f0c395dfaa2e.wav  \n",
            "  inflating: /content/data/wavs/dd752e2ef6083c052c8965537bee2b26.wav  \n",
            "  inflating: /content/data/wavs/d739d17dc3bee3d31e7d704c4c1a7eb6.wav  \n",
            "  inflating: /content/data/wavs/a812227b193eb7ed65fd19d5579ad337.wav  \n",
            "  inflating: /content/data/wavs/9c2560f6d7ca92f1abe93de2f5b8724a.wav  \n",
            "  inflating: /content/data/wavs/418fe54daf177bb09f99f6efb11f588f.wav  \n",
            "  inflating: /content/data/wavs/5a759c637ccc689a6c2abdc791ca3b5e.wav  \n",
            "  inflating: /content/data/wavs/4163c48382f1cabeaee885f16f0ce5c3.wav  \n",
            "  inflating: /content/data/wavs/4c86c6c477ac2f2aecec5b670ac98640.wav  \n",
            "  inflating: /content/data/wavs/3ee3f173b1aff68edd90cb2d0b859e38.wav  \n",
            "  inflating: /content/data/wavs/d5579f9c07d2a72bf1427fbeead147cb.wav  \n",
            "  inflating: /content/data/wavs/6e92d0db9dc6666fa88cf3fe38d04dd9.wav  \n",
            "  inflating: /content/data/wavs/5fce4f1a61147fe6aa5b627061a5e41a.wav  \n",
            "  inflating: /content/data/wavs/e99ab8ddae206317d54b4dab8e8a6b71.wav  \n",
            "  inflating: /content/data/wavs/0232f79c8ce260f04cdc260eae1661b6.wav  \n",
            "  inflating: /content/data/wavs/3fa1e2d638a1e1306fa2f888603a45b6.wav  \n",
            "  inflating: /content/data/wavs/f8206e7589074807891c0365f551c29a.wav  \n",
            "  inflating: /content/data/wavs/1402fb663664cbab490af69462b5a45e.wav  \n",
            "  inflating: /content/data/wavs/9b70cbd46fcb3f8b5194f1a1c5906f3f.wav  \n",
            "  inflating: /content/data/wavs/5061dd649feb2ea2d2b5a724247a4fdd.wav  \n",
            "  inflating: /content/data/wavs/2602da65d6f523d820065ead17599420.wav  \n",
            "  inflating: /content/data/wavs/0c1d4393253e357c8537cea2b261bc1b.wav  \n",
            "  inflating: /content/data/wavs/f43c806f8e099270d4fe212e9115ad2d.wav  \n",
            "  inflating: /content/data/wavs/4171d1d3d2aa4071bb4a460ed68c42c3.wav  \n",
            "  inflating: /content/data/wavs/84099b94b60246d2776738ed3a06a244.wav  \n",
            "  inflating: /content/data/wavs/75da956f07ee888158217b3ad31a4421.wav  \n",
            "  inflating: /content/data/wavs/2db05905e3018f1e268d9cd12e614db9.wav  \n",
            "  inflating: /content/data/wavs/3a18f6552c35569a6ba7c1a83e0e8b74.wav  \n",
            "  inflating: /content/data/wavs/26030dd8ba40b3f77327365dbfcd4b64.wav  \n",
            "  inflating: /content/data/wavs/99df15ce298d0b7c251daf93bc0103c8.wav  \n",
            "  inflating: /content/data/wavs/2365b9c1a050ff0148c49a26058989a8.wav  \n",
            "  inflating: /content/data/wavs/85e1f2dce432ab7229db1a51a76d9a12.wav  \n",
            "  inflating: /content/data/wavs/516ca6bf8e47950e474d7d579d6b4a1f.wav  \n",
            "  inflating: /content/data/wavs/af178b1f142ee0effc6819c98c05392c.wav  \n",
            "  inflating: /content/data/wavs/604cd4b2f07009608db85f57f8434030.wav  \n",
            "  inflating: /content/data/wavs/ff2d36cdb96072555f1bcf87e7452bba.wav  \n",
            "  inflating: /content/data/wavs/e77988994472daa26a245ccfde474268.wav  \n",
            "  inflating: /content/data/wavs/d2d5eca9d5936a899914c47b74a97113.wav  \n",
            "  inflating: /content/data/wavs/fa32cb46f0d7ec37fdcbbc0d776b28ef.wav  \n",
            "  inflating: /content/data/wavs/bc379e9b32c3fdd09a4febb71a78ff7c.wav  \n",
            "  inflating: /content/data/wavs/791e872be3198fefefe8a2a4d39a4077.wav  \n",
            "  inflating: /content/data/wavs/ef8d87203755568e58bf22e0c0600de9.wav  \n",
            "  inflating: /content/data/wavs/16c3bf74bf7a4c8fe45e1206685069af.wav  \n",
            "  inflating: /content/data/wavs/138eb7e69e21c08af2a8399e6fb4af12.wav  \n",
            "  inflating: /content/data/wavs/5c75adc1bc769c5ba872fd5568186d7c.wav  \n",
            "  inflating: /content/data/wavs/eb7b4df8e568633aeb290383fffd4338.wav  \n",
            "  inflating: /content/data/wavs/6c9bfe632391bba8b593c762d54b6d20.wav  \n",
            "  inflating: /content/data/wavs/80f4fd05026da86e02b64f26c201e3b1.wav  \n",
            "  inflating: /content/data/wavs/5bd24081e53cefb324245370317c77c4.wav  \n",
            "  inflating: /content/data/wavs/ff550ca5fa922492178325e81d8e36bb.wav  \n",
            "  inflating: /content/data/wavs/7328c4c60bc3028a4447c787e4a273ca.wav  \n",
            "  inflating: /content/data/wavs/14a1a0bc67597e7a6988eaedf16827f1.wav  \n",
            "  inflating: /content/data/wavs/385600bcf79c275532d3710ba02a6da0.wav  \n",
            "  inflating: /content/data/wavs/5b208b47ac1aba8f02768f555b88364e.wav  \n",
            "  inflating: /content/data/wavs/97a8812111b3df83cd6e03f372d2d16c.wav  \n",
            "  inflating: /content/data/wavs/5c18fdd2426e132191a64c0b223eb000.wav  \n",
            "  inflating: /content/data/wavs/e80054826c3abb9fdc6d2d8ee7f72349.wav  \n",
            "  inflating: /content/data/wavs/e3b80649768f5d56c889c25bb5772848.wav  \n",
            "  inflating: /content/data/wavs/a3d87af115b7de1072afc6449d65b98c.wav  \n",
            "  inflating: /content/data/wavs/0b922b82dab2b2205816b302e72d39b0.wav  \n",
            "  inflating: /content/data/wavs/e8eb56f61681079e3bf9032a81cb251a.wav  \n",
            "  inflating: /content/data/wavs/ab0839c5cec3ee50dc84be581b474bc1.wav  \n",
            "  inflating: /content/data/wavs/d383f512898866842a224460f6007d58.wav  \n",
            "  inflating: /content/data/wavs/5959fff32ee5d9273712ddf2b26f37ee.wav  \n",
            "  inflating: /content/data/wavs/1c103a9d767bd5bf62acb54e63562783.wav  \n",
            "  inflating: /content/data/wavs/c63797b722fb68773a993f744976e08b.wav  \n",
            "  inflating: /content/data/wavs/ae47fc3a9e65c2baf4b96124732b13cc.wav  \n",
            "  inflating: /content/data/wavs/85190633db3e54426071f5e518a2d004.wav  \n",
            "  inflating: /content/data/wavs/06467d6e9fa6b96e5154010741f733a7.wav  \n",
            "  inflating: /content/data/wavs/5c950d56f67aa2a94d8fa696994c1186.wav  \n",
            "  inflating: /content/data/wavs/603a6835beed440834442e6f0b5c57f0.wav  \n",
            "  inflating: /content/data/wavs/ce16ae7ac0c4d0c65842d75eebb6b7b9.wav  \n",
            "  inflating: /content/data/wavs/be7083a32730a56031cc76a7916d497c.wav  \n",
            "  inflating: /content/data/wavs/c9b83ab7b464aab99a98cf45c29db384.wav  \n",
            "  inflating: /content/data/wavs/061a2b8acde62c71b1338df83a636c45.wav  \n",
            "  inflating: /content/data/wavs/c880897e9e803684e8a51f209272942b.wav  \n",
            "  inflating: /content/data/wavs/750c67b878fb5ec123831e14d6d079b5.wav  \n",
            "  inflating: /content/data/wavs/186a90fde885ff45dc7a058ed9e387cc.wav  \n",
            "  inflating: /content/data/wavs/38001183ff52e7d0bd27710fe6c13ced.wav  \n",
            "  inflating: /content/data/wavs/2626b5edb66486c8a949d7c23ec43ccd.wav  \n",
            "  inflating: /content/data/wavs/29877aaf8203e1ecf520a6e2257ecd19.wav  \n",
            "  inflating: /content/data/wavs/3f3f7358f17bb63538fc1ea3b09084e7.wav  \n",
            "  inflating: /content/data/wavs/9c879ef3a42123bc31f0635ec9c6f600.wav  \n",
            "  inflating: /content/data/wavs/ef46764c7bada41bb356c9d96d79b2f6.wav  \n",
            "  inflating: /content/data/wavs/34c71b72b6b27fee2a6e92dc84923624.wav  \n",
            "  inflating: /content/data/wavs/05479589490c5d316590e7553138683c.wav  \n",
            "  inflating: /content/data/wavs/2d23676d5dddc4b5f186444ad33f4c21.wav  \n",
            "  inflating: /content/data/wavs/3e74a8983a750ede569d0dc77a2ce6b8.wav  \n",
            "  inflating: /content/data/wavs/2c4622d4f5f6217f890fe135b2bbed93.wav  \n",
            "  inflating: /content/data/wavs/d733d37b67839ce805b984ef643f254f.wav  \n",
            "  inflating: /content/data/wavs/46b34097f51728fe9c47b77d49a81788.wav  \n",
            "  inflating: /content/data/wavs/f4fdca29f58ef9280c3ebc923bbcb078.wav  \n",
            "  inflating: /content/data/wavs/dd86421a2b4f899830e4bb3d4ba825b5.wav  \n",
            "  inflating: /content/data/wavs/a61d95a72b63b42ec550f416b0d8c6e6.wav  \n",
            "  inflating: /content/data/wavs/43e820294a3460d6c35e4de75b85c105.wav  \n",
            "  inflating: /content/data/wavs/453004a43b55165b04b496c490912684.wav  \n",
            "  inflating: /content/data/wavs/8eebcac5e05256ded37a59f17062e41b.wav  \n",
            "  inflating: /content/data/wavs/605fceee99c085c00ff40da461427f38.wav  \n",
            "  inflating: /content/data/wavs/509751277223d6f9f6a71f4c5b9455ed.wav  \n",
            "  inflating: /content/data/wavs/3f8e9016c369b78aac867c27f99dcf58.wav  \n",
            "  inflating: /content/data/wavs/6643b89305d4df1acc97e0c51db7dc71.wav  \n",
            "  inflating: /content/data/wavs/98f7523c0b6c156093d3ce88b15ff59d.wav  \n",
            "  inflating: /content/data/wavs/f1342b16efab76fef550a2dcacd20aea.wav  \n",
            "  inflating: /content/data/wavs/5a681f09ad8836b31e36d7668d7be39c.wav  \n",
            "  inflating: /content/data/wavs/604e9ce5e90a3e02e339c44d5ba1d2b5.wav  \n",
            "  inflating: /content/data/wavs/811ab92fd0de09864cf152cd31693be7.wav  \n",
            "  inflating: /content/data/wavs/ec47a372e0fff50d64a072ae62de5bd8.wav  \n",
            "  inflating: /content/data/wavs/efa29978ae670f15ae424bbc47c07843.wav  \n",
            "  inflating: /content/data/wavs/39e80c5bbfe809ef4452cb5b83a578fa.wav  \n",
            "  inflating: /content/data/wavs/5a1aa7841e306c3f02f4eea84dca4806.wav  \n",
            "  inflating: /content/data/wavs/90e0f685c52b662b7da2d567e8371a74.wav  \n",
            "  inflating: /content/data/wavs/dfb71f2d76541f59df7deec1be2f90cf.wav  \n",
            "  inflating: /content/data/wavs/f5f42d810dcafbc19f40fa45dc037475.wav  \n",
            "  inflating: /content/data/wavs/a31e86303c58a413c186050a7d5646ef.wav  \n",
            "  inflating: /content/data/wavs/c7e45dc9d79a7549b2027633ecad7303.wav  \n",
            "  inflating: /content/data/wavs/941273d240df95709138289b04e4ef5f.wav  \n",
            "  inflating: /content/data/wavs/93990e842c49163195fb7b8f31f8c29d.wav  \n",
            "  inflating: /content/data/wavs/2556ac71fcd3436e56fd54cc4d86bce9.wav  \n",
            "  inflating: /content/data/wavs/08cbdefd7ac5fabca062b61440d79ceb.wav  \n",
            "  inflating: /content/data/wavs/053de79a9690ae2deaf485346a27901a.wav  \n",
            "  inflating: /content/data/wavs/5f69ceb7e0716935c9e99acbaf4fd460.wav  \n",
            "  inflating: /content/data/wavs/31e8e08c1615581a087352ebe29bf687.wav  \n",
            "  inflating: /content/data/wavs/f3290376dadc2de78c31c83b43e0f897.wav  \n",
            "  inflating: /content/data/wavs/c61bbb2442dd4b519d29a40dd542afbb.wav  \n",
            "  inflating: /content/data/wavs/a5f720c8a4074a38520eb87ca4e58cd9.wav  \n",
            "  inflating: /content/data/wavs/c7f3447327a3cbba08e25170ce0c92d2.wav  \n",
            "  inflating: /content/data/wavs/cfb8bb56c5fe113edda058364857914d.wav  \n",
            "  inflating: /content/data/wavs/00ae51b56e5684983021e8224013fc32.wav  \n",
            "  inflating: /content/data/wavs/0465cd990d23e66c7e59d2fcec0d5c4d.wav  \n",
            "  inflating: /content/data/wavs/58e8a152dfcef085b9c02865a104a958.wav  \n",
            "  inflating: /content/data/wavs/ea298fd79f2b9b1935b09d265e09e772.wav  \n",
            "  inflating: /content/data/wavs/61d22ca768ff270ddddb9c2f20afcf78.wav  \n",
            "  inflating: /content/data/wavs/50cdf731e5ab6ae19d76a195bb2e1b96.wav  \n",
            "  inflating: /content/data/wavs/a00954db9dc5f366aa7f24dab2dc9be4.wav  \n",
            "  inflating: /content/data/wavs/b76c29524954878b0d5d53681d16089a.wav  \n",
            "  inflating: /content/data/wavs/b310cf68b814c06696b1665ce51259f1.wav  \n",
            "  inflating: /content/data/wavs/d040646a4d4881bde4a21dc8587c76b5.wav  \n",
            "  inflating: /content/data/wavs/2ccaf5aa608ee9407858adb10b4fc7c7.wav  \n",
            "  inflating: /content/data/wavs/8e8b8182d20b9c7b5750d51d22bbac52.wav  \n",
            "  inflating: /content/data/wavs/8b498123a51a823f7bf07812f2a922db.wav  \n",
            "  inflating: /content/data/wavs/267bdf2c3b70fef5ab67aa0c8e38d8da.wav  \n",
            "  inflating: /content/data/wavs/c9c13904bc09942739d480bbb66fb176.wav  \n",
            "  inflating: /content/data/wavs/afc02ea3efb8e06d429ec63f95b802c3.wav  \n",
            "  inflating: /content/data/wavs/2a7556c6774c5e1b584a4a8805cf3e42.wav  \n",
            "  inflating: /content/data/wavs/52c57355492baced8c0499396359b844.wav  \n",
            "  inflating: /content/data/wavs/428e4c166ee32ed277b8b7aff099d7f5.wav  \n",
            "  inflating: /content/data/wavs/9ff22e198ca80513d8004dd92859a66c.wav  \n",
            "  inflating: /content/data/wavs/fdef0e926a8b22a2b594312f298bb0d5.wav  \n",
            "  inflating: /content/data/wavs/2c05f4c3e5e41c4e6d577351e39321d8.wav  \n",
            "  inflating: /content/data/wavs/ec1226e9861f5b514c0354ef9b5982b5.wav  \n",
            "  inflating: /content/data/wavs/5f1ba3fbf38b2df2aef656c92dcfc5c1.wav  \n",
            "  inflating: /content/data/wavs/a466a9419916690630d6e7856cdc5ede.wav  \n",
            "  inflating: /content/data/wavs/4ff145c7ac8759208c395b11d6d77efb.wav  \n",
            "  inflating: /content/data/wavs/178896f1a8d4764eac14c7337a2ee9e1.wav  \n",
            "  inflating: /content/data/wavs/205561fc09045789654ede8e77354539.wav  \n",
            "  inflating: /content/data/wavs/e93b3c182192ce5fa738757c94dab3a7.wav  \n",
            "  inflating: /content/data/wavs/4660859b525b51091f01673012342cbc.wav  \n",
            "  inflating: /content/data/wavs/ce752097ff31195981cf3fe5690e1be8.wav  \n",
            "  inflating: /content/data/wavs/7c71a4570283f9c6f6ca0fb893028e51.wav  \n",
            "  inflating: /content/data/wavs/df845f9c10b2567c30048cff4cd6c155.wav  \n",
            "  inflating: /content/data/wavs/0a499dd41c68583d9aed356251485f90.wav  \n",
            "  inflating: /content/data/wavs/87a62e5c81db12425da11743ff9b55b5.wav  \n",
            "  inflating: /content/data/wavs/fb40ef2bb181892c7f83f8e4692033cd.wav  \n",
            "  inflating: /content/data/wavs/328fdb7b1833d0d688338fbae99fc4ea.wav  \n",
            "  inflating: /content/data/wavs/ea7e98b8bdcd6b1d78b954c43cfd381f.wav  \n",
            "  inflating: /content/data/wavs/c272969c3a4138a55c1a04886920eb2c.wav  \n",
            "  inflating: /content/data/wavs/9adc6a27da59c3b6ae0412b72a35f4aa.wav  \n",
            "  inflating: /content/data/wavs/783bdd2bb1181b74882402e5d29c0492.wav  \n",
            "  inflating: /content/data/wavs/db00722b48009681d5878a946641fa75.wav  \n",
            "  inflating: /content/data/wavs/c9d9c6da0a8b0751b265d11d2e29d23a.wav  \n",
            "  inflating: /content/data/wavs/fe9504034df58391d36ee70209e0a8b1.wav  \n",
            "  inflating: /content/data/wavs/6dae6ab1d8de53b3358aa8466e6c0ae5.wav  \n",
            "  inflating: /content/data/wavs/e5fe33e0bb78a4095d06e541f78c2886.wav  \n",
            "  inflating: /content/data/wavs/540c7ed3b6e5fab30a6b1d198cbf6062.wav  \n",
            "  inflating: /content/data/wavs/9389d4bf061fbae5c3c62871683a36f7.wav  \n",
            "  inflating: /content/data/wavs/fb6ba613c3bd4332914658a44de3759d.wav  \n",
            "  inflating: /content/data/wavs/6352315c541dc58379b264793f506b12.wav  \n",
            "  inflating: /content/data/wavs/4f7a3afb5c1b115fe722ec1e8b852d61.wav  \n",
            "  inflating: /content/data/wavs/6d425a7c85b199c1d7a3b6731c731b6b.wav  \n",
            "  inflating: /content/data/wavs/5b2ffceb9d13de731aa82edcbb35cf14.wav  \n",
            "  inflating: /content/data/wavs/d1ab9c4c792b92f355c802ab447ebc47.wav  \n",
            "  inflating: /content/data/wavs/5a57bd37ef6dee2e69dd9f29655361ec.wav  \n",
            "  inflating: /content/data/wavs/99fb0e230a44344184392d8961c0714f.wav  \n",
            "  inflating: /content/data/wavs/2b8fa175da8a9dc7c8085ac96c99b712.wav  \n",
            "  inflating: /content/data/wavs/78b756cfc807b9bab384f35373315427.wav  \n",
            "  inflating: /content/data/wavs/17200c96118a05d425195d0d0a45f7f2.wav  \n",
            "  inflating: /content/data/wavs/b9135cab12035c93877bd7b59558fe9d.wav  \n",
            "  inflating: /content/data/wavs/a5faa740478618bcd835e9569af9794f.wav  \n",
            "  inflating: /content/data/wavs/26495d3fa365e1af4a245cb7acdcb8b9.wav  \n",
            "  inflating: /content/data/wavs/ccd14ed5d93a7b2d3c0571504f9952b9.wav  \n",
            "  inflating: /content/data/wavs/023b5585ce0e19d2bd39685f6acbbcf1.wav  \n",
            "  inflating: /content/data/wavs/831f76d45381fb4d234d283e605e801a.wav  \n",
            "  inflating: /content/data/wavs/cac370294041af73c35452b248539f7b.wav  \n",
            "  inflating: /content/data/wavs/bf61611b33c5d45a40b9aed7e8b289f2.wav  \n",
            "  inflating: /content/data/wavs/f0c10ce9c98c07c32294a4e265f1f9f6.wav  \n",
            "  inflating: /content/data/wavs/3cde7030e9d2ed6e9f8a801eb65200c6.wav  \n",
            "  inflating: /content/data/wavs/4f4a78273aac2c9b473fa1d0e1f68dc3.wav  \n",
            "  inflating: /content/data/wavs/4e18ded8f829b416ea7a0a620d8375ae.wav  \n",
            "  inflating: /content/data/wavs/ee4b1133d23460b598e9b97cfc0b8bb0.wav  \n",
            "  inflating: /content/data/wavs/3d40b7a572783df7bcd2a635be892e64.wav  \n",
            "  inflating: /content/data/wavs/7c0fb74e2a7165fbcb17f49e9c6a94c9.wav  \n",
            "  inflating: /content/data/wavs/b417f58090b539090a0cc296477e9009.wav  \n",
            "  inflating: /content/data/wavs/feaedeb1e54a86dcbf27c0ee2dd4ff89.wav  \n",
            "  inflating: /content/data/wavs/3387cd05212dcf71ac6c8b539a2be7f5.wav  \n",
            "  inflating: /content/data/wavs/289faa4f25fff3e600f3c449f0da1cc0.wav  \n",
            "  inflating: /content/data/wavs/642640aa26e11093469025a4b6436e26.wav  \n",
            "  inflating: /content/data/wavs/a0f5cce64d295da8b840e86287357be6.wav  \n",
            "  inflating: /content/data/wavs/993b8b5c254d1be000702bef7976b328.wav  \n",
            "  inflating: /content/data/wavs/ecda666dcc6f3b60b84f6d44d06ec14f.wav  \n",
            "  inflating: /content/data/wavs/d49f86c9c2354c4988050bb35e681408.wav  \n",
            "  inflating: /content/data/wavs/5793e999a04a72fd9a2a381c6ee02f49.wav  \n",
            "  inflating: /content/data/wavs/3c76928c19b77d2cbbe8f70beed5bcbe.wav  \n",
            "  inflating: /content/data/wavs/0495d7b0b80cdd876f92ed9a2b99186a.wav  \n",
            "  inflating: /content/data/wavs/36038063e25481942fe132d62298f817.wav  \n",
            "  inflating: /content/data/wavs/1913d13e9b39d712dbe5fcee0ecd1d9f.wav  \n",
            "  inflating: /content/data/wavs/7c044d9c29f7fbd2bc255664f05e4e4d.wav  \n",
            "  inflating: /content/data/wavs/54898a84784be7f65f1bc15de7a96351.wav  \n",
            "  inflating: /content/data/wavs/08e121522ec0ee9b60362a0f4becdaca.wav  \n",
            "  inflating: /content/data/wavs/60508995efbcf79b18abba913b687a57.wav  \n",
            "  inflating: /content/data/wavs/282c1d7b889a6d95a78e51cfd87aae3f.wav  \n",
            "  inflating: /content/data/wavs/ea5fdb05ce48f69e29133a32cabd9738.wav  \n",
            "  inflating: /content/data/wavs/d4eb45f609fa97af852d977758715411.wav  \n",
            "  inflating: /content/data/wavs/0fdf7a620319ff4fd4da6bc13e328592.wav  \n",
            "  inflating: /content/data/wavs/5eb9c49fe2595b6caeccae6bee59b808.wav  \n",
            "  inflating: /content/data/wavs/cab78964ff15bfa062dd905667fc4a13.wav  \n",
            "  inflating: /content/data/wavs/ec10f09f8662ff73cd56224044605fce.wav  \n",
            "  inflating: /content/data/wavs/a57ec1324db6c25a53adda17ced95108.wav  \n",
            "  inflating: /content/data/wavs/fc630c365d36f8fd36ce9efd18402472.wav  \n",
            "  inflating: /content/data/wavs/30873019178b4748ae8f3e64e13926db.wav  \n",
            "  inflating: /content/data/wavs/b013c605d59c1a95819135284ef1b99d.wav  \n",
            "  inflating: /content/data/wavs/8de91aaec369c143ae8ac89e3b1946d6.wav  \n",
            "  inflating: /content/data/wavs/938c2ff014abd5de18919013522a3406.wav  \n",
            "  inflating: /content/data/wavs/cffc47779a9d4ac21007d71cc6fc82db.wav  \n",
            "  inflating: /content/data/wavs/27ad7ed52d7ea0667e47db731a7b3ca5.wav  \n",
            "  inflating: /content/data/wavs/f90daeb04a89075c8e47045992c6b419.wav  \n",
            "  inflating: /content/data/wavs/29c73ea67182309099575d07c5c6adf8.wav  \n",
            "  inflating: /content/data/wavs/9817c085d88826d6d44ecb56ffb90813.wav  \n",
            "  inflating: /content/data/wavs/aced3c97f95af7baf70c2bcc8fcd3655.wav  \n",
            "  inflating: /content/data/wavs/2394f752ee13928337c09cbce86f4433.wav  \n",
            "  inflating: /content/data/wavs/cab41954eb65c6ea23015c49217b5cd9.wav  \n",
            "  inflating: /content/data/wavs/33e88218f7236b7c390b1d1da33aa95e.wav  \n",
            "  inflating: /content/data/wavs/a03bf351482a42ef8ebdda20abbd0bbe.wav  \n",
            "  inflating: /content/data/wavs/53c8f708ee26863917836952a71f2a27.wav  \n",
            "  inflating: /content/data/wavs/83aae62e8d4cf3fd8611384147596f60.wav  \n",
            "  inflating: /content/data/wavs/48bc8eecab04361dde9aa085da0029e3.wav  \n",
            "  inflating: /content/data/wavs/5a9079a1c11928293711781c1038f1b0.wav  \n",
            "  inflating: /content/data/wavs/c3a691df065aa299dcd9d8fd8ad4e112.wav  \n",
            "  inflating: /content/data/wavs/076215dee5692ac96cc5b83e110f6b4f.wav  \n",
            "  inflating: /content/data/wavs/a0353b514dace881454caf2799544614.wav  \n",
            "  inflating: /content/data/wavs/10e85625bdb4829aee6f65d324800c22.wav  \n",
            "  inflating: /content/data/wavs/6db397060ae489ccbd9ed580261675b6.wav  \n",
            "  inflating: /content/data/wavs/74651db807be1753503b34bd539392ca.wav  \n",
            "  inflating: /content/data/wavs/531965976f10f47611b5fb53f53896b9.wav  \n",
            "  inflating: /content/data/wavs/0b9841d0cd31fecbe26991000819af87.wav  \n",
            "  inflating: /content/data/wavs/5d4a20dd0b5c152dc6248aa79c35bde2.wav  \n",
            "  inflating: /content/data/wavs/ec2216d288b7ce4ba0c6846f369a3708.wav  \n",
            "  inflating: /content/data/wavs/ce0348ceb420a22b670ccdc95c6027e6.wav  \n",
            "  inflating: /content/data/wavs/825be2a59bfa2208c9d0e7e07d5d4a6c.wav  \n",
            "  inflating: /content/data/wavs/9e3cb4337a2217fb8da02d431c990c92.wav  \n",
            "  inflating: /content/data/wavs/1f18a9b52bdcf88ff6b4a24b55edbb7b.wav  \n",
            "  inflating: /content/data/wavs/ea479151783b74ab53ca9d08ee298239.wav  \n",
            "  inflating: /content/data/wavs/009da679abe5ad9ac1f206f60f6e8dff.wav  \n",
            "  inflating: /content/data/wavs/072e85989445991c7f800be63dbe3192.wav  \n",
            "  inflating: /content/data/wavs/e2b3ada79526566d4e9cd89b5fa28b74.wav  \n",
            "  inflating: /content/data/wavs/d36e0ab809864fc66312ea5a9c9ac8f3.wav  \n",
            "  inflating: /content/data/wavs/1de7f707e1029d7e0d02d15b3e22cfc0.wav  \n",
            "  inflating: /content/data/wavs/6d25f377d85cb199267a21584ba0f68a.wav  \n",
            "  inflating: /content/data/wavs/e2c8f476b0711bdc11a3b7c8975fbc3b.wav  \n",
            "  inflating: /content/data/wavs/679fce893971cca6fa264b219ff975c5.wav  \n",
            "  inflating: /content/data/wavs/78a10558cea9de283046946af351de78.wav  \n",
            "  inflating: /content/data/wavs/8b5faf0345661d6b967f7631b1eb327d.wav  \n",
            "  inflating: /content/data/wavs/741ea014a5f072cd9b0970bbb0c9b762.wav  \n",
            "  inflating: /content/data/wavs/31772bfaa499b4e5a467d26e1cb4bb96.wav  \n",
            "  inflating: /content/data/wavs/6e06d6464c1ec3372c04f3dac2d2bcf9.wav  \n",
            "  inflating: /content/data/wavs/66071edaec8730f98dda58250421314f.wav  \n",
            "  inflating: /content/data/wavs/c5ec27e4ab133505478a712753311392.wav  \n",
            "  inflating: /content/data/wavs/50d28cf54335e1c06ad026ba6a802d46.wav  \n",
            "  inflating: /content/data/wavs/f8967f30d1c2d76ee4915bc0ad60ceaf.wav  \n",
            "  inflating: /content/data/wavs/0d62c14634269e2881901cbfcc7ff309.wav  \n",
            "  inflating: /content/data/wavs/7a8d5cdbec3a11a9b43cdd1f7451e2d0.wav  \n",
            "  inflating: /content/data/wavs/e8a70c9f0647479d75e1b36853c7a5aa.wav  \n",
            "  inflating: /content/data/wavs/c85cc0b2697ec100fc82101a6cd7899b.wav  \n",
            "  inflating: /content/data/wavs/853327071007cd79b827feaa28a99bc3.wav  \n",
            "  inflating: /content/data/wavs/f4dae6f635c06a0b789cec789528cdb6.wav  \n",
            "  inflating: /content/data/wavs/fdb730ca8f8c2373d89a016946bc639c.wav  \n",
            "  inflating: /content/data/wavs/91a3d29fe1b79059e62c0523a5da049d.wav  \n",
            "  inflating: /content/data/wavs/e06bc7e5b20ce11fdd51a0e59d43f301.wav  \n",
            "  inflating: /content/data/wavs/cb4a8e0df180c71b278bf642be924f75.wav  \n",
            "  inflating: /content/data/wavs/d18780b493eed562f49c2fa489ffd5f5.wav  \n",
            "  inflating: /content/data/wavs/45b8d4562e9206835e4cfc82881f4360.wav  \n",
            "  inflating: /content/data/wavs/48e39504963a882f92040d5505076a8c.wav  \n",
            "  inflating: /content/data/wavs/15cc8bc15c0ba123f475479bf6ca0152.wav  \n",
            "  inflating: /content/data/wavs/123c0ec56389238ca1c3ba00f062f165.wav  \n",
            "  inflating: /content/data/wavs/7fd6c446345fd7330f11ace5bee1e38f.wav  \n",
            "  inflating: /content/data/wavs/74a1704e24fdaec7146484e3684a2115.wav  \n",
            "  inflating: /content/data/wavs/b0ebe48efc5b68f221ee24c6dc886ab8.wav  \n",
            "  inflating: /content/data/wavs/198244d34bfa954e692db182faa5513e.wav  \n",
            "  inflating: /content/data/wavs/fa74dfca8f0dfb4f54e5d80576bb21bc.wav  \n",
            "  inflating: /content/data/wavs/e53745e3cda4a90c27140ff576ddf3ca.wav  \n",
            "  inflating: /content/data/wavs/9426bd746ab3718dda4788909926fe04.wav  \n",
            "  inflating: /content/data/wavs/f8057a67e8e12a7a5a38e98107784ea8.wav  \n",
            "  inflating: /content/data/wavs/c55a5b7f2b4f102f0b0cf473237b80d8.wav  \n",
            "  inflating: /content/data/wavs/9eba7a135599e4b2687deb1c9b07a43a.wav  \n",
            "  inflating: /content/data/wavs/f2db7107dfb3a31247182c898d51ec5d.wav  \n",
            "  inflating: /content/data/wavs/a91b83adde6fc4f6e6c10d0a054bfbba.wav  \n",
            "  inflating: /content/data/wavs/1f1b35c6cc0ad0423600e10bfcd6ea86.wav  \n",
            "  inflating: /content/data/wavs/2356a43fe1d2debc008f10007a80e9a7.wav  \n",
            "  inflating: /content/data/wavs/2224af46fda79c97f64f71b7fb3c4621.wav  \n",
            "  inflating: /content/data/wavs/70c8ce9476b2a18941ee99208cf8fa14.wav  \n",
            "  inflating: /content/data/wavs/51c2c6531664f3e3eb8f36449dbcfc48.wav  \n",
            "  inflating: /content/data/wavs/65d3c620fc6bcdedccf69ffaea61bc8e.wav  \n",
            "  inflating: /content/data/wavs/bb5a786f20084e847d401f2879dbd104.wav  \n",
            "  inflating: /content/data/wavs/dd41c4ed7e0e46b02c0d38ad08635b9f.wav  \n",
            "  inflating: /content/data/wavs/68066d84174be7c745c2dcf9cbe04ecf.wav  \n",
            "  inflating: /content/data/wavs/4f244ec83097078514692da9f038cec6.wav  \n",
            "  inflating: /content/data/wavs/1e01cebd6c16ec1c2c7bedb00544cd4d.wav  \n",
            "  inflating: /content/data/wavs/85a1bfb2aa2e175fb4eda523c54f12f1.wav  \n",
            "  inflating: /content/data/wavs/597909de44c35bcacd4739d78c7f34fb.wav  \n",
            "  inflating: /content/data/wavs/67308b719e920a8fd9121e32ba19c1d6.wav  \n",
            "  inflating: /content/data/wavs/128ea3a4c5e3b01c6c44950cb1f1f7cb.wav  \n",
            "  inflating: /content/data/wavs/20b7b0f727cd38921fde883ec627e04b.wav  \n",
            "  inflating: /content/data/wavs/77e90a2978969fd87f9c0b34e4d697e5.wav  \n",
            "  inflating: /content/data/wavs/3ed7f8f263915942576462db8d9ef402.wav  \n",
            "  inflating: /content/data/wavs/250a2e5a3ab9d9dcecf3f5d45467696a.wav  \n",
            "  inflating: /content/data/wavs/701b7d512fc51322bc6dacd0f1090a19.wav  \n",
            "  inflating: /content/data/wavs/df8fcbe7a6973e64a94c8198f122a5e7.wav  \n",
            "  inflating: /content/data/wavs/aa6311a5eeaa056b8c7b3931f47f8b08.wav  \n",
            "  inflating: /content/data/wavs/83072a56b8eabc908fcf8d5ba3dc3648.wav  \n",
            "  inflating: /content/data/wavs/72e9849fe3eb4184db70b85493763b77.wav  \n",
            "  inflating: /content/data/wavs/64da69ebd13a45ddff5c96db9dbe32cd.wav  \n",
            "  inflating: /content/data/wavs/9cd4209f99d9861bf822c9c34cd6905e.wav  \n",
            "  inflating: /content/data/wavs/37623460cf0e7c3c786306e362145c12.wav  \n",
            "  inflating: /content/data/wavs/5883b4210f5e158064deddce06fb6c28.wav  \n",
            "  inflating: /content/data/wavs/f7a66da6285946f0ae0197f0b9e9702e.wav  \n",
            "  inflating: /content/data/wavs/a2fce16e0c5dc809ca50bf3df31f4fad.wav  \n",
            "  inflating: /content/data/wavs/c787ee6a47f94ba200150d77eb4623f5.wav  \n",
            "  inflating: /content/data/wavs/8fd8638996aecde88367e415f077e3f3.wav  \n",
            "  inflating: /content/data/wavs/166b2b0f91edbd31638bca37e22c7465.wav  \n",
            "  inflating: /content/data/wavs/ea63e79f4282cdc8e6a86074c46de28f.wav  \n",
            "  inflating: /content/data/wavs/974fce9092fb0476f60bb1a73a20d714.wav  \n",
            "  inflating: /content/data/wavs/0ee4660b3eb0fa36eb30ea4773d95bb3.wav  \n",
            "  inflating: /content/data/wavs/5358d288906e87d9e727e5e5087154f1.wav  \n",
            "  inflating: /content/data/wavs/e68348f4db503e3b1374997e4043fb14.wav  \n",
            "  inflating: /content/data/wavs/dde8c8695b07c2cce822a0ac0b12297c.wav  \n",
            "  inflating: /content/data/wavs/59c60a55276821c7dc35827e7a44a409.wav  \n",
            "  inflating: /content/data/wavs/52d7a24676cf0c5e68a4586ea38c178c.wav  \n",
            "  inflating: /content/data/wavs/d6f7bc63ed47baf11e82c6bc5939fdbc.wav  \n",
            "  inflating: /content/data/wavs/7cc6cb8910d8c4a4d65b19e48139387d.wav  \n",
            "  inflating: /content/data/wavs/de867c4872b059ebd260d1efc14bdd99.wav  \n",
            "  inflating: /content/data/wavs/93076fad29172477982db55a71c67e0b.wav  \n",
            "  inflating: /content/data/wavs/8776614ff8111693ee70f61ceb3a685f.wav  \n",
            "  inflating: /content/data/wavs/490e3a9d4fe36d9c58a5f8c83212de36.wav  \n",
            "  inflating: /content/data/wavs/731bcffe1789a109852f0390752ef2e2.wav  \n",
            "  inflating: /content/data/wavs/3190a60c01da57a50fd233b8e1e86096.wav  \n",
            "  inflating: /content/data/wavs/7013efdb7ce9dd56d1649da324a47cbb.wav  \n",
            "  inflating: /content/data/wavs/dc057fd502d04849891b68a6c5e95f49.wav  \n",
            "  inflating: /content/data/wavs/83475cef76a88297742e42a7480edb8b.wav  \n",
            "  inflating: /content/data/wavs/e601a6ce9fa6940540c706a61e4830c2.wav  \n",
            "  inflating: /content/data/wavs/7eecf86857aafa3a8c6a0b49aa620116.wav  \n",
            "  inflating: /content/data/wavs/8d12f7377b7401b378c1b5b33c129f1b.wav  \n",
            "  inflating: /content/data/wavs/0e79fb2ed1a38df2247acb5f607f6e7b.wav  \n",
            "  inflating: /content/data/wavs/8155fcb5028a00c892e101f4c103a028.wav  \n",
            "  inflating: /content/data/wavs/b4c3afb601e96ddd9ef8fcb9d099c66e.wav  \n",
            "  inflating: /content/data/wavs/a4ba34181a6d48486744d5472f63e39b.wav  \n",
            "  inflating: /content/data/wavs/3baa4cd72e52f5bef1eb2011fd971555.wav  \n",
            "  inflating: /content/data/wavs/48c1f9235986c8d9b471b6b6c7fd3e96.wav  \n",
            "  inflating: /content/data/wavs/7439a44724b03eef60511f6baa2630fd.wav  \n",
            "  inflating: /content/data/wavs/29479f4eb972b2d79c460d6412785649.wav  \n",
            "  inflating: /content/data/wavs/4248c51a7256208e384773b925a8c69a.wav  \n",
            "  inflating: /content/data/wavs/bc9ba195b4d29d114e16e6abcd787b17.wav  \n",
            "  inflating: /content/data/wavs/ad00bbd25c9a79cbb4a2b95db5e7f9e2.wav  \n",
            "  inflating: /content/data/wavs/8236735e25d3e5caf11e94499db8689e.wav  \n",
            "  inflating: /content/data/wavs/8393c39f382ee69ac84d94227fbd8a13.wav  \n",
            "  inflating: /content/data/wavs/43284fd0f9963434ffa28e681a534d9e.wav  \n",
            "  inflating: /content/data/wavs/8219174bcf7337bad1e3e960e4222e36.wav  \n",
            "  inflating: /content/data/wavs/5754df4b17c80ebea953b38a0793a6f9.wav  \n",
            "  inflating: /content/data/wavs/28f8860697153a9b63f4bdb1fff77ccb.wav  \n",
            "  inflating: /content/data/wavs/45c89d661a3d863323677fdc5c13728f.wav  \n",
            "  inflating: /content/data/wavs/508ac3555136ace3bbc354de59950a01.wav  \n",
            "  inflating: /content/data/wavs/5e2bfae842452dd05e5a2531d794afcf.wav  \n",
            "  inflating: /content/data/wavs/c3536bd28c43a05bb453369f1331e1ac.wav  \n",
            "  inflating: /content/data/wavs/6374056b986eec0d06b8faef336fc500.wav  \n",
            "  inflating: /content/data/wavs/e25a29fe098a0eba09ccba75a20c547f.wav  \n",
            "  inflating: /content/data/wavs/f09807d80ccf3374bd286e33dd2348b0.wav  \n",
            "  inflating: /content/data/wavs/0e1879f9c0ac080d0ed8cf3f54239607.wav  \n",
            "  inflating: /content/data/wavs/a6ce2fe98658818f263a6dbc2b3a947f.wav  \n",
            "  inflating: /content/data/wavs/cb8b8d6c74fecc2c91134f94eb4e93a2.wav  \n",
            "  inflating: /content/data/wavs/0fc470dd92e9fba3142737c0e8493c01.wav  \n",
            "  inflating: /content/data/wavs/5a0c386dbacbecbaa3d66d252b7e774a.wav  \n",
            "  inflating: /content/data/wavs/1ea8598ad5a110c14c35bb1a5d6afb9a.wav  \n",
            "  inflating: /content/data/wavs/6eaf1186c9ac1c138f6ffcf13d30faef.wav  \n",
            "  inflating: /content/data/wavs/3cb459dc07fab0ccb5ad20d93af3b3b0.wav  \n",
            "  inflating: /content/data/wavs/3ab9d05d9179ba31196435ee2f431db9.wav  \n",
            "  inflating: /content/data/wavs/bc88768f046f189398307088c90c4a06.wav  \n",
            "  inflating: /content/data/wavs/b33a44453a847aa1f4dc8bd58b728c97.wav  \n",
            "  inflating: /content/data/wavs/c5b111da1a1bf3ed272365d623bae9c0.wav  \n",
            "  inflating: /content/data/wavs/8f3f7169bd19da64a63c325455d18e3d.wav  \n",
            "  inflating: /content/data/wavs/5487ceb3910decbd8ef4bae63f5e4b61.wav  \n",
            "  inflating: /content/data/wavs/c1af7cb1106ba32fc5737959aef396f7.wav  \n",
            "  inflating: /content/data/wavs/d39ed679e7cdcd28068e78457a4c409d.wav  \n",
            "  inflating: /content/data/wavs/45d5ed826672c21adb44736d30fd92c0.wav  \n",
            "  inflating: /content/data/wavs/2f82730c3edf63cb841124925415be00.wav  \n",
            "  inflating: /content/data/wavs/c467ff089cd20b788d14524c6fb07efe.wav  \n",
            "  inflating: /content/data/wavs/66c42d45307b6b21107e371dc469add3.wav  \n",
            "  inflating: /content/data/wavs/73f99150d042313864141fc1651ffa41.wav  \n",
            "  inflating: /content/data/wavs/4f4ce054516da40b7fd098e4254e1757.wav  \n",
            "  inflating: /content/data/wavs/e30fb57fdacd3f61d78d4c0776b73087.wav  \n",
            "  inflating: /content/data/wavs/861d7d41b78eab110dc74594d9357b28.wav  \n",
            "  inflating: /content/data/wavs/bd5ff343bf52b3562f12842b4dd7a9e0.wav  \n",
            "  inflating: /content/data/wavs/fa3ae6fd6fc7a2572c4dc31df79e9c00.wav  \n",
            "  inflating: /content/data/wavs/b3464055b9103b66929da33edb87486b.wav  \n",
            "  inflating: /content/data/wavs/23f39307e32ffbb5e2f77fbe43e371bd.wav  \n",
            "  inflating: /content/data/wavs/ed8dfb5864968471a809b957ff58aad1.wav  \n",
            "  inflating: /content/data/wavs/7c9014657ea8b0929370370e565c35da.wav  \n",
            "  inflating: /content/data/wavs/eb1600a1976c2b0692c57eca6d9597d7.wav  \n",
            "  inflating: /content/data/wavs/68661705782e54652477453ff9972d70.wav  \n",
            "  inflating: /content/data/wavs/9e864257f2606afd3ba494ccd143a3e9.wav  \n",
            "  inflating: /content/data/wavs/820d7ed02544391bb1a57027de9b495b.wav  \n",
            "  inflating: /content/data/wavs/93a6d6279a844aceb025ddca8e63dd57.wav  \n",
            "  inflating: /content/data/wavs/0a912dfd287430da36a2a6988eb2e2b8.wav  \n",
            "  inflating: /content/data/wavs/28a0e6d3001c0348c1894245b47b6767.wav  \n",
            "  inflating: /content/data/wavs/5cfda10927d87ee6c71cba0ceac6fe0e.wav  \n",
            "  inflating: /content/data/wavs/cc9e5dc96ce308552f130708be69b715.wav  \n",
            "  inflating: /content/data/wavs/972ef58137b18d06c2e7a8454aacbc06.wav  \n",
            "  inflating: /content/data/wavs/a95f37a9ce5d001c7217fc2468e6cf43.wav  \n",
            "  inflating: /content/data/wavs/bf7331a40e4a51af9790ab7b9ae5e58c.wav  \n",
            "  inflating: /content/data/wavs/15a36a161afd98a987d3837034ab40bf.wav  \n",
            "  inflating: /content/data/wavs/b2c6c8098c2cee2c8be6f4359fdacd38.wav  \n",
            "  inflating: /content/data/wavs/6ade8c5feb4d3773a1e5a17ce7428b9a.wav  \n",
            "  inflating: /content/data/wavs/916cbd959bee03642b48acea40c9a1a3.wav  \n",
            "  inflating: /content/data/wavs/9a087801813ba74c59a46bb800880749.wav  \n",
            "  inflating: /content/data/wavs/a77301d336324ba5ccd18eeb9415db7e.wav  \n",
            "  inflating: /content/data/wavs/8847c778724cd4321ee8bd15c1310735.wav  \n",
            "  inflating: /content/data/wavs/e49c8c36c6a7c4f47d71c2f0513c072e.wav  \n",
            "  inflating: /content/data/wavs/5cff4bbe0376efe3797ee2ac7a661510.wav  \n",
            "  inflating: /content/data/wavs/ca51f04a4cc52055a1c9b1b3501f7c6b.wav  \n",
            "  inflating: /content/data/wavs/76f99dc69a0900ea4c2dc409f743e781.wav  \n",
            "  inflating: /content/data/wavs/03d3840db3d41b2fabd0d3a33d6e6a7c.wav  \n",
            "  inflating: /content/data/wavs/31c10b8de3db855c07b937ca50994efa.wav  \n",
            "  inflating: /content/data/wavs/660e968a02ee325a5c41a49d16d20669.wav  \n",
            "  inflating: /content/data/wavs/76b5d8cdddd06c21a92e9e5fe5056004.wav  \n",
            "  inflating: /content/data/wavs/286aa0990d8ec8d0228be9936c023067.wav  \n",
            "  inflating: /content/data/wavs/019bf66d74987055f9a861e93d1aece5.wav  \n",
            "  inflating: /content/data/wavs/824c67baa999b877bdeb777db40dc123.wav  \n",
            "  inflating: /content/data/wavs/710f71cfc54feb9854a737118444175e.wav  \n",
            "  inflating: /content/data/wavs/6cf590e0636ef1b30a6e6f3f2e64be3e.wav  \n",
            "  inflating: /content/data/wavs/55fd22bd12fcdb8f852d8e785ef52b7b.wav  \n",
            "  inflating: /content/data/wavs/46ae0f2c01aad80ade4a216b0ddf150f.wav  \n",
            "  inflating: /content/data/wavs/a6d28a88c49dd1c8a3b1d2c55ea39993.wav  \n",
            "  inflating: /content/data/wavs/ef1b7701822ca2c7b047bdb3606a25b8.wav  \n",
            "  inflating: /content/data/wavs/5aa7b02c15700166d81baed501753291.wav  \n",
            "  inflating: /content/data/wavs/12306b0e58da1829921bf79a2132668b.wav  \n",
            "  inflating: /content/data/wavs/e9656a1181ee8db88738b947e6bd8fe6.wav  \n",
            "  inflating: /content/data/wavs/689369a21e3707d77c99d01bc123a8bb.wav  \n",
            "  inflating: /content/data/wavs/28bfd3f63a5d5fc3f75bffd2c8947932.wav  \n",
            "  inflating: /content/data/wavs/5cb6659ed61bccfd0ec199ce81502d39.wav  \n",
            "  inflating: /content/data/wavs/8cd45f5cc81c9de9ff2f80711407b08e.wav  \n",
            "  inflating: /content/data/wavs/6702e0a8a7825e9a9a8007dd1891890a.wav  \n",
            "  inflating: /content/data/wavs/f785ba880e1f478becc3e3efe241bd19.wav  \n",
            "  inflating: /content/data/wavs/ec4efbf2e648b4d422ccd6ff2ba76b80.wav  \n",
            "  inflating: /content/data/wavs/b66f56ec472b0fdab270fb5b9ed7ffed.wav  \n",
            "  inflating: /content/data/wavs/9717c1a0febfd9bfdcee1a244d7af6da.wav  \n",
            "  inflating: /content/data/wavs/084af0c942ac68d704d7166e57a6b7cc.wav  \n",
            "  inflating: /content/data/wavs/f841b961189b58c3439cb064d49011c9.wav  \n",
            "  inflating: /content/data/wavs/ec307983e36f6e55aa5facdfc179d4c5.wav  \n",
            "  inflating: /content/data/wavs/2d0bd77357a85c670bfdba51f59687dd.wav  \n",
            "  inflating: /content/data/wavs/405ef1d81be0b2ab9db902c4313dfda9.wav  \n",
            "  inflating: /content/data/wavs/7971da6aab75ea806bfb52b45cb0fb39.wav  \n",
            "  inflating: /content/data/wavs/8d43f1de0dcc65a9c0a702e8eefc62b1.wav  \n",
            "  inflating: /content/data/wavs/f3fe471c3366318f87d71bfc54b7c78c.wav  \n",
            "  inflating: /content/data/wavs/f605285458eb003728be07ec0554793a.wav  \n",
            "  inflating: /content/data/wavs/5377738999bcdbefba884c6d0f2bc164.wav  \n",
            "  inflating: /content/data/wavs/e8caa96067ee4d9bc5e02fd92b1261fa.wav  \n",
            "  inflating: /content/data/wavs/1cfa9ab737bf6e7b75a1e3adfd8c1948.wav  \n",
            "  inflating: /content/data/wavs/f600463907d5a63cdf1d374de71d3e86.wav  \n",
            "  inflating: /content/data/wavs/32b679a37278e3821092c84df5d9fcaf.wav  \n",
            "  inflating: /content/data/wavs/f256c5c325185b5a714c43a3f5827c84.wav  \n",
            "  inflating: /content/data/wavs/8e36b71d729709b5421b0224d56d53e4.wav  \n",
            "  inflating: /content/data/wavs/718120ec3328363e2596d3fabe7a2251.wav  \n",
            "  inflating: /content/data/wavs/a52077335ba97736d96840878c6feb0a.wav  \n",
            "  inflating: /content/data/wavs/a853d314e4f497ea76dbedce2df6eb9b.wav  \n",
            "  inflating: /content/data/wavs/9075c24cd9dc6c6f209d9ef56b4baf6c.wav  \n",
            "  inflating: /content/data/wavs/88779c22661d23326cb205eb60a410ff.wav  \n",
            "  inflating: /content/data/wavs/883e6cb472651d540dde926b3d9f6840.wav  \n",
            "  inflating: /content/data/wavs/c85d5d89e86bce783a3a1614afac929b.wav  \n",
            "  inflating: /content/data/wavs/f118dfb00f9f2ff1ad8ee9e5a783cd09.wav  \n",
            "  inflating: /content/data/wavs/c5abd14e051c200a856a3aef72e8517f.wav  \n",
            "  inflating: /content/data/wavs/8d6b687dc5860b398855daff26557b71.wav  \n",
            "  inflating: /content/data/wavs/d3f30167aab0ebac8996df1d5e21ca03.wav  \n",
            "  inflating: /content/data/wavs/841ec098babd2723f262ef5e9b623b86.wav  \n",
            "  inflating: /content/data/wavs/64ee4f072b1d8b24734986cc3a8265aa.wav  \n",
            "  inflating: /content/data/wavs/cb49c6cee918b958397add35be5c9455.wav  \n",
            "  inflating: /content/data/wavs/7ff7a24efd20dfed7f30db5e9d87357f.wav  \n",
            "  inflating: /content/data/wavs/8c070af874cc247e5953832b042e4b24.wav  \n",
            "  inflating: /content/data/wavs/53b0d0f7966577d4cf922673fdb66d28.wav  \n",
            "  inflating: /content/data/wavs/963c7cfb795e421316a9f5225dd2d66a.wav  \n",
            "  inflating: /content/data/wavs/2022b40cda36e7e4da73ebab2da96534.wav  \n",
            "  inflating: /content/data/wavs/cb05e4ef6f01104db19e9bc669250689.wav  \n",
            "  inflating: /content/data/wavs/0abbc32882e8965845aa8c70db458ede.wav  \n",
            "  inflating: /content/data/wavs/325c2b20196621335db13d85534f7371.wav  \n",
            "  inflating: /content/data/wavs/8bd533ecebfdd8760aa53d2b76fb0ad6.wav  \n",
            "  inflating: /content/data/wavs/f5c5ce20a785dafd8510980263ad5076.wav  \n",
            "  inflating: /content/data/wavs/3f9a70ca35091d6615003573bd69fc8e.wav  \n",
            "  inflating: /content/data/wavs/13f981cd0ed6199d3cddf4b2d89d86fa.wav  \n",
            "  inflating: /content/data/wavs/827ce59035ab84c8c7427c223f390892.wav  \n",
            "  inflating: /content/data/wavs/984b3d2cc780e2dbd80c5b26e0d840bd.wav  \n",
            "  inflating: /content/data/wavs/29a985cf5eb1419f6f6374458ecf47ff.wav  \n",
            "  inflating: /content/data/wavs/e318b28f64a057115b1d4cfd6656949e.wav  \n",
            "  inflating: /content/data/wavs/18538fa16dd68e968751e836128484b6.wav  \n",
            "  inflating: /content/data/wavs/a6cd710c538612e6bea76e40313abafd.wav  \n",
            "  inflating: /content/data/wavs/1f388e83d3580258e14319da14840bbf.wav  \n",
            "  inflating: /content/data/wavs/7f98650e1abf8627beda3c6f79ad5002.wav  \n",
            "  inflating: /content/data/wavs/ab855665eed5852b816c6eed6edafc94.wav  \n",
            "  inflating: /content/data/wavs/f9d8f07ef011dafb197cd16f5a0e334a.wav  \n",
            "  inflating: /content/data/wavs/a93d4265e20302461c54d4f965d58e05.wav  \n",
            "  inflating: /content/data/wavs/f711ec5d85e985e8fdbb333fc2e034ec.wav  \n",
            "  inflating: /content/data/wavs/56d425126b030fecd28f742c0edeb3fc.wav  \n",
            "  inflating: /content/data/wavs/9e1d942795bbb26daada9b942457f99e.wav  \n",
            "  inflating: /content/data/wavs/f1f6d1516e903a245c3647306e51f97c.wav  \n",
            "  inflating: /content/data/wavs/5093e5f378a2676afeca57ac50147680.wav  \n",
            "  inflating: /content/data/wavs/863ee1824cb320a5d48480310aa85d48.wav  \n",
            "  inflating: /content/data/wavs/caf69a7022b3e73cac3094bda6221923.wav  \n",
            "  inflating: /content/data/wavs/9fa77b2eef1bb174e442ea2b64cc34a3.wav  \n",
            "  inflating: /content/data/wavs/7efd7962ca2715a1058126eddc9224b0.wav  \n",
            "  inflating: /content/data/wavs/b31983554094a8f0544089b40b7a09c6.wav  \n",
            "  inflating: /content/data/wavs/92a651dfb7675950c096b19c46a7fe85.wav  \n",
            "  inflating: /content/data/wavs/82af3aa565c64b1c21492e0ed4a4469d.wav  \n",
            "  inflating: /content/data/wavs/01be0bd60764314cec72959881f2f0e2.wav  \n",
            "  inflating: /content/data/wavs/16952262a3dd327776852ebfbf2b8d76.wav  \n",
            "  inflating: /content/data/wavs/211b0b21707a121b8bc0bfe7efa4cdf7.wav  \n",
            "  inflating: /content/data/wavs/32b07bf62db3060909020047376e5d7e.wav  \n",
            "  inflating: /content/data/wavs/e7e120ed391b369cf24d6877cc1eabff.wav  \n",
            "  inflating: /content/data/wavs/e09c80d31d029df50330bc1e0d77a8de.wav  \n",
            "  inflating: /content/data/wavs/38c71c1b2ab8dfd7c9ea14f1ae059ceb.wav  \n",
            "  inflating: /content/data/wavs/a2226dd6d30c94a32a6ed6820e9afee6.wav  \n",
            "  inflating: /content/data/wavs/aeff458a6cf4b288dfd42a84a546bea5.wav  \n",
            "  inflating: /content/data/wavs/19e54c93cc14f0d41dcc59fcb5896bd9.wav  \n",
            "  inflating: /content/data/wavs/0b66c73dc36840abc2029ea2db9ccb3c.wav  \n",
            "  inflating: /content/data/wavs/96f8f398063ab6e032cb098a26f515f1.wav  \n",
            "  inflating: /content/data/wavs/f007a9dd9e469785090cc92a22594375.wav  \n",
            "  inflating: /content/data/wavs/ba0f21c01d22b31add0bd58b8b1307b0.wav  \n",
            "  inflating: /content/data/wavs/18794859e103091783f3eeb656980870.wav  \n",
            "  inflating: /content/data/wavs/83ecc4f9dd4c8a574b5f7e4a7190a518.wav  \n",
            "  inflating: /content/data/wavs/e97fabea95f74313cac330ad320d9ded.wav  \n",
            "  inflating: /content/data/wavs/a5424004e5f041aa36183561ba3ce29e.wav  \n",
            "  inflating: /content/data/wavs/eba6f5321552fb52e905067be05b5a92.wav  \n",
            "  inflating: /content/data/wavs/4980e046ea348f27f98df4cce7bfc3f9.wav  \n",
            "  inflating: /content/data/wavs/d533bef2ccb3413bcade8a3925a1c2b0.wav  \n",
            "  inflating: /content/data/wavs/a69bd5da19c8415397dea070d84f6640.wav  \n",
            "  inflating: /content/data/wavs/edf00191e07403f6f5552e61c5d6bbe4.wav  \n",
            "  inflating: /content/data/wavs/7c9267b83e638ae5ea13fdfb7c5a5ba7.wav  \n",
            "  inflating: /content/data/wavs/57b0addf3e4f89113f12bb9c5ba949c6.wav  \n",
            "  inflating: /content/data/wavs/97f45e8d66f3281860a5e1064e759e28.wav  \n",
            "  inflating: /content/data/wavs/963037f1cd70be5e91ea0482d80d6d88.wav  \n",
            "  inflating: /content/data/wavs/50e15bd21626bc7e4c3c5f405733614c.wav  \n",
            "  inflating: /content/data/wavs/320e1b86b8b1708be27665b13f1bf8ae.wav  \n",
            "  inflating: /content/data/wavs/391b3cd25a0fabe8c13cff4f5d5fb8ea.wav  \n",
            "  inflating: /content/data/wavs/d0c7f8fa7aced2c8e85ce553203d95ed.wav  \n",
            "  inflating: /content/data/wavs/89fdc3724a7fe3790364c66556ae0990.wav  \n",
            "  inflating: /content/data/wavs/a340e76fcf0883d631ae39882b116f25.wav  \n",
            "  inflating: /content/data/wavs/f247725003dad378674a512bf3eae78f.wav  \n",
            "  inflating: /content/data/wavs/9e5c6c11628b8d24a222bef26b04da0a.wav  \n",
            "  inflating: /content/data/wavs/b3617b586b73dd7c4f6587b8a8df14ad.wav  \n",
            "  inflating: /content/data/wavs/7839fec31ad580e9c70cbf9d655959f0.wav  \n",
            "  inflating: /content/data/wavs/f490c9ce0c08d4edcd6221b96f8baf71.wav  \n",
            "  inflating: /content/data/wavs/d15cf09029fa2f65535347a8fafadd02.wav  \n",
            "  inflating: /content/data/wavs/20b6536de1e2f0f109d54877e86a5fbc.wav  \n",
            "  inflating: /content/data/wavs/278c4f39882dfc647b51f7cd0699a43c.wav  \n",
            "  inflating: /content/data/wavs/ca5242a9a044b98f24256406d2bfa072.wav  \n",
            "  inflating: /content/data/wavs/d9ba0d7bbc95d11f1f2a5924e3f288a3.wav  \n",
            "  inflating: /content/data/wavs/5bdbdaa8a4f6cecefaf07f2ce6ec59ff.wav  \n",
            "  inflating: /content/data/wavs/70e5ab7173c3f8127ac7f5da08268991.wav  \n",
            "  inflating: /content/data/wavs/f67db2bf584787e4d64ffe698a4ce9fd.wav  \n",
            "  inflating: /content/data/wavs/f33415e5ed2dddf974d274e78bae4dc0.wav  \n",
            "  inflating: /content/data/wavs/aa38d19c81166f87ccd42d5c4ec15ae6.wav  \n",
            "  inflating: /content/data/wavs/e1071fc404d00f98ec92a73ff54741ee.wav  \n",
            "  inflating: /content/data/wavs/eea6a7d1f1dd62775abd020a24d221cf.wav  \n",
            "  inflating: /content/data/wavs/906aed6356df5adcbf55212907e4b021.wav  \n",
            "  inflating: /content/data/wavs/8e4612d5d993f040e915ebe1ed2d9a71.wav  \n",
            "  inflating: /content/data/wavs/1dbbbef3b25ddb3064c14bb6784a066d.wav  \n",
            "  inflating: /content/data/wavs/62596766fab45babe9fa60f3f1ebc26c.wav  \n",
            "  inflating: /content/data/wavs/c3f24ce8d1263f673d53058be1bd2fb4.wav  \n",
            "  inflating: /content/data/wavs/014325d39c59151f36dbac0dd41d1d4e.wav  \n",
            "  inflating: /content/data/wavs/d28cb04a4a67656a562842b6d3bd30e4.wav  \n",
            "  inflating: /content/data/wavs/43b9f02434518349f2f31a6bb4b99730.wav  \n",
            "  inflating: /content/data/wavs/14e3167ecc3c96f205d06079e23d468b.wav  \n",
            "  inflating: /content/data/wavs/851887a6d32e4a0be616db17606e101e.wav  \n",
            "  inflating: /content/data/wavs/560767df8d30cae39a0ad7341ab4983a.wav  \n",
            "  inflating: /content/data/wavs/bb131bda11d915dd4ab91398b07f4f79.wav  \n",
            "  inflating: /content/data/wavs/37251c3b6ce728d9729ceb0afa77b527.wav  \n",
            "  inflating: /content/data/wavs/bb8c55d1a72974ac708765fcdfbe3e57.wav  \n",
            "  inflating: /content/data/wavs/1da607c93b47ebfe895572bd95189253.wav  \n",
            "  inflating: /content/data/wavs/12980e1338676d422adf280a439966e9.wav  \n",
            "  inflating: /content/data/wavs/66828828ba5a4ab8809f4c0ccbf7313c.wav  \n",
            "  inflating: /content/data/wavs/e8552fc6cb5a3b25ad001577ffe9170b.wav  \n",
            "  inflating: /content/data/wavs/789fb3c3905353759c193ac4415c26c6.wav  \n",
            "  inflating: /content/data/wavs/3922eb7192673236f8aee97e88403880.wav  \n",
            "  inflating: /content/data/wavs/3564d5665a25fe64998829b657193dfe.wav  \n",
            "  inflating: /content/data/wavs/e1f413d3799e36924929c4a2739dbb2c.wav  \n",
            "  inflating: /content/data/wavs/5fbd0b9fdd2a21b90b3dccbfa680c92f.wav  \n",
            "  inflating: /content/data/wavs/e02e033f4d024686908a62ac03423a94.wav  \n",
            "  inflating: /content/data/wavs/d24af50effb53db88a2162e6f82bddaf.wav  \n",
            "  inflating: /content/data/wavs/39ff49f791dd6a3cc60d94845304e195.wav  \n",
            "  inflating: /content/data/wavs/ecdd77debaf779b2862a32d7834f7831.wav  \n",
            "  inflating: /content/data/wavs/5f5a56d8204496ef19bfce3fc96d21f1.wav  \n",
            "  inflating: /content/data/wavs/158f65c8e1246b128624db8ace4024f8.wav  \n",
            "  inflating: /content/data/wavs/458a12ccdadca892d54e614d3ad76572.wav  \n",
            "  inflating: /content/data/wavs/092fa21b4b538d74f76e81505631a74e.wav  \n",
            "  inflating: /content/data/wavs/96c77e548d8858855dfcc10108654644.wav  \n",
            "  inflating: /content/data/wavs/7ae55c982330770f1f02f845f8a2fe46.wav  \n",
            "  inflating: /content/data/wavs/759f475d2dbbca7e7740789ee794ed68.wav  \n",
            "  inflating: /content/data/wavs/c267b2ccc5002030fad943b191deb8b2.wav  \n",
            "  inflating: /content/data/wavs/5599ec750ac6fe322bf2715cb0f92a99.wav  \n",
            "  inflating: /content/data/wavs/eccff8c54181e2309dfc6792153b8606.wav  \n",
            "  inflating: /content/data/wavs/0b72e4566095efd5d349beab5e605a69.wav  \n",
            "  inflating: /content/data/wavs/93b5a0692df74d47b5911fd110af5c23.wav  \n",
            "  inflating: /content/data/wavs/f839a48c0fc6534efbfc734ccca3f0ae.wav  \n",
            "  inflating: /content/data/wavs/80982dbe5eea9fe454bf281aca413194.wav  \n",
            "  inflating: /content/data/wavs/775f6ddf3e70a3fee9012f6d3fb270d3.wav  \n",
            "  inflating: /content/data/wavs/dcdf32baa4b15737200242b5c5307d3e.wav  \n",
            "  inflating: /content/data/wavs/acba3105499713ae19a9dbc90261c426.wav  \n",
            "  inflating: /content/data/wavs/69000ca867af15c378a08ef1abbd586b.wav  \n",
            "  inflating: /content/data/wavs/c8bd2c936d4eef38a9b695439dcb6be0.wav  \n",
            "  inflating: /content/data/wavs/9443730e33795900278512608e76bc14.wav  \n",
            "  inflating: /content/data/wavs/948c62d2854000b84e297bd40f2a8e3d.wav  \n",
            "  inflating: /content/data/wavs/1568ba65040368abd7b1cee9443e7508.wav  \n",
            "  inflating: /content/data/wavs/ab7fbdea4860d9f4ed2c06bc205c1389.wav  \n",
            "  inflating: /content/data/wavs/b0982905f48794d7cc368c64805fd77c.wav  \n",
            "  inflating: /content/data/wavs/f804d53f344fde90f46e7e5eed650833.wav  \n",
            "  inflating: /content/data/wavs/f0238e157c0c402752e6f5f01baa0c84.wav  \n",
            "  inflating: /content/data/wavs/888773742425ffe345085102da8d30ac.wav  \n",
            "  inflating: /content/data/wavs/05aff0e39b3e8f9aef112e4015eb5249.wav  \n",
            "  inflating: /content/data/wavs/20be0acbb8b5317c5d82e912daf54d13.wav  \n",
            "  inflating: /content/data/wavs/58854d5c980041d8b17b74f66be29c9d.wav  \n",
            "  inflating: /content/data/wavs/4cab2adae6f234d1cc54150a0a4524e6.wav  \n",
            "  inflating: /content/data/wavs/7e02f7a7b410009a7f0281f8d656ddc4.wav  \n",
            "  inflating: /content/data/wavs/283a8e7e183c00b1ced1cfe71e73db99.wav  \n",
            "  inflating: /content/data/wavs/8643a9267a611c87bc80ea7bf40c80c3.wav  \n",
            "  inflating: /content/data/wavs/b63f75d0bddef4e0b70a7e710d5959e2.wav  \n",
            "  inflating: /content/data/wavs/6d473013ea8b6f5c3d6f90ee2899470c.wav  \n",
            "  inflating: /content/data/wavs/a8ccf28c0592fe1d8fbae87f02717ffc.wav  \n",
            "  inflating: /content/data/wavs/94bccc5e639e9ae4e84823826e80fa52.wav  \n",
            "  inflating: /content/data/wavs/5c6e6e86ef30cb4c6dcc31ce25d9a234.wav  \n",
            "  inflating: /content/data/wavs/26a035b23e636f6fe46dac5e30928b67.wav  \n",
            "  inflating: /content/data/wavs/d5719468ec1f4c64f577d2b28fee4217.wav  \n",
            "  inflating: /content/data/wavs/32fd62759e253f06e293542292ee1383.wav  \n",
            "  inflating: /content/data/wavs/fa92e5c7995c39917ee3b9ad0f15722a.wav  \n",
            "  inflating: /content/data/wavs/36d896737303e84ee36a7e2e456aa533.wav  \n",
            "  inflating: /content/data/wavs/2030a5e5a8224fbd0ceed395bbfb01ad.wav  \n",
            "  inflating: /content/data/wavs/f62e84059d8cb85be3eedda97a0888a6.wav  \n",
            "  inflating: /content/data/wavs/c3825efea57bf04ec6ce3036316e69ff.wav  \n",
            "  inflating: /content/data/wavs/ab7a7ad1742fdeae7155fb11781f0514.wav  \n",
            "  inflating: /content/data/wavs/9b846a29a1f3de2807b983903aa664bc.wav  \n",
            "  inflating: /content/data/wavs/d993b8b97425a850d481dd0ef368846c.wav  \n",
            "  inflating: /content/data/wavs/86a80f2b52df9ce24a294f3030e415fd.wav  \n",
            "  inflating: /content/data/wavs/035f1eecbd7dad413ed3a063346df19c.wav  \n",
            "  inflating: /content/data/wavs/53ea283eab41eab045027fbc02fad2aa.wav  \n",
            "  inflating: /content/data/wavs/eb9f1853e4347f616ac43d59e90d249d.wav  \n",
            "  inflating: /content/data/wavs/9278dc20c56b814429121c4db0d3b423.wav  \n",
            "  inflating: /content/data/wavs/d9284d71682708d55dfde97f756d0adb.wav  \n",
            "  inflating: /content/data/wavs/9613cdc29b016154705bb8d0b16ee4fb.wav  \n",
            "  inflating: /content/data/wavs/33de58e0e4399701455a6cb39acd731d.wav  \n",
            "  inflating: /content/data/wavs/d9d7e5bdd876a618ad93598b3d71c280.wav  \n",
            "  inflating: /content/data/wavs/c0508c619755bee48bd9e0bdf5659ae6.wav  \n",
            "  inflating: /content/data/wavs/6d782b785d94363d83fb7980ffdc7cf0.wav  \n",
            "  inflating: /content/data/wavs/784e5b2632bef3191e70d2b8c9c37a72.wav  \n",
            "  inflating: /content/data/wavs/06e9f3fecdaeb5d83d1ee128bbd8ef96.wav  \n",
            "  inflating: /content/data/wavs/80dc48aea67aca6da6cdbd10358a4d42.wav  \n",
            "  inflating: /content/data/wavs/beb7e1cdf83d134d20fae97ac5f0c0e6.wav  \n",
            "  inflating: /content/data/wavs/e286f717a28df992dbc10bf2271d3933.wav  \n",
            "  inflating: /content/data/wavs/e3f304085c02e888d325306266de6294.wav  \n",
            "  inflating: /content/data/wavs/78becec8b79e4c48bda1f0e31ca0abc5.wav  \n",
            "  inflating: /content/data/wavs/35ea7b88d348a944e96ec53e7e70e692.wav  \n",
            "  inflating: /content/data/wavs/3596fbe5e1fdecdaa695bfd32ad6bdc7.wav  \n",
            "  inflating: /content/data/wavs/e2c0d5814de9513c169004549761aec8.wav  \n",
            "  inflating: /content/data/wavs/f265c575ac5afa4e1f3e8911e5f1dfba.wav  \n",
            "  inflating: /content/data/wavs/9ab913e46f61f5c6ace461af1daa2ec4.wav  \n",
            "  inflating: /content/data/wavs/dce88d2522abbf0347b93ae042233518.wav  \n",
            "  inflating: /content/data/wavs/7cc3908f498a85d5444fba25a9400aad.wav  \n",
            "  inflating: /content/data/wavs/9b3655b2e46b12ad563f752678efa618.wav  \n",
            "  inflating: /content/data/wavs/4a4f5f9dae52ba5af6f3440de549031a.wav  \n",
            "  inflating: /content/data/wavs/0242ac6071311d52a84474d5f1063cde.wav  \n",
            "  inflating: /content/data/wavs/43d6454cf11ccc35d0315b209eb35f6b.wav  \n",
            "  inflating: /content/data/wavs/a58d5e088589a8296a31451e24425de4.wav  \n",
            "  inflating: /content/data/wavs/485aa8fd76b99aa44f5b9f4bdc5f02e0.wav  \n",
            "  inflating: /content/data/wavs/48357fb35409adc7e67d3604ad3d5e4a.wav  \n",
            "  inflating: /content/data/wavs/cfdc6ad64669118b65eb6ae51d6ef1a9.wav  \n",
            "  inflating: /content/data/wavs/2e174d3bb3784aad8c919c8f8c1b517a.wav  \n",
            "  inflating: /content/data/wavs/f3e77be61692c95952e544607cb5dde3.wav  \n",
            "  inflating: /content/data/wavs/78ea0a8dcd89b9593037527f7a02b006.wav  \n",
            "  inflating: /content/data/wavs/0664c66a1dab59bee107ba92748288d0.wav  \n",
            "  inflating: /content/data/wavs/b4c599a3195084c3eae1a31db3966b01.wav  \n",
            "  inflating: /content/data/wavs/132b2b60a32f646e7c99d62e470a5494.wav  \n",
            "  inflating: /content/data/wavs/33d7f2f1195cb3ff03523811a8f783a7.wav  \n",
            "  inflating: /content/data/wavs/2dba030da1d663ecff0b4601d292efaa.wav  \n",
            "  inflating: /content/data/wavs/b5d2b35dc001a88beaf7a534f6914017.wav  \n",
            "  inflating: /content/data/wavs/6bf78d578faaa09908c8188af5ff0eed.wav  \n",
            "  inflating: /content/data/wavs/60ed0cb7756b1d0400f1eaddb0085efd.wav  \n",
            "  inflating: /content/data/wavs/d13092c579414561c188e4e229691f55.wav  \n",
            "  inflating: /content/data/wavs/348803b7a3431160794fe8900f6b20d8.wav  \n",
            "  inflating: /content/data/wavs/b8386a738196284463893354fd63bad4.wav  \n",
            "  inflating: /content/data/wavs/172064dff513c7bd8614256012b6e505.wav  \n",
            "  inflating: /content/data/wavs/ff5833c17b67df3b116cff21dea90745.wav  \n",
            "  inflating: /content/data/wavs/d8c7736b2f4c9824e145efc3431cb5e2.wav  \n",
            "  inflating: /content/data/wavs/4bf4334ffdef85629f648de71ecb57c0.wav  \n",
            "  inflating: /content/data/wavs/d44277d8cd52c7a8ff5627803a31c913.wav  \n",
            "  inflating: /content/data/wavs/11b090430f493625b0e2e81f5bf99e93.wav  \n",
            "  inflating: /content/data/wavs/99adfa3f611108b4ab8e812e22360f9d.wav  \n",
            "  inflating: /content/data/wavs/4e6bff19fb49e3669fa4da866cf49a76.wav  \n",
            "  inflating: /content/data/wavs/b88edfa87eace1632c266cca94693462.wav  \n",
            "  inflating: /content/data/wavs/bae2386cb3052c08e5ee762492b17378.wav  \n",
            "  inflating: /content/data/wavs/4e7b2ed0542aa093e2b2551f1b1185be.wav  \n",
            "  inflating: /content/data/wavs/7d7c9b9047b32918b5e6257f317944bc.wav  \n",
            "  inflating: /content/data/wavs/10517d79cd22aeb605eb04e5d7e02bd2.wav  \n",
            "  inflating: /content/data/wavs/db36957fcdab61439fc4e2af61a53b8b.wav  \n",
            "  inflating: /content/data/wavs/5ed6e021b37cc6b3cfbdb97082b94469.wav  \n",
            "  inflating: /content/data/wavs/8e3438f9ceb0c4cb3a222cb5dac6e2e6.wav  \n",
            "  inflating: /content/data/wavs/0dccf666fabd2d4b0be5e7c9c6418f10.wav  \n",
            "  inflating: /content/data/wavs/27228cb3b6f2921299a508fdf5f452b9.wav  \n",
            "  inflating: /content/data/wavs/a6a5027ca0fd6a94e0e2a76c8f7ead94.wav  \n",
            "  inflating: /content/data/wavs/f56712afe9cfe2692d1ef8e4388342f1.wav  \n",
            "  inflating: /content/data/wavs/6d9dfd3e1872aa671f0640eddafd6e3d.wav  \n",
            "  inflating: /content/data/wavs/25e870fb099c6a691eb6cde8f77e061e.wav  \n",
            "  inflating: /content/data/wavs/777cc87c38434fdefebc5c3d29b4959f.wav  \n",
            "  inflating: /content/data/wavs/18f88472ca93127793e457c7f09224bf.wav  \n",
            "  inflating: /content/data/wavs/b8c68c660fb7bef37dfde2d699a3b189.wav  \n",
            "  inflating: /content/data/wavs/eaf4df685b61f91f6d009c2bcc3ffc43.wav  \n",
            "  inflating: /content/data/wavs/7efd564d80d83b9b2c76bd9bbac0f2d2.wav  \n",
            "  inflating: /content/data/wavs/928da0c5a7fb32d1068f04731f41f281.wav  \n",
            "  inflating: /content/data/wavs/476e1b8bdeeb9d7b1e89a1fcaa797dea.wav  \n",
            "  inflating: /content/data/wavs/0610c9bfdabf400336a1de247beb4f78.wav  \n",
            "  inflating: /content/data/wavs/0c37b23ba3151b14fb5ea07e77536339.wav  \n",
            "  inflating: /content/data/wavs/d4b71363761bbb8697388d45c719f712.wav  \n",
            "  inflating: /content/data/wavs/10292c1c9ee63605a3ba3c5a89a70a2c.wav  \n",
            "  inflating: /content/data/wavs/78a2dee717868d072307752e1a3d2ff4.wav  \n",
            "  inflating: /content/data/wavs/8408227b4e6150699d67a705dc64b3b4.wav  \n",
            "  inflating: /content/data/wavs/ee8f955697739a77c053d3c00661e7c8.wav  \n",
            "  inflating: /content/data/wavs/8dff2d264ffd4fc2a5ef29144d7d4325.wav  \n",
            "  inflating: /content/data/wavs/478b48da1830020e41eb396542656652.wav  \n",
            "  inflating: /content/data/wavs/8d6310610f63245f538c2634a0274483.wav  \n",
            "  inflating: /content/data/wavs/8e48cc19f789d6e5b72dabfc5f6d3b68.wav  \n",
            "  inflating: /content/data/wavs/b1908e4c7dc62d977ee3092e73867447.wav  \n",
            "  inflating: /content/data/wavs/304a86f63346d5a60221b112a8fd35c5.wav  \n",
            "  inflating: /content/data/wavs/eb690aa4817e54488eac331a1c52f376.wav  \n",
            "  inflating: /content/data/wavs/b8aa1c2876dcb64198e43689fef13b4a.wav  \n",
            "  inflating: /content/data/wavs/749a507bee1c81f8197856a2090f470a.wav  \n",
            "  inflating: /content/data/wavs/fdd7bb0c87dcd369f024235a859230db.wav  \n",
            "  inflating: /content/data/wavs/22d6cb2a36632a457e0160f33d132389.wav  \n",
            "  inflating: /content/data/wavs/4640602b16f33dd8f3d9e617c402f9a4.wav  \n",
            "  inflating: /content/data/wavs/c38f8ee619a27bd5d19d752b8d1351e4.wav  \n",
            "  inflating: /content/data/wavs/95bcc7cc0c351f30461d074e94c87a32.wav  \n",
            "  inflating: /content/data/wavs/910e244815c99aadb9e42883d8ef1c88.wav  \n",
            "  inflating: /content/data/wavs/1151bfdb150449a3276bc8eba6bbf059.wav  \n",
            "  inflating: /content/data/wavs/f156b5f55b809c34d4af1491c13582cf.wav  \n",
            "  inflating: /content/data/wavs/53aac75e5d8e79e5e1e4a84d859194db.wav  \n",
            "  inflating: /content/data/wavs/01a42f9c3945d7cca9f65abea97b914f.wav  \n",
            "  inflating: /content/data/wavs/98100a5e990ff5a339cdd663cadd2104.wav  \n",
            "  inflating: /content/data/wavs/6ccc7ec88d1aef399c14032c91c39892.wav  \n",
            "  inflating: /content/data/wavs/91e36b6cb1f1219868620ea9d111daa3.wav  \n",
            "  inflating: /content/data/wavs/d76a1795cd7b0918dcc69e8ed18bdc82.wav  \n",
            "  inflating: /content/data/wavs/0437197fa042b4560a24b15044723b81.wav  \n",
            "  inflating: /content/data/wavs/29eba95b980c0ad042bca82f1540caf2.wav  \n",
            "  inflating: /content/data/wavs/67aded327b681308619349d265604bf3.wav  \n",
            "  inflating: /content/data/wavs/9340b4289c38342a6404f901ebbd503b.wav  \n",
            "  inflating: /content/data/wavs/1d32f6ee62fdf9a0be1762fc851b6730.wav  \n",
            "  inflating: /content/data/wavs/b896295384ed84c87e460954871d8e6b.wav  \n",
            "  inflating: /content/data/wavs/d76928da8055c27e6d07e645f3fb9bdc.wav  \n",
            "  inflating: /content/data/wavs/aa293cdbbb64db9c0adc31956fedb8e4.wav  \n",
            "  inflating: /content/data/wavs/32257d2bca46abbed604065308a7e925.wav  \n",
            "  inflating: /content/data/wavs/5f64032da1ccfadec3d4e3fd8ef8a618.wav  \n",
            "  inflating: /content/data/wavs/9c7d2e073c7f7d6ddff537e58da7daea.wav  \n",
            "  inflating: /content/data/wavs/a6605c9dfc554157f534037e39bf9648.wav  \n",
            "  inflating: /content/data/wavs/c773677c398824c76174e4fb0227c562.wav  \n",
            "  inflating: /content/data/wavs/bbf7ff35ae4d9f26f96fa98271ab7338.wav  \n",
            "  inflating: /content/data/wavs/244f0eb51fac9d1a6d6ff313b20c7c7d.wav  \n",
            "  inflating: /content/data/wavs/beddd7ca57d27ef8386e20fa215e3bb2.wav  \n",
            "  inflating: /content/data/wavs/fa60c873e77337937ffdaf6f5ebd7368.wav  \n",
            "  inflating: /content/data/wavs/f7283e1ab30002596628894d2320bef9.wav  \n",
            "  inflating: /content/data/wavs/b08c2996dc8889a2be9b6c7f1170d16d.wav  \n",
            "  inflating: /content/data/wavs/4255c5309b18f745097644dcb1b716ba.wav  \n",
            "  inflating: /content/data/wavs/0d182667d12c16f2090c17775d1be7dd.wav  \n",
            "  inflating: /content/data/wavs/109da918ed4c82e9bcb33cda03ac6f0a.wav  \n",
            "  inflating: /content/data/wavs/8c7254c47b150f711c148279ec047677.wav  \n",
            "  inflating: /content/data/wavs/e30879285ef8f9117d80316db356b3fa.wav  \n",
            "  inflating: /content/data/wavs/32cd669d38ec38129f30ac4fe4c16739.wav  \n",
            "  inflating: /content/data/wavs/a949e20d27286fbdef26464e358f050a.wav  \n",
            "  inflating: /content/data/wavs/2266391715a6fe63a8abcc0ced6cc843.wav  \n",
            "  inflating: /content/data/wavs/4bad2fdc0501c933007dda039a72be1f.wav  \n",
            "  inflating: /content/data/wavs/9b199faef8fca373666668fb544d8434.wav  \n",
            "  inflating: /content/data/wavs/f8f9e595005334c2d63ac86fdfd713aa.wav  \n",
            "  inflating: /content/data/wavs/c33796e22db951ccd53fc1c7b1356206.wav  \n",
            "  inflating: /content/data/wavs/2fe77a60490947234fe79cc18db6e3b4.wav  \n",
            "  inflating: /content/data/wavs/a7ff3853cda0f184d6b132b2084cd089.wav  \n",
            "  inflating: /content/data/wavs/0ced46fac1676cb027ef3ad02323487a.wav  \n",
            "  inflating: /content/data/wavs/a1128ab6a5a321742f2345825c7e4a70.wav  \n",
            "  inflating: /content/data/wavs/68d5a31d54e6590a3bc60a0d38a51f14.wav  \n",
            "  inflating: /content/data/wavs/1b0e7f6e05e3827c766a8cb387ed79cd.wav  \n",
            "  inflating: /content/data/wavs/5a1af397a610eb7a3f19f516278da9a1.wav  \n",
            "  inflating: /content/data/wavs/ed6ffbf02ef403a9e674454457e44cf2.wav  \n",
            "  inflating: /content/data/wavs/35f8b5fd6d9955c71356041fb2524215.wav  \n",
            "  inflating: /content/data/wavs/6120738d9ace340a27e87b6817eccb86.wav  \n",
            "  inflating: /content/data/wavs/11db0798b9a622573f148d44aa03c666.wav  \n",
            "  inflating: /content/data/wavs/b7bf53adf61bbfa771e7f724ef05bf94.wav  \n",
            "  inflating: /content/data/wavs/6be18d45eb797f7696251b47a60e4281.wav  \n",
            "  inflating: /content/data/wavs/def44e0c61f52210cd7ba505b3b85304.wav  \n",
            "  inflating: /content/data/wavs/88ebafb4b38590b83315093519fd00af.wav  \n",
            "  inflating: /content/data/wavs/85e5020d5e09a17568a3b0c1a217cec6.wav  \n",
            "  inflating: /content/data/wavs/edc8176e64691912dc4edc074ef174a7.wav  \n",
            "  inflating: /content/data/wavs/3a511a5a295c6e1b7c1ab804e63c1a1d.wav  \n",
            "  inflating: /content/data/wavs/f57a9c0646041e22bf9d16ae2ef24548.wav  \n",
            "  inflating: /content/data/wavs/04eda949f2b678eb9bcb07f631cf5bcb.wav  \n",
            "  inflating: /content/data/wavs/841dbc72fe976719fac0b80fdd9dbf4e.wav  \n",
            "  inflating: /content/data/wavs/a8c42818eb356a119e2fed1ed1b722be.wav  \n",
            "  inflating: /content/data/wavs/070c6895a020ca64d58e6d2c462851a0.wav  \n",
            "  inflating: /content/data/wavs/7d68efe8cffabf793875ec128c9f8034.wav  \n",
            "  inflating: /content/data/wavs/000e9484e15057da099a9e778f2b8261.wav  \n",
            "  inflating: /content/data/wavs/8bd734e1cf511d9c0d5bab1e5c09896e.wav  \n",
            "  inflating: /content/data/wavs/97649db80aae007eb2c92fded91083a4.wav  \n",
            "  inflating: /content/data/wavs/062103527f75fe495ce635f4f5c32c02.wav  \n",
            "  inflating: /content/data/wavs/1e5f702666a745b037178faed47157a7.wav  \n",
            "  inflating: /content/data/wavs/6c0effb5161441e6e1ee9a3fbe10c355.wav  \n",
            "  inflating: /content/data/wavs/1486f90d4bd056407f7c4b3622736147.wav  \n",
            "  inflating: /content/data/wavs/4ecccd9da52f4d655244467f6403f4b4.wav  \n",
            "  inflating: /content/data/wavs/266affece05fcca6a4f564710ddbb6dc.wav  \n",
            "  inflating: /content/data/wavs/f23dcc32037a199624db5307e01d61ad.wav  \n",
            "  inflating: /content/data/wavs/602c2d2a258f58f3cb6aefc2e52a67ea.wav  \n",
            "  inflating: /content/data/wavs/3cc90ab4221e54b8005c50a00bce87a0.wav  \n",
            "  inflating: /content/data/wavs/68391bdf56d4c0d31b86e0867a33dfd2.wav  \n",
            "  inflating: /content/data/wavs/bedd0b611f37c4e78905e3a75c93703b.wav  \n",
            "  inflating: /content/data/wavs/0c3d90294375f1d646a29a4f4e8d32bd.wav  \n",
            "  inflating: /content/data/wavs/103bb2b91b0ee3dd67992f332e14d680.wav  \n",
            "  inflating: /content/data/wavs/58bc412ef191e2a5013cf36951247293.wav  \n",
            "  inflating: /content/data/wavs/fbaa21fb6e5c2082c816dbecb72f61fb.wav  \n",
            "  inflating: /content/data/wavs/dcf19a0ecd766133018c6c25ec0b5cd1.wav  \n",
            "  inflating: /content/data/wavs/b80d4819f1f155f0488ad7371c8a584b.wav  \n",
            "  inflating: /content/data/wavs/9c95401defa11f864b8dd9e076054793.wav  \n",
            "  inflating: /content/data/wavs/d3fce96b89f232a2bf6cf856697573b8.wav  \n",
            "  inflating: /content/data/wavs/5c986abe4e0bbc9158666bb2a43a7cb4.wav  \n",
            "  inflating: /content/data/wavs/ee87a7000732bca5585174caca9d7270.wav  \n",
            "  inflating: /content/data/wavs/8ca265a6c9ffe3cf6b685d369019d05d.wav  \n",
            "  inflating: /content/data/wavs/80beba1947df3178292bc4f22b249f72.wav  \n",
            "  inflating: /content/data/wavs/bf2e8b75a65cdd04fddb0f4d3e992925.wav  \n",
            "  inflating: /content/data/wavs/6d7ae245676d12180cdd16e91e29f1f0.wav  \n",
            "  inflating: /content/data/wavs/e37d83611e8c60e66794bc173c0ad34f.wav  \n",
            "  inflating: /content/data/wavs/30fb3bb9ce59d7d46a4f393709e9decf.wav  \n",
            "  inflating: /content/data/wavs/07222df7e3b3dcab3572404ab6b2dd86.wav  \n",
            "  inflating: /content/data/wavs/d9734496c6637f55933d184afb4042ae.wav  \n",
            "  inflating: /content/data/wavs/812baace6c45ea9c64217a3bec5bc681.wav  \n",
            "  inflating: /content/data/wavs/65032340fc18c667e96dec579193d489.wav  \n",
            "  inflating: /content/data/wavs/4726f5987540573d06f66ebbcd459db1.wav  \n",
            "  inflating: /content/data/wavs/3a44983774f1e1cb15e0411ab672163e.wav  \n",
            "  inflating: /content/data/wavs/3f819d489918a496edb0d04fba40365a.wav  \n",
            "  inflating: /content/data/wavs/6da94a157746a3c3d2d4945a25d420d8.wav  \n",
            "  inflating: /content/data/wavs/cb79f835041983d90aa5e0c4046656ef.wav  \n",
            "  inflating: /content/data/wavs/91014118245bd7a290af0c16b5bfb82e.wav  \n",
            "  inflating: /content/data/wavs/63950a66fd156d91060f475a771438da.wav  \n",
            "  inflating: /content/data/wavs/8350f713caa16220832d9835c5e8d7a8.wav  \n",
            "  inflating: /content/data/wavs/b756360eb3051d5b2d3fadedb9e4c9eb.wav  \n",
            "  inflating: /content/data/wavs/b531e746d393b3879c9e80c63f30fddf.wav  \n",
            "  inflating: /content/data/wavs/0624c4d962edec1b2d8fb83f47198cb6.wav  \n",
            "  inflating: /content/data/wavs/f449ccd375624dfe709d352bfbe4c7bc.wav  \n",
            "  inflating: /content/data/wavs/65297322cca316591e4485246b3a8b15.wav  \n",
            "  inflating: /content/data/wavs/c996531f0554517539fbaf64a67013d9.wav  \n",
            "  inflating: /content/data/wavs/f95cd40328301ccdfdb49ae0ad9fe653.wav  \n",
            "  inflating: /content/data/wavs/2f20265af06cd9aa80fbbb8909c1f750.wav  \n",
            "  inflating: /content/data/wavs/3260d251111c4c8c78f8c5a41887cf04.wav  \n",
            "  inflating: /content/data/wavs/ba520f86fea0f4e812d3d365e651fcc1.wav  \n",
            "  inflating: /content/data/wavs/b86dd5b89002a704ca9549b27aeedbee.wav  \n",
            "  inflating: /content/data/wavs/1e0de58229551ea79e1906b0d465c5bb.wav  \n",
            "  inflating: /content/data/wavs/f7244904fe814f7f37fb88a605ff6bb6.wav  \n",
            "  inflating: /content/data/wavs/2ed41c511cb3ae529b6fc9cdb65a031e.wav  \n",
            "  inflating: /content/data/wavs/877f54e5d60b7298478a57675cc0a6aa.wav  \n",
            "  inflating: /content/data/wavs/7e00d14b37651e1f23f545828092d779.wav  \n",
            "  inflating: /content/data/wavs/d4f523ab01aa48f5e14a1187bcbded64.wav  \n",
            "  inflating: /content/data/wavs/0670047121773a281f7918dce91e9914.wav  \n",
            "  inflating: /content/data/wavs/0def22dd7a2780ca3691b90e60acd646.wav  \n",
            "  inflating: /content/data/wavs/adb0ec1374efd4a04ec873b45dbb83b9.wav  \n",
            "  inflating: /content/data/wavs/9bf05e2777b1ccd7ee5f0977ab08add3.wav  \n",
            "  inflating: /content/data/wavs/bb3c69faa8aa75107993177d709fd9b3.wav  \n",
            "  inflating: /content/data/wavs/bafbb0d798e6ddc2a93ad44cea1a0a28.wav  \n",
            "  inflating: /content/data/wavs/0f6241c2a452c2d28471a51d8cedfcb2.wav  \n",
            "  inflating: /content/data/wavs/f9b6d0a811dce759bfdd9df1b521ea69.wav  \n",
            "  inflating: /content/data/wavs/e134b73e1a0d654d61debb6748fc47ad.wav  \n",
            "  inflating: /content/data/wavs/141c324f252a947f093d044d7c32932d.wav  \n",
            "  inflating: /content/data/wavs/0dd9d9220662eb908d0011039c9fb621.wav  \n",
            "  inflating: /content/data/wavs/f7ac3ec4e902ea42db9f7104d763e023.wav  \n",
            "  inflating: /content/data/wavs/b874fff9dcd3fcda2eb099ef1a87f74c.wav  \n",
            "  inflating: /content/data/wavs/e9d30d881f8f988cb8614a8824acd93d.wav  \n",
            "  inflating: /content/data/wavs/79b3c2227fb9c5e69a053d2a276de51e.wav  \n",
            "  inflating: /content/data/wavs/15c9bc6d59e0c960245f88cea274db18.wav  \n",
            "  inflating: /content/data/wavs/ebe8cef0c004b44e190870decb538e7d.wav  \n",
            "  inflating: /content/data/wavs/fe9ca3df1472fbf9145479987fc10068.wav  \n",
            "  inflating: /content/data/wavs/75bd10db8898f404083f18b22b25682e.wav  \n",
            "  inflating: /content/data/wavs/a8ea7be7066a1041e9a569c094a66746.wav  \n",
            "  inflating: /content/data/wavs/8defdb8052a1c93b2349d526a3f26765.wav  \n",
            "  inflating: /content/data/wavs/f435ce83b502c627d699ef75302fef7f.wav  \n",
            "  inflating: /content/data/wavs/6bc9b28d7cea7cd21152dcc5dc63cfa7.wav  \n",
            "  inflating: /content/data/wavs/68262ce79f6910b0f2ed7cef008016cb.wav  \n",
            "  inflating: /content/data/wavs/e15eb240d8f0f89ae22eaeb6004eeb35.wav  \n",
            "  inflating: /content/data/wavs/dbc68b1bd7a768375d17e0a31d61abaa.wav  \n",
            "  inflating: /content/data/wavs/35e42436c079ee80dc0698a0715c8f7e.wav  \n",
            "  inflating: /content/data/wavs/6854ae2743df5cfacef104778dc84547.wav  \n",
            "  inflating: /content/data/wavs/e1f5fe5e66ee4f0a3d6efc72b7d0df41.wav  \n",
            "  inflating: /content/data/wavs/efd7a2afd29e899c223abdc836d796ad.wav  \n",
            "  inflating: /content/data/wavs/c709ad3372a7cb774f06faa2f0c57b84.wav  \n",
            "  inflating: /content/data/wavs/760d012c2f2dae2dbf68661e6d0bfedd.wav  \n",
            "  inflating: /content/data/wavs/01ee7d25c6782a292de2aad8d9c8baf0.wav  \n",
            "  inflating: /content/data/wavs/83c3254a56329ffbfcbefa48f3f69688.wav  \n",
            "  inflating: /content/data/wavs/71ce78be36b1e55d73255b15f8d1e504.wav  \n",
            "  inflating: /content/data/wavs/edbc0c55e8c42a91ee8013fba7e01451.wav  \n",
            "  inflating: /content/data/wavs/793c5088d5f0911a6edddc6c1326b7b8.wav  \n",
            "  inflating: /content/data/wavs/d3b9f2a8e9c9e8e5a26e7c3527d7dff0.wav  \n",
            "  inflating: /content/data/wavs/2c257405379897ab2864d314b69e5577.wav  \n",
            "  inflating: /content/data/wavs/c2199ce4955fd0f13f9eca4995e67a5c.wav  \n",
            "  inflating: /content/data/wavs/0d3b30db9ec153f509820ce53d08203e.wav  \n",
            "  inflating: /content/data/wavs/29ee0670a1c5159e1f6ab7dc98f18485.wav  \n",
            "  inflating: /content/data/wavs/5405b63e643b9437f3520f00cc6852ff.wav  \n",
            "  inflating: /content/data/wavs/6ecb12eca5deb5aaa50a2bef343a5d44.wav  \n",
            "  inflating: /content/data/wavs/bf413125ec98a6fa680802cf1a65952a.wav  \n",
            "  inflating: /content/data/wavs/be084503bc28b15dab8bac0f1ac2077c.wav  \n",
            "  inflating: /content/data/wavs/bae99d63815c8910f39fb3cd2e47ee87.wav  \n",
            "  inflating: /content/data/wavs/e498baa376f27a0bc67d706a1bd5b8fb.wav  \n",
            "  inflating: /content/data/wavs/8adb7523586345fdb290b71ef2ff1663.wav  \n",
            "  inflating: /content/data/wavs/da864bd900ba78e1ac57b01dda457a28.wav  \n",
            "  inflating: /content/data/wavs/b94d005d441e12b4ec7d2536a46566e3.wav  \n",
            "  inflating: /content/data/wavs/3bec513687a040dd5a6f0f048add726b.wav  \n",
            "  inflating: /content/data/wavs/216c16fdf77fa5f8c192285132055b00.wav  \n",
            "  inflating: /content/data/wavs/a8299fb35ff44cafe2c909c568d619e8.wav  \n",
            "  inflating: /content/data/wavs/3ee22ad47aaeb3a0c5cec7846e09dcaa.wav  \n",
            "  inflating: /content/data/wavs/dbec65b23ec23e4ea9020125d1e3efde.wav  \n",
            "  inflating: /content/data/wavs/c5850a9ea4330f72155abb552ae29d9b.wav  \n",
            "  inflating: /content/data/wavs/a7aebaefe1600b61d2cc744b48d470f4.wav  \n",
            "  inflating: /content/data/wavs/d6c3a05e1d91aa1020ab016182134e73.wav  \n",
            "  inflating: /content/data/wavs/04ed0bfc8d11abce8f552fa08d25ce81.wav  \n",
            "  inflating: /content/data/wavs/d9f0699671f540ba8b04ef0f09987649.wav  \n",
            "  inflating: /content/data/wavs/538875f2ed8767238109b199548a4cd3.wav  \n",
            "  inflating: /content/data/wavs/5232dbd1ce590edb646ef65b073bcaf9.wav  \n",
            "  inflating: /content/data/wavs/fde00b83fb3273bd4fd0edc22af614bf.wav  \n",
            "  inflating: /content/data/wavs/10e0de735d24d25a1825358357af27b0.wav  \n",
            "  inflating: /content/data/wavs/bff88a1e61a1beb5ae2d7864e514a21b.wav  \n",
            "  inflating: /content/data/wavs/f3c8145046426faa9005936d30e97c94.wav  \n",
            "  inflating: /content/data/wavs/6376db6dae78a0cde9acb0f1f568e6b7.wav  \n",
            "  inflating: /content/data/wavs/5acb8ff2c2fcf4a1fefcf617112ff442.wav  \n",
            "  inflating: /content/data/wavs/005d478748afc1f7147c81b00e5ba610.wav  \n",
            "  inflating: /content/data/wavs/caef0259cf014080c79eed1bd36bef5c.wav  \n",
            "  inflating: /content/data/wavs/68d040f991bc57a96e42572de0e431e6.wav  \n",
            "  inflating: /content/data/wavs/5959fbc4d71c2c1ba7cf5f13dc104f97.wav  \n",
            "  inflating: /content/data/wavs/662ec1512ff26f8b3e9483bf04eb541c.wav  \n",
            "  inflating: /content/data/wavs/d8ec2324e888390d44335c3882f572e8.wav  \n",
            "  inflating: /content/data/wavs/a66fe4ffee79acfd14f7e8b3819cedc3.wav  \n",
            "  inflating: /content/data/wavs/6cc2d35b26592e835e9e0abe42c7ca91.wav  \n",
            "  inflating: /content/data/wavs/380b8e816dba9f76959734fa4abec9fe.wav  \n",
            "  inflating: /content/data/wavs/2dce803ec59342a0c005c62054f169b6.wav  \n",
            "  inflating: /content/data/wavs/779bab9c86edeb0b412ee133ba534689.wav  \n",
            "  inflating: /content/data/wavs/40d4045848bb6c7418087d1f8185f006.wav  \n",
            "  inflating: /content/data/wavs/689c9ee4e21c0a73779d55433a3d394a.wav  \n",
            "  inflating: /content/data/wavs/c81adbb032f1f37bcea3a26e1b1c0183.wav  \n",
            "  inflating: /content/data/wavs/e3bff203062596a5cf51e1ede580fb79.wav  \n",
            "  inflating: /content/data/wavs/92fb9f09bba166a3b750a1e8a0d5107a.wav  \n",
            "  inflating: /content/data/wavs/76a2f76ac2c01cad0aaaaa5ea391cdf3.wav  \n",
            "  inflating: /content/data/wavs/d1773995c2cfb6a189fdb715788f75a7.wav  \n",
            "  inflating: /content/data/wavs/266432ae7419e1ba4116d3c1790a3cf5.wav  \n",
            "  inflating: /content/data/wavs/1603f768c4b044c2d69bf3a601eabee1.wav  \n",
            "  inflating: /content/data/wavs/da69b0432076e019c8ec2ebd1d1f75e3.wav  \n",
            "  inflating: /content/data/wavs/6aa05faabbd71b5d7b9c847074a7fdfb.wav  \n",
            "  inflating: /content/data/wavs/ff8905b72db1da7036b0bf520b21a61d.wav  \n",
            "  inflating: /content/data/wavs/94080ab7e654cfc2caf3e53f2d252384.wav  \n",
            "  inflating: /content/data/wavs/0d1dd3ca9c65a0b3c79c1fa95461aeee.wav  \n",
            "  inflating: /content/data/wavs/91ba10814a9a02633e75c405be8044cb.wav  \n",
            "  inflating: /content/data/wavs/f38cbdac2668ade5b939597785bf0606.wav  \n",
            "  inflating: /content/data/wavs/5017864f8a59f9a79e79756d0b2a25b1.wav  \n",
            "  inflating: /content/data/wavs/faa5d2136d688d664342271dcb7d3591.wav  \n",
            "  inflating: /content/data/wavs/6c860208916bd7001a03488933407cb5.wav  \n",
            "  inflating: /content/data/wavs/7042c6e1bc7abecda93691782770e0f7.wav  \n",
            "  inflating: /content/data/wavs/f8357a6f89ea49ec021596b1264ce3c7.wav  \n",
            "  inflating: /content/data/wavs/b5ea7e32d138bfa2aeee6928408976ed.wav  \n",
            "  inflating: /content/data/wavs/de5892d8583f63d0b9c344157af84372.wav  \n",
            "  inflating: /content/data/wavs/420ec8442dc292a7425dfb6ed89d7163.wav  \n",
            "  inflating: /content/data/wavs/6885f4d91b67595dfe73be1260047793.wav  \n",
            "  inflating: /content/data/wavs/aeea7c1c2ee17bfa2cedf7d36cd7ed8c.wav  \n",
            "  inflating: /content/data/wavs/5515fb12be7ebc4645e020bb47b2ef09.wav  \n",
            "  inflating: /content/data/wavs/88121b76c1d84a7dda2a3b82291c6cb6.wav  \n",
            "  inflating: /content/data/wavs/71dc21aa0a9e18af49d624d89b9e2bc7.wav  \n",
            "  inflating: /content/data/wavs/ffc593e617874db4607eb72e07ba29fe.wav  \n",
            "  inflating: /content/data/wavs/8f112329b91cf0bc10548eb75a13276d.wav  \n",
            "  inflating: /content/data/wavs/cf0ccbf6f6af640e05cdefc83e1c0809.wav  \n",
            "  inflating: /content/data/wavs/f3568d88337fa9f312e7d119f5827df0.wav  \n",
            "  inflating: /content/data/wavs/5b789fe0058f7dd5c3fd7873cfe58add.wav  \n",
            "  inflating: /content/data/wavs/0d466238005cfe9f0e1ddd681a58b716.wav  \n",
            "  inflating: /content/data/wavs/0b806d5033cbe0b381cfd2ba79883b84.wav  \n",
            "  inflating: /content/data/wavs/f2d318f40f0f6c5a6d29acf3c7e54ab6.wav  \n",
            "  inflating: /content/data/wavs/7318297c5ba7e9deab9c782d2a166439.wav  \n",
            "  inflating: /content/data/wavs/46d5883fb29fab41448d23c78125566f.wav  \n",
            "  inflating: /content/data/wavs/9585880ed02ac7ff7f35bb33a0f047d0.wav  \n",
            "  inflating: /content/data/wavs/ab6397c9fe413e342fadbb430b137997.wav  \n",
            "  inflating: /content/data/wavs/a27626093a5254341f26baf9fe70aa54.wav  \n",
            "  inflating: /content/data/wavs/95323557fa350766b2f3627133fac11e.wav  \n",
            "  inflating: /content/data/wavs/9f36002918d1c09549907db81e620cdd.wav  \n",
            "  inflating: /content/data/wavs/8a69263ca87ff9bc64474ea4009cfcf9.wav  \n",
            "  inflating: /content/data/wavs/b32fa3978b951bfc95263170437f24db.wav  \n",
            "  inflating: /content/data/wavs/73462161a85ada14273cbce56e17d98d.wav  \n",
            "  inflating: /content/data/wavs/900669658e601280ebefc03bfd626fa4.wav  \n",
            "  inflating: /content/data/wavs/35d42f9f5128fda86df8303a0001f588.wav  \n",
            "  inflating: /content/data/wavs/9dda1610d85c99a675f881f727605300.wav  \n",
            "  inflating: /content/data/wavs/bd5a45131269d20e176185ae0bb7939f.wav  \n",
            "  inflating: /content/data/wavs/6048e2f61c4dd38ada97499ed70b9554.wav  \n",
            "  inflating: /content/data/wavs/f90b83e15340b48edc21c39deade3717.wav  \n",
            "  inflating: /content/data/wavs/aa52f3b0c74a9bd6b898c2f553bd9d24.wav  \n",
            "  inflating: /content/data/wavs/1d38511f9d95cd1dae501c3b8bd9578d.wav  \n",
            "  inflating: /content/data/wavs/b531e553c4af7abc68bd4a19fe75b90a.wav  \n",
            "  inflating: /content/data/wavs/690b90065e6e933bf8c57cd044e40257.wav  \n",
            "  inflating: /content/data/wavs/c8a1a5c1b35d2ef2f2586fb762d1e661.wav  \n",
            "  inflating: /content/data/wavs/827ae0837cb3a004ba482f4edf1658f1.wav  \n",
            "  inflating: /content/data/wavs/ba7cc165ff2b4dbcb30212685cd0ef3a.wav  \n",
            "  inflating: /content/data/wavs/743de661e8e0c276b90643587c46dc84.wav  \n",
            "  inflating: /content/data/wavs/a9945c6aabcb6731bcb743903257d383.wav  \n",
            "  inflating: /content/data/wavs/8b0b2c284c4adee5c2fd1657e815406d.wav  \n",
            "  inflating: /content/data/wavs/9c0eb8432312898a59f256ee6eaf2904.wav  \n",
            "  inflating: /content/data/wavs/5b402361fc2fa6e0895d73deeadfd9f3.wav  \n",
            "  inflating: /content/data/wavs/3c831c77605d989bb973a78e6dfe35a5.wav  \n",
            "  inflating: /content/data/wavs/2523728d341651fa8bbec36b467d69c7.wav  \n",
            "  inflating: /content/data/wavs/b276297bb4488d297fe5e48ec6c60820.wav  \n",
            "  inflating: /content/data/wavs/1ed2c3f8b2807a3938bc462a60fafb5a.wav  \n",
            "  inflating: /content/data/wavs/f73fe8555fba889a0fbe5ef6e87b1111.wav  \n",
            "  inflating: /content/data/wavs/5a811066ff1af3926246dedced323227.wav  \n",
            "  inflating: /content/data/wavs/127a1e21e5c8ebf72f1b1db354070826.wav  \n",
            "  inflating: /content/data/wavs/545a057ceb6370de2f34faafa2eb3056.wav  \n",
            "  inflating: /content/data/wavs/07a6f55ed744016d26f0b9b323dcd63d.wav  \n",
            "  inflating: /content/data/wavs/d9c7b65b6f698f6f867ae195ba40460d.wav  \n",
            "  inflating: /content/data/wavs/56583d997f3038dff699308c2685db3d.wav  \n",
            "  inflating: /content/data/wavs/76fb6137c7cc32be6a64d0ba8a3e2faf.wav  \n",
            "  inflating: /content/data/wavs/b2e8021e56fd3a9b23171c3eeb08c42a.wav  \n",
            "  inflating: /content/data/wavs/02455d5ffb3d227fff0081968845bcfd.wav  \n",
            "  inflating: /content/data/wavs/a01ebbafd503510d0ac5d510a64bdb5f.wav  \n",
            "  inflating: /content/data/wavs/d61e999df7831df2cda45c9df5b3891b.wav  \n",
            "  inflating: /content/data/wavs/160a3c6c185fca2e4b5e8bcadb1c717c.wav  \n",
            "  inflating: /content/data/wavs/22a0fa20e8c3bd5396a65c92566242fe.wav  \n",
            "  inflating: /content/data/wavs/9b1ed81351187ea375232fc481083b35.wav  \n",
            "  inflating: /content/data/wavs/6c95b6403486b6b7a72b15de3d311c8f.wav  \n",
            "  inflating: /content/data/wavs/595cdf5ef1d687e6fb37f0bb6f950402.wav  \n",
            "  inflating: /content/data/wavs/01639da2e1c47c0070bcee7d68472194.wav  \n",
            "  inflating: /content/data/wavs/89cccd04fccb3d3459b0484ce1866226.wav  \n",
            "  inflating: /content/data/wavs/ecfe0f520a19fb27e33de4029059b7c0.wav  \n",
            "  inflating: /content/data/wavs/78ac71609b7177ba72fd6876b859e908.wav  \n",
            "  inflating: /content/data/wavs/9da6e85bffce0556ed29ddb1f737aae9.wav  \n",
            "  inflating: /content/data/wavs/cdfeeaac050b34a545c0fbf19894979a.wav  \n",
            "  inflating: /content/data/wavs/7d5b828caf5d066e4d8b258e51dfaed1.wav  \n",
            "  inflating: /content/data/wavs/dea3238c7f234b7a7cd08233c8d17bda.wav  \n",
            "  inflating: /content/data/wavs/becdec2b606762928304d235d39aaf75.wav  \n",
            "  inflating: /content/data/wavs/35b589d5f662fa46d08679356b6ddb66.wav  \n",
            "  inflating: /content/data/wavs/f200da08f9caa5ed972dec84b683bd42.wav  \n",
            "  inflating: /content/data/wavs/594b28336724a6060f158a5d53eb07e5.wav  \n",
            "  inflating: /content/data/wavs/aedbeed3eeeba64f40e9467e4f27a8f4.wav  \n",
            "  inflating: /content/data/wavs/2c38f425dda4eab5738c37bcb82c4cf3.wav  \n",
            "  inflating: /content/data/wavs/dc5d831f190f14a08ebcd2a9bd502a27.wav  \n",
            "  inflating: /content/data/wavs/97070cc96136f92ab58e06b17dbb7c33.wav  \n",
            "  inflating: /content/data/wavs/98d5cb12367817934597dc5ccfb93efe.wav  \n",
            "  inflating: /content/data/wavs/c2efc79eb51e22194d6c3d9cc66d4b22.wav  \n",
            "  inflating: /content/data/wavs/0f6524199419c6bcd85613805b789c66.wav  \n",
            "  inflating: /content/data/wavs/ae7e3cf00390bba95fce86b382a72552.wav  \n",
            "  inflating: /content/data/wavs/2f492d1f1613e1eed4efc36cedcd9251.wav  \n",
            "  inflating: /content/data/wavs/b7e719bb625a23289939b4e680d4272e.wav  \n",
            "  inflating: /content/data/wavs/1c4941b0f71bdf2f6aa8024b834fc1f8.wav  \n",
            "  inflating: /content/data/wavs/41a7b9f46996d81866f6d5720d704de7.wav  \n",
            "  inflating: /content/data/wavs/3f99e88bda5577c7f7ec26268eac6d85.wav  \n",
            "  inflating: /content/data/wavs/09d5f073625e58e02b0416f22422b9e1.wav  \n",
            "  inflating: /content/data/wavs/9a9979007dab75a1efeb95b181988f09.wav  \n",
            "  inflating: /content/data/wavs/5fee78e311d22ea47d9d2ff181b9432a.wav  \n",
            "  inflating: /content/data/wavs/547f8a0648b4c0cb55d35496a5f5c58d.wav  \n",
            "  inflating: /content/data/wavs/da8ac4c4ed9d058e9c2cb86d68f8ebe1.wav  \n",
            "  inflating: /content/data/wavs/2264e4531d9f6dfc032df8ad6eb8fdcd.wav  \n",
            "  inflating: /content/data/wavs/4ac0ef8399a26eb65cc57b3fd951c349.wav  \n",
            "  inflating: /content/data/wavs/4f6bced91e3e809d179384521f2acad2.wav  \n",
            "  inflating: /content/data/wavs/245326a2e0a0df3945a75cf27acc7c04.wav  \n",
            "  inflating: /content/data/wavs/bfce8c8916f4dca13f772717dae81c97.wav  \n",
            "  inflating: /content/data/wavs/1c26894f123eda6291ddfd26b3603eca.wav  \n",
            "  inflating: /content/data/wavs/f39d1a084463944be75b5c01c8c84a4c.wav  \n",
            "  inflating: /content/data/wavs/84ee3e9bb340401c0425a73494655d62.wav  \n",
            "  inflating: /content/data/wavs/abccdd46f2b1bd229b8c574c61adc8bd.wav  \n",
            "  inflating: /content/data/wavs/a66e78db905dd9725a6d592ca041e06f.wav  \n",
            "  inflating: /content/data/wavs/19fb44562a86ce32a75c77142b077fad.wav  \n",
            "  inflating: /content/data/wavs/35815a56f306d3f8b0d3e72055129a64.wav  \n",
            "  inflating: /content/data/wavs/24d0cbec8ca43d6d09edca5f4caf4775.wav  \n",
            "  inflating: /content/data/wavs/48c823d7864561295c3c399b688d3f1e.wav  \n",
            "  inflating: /content/data/wavs/d42fd4a396951bbfcd9756de238d3da4.wav  \n",
            "  inflating: /content/data/wavs/b6eb3c0054f278ff6c9deee5a80effc1.wav  \n",
            "  inflating: /content/data/wavs/982e1dc2f6dc632429aeb6ff0515b159.wav  \n",
            "  inflating: /content/data/wavs/96dc601717486f24f1811b5d54040e8d.wav  \n",
            "  inflating: /content/data/wavs/0210cbfceeaaee17425eb6fe643b7788.wav  \n",
            "  inflating: /content/data/wavs/7721da1245e5301994afc4c4937a1390.wav  \n",
            "  inflating: /content/data/wavs/13a5560fd63b7ba6ff1d39e052bff34f.wav  \n",
            "  inflating: /content/data/wavs/5a1789e385a37d30488a3052b45c1cf8.wav  \n",
            "  inflating: /content/data/wavs/5478e9a116f9914d0ac4c6f62aa703c3.wav  \n",
            "  inflating: /content/data/wavs/39102ccb9d355d029656c0e6aeb7c305.wav  \n",
            "  inflating: /content/data/wavs/6dd6454e73403d4f5b3ca1ec6d675445.wav  \n",
            "  inflating: /content/data/wavs/7f2fb4d60a6aef1c87b471d39364d099.wav  \n",
            "  inflating: /content/data/wavs/733f5646005ece4d08599dbee9f07b9e.wav  \n",
            "  inflating: /content/data/wavs/34f3f310e08f5ec68b0818655cd8ed86.wav  \n",
            "  inflating: /content/data/wavs/52c2cc4305d4d7fc1267eec2413aa1a3.wav  \n",
            "  inflating: /content/data/wavs/fd90753454adce4b773deada855e5b74.wav  \n",
            "  inflating: /content/data/wavs/77b2612cc4f2187869e03c87175698c5.wav  \n",
            "  inflating: /content/data/wavs/e0cd5e411af4e580dd8775a211274856.wav  \n",
            "  inflating: /content/data/wavs/73f894d201e8cc6077e541ddb1824d9c.wav  \n",
            "  inflating: /content/data/wavs/9481f97bae6e0d98134ce02edc6cddaa.wav  \n",
            "  inflating: /content/data/wavs/8fef87fceeb30628287336ea93e99ac7.wav  \n",
            "  inflating: /content/data/wavs/e06ec4a5ca0c235b00a082ce4771ead3.wav  \n",
            "  inflating: /content/data/wavs/9db461fdc33c2bfd7983bf349a26457d.wav  \n",
            "  inflating: /content/data/wavs/369ca27069374897e40ea7cf4c43c6a9.wav  \n",
            "  inflating: /content/data/wavs/6f3a364521fb34a263278f04bb305730.wav  \n",
            "  inflating: /content/data/wavs/29ffa6094b37686923f465dd17dd8104.wav  \n",
            "  inflating: /content/data/wavs/44c1bdfb7a5dd45e7b141eb3e3d4224c.wav  \n",
            "  inflating: /content/data/wavs/f0bd2c8570e1d0b7fe09af0a13e0a2be.wav  \n",
            "  inflating: /content/data/wavs/0111abcf48df533bcab8663f57259f61.wav  \n",
            "  inflating: /content/data/wavs/a03c1ca490b5a215f02e9d9b83ac727d.wav  \n",
            "  inflating: /content/data/wavs/ddb9b4cb35b6435dc35a742c2dba7214.wav  \n",
            "  inflating: /content/data/wavs/d440d80f757dd25a220d354867be4a94.wav  \n",
            "  inflating: /content/data/wavs/3b31f16e53ea68f27c566a214d8a9cdc.wav  \n",
            "  inflating: /content/data/wavs/a6422643021faee49d7dafdda24ad885.wav  \n",
            "  inflating: /content/data/wavs/0614cd5566ce9c4d6acd264ec39b6836.wav  \n",
            "  inflating: /content/data/wavs/2f181e4995c2463e986a379bc9f39dca.wav  \n",
            "  inflating: /content/data/wavs/f1c2375043818bf00cfc6e057ab0b331.wav  \n",
            "  inflating: /content/data/wavs/8dc96d8d02650982523b52935566a147.wav  \n",
            "  inflating: /content/data/wavs/8e19be29630a6dfaa5070d8edf82cd99.wav  \n",
            "  inflating: /content/data/wavs/a1dd4024845b92dc8efe9f40273680f9.wav  \n",
            "  inflating: /content/data/wavs/b24dcfc1cbfe8f70c1094b1c76d95eb4.wav  \n",
            "  inflating: /content/data/wavs/153a39b4899a7e1fff11bc6762f131f9.wav  \n",
            "  inflating: /content/data/wavs/c9aaf353c371d369db3202dc54380579.wav  \n",
            "  inflating: /content/data/wavs/7d4d93d566d80fe82dce8773f3a24159.wav  \n",
            "  inflating: /content/data/wavs/3bc29caf21e8bea383aae568228c6437.wav  \n",
            "  inflating: /content/data/wavs/d028f54693134fe85d043a1dbf312231.wav  \n",
            "  inflating: /content/data/wavs/dc1368c5e8d2760539774da5898ec6e4.wav  \n",
            "  inflating: /content/data/wavs/af35d881817f761e014484c58839cdef.wav  \n",
            "  inflating: /content/data/wavs/e03de6131274ee46cca98e44bd6b4700.wav  \n",
            "  inflating: /content/data/wavs/39127992d3acbd179f5310f2bb908e0b.wav  \n",
            "  inflating: /content/data/wavs/044cf7a5978e9b552cf7eb129e122345.wav  \n",
            "  inflating: /content/data/wavs/2b9dfa585623597728e2fba901cfb739.wav  \n",
            "  inflating: /content/data/wavs/7f5c9c13029751ed54dcb4bed56c76d4.wav  \n",
            "  inflating: /content/data/wavs/b67b2f79a66c78cbd61c35461a1a1324.wav  \n",
            "  inflating: /content/data/wavs/6484fc9002cc12572f82bd160c4337b5.wav  \n",
            "  inflating: /content/data/wavs/3b1380787992b9d83b8d6f71cfb286b6.wav  \n",
            "  inflating: /content/data/wavs/07e3a74ec08aa060f9943e710ce0e3bf.wav  \n",
            "  inflating: /content/data/wavs/d74978da5cd55daf61e00b2582e25434.wav  \n",
            "  inflating: /content/data/wavs/e033ab045ad0e351f1a5f6faf36ab1ba.wav  \n",
            "  inflating: /content/data/wavs/1d660fe40ff9c2756fb983fb3daec74c.wav  \n",
            "  inflating: /content/data/wavs/daa5343e5930a123a4c218a7611e949f.wav  \n",
            "  inflating: /content/data/wavs/b1d5440bfa0e224f33e6c83811a76299.wav  \n",
            "  inflating: /content/data/wavs/11dfa93256f08ad32542eec2ee537226.wav  \n",
            "  inflating: /content/data/wavs/c324a129ba55525b95c6d7ff1eac13b8.wav  \n",
            "  inflating: /content/data/wavs/53d7382f3ede58ff1fc78f0926b6900a.wav  \n",
            "  inflating: /content/data/wavs/0ed9a223a0af242635a8760d7c444a2e.wav  \n",
            "  inflating: /content/data/wavs/b25ca36654ddc9d48b3321a55d1c7209.wav  \n",
            "  inflating: /content/data/wavs/eac7e53254dc76db213707b7c168806e.wav  \n",
            "  inflating: /content/data/wavs/d286df41f6541809b17c465518809254.wav  \n",
            "  inflating: /content/data/wavs/08680574b7abbec6c8eb9632a1cc6c44.wav  \n",
            "  inflating: /content/data/wavs/0382df3aee5523941ae92c27e0d80f2a.wav  \n",
            "  inflating: /content/data/wavs/1dc0b7e283db6203282338cb9a317736.wav  \n",
            "  inflating: /content/data/wavs/3395ee4a6b143b5b00c6d3bd6810c7d6.wav  \n",
            "  inflating: /content/data/wavs/e87a4ec3094246b84f267e5a27e684fe.wav  \n",
            "  inflating: /content/data/wavs/2538be90d12433c1d4cab92285e85ede.wav  \n",
            "  inflating: /content/data/wavs/a34d0462971ee6099515639c5015e770.wav  \n",
            "  inflating: /content/data/wavs/66dcc850cdbf9e043bbcb4a68f219132.wav  \n",
            "  inflating: /content/data/wavs/67a8d1f81bc5347c079af2eb6a631831.wav  \n",
            "  inflating: /content/data/wavs/adfae48c3376b8e3ad0574f966c9d7a1.wav  \n",
            "  inflating: /content/data/wavs/2e30b7ac2faed858b95ff706623bc82c.wav  \n",
            "  inflating: /content/data/wavs/499d91f03e9496ac2ddc8a99026a0c97.wav  \n",
            "  inflating: /content/data/wavs/8f1978d3d73dc4d76e1f1a4f286bab6e.wav  \n",
            "  inflating: /content/data/wavs/49fe44de483ee97afb4cfb31a3fab376.wav  \n",
            "  inflating: /content/data/wavs/6cb132e915c961e3881812a02f43cdc9.wav  \n",
            "  inflating: /content/data/wavs/cbc32d85c3b77f776c790a618e96e9e4.wav  \n",
            "  inflating: /content/data/wavs/78f00a854cabd7b77256b3a90e54c63c.wav  \n",
            "  inflating: /content/data/wavs/adada1773b3d6665e9d4969714afa293.wav  \n",
            "  inflating: /content/data/wavs/cfe863ec53246dfe85cbce194318c411.wav  \n",
            "  inflating: /content/data/wavs/e0cafe7049f81059251a12c74cc3e7b1.wav  \n",
            "  inflating: /content/data/wavs/4f01dd9ff7d3f9fee663b88cc77fd0f9.wav  \n",
            "  inflating: /content/data/wavs/e4c8d9f02b0cdf317cc680c50eec3d64.wav  \n",
            "  inflating: /content/data/wavs/e40c4273fa271be696e59b034bd9bfd0.wav  \n",
            "  inflating: /content/data/wavs/bbdd3603528f2fb0e86157b86e91f94d.wav  \n",
            "  inflating: /content/data/wavs/f8e5b21074d2ca55edd11ef4e82f9cf2.wav  \n",
            "  inflating: /content/data/wavs/201e1dd4210404d3c5b1596653b6e409.wav  \n",
            "  inflating: /content/data/wavs/c7ac2694361e8c08fc12da432abdcf60.wav  \n",
            "  inflating: /content/data/wavs/ae229ed9d6e015d734610da3c12e8b8e.wav  \n",
            "  inflating: /content/data/wavs/0235701f430e424fa3e73d33ecc4f420.wav  \n",
            "  inflating: /content/data/wavs/721bff6aff43ebbae46e74f587e7a342.wav  \n",
            "  inflating: /content/data/wavs/55e544e68f06a3772d1c397729b8ebdf.wav  \n",
            "  inflating: /content/data/wavs/13969b479957cfc40cbb84aee16c33ed.wav  \n",
            "  inflating: /content/data/wavs/deb2f17463e5c7977f2e3e87d30611dc.wav  \n",
            "  inflating: /content/data/wavs/7d2bcd9fc4722c78ca00b22b9c6ef12c.wav  \n",
            "  inflating: /content/data/wavs/5dfc37b933974290a3b3be8d7755286b.wav  \n",
            "  inflating: /content/data/wavs/054342cba69f8ba4a2e89ec2b8d430ae.wav  \n",
            "  inflating: /content/data/wavs/711295929c14d7b2b98eb54f998a0fd3.wav  \n",
            "  inflating: /content/data/wavs/128331a6f0c0a4ac1d2d780981f6b4c5.wav  \n",
            "  inflating: /content/data/wavs/8aa628211cbd49d6138722763fb6a611.wav  \n",
            "  inflating: /content/data/wavs/d639ea07d70e229e4ddc867a43f13d85.wav  \n",
            "  inflating: /content/data/wavs/65775c9ff7dbcba93bbef83ce47c5202.wav  \n",
            "  inflating: /content/data/wavs/571b2a7e2ad1efb1235231efb85d07a7.wav  \n",
            "  inflating: /content/data/wavs/f012be7bc705b76284f6fcf3bc646634.wav  \n",
            "  inflating: /content/data/wavs/f1ac5e3b0ecd4440722e08f84034b60e.wav  \n",
            "  inflating: /content/data/wavs/b5210a1ac4ce845533cc7b0fb5bf5e1a.wav  \n",
            "  inflating: /content/data/wavs/3b6f705206a3dd7ad02607be3d9b063c.wav  \n",
            "  inflating: /content/data/wavs/b9b219a5288431adc2562382bc7d2d4b.wav  \n",
            "  inflating: /content/data/wavs/1eae64c3f9836c5b87417ddd20833940.wav  \n",
            "  inflating: /content/data/wavs/fd2cf631df884c37c47dd81586322699.wav  \n",
            "  inflating: /content/data/wavs/fb7e018f8264936b414aafcdb9169db6.wav  \n",
            "  inflating: /content/data/wavs/0e3157b75a58693c33e235d7e46eeaf8.wav  \n",
            "  inflating: /content/data/wavs/4a714235e3a3c8ba545f8a9a5ef726cf.wav  \n",
            "  inflating: /content/data/wavs/12a5ddf593282da63b66171314f292e7.wav  \n",
            "  inflating: /content/data/wavs/7bbe1a0fd544cb98c3dc92b5f32e3386.wav  \n",
            "  inflating: /content/data/wavs/fc21128baa78b3575183eb27303b928f.wav  \n",
            "  inflating: /content/data/wavs/778e805abf2d89a46e12a9a49673c492.wav  \n",
            "  inflating: /content/data/wavs/980c78b727ebe14cf7c9263ca5ff2fbe.wav  \n",
            "  inflating: /content/data/wavs/8062860d55f82353ba1e615353856f04.wav  \n",
            "  inflating: /content/data/wavs/021d755e98b923f81ba85b02649a8b1f.wav  \n",
            "  inflating: /content/data/wavs/b4628763f4e7657831f4aa30163217ce.wav  \n",
            "  inflating: /content/data/wavs/263c56809806306f24c20c4071983a54.wav  \n",
            "  inflating: /content/data/wavs/fbe13f137603f7502d3394cbc626a002.wav  \n",
            "  inflating: /content/data/wavs/c833fd799b688381141b6db4c0502bcf.wav  \n",
            "  inflating: /content/data/wavs/9b11979319a9b5c1f787080c691aebe3.wav  \n",
            "  inflating: /content/data/wavs/0a415ddb11dae2a80fa31da3b0f48a97.wav  \n",
            "  inflating: /content/data/wavs/f47f378695683aeb71f8f203043df33f.wav  \n",
            "  inflating: /content/data/wavs/0c338a88b8e576c5365a33cd6372bb30.wav  \n",
            "  inflating: /content/data/wavs/dea0204455186fe3b0e1164c089ac587.wav  \n",
            "  inflating: /content/data/wavs/7842561c4babe3301a5c226d8fc990e1.wav  \n",
            "  inflating: /content/data/wavs/add9440d1cf1947a07da2af771d990d4.wav  \n",
            "  inflating: /content/data/wavs/112ec7e4ae64e2ce6fc0505b0ed03ce2.wav  \n",
            "  inflating: /content/data/wavs/b692704e070424f91fbb794cc61d6fb0.wav  \n",
            "  inflating: /content/data/wavs/2030373cc971fd4a3b22d48bd59a0a45.wav  \n",
            "  inflating: /content/data/wavs/b14a8b58a0ca0dd1cb185497e73b0c69.wav  \n",
            "  inflating: /content/data/wavs/82914fae30061bdf0a2af9445c1ae7bb.wav  \n",
            "  inflating: /content/data/wavs/3d2bc764051636d210ba0623626af42f.wav  \n",
            "  inflating: /content/data/wavs/56a7644f1a06592c86ab174380df13c9.wav  \n",
            "  inflating: /content/data/wavs/c7d2e76b0b7ecda7965fee1d53a96f7d.wav  \n",
            "  inflating: /content/data/wavs/96845c41d46e8a3f5efc34dc63991c41.wav  \n",
            "  inflating: /content/data/wavs/e4d55b0856f1e66aa27599eccb6dba61.wav  \n",
            "  inflating: /content/data/wavs/14d6c40747edd426963eb2b207f0381d.wav  \n",
            "  inflating: /content/data/wavs/19c2fbf07be6a843ca7df988ee90a552.wav  \n",
            "  inflating: /content/data/wavs/d691912b36607b9d6e81d82df8751086.wav  \n",
            "  inflating: /content/data/wavs/f41cae019abe67fa72f2eb7717f1f115.wav  \n",
            "  inflating: /content/data/wavs/ae3b0a873b13db41e2dc6702076ccd19.wav  \n",
            "  inflating: /content/data/wavs/77d8f4b15695ce3120455c4e87ba10f9.wav  \n",
            "  inflating: /content/data/wavs/3ee248fb3dd49c87ee0db54186963e9e.wav  \n",
            "  inflating: /content/data/wavs/d6ed3075b5832951f16a4543beecfded.wav  \n",
            "  inflating: /content/data/wavs/ef809d6b4ab6fa24df08e6483df5e023.wav  \n",
            "  inflating: /content/data/wavs/6fd501ccd620bba6ed638344785f246d.wav  \n",
            "  inflating: /content/data/wavs/9970292251ed9db1a07800f70274efc2.wav  \n",
            "  inflating: /content/data/wavs/e6bbee02dec8685867cd06ff2b9046de.wav  \n",
            "  inflating: /content/data/wavs/eef78b7a04992fb797cb6642467af6ad.wav  \n",
            "  inflating: /content/data/wavs/8e244bc4f8306ea855a04e29e04c1b0a.wav  \n",
            "  inflating: /content/data/wavs/099254c323ab16364804ce5f7eeb8984.wav  \n",
            "  inflating: /content/data/wavs/8eed921c6cba37e9b8dfa125c87d9054.wav  \n",
            "  inflating: /content/data/wavs/45e1a2ee29039feab130d5d30e1f57a4.wav  \n",
            "  inflating: /content/data/wavs/59585af89adc9202f77f20b8d5c4f131.wav  \n",
            "  inflating: /content/data/wavs/cd0de27f4efb09bdf77e6712ce04686c.wav  \n",
            "  inflating: /content/data/wavs/fda6f67aaf46f065441723de6e84420f.wav  \n",
            "  inflating: /content/data/wavs/af10cd4ac195e593b18ae724d748895e.wav  \n",
            "  inflating: /content/data/wavs/7431b532a41ddd1aeb5dd5e6c2825eeb.wav  \n",
            "  inflating: /content/data/wavs/5702d065f20c807fc3a6ffdb1b4ceb34.wav  \n",
            "  inflating: /content/data/wavs/e961feb4d41808e4ac6c5ddd7d3eaf07.wav  \n",
            "  inflating: /content/data/wavs/52a42a4fe30be9fc54ce9aef27ba3c0f.wav  \n",
            "  inflating: /content/data/wavs/728e0bbcbf694d657dbdcf61f16ca030.wav  \n",
            "  inflating: /content/data/wavs/69d2c972b618f39b9a2a12ed0058bcc1.wav  \n",
            "  inflating: /content/data/wavs/7c16afd37ee5c02180b26af75dfc0f5b.wav  \n",
            "  inflating: /content/data/wavs/c049bf13f5ede562bfb68bf9c4021ff2.wav  \n",
            "  inflating: /content/data/wavs/458d86bf7d7ab55a8fc0bd4863a574f7.wav  \n",
            "  inflating: /content/data/wavs/54ad767429cb8a8a4beb3b0cf7762afd.wav  \n",
            "  inflating: /content/data/wavs/a0f5c471ef5bed38c86086b4e4dbcc83.wav  \n",
            "  inflating: /content/data/wavs/cf4e6e813773b8e41822a896f393923b.wav  \n",
            "  inflating: /content/data/wavs/0010f18301b3a0b07528c927c5543436.wav  \n",
            "  inflating: /content/data/wavs/a85563c768da74439d59eb1dc7503a19.wav  \n",
            "  inflating: /content/data/wavs/32511506f3ea49bac049f2c4e5c28ed1.wav  \n",
            "  inflating: /content/data/wavs/f9b57765586fa3009f489d7e5040fa08.wav  \n",
            "  inflating: /content/data/wavs/7088cd07585e318de3f8d3ffd8e546a8.wav  \n",
            "  inflating: /content/data/wavs/112985162b16cebe4859564a93719c94.wav  \n",
            "  inflating: /content/data/wavs/ffca6bb9a3eb9668f64f4efd10ecf21c.wav  \n",
            "  inflating: /content/data/wavs/5a7d54c5a7e8e1f90422cc538b6483f4.wav  \n",
            "  inflating: /content/data/wavs/182586d1827a8c48d399b860828d52ac.wav  \n",
            "  inflating: /content/data/wavs/5b3f24e31b15c65711a3203770c27072.wav  \n",
            "  inflating: /content/data/wavs/812722876d3af5457ec8e396f4f1cf72.wav  \n",
            "  inflating: /content/data/wavs/bc766916cb545dc5eff3db0407207cc9.wav  \n",
            "  inflating: /content/data/wavs/23b4351484a548fccb5c55bdfe12a1f6.wav  \n",
            "  inflating: /content/data/wavs/7f6f0b7e4b64d8c3712fd8d15ac1b5c3.wav  \n",
            "  inflating: /content/data/wavs/5343ef7abd1a44dd4b8d336db5b0103b.wav  \n",
            "  inflating: /content/data/wavs/23f85aaead0566fcb20b6c6c1fe536b1.wav  \n",
            "  inflating: /content/data/wavs/ba71e4c50eee2266794419459b51e0f4.wav  \n",
            "  inflating: /content/data/wavs/b01f3648e8245b2725e9c5cf8b57730c.wav  \n",
            "  inflating: /content/data/wavs/dd593756b51933f70c222ca6781d2ddd.wav  \n",
            "  inflating: /content/data/wavs/2562c6e7f0b3f8a836228d63092ba97b.wav  \n",
            "  inflating: /content/data/wavs/a13f26a5c7a5ab15d5021b5f0f2a904a.wav  \n",
            "  inflating: /content/data/wavs/4885a3f7ba1d0e429412aceb4e7325c3.wav  \n",
            "  inflating: /content/data/wavs/3d61565b994e27bca8e0b485114e527e.wav  \n",
            "  inflating: /content/data/wavs/a6c67b610d7b882fd957e0f9a09b50fb.wav  \n",
            "  inflating: /content/data/wavs/30762c2f4aab8b2a51dbefe6ad745cf5.wav  \n",
            "  inflating: /content/data/wavs/f300300dd1cf5d3b103e0281c77742ef.wav  \n",
            "  inflating: /content/data/wavs/50bdf17dfafc49bca8e4f609c29d97e2.wav  \n",
            "  inflating: /content/data/wavs/5fd877c9faf2ef17f352930cba487aa5.wav  \n",
            "  inflating: /content/data/wavs/409077744bed0487631e2d73b3d1bd1b.wav  \n",
            "  inflating: /content/data/wavs/cae24947ce3307d4ae173760657f848e.wav  \n",
            "  inflating: /content/data/wavs/305dbd7e9420d7c3e631fd16156bd105.wav  \n",
            "  inflating: /content/data/wavs/25de26f2fc765f379aa3e202b7f6f606.wav  \n",
            "  inflating: /content/data/wavs/7dec5c430c1b243a5425f1f71a78e766.wav  \n",
            "  inflating: /content/data/wavs/0fcb79725350c4cde834efcdb297e234.wav  \n",
            "  inflating: /content/data/wavs/3c05dab97fd7a6549f166f093f5e3b65.wav  \n",
            "  inflating: /content/data/wavs/82a02cf2b973de63c53d3acbaec22a4b.wav  \n",
            "  inflating: /content/data/wavs/83f879eeedc8b0683463cb5bfa2a88e3.wav  \n",
            "  inflating: /content/data/wavs/62b449f151a0d55af09f7394b3e19f67.wav  \n",
            "  inflating: /content/data/wavs/2bdeec31b7d5f902d9d14d705529254e.wav  \n",
            "  inflating: /content/data/wavs/eb4a8474f9fe608ffd251af1caaf927d.wav  \n",
            "  inflating: /content/data/wavs/a2d43523a0a4b508b30183300d8b77c6.wav  \n",
            "  inflating: /content/data/wavs/df53da215f282ed3f736dcb1578e9149.wav  \n",
            "  inflating: /content/data/wavs/b96bd4afc3222e388c72b4b62eeb5a54.wav  \n",
            "  inflating: /content/data/wavs/d6ef9d603536381c1cf16a7ae29a1579.wav  \n",
            "  inflating: /content/data/wavs/ae3753c578c2a1d44e53d482148808fb.wav  \n",
            "  inflating: /content/data/wavs/0c19093b22632bad7a3d1060c576e0cb.wav  \n",
            "  inflating: /content/data/wavs/8082a1893cd97dd5f646b4900c77b646.wav  \n",
            "  inflating: /content/data/wavs/785f1cdfa48b77bea8270f3eb6ff7ba9.wav  \n",
            "  inflating: /content/data/wavs/02f7f359e5826df94f2b7715f0a16ccd.wav  \n",
            "  inflating: /content/data/wavs/8c46e4480a3beb381d518bade4c0c6e5.wav  \n",
            "  inflating: /content/data/wavs/f5e080ab116d592e987b3530016b09f1.wav  \n",
            "  inflating: /content/data/wavs/0b4b0308b7c25cbe86d8ef0a4db4fd56.wav  \n",
            "  inflating: /content/data/wavs/2792bf9c7636410ba6f24d3e18b0d013.wav  \n",
            "  inflating: /content/data/wavs/0988cd90d0d501699765fa10811f5aab.wav  \n",
            "  inflating: /content/data/wavs/ce046d20ebe8d557c490cda281c8cb16.wav  \n",
            "  inflating: /content/data/wavs/ae09472a22f0cf4a9bfaf1a05d6e17d0.wav  \n",
            "  inflating: /content/data/wavs/4680b8d47fea797e02e2c8257b67cc14.wav  \n",
            "  inflating: /content/data/wavs/27be93339443adec9c97da93b9cd0515.wav  \n",
            "  inflating: /content/data/wavs/fa8b066ecad8176c13d3054fc996481b.wav  \n",
            "  inflating: /content/data/wavs/29930bb0c78622b5e7798a223b342cfb.wav  \n",
            "  inflating: /content/data/wavs/1fb5ec8eae6d937976eccbaa2ba43439.wav  \n",
            "  inflating: /content/data/wavs/7f0a43d60ab1aa1118f3d5f6f373888c.wav  \n",
            "  inflating: /content/data/wavs/547fa013101a02b54ac5e9fe79ad5371.wav  \n",
            "  inflating: /content/data/wavs/d6c61664ba332443ade76c65ec3ca0bb.wav  \n",
            "  inflating: /content/data/wavs/67c20613dde6b102142f03bc2f38a42a.wav  \n",
            "  inflating: /content/data/wavs/b58e83b14716eb5a60fa813991baa976.wav  \n",
            "  inflating: /content/data/wavs/62c5ad4a48bc16e6f8233355f9b4e03b.wav  \n",
            "  inflating: /content/data/wavs/5bc9d7c70af97e16d8cca4fdc32e6b8a.wav  \n",
            "  inflating: /content/data/wavs/d6b8c0c4e02243e881c471cc453e83a1.wav  \n",
            "  inflating: /content/data/wavs/bd7db14acf742dbb5671e85c1d20b3e4.wav  \n",
            "  inflating: /content/data/wavs/8d1a73494bac48d239aa18d8e4cfdec6.wav  \n",
            "  inflating: /content/data/wavs/7dfdd3bd14fdb67adcae3be3c41dbd3f.wav  \n",
            "  inflating: /content/data/wavs/9007fcc00ea1193e5913ea17a6fe9efe.wav  \n",
            "  inflating: /content/data/wavs/00721acc25b71fe9d25bce085ee31a79.wav  \n",
            "  inflating: /content/data/wavs/2d49096f6c02ebcec2aba51056b668d4.wav  \n",
            "  inflating: /content/data/wavs/db418f468a619b880d2d16cd7921b9e1.wav  \n",
            "  inflating: /content/data/wavs/227ccbc2b95ce73e86be5ce146b38dea.wav  \n",
            "  inflating: /content/data/wavs/2308d87f03517f3e03e6ad551e719814.wav  \n",
            "  inflating: /content/data/wavs/627d77bafe9bff8932518725a6b7d51c.wav  \n",
            "  inflating: /content/data/wavs/35ca86deac57667890373763782157f3.wav  \n",
            "  inflating: /content/data/wavs/44c476a3eacf3ad10196a9c054f14f99.wav  \n",
            "  inflating: /content/data/wavs/ee2285dd16b7f026a87d4a64737d6154.wav  \n",
            "  inflating: /content/data/wavs/d3803ef1f1b38f6c53597afab539d7a2.wav  \n",
            "  inflating: /content/data/wavs/e31178a6162d0fe8e590f33d49e25402.wav  \n",
            "  inflating: /content/data/wavs/3ecd07165edd0a7b2269117b0744629d.wav  \n",
            "  inflating: /content/data/wavs/ed13b3de92094bbd70c8cd0f6fff8baa.wav  \n",
            "  inflating: /content/data/wavs/ff985886a8c47283ed99f15fb802ec4e.wav  \n",
            "  inflating: /content/data/wavs/ba31125ce58b72a7992c9e9a15bbbefd.wav  \n",
            "  inflating: /content/data/wavs/afe330df2f79dd7b28480bf213394c73.wav  \n",
            "  inflating: /content/data/wavs/a9a67d3132ac096c4211d41f4e7fc82d.wav  \n",
            "  inflating: /content/data/wavs/d768ae3c01cf8bfe69f80e53430140fa.wav  \n",
            "  inflating: /content/data/wavs/742dd69c811521123f877b6ff4d2fdc2.wav  \n",
            "  inflating: /content/data/wavs/8b65f927235406c2f45292b629cdbbdc.wav  \n",
            "  inflating: /content/data/wavs/e991c01b3a8f1893a4176701b7038a57.wav  \n",
            "  inflating: /content/data/wavs/4e699b4ab0346286553177f72c240fa6.wav  \n",
            "  inflating: /content/data/wavs/acd96d24757f3a9fc26864604e909795.wav  \n",
            "  inflating: /content/data/wavs/61fb5271afeb925b7707b0f24fbc37bf.wav  \n",
            "  inflating: /content/data/wavs/9c58af7e7441ea808647049d1623c187.wav  \n",
            "  inflating: /content/data/wavs/8faf716bdcfce8863f09a0870c544810.wav  \n",
            "  inflating: /content/data/wavs/b561d7264de4ce4d717ed3f6c432f8c0.wav  \n",
            "  inflating: /content/data/wavs/6b52d7f7ddf272792d7508379f120110.wav  \n",
            "  inflating: /content/data/wavs/f326c8a88c34680b0da9b238d8883cef.wav  \n",
            "  inflating: /content/data/wavs/b4fcaff18c1e866d18727cf4d5295a2c.wav  \n",
            "  inflating: /content/data/wavs/a8e7f59db13f1cf67939f83b1de0f6cf.wav  \n",
            "  inflating: /content/data/wavs/816b30c4ae8ab85b229404b45a9fb0c9.wav  \n",
            "  inflating: /content/data/wavs/b8550a0dce88ff52f98b2ce4af0dbece.wav  \n",
            "  inflating: /content/data/wavs/9681292173820fbf5f00ea8813e1a4ea.wav  \n",
            "  inflating: /content/data/wavs/cc116e53c03fa09203aea8658cce3599.wav  \n",
            "  inflating: /content/data/wavs/587efe7cf33c3f4b4d0b76c6bb1b757c.wav  \n",
            "  inflating: /content/data/wavs/7214275b473691a0bafb55422b683d96.wav  \n",
            "  inflating: /content/data/wavs/05274fcea3218b3383730d94ebbd2523.wav  \n",
            "  inflating: /content/data/wavs/fa36f1598a7120e4ef260c4fd4640281.wav  \n",
            "  inflating: /content/data/wavs/76ed36c0a7bc375e862115d2552df13c.wav  \n",
            "  inflating: /content/data/wavs/05d7a1d46b733a9809d81553e5eeabdc.wav  \n",
            "  inflating: /content/data/wavs/c5bb234febb358a0c7208db3328e0e98.wav  \n",
            "  inflating: /content/data/wavs/5d057b37e029e509444f138a82c413e8.wav  \n",
            "  inflating: /content/data/wavs/6631c80b830d1bd8a1ed8d6c3794abf6.wav  \n",
            "  inflating: /content/data/wavs/59ea7084be601981bebe1a75d18e674e.wav  \n",
            "  inflating: /content/data/wavs/a483fbdb7f00df7080775b0026229106.wav  \n",
            "  inflating: /content/data/wavs/d4edbc17c0586ffa49e25eb9c2ba7fa8.wav  \n",
            "  inflating: /content/data/wavs/2fa4ad5396ce128f127e3ab7b4375f0a.wav  \n",
            "  inflating: /content/data/wavs/e98ccf579424d031de30b252e1fe6d97.wav  \n",
            "  inflating: /content/data/wavs/8711cb413b3f2948e6340a98300bf007.wav  \n",
            "  inflating: /content/data/wavs/bb76db7d35fb9707d4759b86e75e0556.wav  \n",
            "  inflating: /content/data/wavs/421a849e9bb1fc0a6426579f6a01d51c.wav  \n",
            "  inflating: /content/data/wavs/f4a217eca752373bdb4e8994e6639a3b.wav  \n",
            "  inflating: /content/data/wavs/5f665427bdb1d8ec5c8da569f902c813.wav  \n",
            "  inflating: /content/data/wavs/451dd626f3f6e45085f986fcc0acf114.wav  \n",
            "  inflating: /content/data/wavs/f07a82f607765924c878cb931ad13fb7.wav  \n",
            "  inflating: /content/data/wavs/04aab9331c94aaf10e62bfe3b64dec2d.wav  \n",
            "  inflating: /content/data/wavs/89b1e376476d1e11f369a456d097b655.wav  \n",
            "  inflating: /content/data/wavs/9155045b883431c3b6e0f5ba2b156c14.wav  \n",
            "  inflating: /content/data/wavs/41cdd47911d3bc2056411ed3ffaf0774.wav  \n",
            "  inflating: /content/data/wavs/17c8619d975d8332117ed1f81a7f3b4f.wav  \n",
            "  inflating: /content/data/wavs/ff3a6b5ca3dd96369a45132e4a6450ee.wav  \n",
            "  inflating: /content/data/wavs/9f9515086c29b16e6606b5d5e5c7b1c6.wav  \n",
            "  inflating: /content/data/wavs/ba6a18503a707c072f0fe075842cfd7f.wav  \n",
            "  inflating: /content/data/wavs/eab955354dfb57185a6ddd5698b90257.wav  \n",
            "  inflating: /content/data/wavs/6ce3768b7c3952338d8bc56b564f3441.wav  \n",
            "  inflating: /content/data/wavs/ddf16aa7166ccee7337fb698e063c8a8.wav  \n",
            "  inflating: /content/data/wavs/01fd3b23559551352d35e78e214111fd.wav  \n",
            "  inflating: /content/data/wavs/02ed39470f92fc0c08d9284bd1d556fc.wav  \n",
            "  inflating: /content/data/wavs/65415c6135f30e847211cfb76eb1856e.wav  \n",
            "  inflating: /content/data/wavs/fb1d54c3427d5842902a5d59443f4d98.wav  \n",
            "  inflating: /content/data/wavs/e5a601f762b4f9be9d5dfc1352b51a0f.wav  \n",
            "  inflating: /content/data/wavs/1e3bc82a957cecdef26e84be6fc24449.wav  \n",
            "  inflating: /content/data/wavs/27e80cf4f2dca5218ac1d89c2d1a1e1a.wav  \n",
            "  inflating: /content/data/wavs/d097396355c36000d2b8b7286be316ea.wav  \n",
            "  inflating: /content/data/wavs/fe42c277be3f1a78395821f4a0efec0b.wav  \n",
            "  inflating: /content/data/wavs/b2c8903a240bdbb344c9ebe63074e759.wav  \n",
            "  inflating: /content/data/wavs/29e0cbef75a11b948b688db3d3be58dd.wav  \n",
            "  inflating: /content/data/wavs/69ae81e28e56c72e480990c6e0f92d64.wav  \n",
            "  inflating: /content/data/wavs/6ac4e212ec40fc8e34915b56b3475481.wav  \n",
            "  inflating: /content/data/wavs/ff1b3b05aa87a1574f247957397daaaf.wav  \n",
            "  inflating: /content/data/wavs/92194a2d05ad01940949549ebc668d4f.wav  \n",
            "  inflating: /content/data/wavs/891b074d630dc68be5074393b21dd2e1.wav  \n",
            "  inflating: /content/data/wavs/61678041c6a05167af39c05065bf496a.wav  \n",
            "  inflating: /content/data/wavs/710a86b51d2a2a828bb6fd93f923c440.wav  \n",
            "  inflating: /content/data/wavs/6d9a9d977c984c514d1cb85c97138148.wav  \n",
            "  inflating: /content/data/wavs/42bade7238e6c69d7e429e0bb2e06163.wav  \n",
            "  inflating: /content/data/wavs/485a75e157b159d104cfda7481f3203b.wav  \n",
            "  inflating: /content/data/wavs/7e0f4cd0eb217edb990c00e95a20556e.wav  \n",
            "  inflating: /content/data/wavs/767cc3a2387d5692b4675201477ea8c6.wav  \n",
            "  inflating: /content/data/wavs/76430bca1590891edffd40ac51517f16.wav  \n",
            "  inflating: /content/data/wavs/fa01e57918df36f3ca2006b3d31732f4.wav  \n",
            "  inflating: /content/data/wavs/38879fda90ef109b8ff0c091c8cabc2d.wav  \n",
            "  inflating: /content/data/wavs/fd74b62b969175cf155a0a4f820ec4a5.wav  \n",
            "  inflating: /content/data/wavs/8e1adcf876d7a9f56228bd2165dbe116.wav  \n",
            "  inflating: /content/data/wavs/de506c6dd9fbe3f1975e5b49cb4fcb02.wav  \n",
            "  inflating: /content/data/wavs/45e791b178d3b6c10ffd6f21bc8edfcd.wav  \n",
            "  inflating: /content/data/wavs/30acc656951c51bdd4004e670c4fad3b.wav  \n",
            "  inflating: /content/data/wavs/0cff626c59295d19d7e72d2abced62f9.wav  \n",
            "  inflating: /content/data/wavs/3dd1e4feddb30ee1b84327d27b498b95.wav  \n",
            "  inflating: /content/data/wavs/f09200183095686561d81dc48609c8c0.wav  \n",
            "  inflating: /content/data/wavs/9702172eb54e638a0f0bdfd886039c8e.wav  \n",
            "  inflating: /content/data/wavs/cdcd65d9ad439a4549a528d7116ecb05.wav  \n",
            "  inflating: /content/data/wavs/758b929eca2347f581ee8147df150ac6.wav  \n",
            "  inflating: /content/data/wavs/a353b2ca90ec6aeff6d056e2947e1d9a.wav  \n",
            "  inflating: /content/data/wavs/e8fd406263d215af8333b1d4c1fc26b6.wav  \n",
            "  inflating: /content/data/wavs/fb9e67f0fba34ae11952e80173b8acfc.wav  \n",
            "  inflating: /content/data/wavs/5c4ff504435c3f26e7f39ce4cc650b49.wav  \n",
            "  inflating: /content/data/wavs/ba54e12ec20c362fc829387401934bba.wav  \n",
            "  inflating: /content/data/wavs/9181378ebf3ec968fe9760b8e9d1c1bb.wav  \n",
            "  inflating: /content/data/wavs/29028c9cf9c14439ec691adb1191d3e0.wav  \n",
            "  inflating: /content/data/wavs/fca22a36240c7ce336e45a0a340df60e.wav  \n",
            "  inflating: /content/data/wavs/178acc087b9ff5b94e72a131977f12b6.wav  \n",
            "  inflating: /content/data/wavs/2d877e61bc9d3c7d72af226eed01a6b3.wav  \n",
            "  inflating: /content/data/wavs/f65df31a4b270dafea7ad81fb8d3ea9f.wav  \n",
            "  inflating: /content/data/wavs/98944f8d1d289a6aea1cbdf425042c61.wav  \n",
            "  inflating: /content/data/wavs/5ed23a72f6d3a61f7e368e5d58cb5691.wav  \n",
            "  inflating: /content/data/wavs/4b9799e5ae32e6baf8f94d146f7cad5b.wav  \n",
            "  inflating: /content/data/wavs/589dc97158bed704e42c9ec7bf8393c4.wav  \n",
            "  inflating: /content/data/wavs/8927ac891297b95182447900382514bd.wav  \n",
            "  inflating: /content/data/wavs/3eeb7f2641ad2a23a08c3dbfc8d96273.wav  \n",
            "  inflating: /content/data/wavs/fa37d14ceb086836ceb1be249bdc9b01.wav  \n",
            "  inflating: /content/data/wavs/1051c43ef72ecc704911f28524646143.wav  \n",
            "  inflating: /content/data/wavs/5f2472787b7e281ce8a71197fc1d89d1.wav  \n",
            "  inflating: /content/data/wavs/bc79ffd3afa5cf4d9cc65d1ac7830e68.wav  \n",
            "  inflating: /content/data/wavs/43bfe95b666300d9b7d54d170c36840a.wav  \n",
            "  inflating: /content/data/wavs/e0ba4fcf42c0f1bc80fc00fcd3c25d06.wav  \n",
            "  inflating: /content/data/wavs/3ba826d9e2d642dcc7e4c3ecbaabac7f.wav  \n",
            "  inflating: /content/data/wavs/9e4398f3369480a03f4e3ce7f27d7a97.wav  \n",
            "  inflating: /content/data/wavs/3eacb05ec950b6ab5cb0baafd6269b5e.wav  \n",
            "  inflating: /content/data/wavs/e1b60933cf875554c9f8ec10e0b07f58.wav  \n",
            "  inflating: /content/data/wavs/6409acfd7168783cb93646cfe918c8bc.wav  \n",
            "  inflating: /content/data/wavs/4d3484390e073342c2398662c3c555fa.wav  \n",
            "  inflating: /content/data/wavs/3d48e7e80c0c5b6fc27e627e04234bbe.wav  \n",
            "  inflating: /content/data/wavs/abbae1e9a6350882cbebd7a5d860c8fa.wav  \n",
            "  inflating: /content/data/wavs/bda7c80d8846cb7d5fa700f1aaaf31ac.wav  \n",
            "  inflating: /content/data/wavs/e453594108c3e4750ce754c155c0b5d0.wav  \n",
            "  inflating: /content/data/wavs/06868256b13e8e5871450d4f8238e81b.wav  \n",
            "  inflating: /content/data/wavs/57ba651d269c1466304ee7a85bc2f89d.wav  \n",
            "  inflating: /content/data/wavs/7017f5726df8288cd44f964d4825e3c4.wav  \n",
            "  inflating: /content/data/wavs/27b9dc7a4ad31bfa5c9bc487762e11b6.wav  \n",
            "  inflating: /content/data/wavs/4156006657c8b3839889bedf50834f21.wav  \n",
            "  inflating: /content/data/wavs/a07d3ce2f96bc62fcf8b40ff7c9b3293.wav  \n",
            "  inflating: /content/data/wavs/775483d1d726b303b0a2f4500cff8756.wav  \n",
            "  inflating: /content/data/wavs/f38fcc65fd38742d6d3e6ef0ec690701.wav  \n",
            "  inflating: /content/data/wavs/53b9dfd0cf25d82c1f6901934955d18a.wav  \n",
            "  inflating: /content/data/wavs/5eea62000057e20881e066ab62ba2669.wav  \n",
            "  inflating: /content/data/wavs/55665f82e145bf184974c8f5b53f9e0a.wav  \n",
            "  inflating: /content/data/wavs/9ab42bdbba856535fbe84d67dd8d8a93.wav  \n",
            "  inflating: /content/data/wavs/f0260d59511ee3794716e3fb1052daa9.wav  \n",
            "  inflating: /content/data/wavs/4135c13b0d4ea72054569a7e62d8d47a.wav  \n",
            "  inflating: /content/data/wavs/ec4d21a013521c2f29c86e020e4a3243.wav  \n",
            "  inflating: /content/data/wavs/a72fba8205737fa37c0b6d32e70f9f35.wav  \n",
            "  inflating: /content/data/wavs/25d84567e477924c16f9ba975a835897.wav  \n",
            "  inflating: /content/data/wavs/a8c4cc3daa731c5ef5185d4a7feb676b.wav  \n",
            "  inflating: /content/data/wavs/62afb99be760d0641e7246882bf50016.wav  \n",
            "  inflating: /content/data/wavs/d8833bc3665a55712b27edc51b1e300e.wav  \n",
            "  inflating: /content/data/wavs/dd0c10ff4ad0e1b5cde222d5c5df3abc.wav  \n",
            "  inflating: /content/data/wavs/e31fdc6a386cf49863c5998171cc4499.wav  \n",
            "  inflating: /content/data/wavs/50f3b02dd85a99ea337d06ee8e8caa16.wav  \n",
            "  inflating: /content/data/wavs/4b4380ae5498cb451a3bd65944703076.wav  \n",
            "  inflating: /content/data/wavs/09121b2a0e73d7e20a765abf42b00043.wav  \n",
            "  inflating: /content/data/wavs/b03f5da041e0c7afe5e45a6a72b48a51.wav  \n",
            "  inflating: /content/data/wavs/739cdb9fa10793a923daf39d676c0cc5.wav  \n",
            "  inflating: /content/data/wavs/fc8d2de1d1b9a25d92c717df8393748f.wav  \n",
            "  inflating: /content/data/wavs/2d3da389702d188907d038ed1e2968bf.wav  \n",
            "  inflating: /content/data/wavs/762c7d7e5c0e6da6d6c609d09e42f903.wav  \n",
            "  inflating: /content/data/wavs/63b82012629b2614005c44333d179be5.wav  \n",
            "  inflating: /content/data/wavs/0590040671f414406ccb0e8c47490eae.wav  \n",
            "  inflating: /content/data/wavs/814b305e618784555bbcf0b43c645514.wav  \n",
            "  inflating: /content/data/wavs/d4c33686ec80ab2560a9382be1798234.wav  \n",
            "  inflating: /content/data/wavs/7ef4e8ff619ce6dece1bc0122fb4dd58.wav  \n",
            "  inflating: /content/data/wavs/40a2396529fd901a85912230363ef9e8.wav  \n",
            "  inflating: /content/data/wavs/91cfd8b4efa1a360d3410ea7f1a7ca35.wav  \n",
            "  inflating: /content/data/wavs/a3a7b84aafb1829f5b2a8e7a3ce1c3fd.wav  \n",
            "  inflating: /content/data/wavs/d64bc690ace638bc2a4e4295cc14fa5f.wav  \n",
            "  inflating: /content/data/wavs/43704b06e388fc07bd39efa0c1625fc6.wav  \n",
            "  inflating: /content/data/wavs/7984dd9e1fb46c5f54fbbc818d6ef5a1.wav  \n",
            "  inflating: /content/data/wavs/1a2558d371cf7a9cab2edc0fb4f97629.wav  \n",
            "  inflating: /content/data/wavs/4494b935f42c6c816cc40f6e266bb329.wav  \n",
            "  inflating: /content/data/wavs/01822b9e4a49835ad06ad5f3977e6557.wav  \n",
            "  inflating: /content/data/wavs/0e86fb095b0cc5e14e9c8ef25e3cbf6d.wav  \n",
            "  inflating: /content/data/wavs/2a49c9f55e797597dad438521d80a705.wav  \n",
            "  inflating: /content/data/wavs/89dd16ad2948ca9ba4e6b82aad32b89d.wav  \n",
            "  inflating: /content/data/wavs/3d0d8f808bb4de1e803488f8b6b630dd.wav  \n",
            "  inflating: /content/data/wavs/f7929596ec88a7bb53ad3fa11af25711.wav  \n",
            "  inflating: /content/data/wavs/9fdc004574239b5b5a0b1900b073fbf6.wav  \n",
            "  inflating: /content/data/wavs/136e1b7dacaa94fdf978c1ed832e49fe.wav  \n",
            "  inflating: /content/data/wavs/f59e3186737a907f10164c6bd06a863f.wav  \n",
            "  inflating: /content/data/wavs/4068d1ef6a3f2ae804b177b0696ef24d.wav  \n",
            "  inflating: /content/data/wavs/7bcd15658be4a5791a96fe145ce9ca21.wav  \n",
            "  inflating: /content/data/wavs/0c84106e37bfda5df2daa2f356ff1d6c.wav  \n",
            "  inflating: /content/data/wavs/9cfd6fecbc4c13165d201b9194cdfab7.wav  \n",
            "  inflating: /content/data/wavs/2e09cd666a85e854ae579d099e3e44f3.wav  \n",
            "  inflating: /content/data/wavs/3acf8659958a36f399be16b05edbd72b.wav  \n",
            "  inflating: /content/data/wavs/26ed3015345f725356e4b3efdc91e4f6.wav  \n",
            "  inflating: /content/data/wavs/af938e0dee1e9475803f58d838732147.wav  \n",
            "  inflating: /content/data/wavs/db702ab1077eb4d41a5c03bc7f5577a4.wav  \n",
            "  inflating: /content/data/wavs/62ca6b7b2955483f0883e8220af798b6.wav  \n",
            "  inflating: /content/data/wavs/2a84d391866467c04dca85de735c4855.wav  \n",
            "  inflating: /content/data/wavs/c214eae2e972aed36a98c667579c764b.wav  \n",
            "  inflating: /content/data/wavs/9be19d520a97c7465f76257034d325c3.wav  \n",
            "  inflating: /content/data/wavs/28a24589918da335128cdd09292df8b5.wav  \n",
            "  inflating: /content/data/wavs/e6c209ff6b1e8f4e6bef3604db98f2d3.wav  \n",
            "  inflating: /content/data/wavs/f35df03bb29b394c131e9c3a3020a6f7.wav  \n",
            "  inflating: /content/data/wavs/dac8b39104c22ba1b63f53e60121b0f7.wav  \n",
            "  inflating: /content/data/wavs/9f86d33308e7131e69cffa234d76b08b.wav  \n",
            "  inflating: /content/data/wavs/69f4ca4a657d753b5530f59b5ca0de80.wav  \n",
            "  inflating: /content/data/wavs/8e793bb7ba7a54d4c5e977af5da21d03.wav  \n",
            "  inflating: /content/data/wavs/473d5387b0787a4170f465e16f40926b.wav  \n",
            "  inflating: /content/data/wavs/dff28fef59f062681bc5d239572dccec.wav  \n",
            "  inflating: /content/data/wavs/d5b5d899319039324a605cc8837e3727.wav  \n",
            "  inflating: /content/data/wavs/4f3e34be19f8e1637a85622ebe0a84e8.wav  \n",
            "  inflating: /content/data/wavs/9bbe13d2606a08dfec3ac79054aea8d7.wav  \n",
            "  inflating: /content/data/wavs/9ab6b22297263b891976255051350470.wav  \n",
            "  inflating: /content/data/wavs/6adaa501ee07f866bb2f8cb8a4cf283a.wav  \n",
            "  inflating: /content/data/wavs/f51d7b3e21ceda08319687a948b8d9ef.wav  \n",
            "  inflating: /content/data/wavs/f076e72558ba7118f1403e69ab1c20b0.wav  \n",
            "  inflating: /content/data/wavs/c83f3bb3c1eb00cff56450afb7295b3e.wav  \n",
            "  inflating: /content/data/wavs/b66a925d25eedbd9deb7c37812eb4f4f.wav  \n",
            "  inflating: /content/data/wavs/445d8e72f26fe661828a27ec018cd6ca.wav  \n",
            "  inflating: /content/data/wavs/c7bddd7a11f804ffc6142c68eb870bda.wav  \n",
            "  inflating: /content/data/wavs/889b7567a439428d9c2590f4ba952d8b.wav  \n",
            "  inflating: /content/data/wavs/9c7252d7433cf1eef68b27a107c2e431.wav  \n",
            "  inflating: /content/data/wavs/ff773bc38f0addb1bbfdf6043e11b330.wav  \n",
            "  inflating: /content/data/wavs/8d3ff6d8bffff57870ca1408b5f759d0.wav  \n",
            "  inflating: /content/data/wavs/934671ccb46dc5f15a1154f628e9159c.wav  \n",
            "  inflating: /content/data/wavs/a384af110b4d2957133b76d0e7d4adf4.wav  \n",
            "  inflating: /content/data/wavs/c121c809700d6399ddd3ad537ff8a8a2.wav  \n",
            "  inflating: /content/data/wavs/74ca04aa586c53e2b60b446773195960.wav  \n",
            "  inflating: /content/data/wavs/6e6f0154f83eae6a783c6f9935d1167f.wav  \n",
            "  inflating: /content/data/wavs/2904fd9f7aea70a41a6c1b28922cf25a.wav  \n",
            "  inflating: /content/data/wavs/b3b28b2d9769afbe53ea65f6f5328192.wav  \n",
            "  inflating: /content/data/wavs/7148efa84737eadd29164309c93573ba.wav  \n",
            "  inflating: /content/data/wavs/cde5b6b22a36e8cb245a5ddabeede4fb.wav  \n",
            "  inflating: /content/data/wavs/da2f1ef4c5f4f18164291bc5d8f12928.wav  \n",
            "  inflating: /content/data/wavs/0019c79cf749b4fdc36fb66d45fde055.wav  \n",
            "  inflating: /content/data/wavs/f96fd0800065bac1cbd394a31d554e9e.wav  \n",
            "  inflating: /content/data/wavs/6ce2dafa9636ff83b073de26b76064ea.wav  \n",
            "  inflating: /content/data/wavs/4c2451b291f54c72abe974058990c0cf.wav  \n",
            "  inflating: /content/data/wavs/379f3aacc2e740c6c421ef8f8c3ba64f.wav  \n",
            "  inflating: /content/data/wavs/22c513c3d13f78f28afa73f8c89105ab.wav  \n",
            "  inflating: /content/data/wavs/665d37ff0fd5d630c4c1162235d56a6b.wav  \n",
            "  inflating: /content/data/wavs/57a6d0f61b079fbcb8906f95bc6310ea.wav  \n",
            "  inflating: /content/data/wavs/fab97248703177744168ae8fbf871f05.wav  \n",
            "  inflating: /content/data/wavs/94823846c6eaf806f7945bfef7bce87d.wav  \n",
            "  inflating: /content/data/wavs/624186230fb7a7a011cf5122dcaa88e7.wav  \n",
            "  inflating: /content/data/wavs/d9c98ed02f746e25d4ee1cbfe58b165d.wav  \n",
            "  inflating: /content/data/wavs/400bc1881929aaa23f9208dc4a11e466.wav  \n",
            "  inflating: /content/data/wavs/443bcf7195d5c2a0f2d3e796a246d5a2.wav  \n",
            "  inflating: /content/data/wavs/ef789be175dccbc7d1266277fc79e296.wav  \n",
            "  inflating: /content/data/wavs/0bde77826cfeca8c26eada4794cbfcb5.wav  \n",
            "  inflating: /content/data/wavs/ca68e8442afe1a0b25d75c17135a10fc.wav  \n",
            "  inflating: /content/data/wavs/e4668187d7dd747b24efb92220e360f5.wav  \n",
            "  inflating: /content/data/wavs/674507c3f61d0b27064cd160fc059d43.wav  \n",
            "  inflating: /content/data/wavs/b7a97cb5687a1b365d555f36dabc83b9.wav  \n",
            "  inflating: /content/data/wavs/64315bcaaf5514fc5b5c51a6b2a29592.wav  \n",
            "  inflating: /content/data/wavs/d5f76596d94c03788fcd6f138ae8bf14.wav  \n",
            "  inflating: /content/data/wavs/0cdc05dab9f37fa5e774770d6c487952.wav  \n",
            "  inflating: /content/data/wavs/88b366a4c3ac02de383e2314cb53819b.wav  \n",
            "  inflating: /content/data/wavs/a36a5a98e589ae97c3cd61daf371d7da.wav  \n",
            "  inflating: /content/data/wavs/83763b255a96cd2ee83a57ff229f8b32.wav  \n",
            "  inflating: /content/data/wavs/4868b0f05000e19df48c4d41524c3764.wav  \n",
            "  inflating: /content/data/wavs/4ca0f313d2cdf12e2669a490694517d4.wav  \n",
            "  inflating: /content/data/wavs/e6a0af21fb772e08b926918fe4043c09.wav  \n",
            "  inflating: /content/data/wavs/64e371a68e21ec019277edddb61607b9.wav  \n",
            "  inflating: /content/data/wavs/c566ab937855c9efa0bba667d60a813b.wav  \n",
            "  inflating: /content/data/wavs/e202291803028177e7b21fa9854217ff.wav  \n",
            "  inflating: /content/data/wavs/bc1b683b464827ce72dc2a3783247eed.wav  \n",
            "  inflating: /content/data/wavs/a082e87de5cb41952434b090b09a37e7.wav  \n",
            "  inflating: /content/data/wavs/3583e1836d9f8a540999158239de7371.wav  \n",
            "  inflating: /content/data/wavs/9724ed73be8922f770e85b04aec38749.wav  \n",
            "  inflating: /content/data/wavs/2e18a4a11d8571bd1cf425c2cfb52612.wav  \n",
            "  inflating: /content/data/wavs/1b853fa2de6fa2cc9f2635a6f3392e45.wav  \n",
            "  inflating: /content/data/wavs/33b6de6a78f4d3d191e97db9fac4327c.wav  \n",
            "  inflating: /content/data/wavs/6a76b7bd81fd631969c025b01d831dd7.wav  \n",
            "  inflating: /content/data/wavs/d1e560a4c090272c43c91cccdd26033d.wav  \n",
            "  inflating: /content/data/wavs/74056afbda843162fe335fbaf9dd67ee.wav  \n",
            "  inflating: /content/data/wavs/4debfd2f807aeb8bd7ecb7df4d58ec24.wav  \n",
            "  inflating: /content/data/wavs/3c9855f1a2b422231cb75dac20508cf4.wav  \n",
            "  inflating: /content/data/wavs/d8919094c3327c397bd6902a8c0f32e4.wav  \n",
            "  inflating: /content/data/wavs/3a848e5127b6b4afeef92592f09435c0.wav  \n",
            "  inflating: /content/data/wavs/d629381b3e225dd7ab11890d6ff9953f.wav  \n",
            "  inflating: /content/data/wavs/df77e7b0f3babefce25dff18c745cd0b.wav  \n",
            "  inflating: /content/data/wavs/3faa3f9dd278e9e25f1f874ed99b7524.wav  \n",
            "  inflating: /content/data/wavs/b6bc2a281d088a7f55667bf09153f21b.wav  \n",
            "  inflating: /content/data/wavs/488e071d1842e78733da42c6e88e87f8.wav  \n",
            "  inflating: /content/data/wavs/745eebbc3eab4ee139c0425d044cc690.wav  \n",
            "  inflating: /content/data/wavs/254a749d466037b1548e85816e251451.wav  \n",
            "  inflating: /content/data/wavs/8028ba956620ea42cd33e3e5c16bd5c8.wav  \n",
            "  inflating: /content/data/wavs/b8c632418666032efedbaa6c189cb198.wav  \n",
            "  inflating: /content/data/wavs/cc4de4a0e0dd15cf9ef9305a9c8f9f6c.wav  \n",
            "  inflating: /content/data/wavs/e8539bb255e380ff74c53576560aefb1.wav  \n",
            "  inflating: /content/data/wavs/ad2f503a1dd1db59f49c4a0a712d2958.wav  \n",
            "  inflating: /content/data/wavs/afcb12bd4f0e468d79a9cf4138880e5a.wav  \n",
            "  inflating: /content/data/wavs/c76472d9a6640ae542629e14cdde3b0a.wav  \n",
            "  inflating: /content/data/wavs/a2763b6ffaf50cc654cb17f251b15e31.wav  \n",
            "  inflating: /content/data/wavs/699810d55537482fda79432653bf2289.wav  \n",
            "  inflating: /content/data/wavs/a74702219e14b955ccb14b2af9bd9b15.wav  \n",
            "  inflating: /content/data/wavs/ac23573810341ccd48813ef8f039ee84.wav  \n",
            "  inflating: /content/data/wavs/d45f6bed4967549b0ba0e5b65ca6ce20.wav  \n",
            "  inflating: /content/data/wavs/c11dcf63c08069ad6f5b022398ca8aa8.wav  \n",
            "  inflating: /content/data/wavs/6c74239181d1edd8e585c8271ea4800e.wav  \n",
            "  inflating: /content/data/wavs/bb9557a5da29a385b2426b62df394731.wav  \n",
            "  inflating: /content/data/wavs/35345b615d41b37774762c9cfdc9de43.wav  \n",
            "  inflating: /content/data/wavs/ea85fff6bd82f6149dd9b2ca7b67456e.wav  \n",
            "  inflating: /content/data/wavs/0920497c495dc734ead7848b714209a0.wav  \n",
            "  inflating: /content/data/wavs/b83b724d481cda62661a472c187c9e4d.wav  \n",
            "  inflating: /content/data/wavs/3bcbbe4a908de45dcaf93b6335554f81.wav  \n",
            "  inflating: /content/data/wavs/b53b3faa6cd60ac26a571de2c443a00e.wav  \n",
            "  inflating: /content/data/wavs/8a7135e387ff76664b6ead2284e46664.wav  \n",
            "  inflating: /content/data/wavs/c1be130e64a3827d0c8e09aa5f8b331d.wav  \n",
            "  inflating: /content/data/wavs/6b67d9d6677d7862430ccdace1100de3.wav  \n",
            "  inflating: /content/data/wavs/a6b20ecace1765a6d1d3e549385ce745.wav  \n",
            "  inflating: /content/data/wavs/ffd800b9cd1b153d7f8a7cbbf3ed82c6.wav  \n",
            "  inflating: /content/data/wavs/e0f3c23fa9d6102b3c61baff1ae3d22d.wav  \n",
            "  inflating: /content/data/wavs/8cae8ffd69082402cf160847f2496372.wav  \n",
            "  inflating: /content/data/wavs/96efae1b4222c2c05a0e9696a721cd58.wav  \n",
            "  inflating: /content/data/wavs/9f4de1b30237dc3956ed0fc6646e5a1b.wav  \n",
            "  inflating: /content/data/wavs/a30e422115c9961862843542d4586ac8.wav  \n",
            "  inflating: /content/data/wavs/c08f815a8bdae402e5b36795cbfef35c.wav  \n",
            "  inflating: /content/data/wavs/4b74114b7b8074d2f9ff6d86539b800b.wav  \n",
            "  inflating: /content/data/wavs/cd87449c71b814f74b8c230009ee266f.wav  \n",
            "  inflating: /content/data/wavs/87137f55d4d87caa2b47c3afa0250ef9.wav  \n",
            "  inflating: /content/data/wavs/53344b2e501818a1578fa5205c428463.wav  \n",
            "  inflating: /content/data/wavs/d11fbd4058051247816c317616f8262d.wav  \n",
            "  inflating: /content/data/wavs/3a7079ab497c0acdbf61d9217e02cfd8.wav  \n",
            "  inflating: /content/data/wavs/5e3e8cdece6329752c6cf995a611dd4c.wav  \n",
            "  inflating: /content/data/wavs/1f85432274ebc85121127ad5b4ecc761.wav  \n",
            "  inflating: /content/data/wavs/ddbef8574eaa43b2d700eec709ef60d9.wav  \n",
            "  inflating: /content/data/wavs/c3e4fe978d2bd2d8df192ec232d0f591.wav  \n",
            "  inflating: /content/data/wavs/eaa3befd2e145d859c5537c0003af76a.wav  \n",
            "  inflating: /content/data/wavs/cce2baba87c7dbdb493fb9497bdc533d.wav  \n",
            "  inflating: /content/data/wavs/41e9294895082946298d4dc7240889a7.wav  \n",
            "  inflating: /content/data/wavs/eca3551593e2ed56251ec6de1409a613.wav  \n",
            "  inflating: /content/data/wavs/9ff4a4eced2d338bb8c7038bb277fbce.wav  \n",
            "  inflating: /content/data/wavs/c6bebfa739c4aaa22011dbaf2a0cf600.wav  \n",
            "  inflating: /content/data/wavs/e6a6b3571bf9427297e1a2d268c6cf5b.wav  \n",
            "  inflating: /content/data/wavs/36571bdc628b6c5cbba2b2f6812ce1c9.wav  \n",
            "  inflating: /content/data/wavs/7e92eac4cd13c89556e3ca1632bec5c4.wav  \n",
            "  inflating: /content/data/wavs/cea3dce62d044352297fc73290feba89.wav  \n",
            "  inflating: /content/data/wavs/f07ab0034024466eacd66f02247c1022.wav  \n",
            "  inflating: /content/data/wavs/43725ad476fa6d571fa161295b478891.wav  \n",
            "  inflating: /content/data/wavs/8ef17fbadaf15bb273c33cfcc9b47703.wav  \n",
            "  inflating: /content/data/wavs/7a3f82d366f656122cd7ddde6e9a69a9.wav  \n",
            "  inflating: /content/data/wavs/a03bdf3cf585ae471990e448625e54ba.wav  \n",
            "  inflating: /content/data/wavs/8e652d1b549f07edde8e2d51cf40d206.wav  \n",
            "  inflating: /content/data/wavs/f742a112c79f3b7d17d67a367963511c.wav  \n",
            "  inflating: /content/data/wavs/b8868531d8992051b96bdbfb06a7c93a.wav  \n",
            "  inflating: /content/data/wavs/402dcda4e30ec4c88ab65ec873fd5116.wav  \n",
            "  inflating: /content/data/wavs/a94738b44e94f8fc045375c0ee094634.wav  \n",
            "  inflating: /content/data/wavs/f8150db03b613a5898c9b5b46867c91e.wav  \n",
            "  inflating: /content/data/wavs/f9b954f56068623f9b2ee4053f1864dd.wav  \n",
            "  inflating: /content/data/wavs/373e1521a0fe78ffd7b4b27bd071355c.wav  \n",
            "  inflating: /content/data/wavs/272a9ecd564f41380a8b55510a27839b.wav  \n",
            "  inflating: /content/data/wavs/0e1d23dbbcdbb0eae8b66d7967f1a3bd.wav  \n",
            "  inflating: /content/data/wavs/8a4b20c7c804bcd1a62d2dc27c14f3bb.wav  \n",
            "  inflating: /content/data/wavs/b6065604c8a346c1e68c9b9fa134adf0.wav  \n",
            "  inflating: /content/data/wavs/d447104ea05e36c29cfeec178cf111f6.wav  \n",
            "  inflating: /content/data/wavs/d52f6b7e6e7e74296216026970af64dc.wav  \n",
            "  inflating: /content/data/wavs/40b7d1f0c5cc05335e3033f4280f063b.wav  \n",
            "  inflating: /content/data/wavs/bc9001fc553f4ad61f79f0ee167875ed.wav  \n",
            "  inflating: /content/data/wavs/98746c1f3c40ddb41a5b436117f05369.wav  \n",
            "  inflating: /content/data/wavs/9a171fd9188e4e69d7fe3ecca2cdfa08.wav  \n",
            "  inflating: /content/data/wavs/1cff5bcccc2df36c1821c833f2fd9457.wav  \n",
            "  inflating: /content/data/wavs/b46825023cdefc4ccd7211de1067fbcb.wav  \n",
            "  inflating: /content/data/wavs/d4e03965e3ae0add0f42b90a05385ee3.wav  \n",
            "  inflating: /content/data/wavs/e3a08a6ffde956bb9fd6c22720979144.wav  \n",
            "  inflating: /content/data/wavs/bdbab1cc46450636a85870b074e78bd8.wav  \n",
            "  inflating: /content/data/wavs/1a85c51820292009428d910dc8e7b0f1.wav  \n",
            "  inflating: /content/data/wavs/3a32c67f5ead5fadbf84ee5dabdd40d6.wav  \n",
            "  inflating: /content/data/wavs/ff21f073cdfd4b7ebd09e53f07750f9f.wav  \n",
            "  inflating: /content/data/wavs/485422ecc57a500281932c685065a1b5.wav  \n",
            "  inflating: /content/data/wavs/75aec85e70b1c58ef2f2250edd00a89c.wav  \n",
            "  inflating: /content/data/wavs/0dbdc315b7e6a230fce984dad5cb77fc.wav  \n",
            "  inflating: /content/data/wavs/eb75672c3661db7e9a3b6ab357502ec2.wav  \n",
            "  inflating: /content/data/wavs/fe40bd7dee8371de368565c77fce6403.wav  \n",
            "  inflating: /content/data/wavs/ad93abc24a339e507fa9e26f0afb00b5.wav  \n",
            "  inflating: /content/data/wavs/7ee7e85ed6a785e43cee32fa28ff207f.wav  \n",
            "  inflating: /content/data/wavs/47ff4d7dabe9c3712e548e29a7727e1e.wav  \n",
            "  inflating: /content/data/wavs/5612c79312c6cbe21ed9c6c78cb210ea.wav  \n",
            "  inflating: /content/data/wavs/d9611b47a5563d882ccf9c3a753152de.wav  \n",
            "  inflating: /content/data/wavs/f0fc18a49f35890ecc855bf6aeddb21f.wav  \n",
            "  inflating: /content/data/wavs/d8e4aa75b10c5309dda1e8157573b5a4.wav  \n",
            "  inflating: /content/data/wavs/bbfca522466fae9b92577d4635ae1679.wav  \n",
            "  inflating: /content/data/wavs/4fa0efc01264e2aa946d3ad5c060e3bc.wav  \n",
            "  inflating: /content/data/wavs/6cc006e34bc53a8700aa3af8694f2dbd.wav  \n",
            "  inflating: /content/data/wavs/98ddf9b1126d1a953d63e689f7c77d28.wav  \n",
            "  inflating: /content/data/wavs/40f9ee8d0d8872c5fa13ab912970b00b.wav  \n",
            "  inflating: /content/data/wavs/66e3cc0a44470e146a5bd11d6a22637f.wav  \n",
            "  inflating: /content/data/wavs/6efc2535d4fb1646f13fcf0caca41aa4.wav  \n",
            "  inflating: /content/data/wavs/7e8aed52709165ef31d3d142743369ee.wav  \n",
            "  inflating: /content/data/wavs/65c3c688f97a036ca74b314087f5235d.wav  \n",
            "  inflating: /content/data/wavs/cdbd6ed6e1eb44b09ac35d1eaface87e.wav  \n",
            "  inflating: /content/data/wavs/b55c002378a7aa8b9cbf7d1f8c274780.wav  \n",
            "  inflating: /content/data/wavs/7ecc7d6bcd7c2246b675c60608aaceb9.wav  \n",
            "  inflating: /content/data/wavs/38a1048b2c9998fdcadecbea60d9d755.wav  \n",
            "  inflating: /content/data/wavs/b8a1ad1b2370347d95abc07c5d302ede.wav  \n",
            "  inflating: /content/data/wavs/b6e95c4752bc101670b4283175b815c8.wav  \n",
            "  inflating: /content/data/wavs/a62be19a01593a5f3e807d074ca9a042.wav  \n",
            "  inflating: /content/data/wavs/5b8cb7b77db8531248261622b8aceef3.wav  \n",
            "  inflating: /content/data/wavs/7553bf3ed3eb22b37214fd10bd841974.wav  \n",
            "  inflating: /content/data/wavs/4368e815b494b7d60c50c70c0231f569.wav  \n",
            "  inflating: /content/data/wavs/eb4958f55076312796928757eaed2ac9.wav  \n",
            "  inflating: /content/data/wavs/c3d3d4efbc74809c5b8874862b5b958b.wav  \n",
            "  inflating: /content/data/wavs/42caaa819f6f1e45ab934d2481f436ce.wav  \n",
            "  inflating: /content/data/wavs/d7d0748ffc6e325e442400ca6fed5ce1.wav  \n",
            "  inflating: /content/data/wavs/2e5bcde0e4550c45cea31085ecc42eb6.wav  \n",
            "  inflating: /content/data/wavs/68ae823d52c1041bffd45af86597b616.wav  \n",
            "  inflating: /content/data/wavs/223ee403ee4e7a3dba34fa84cf8d118a.wav  \n",
            "  inflating: /content/data/wavs/bf77e539f9323fe0f0a6bd68ee84f729.wav  \n",
            "  inflating: /content/data/wavs/6bf0660a465ed69ccef483b3218c2f2c.wav  \n",
            "  inflating: /content/data/wavs/1e93b80c4fa6bdc577291ea620b57285.wav  \n",
            "  inflating: /content/data/wavs/b2fcb7fe420172e38c85f29f9b17723f.wav  \n",
            "  inflating: /content/data/wavs/864ab43a915efd53ff69c78f7ec82d94.wav  \n",
            "  inflating: /content/data/wavs/d5bb42af9118adabdd511c6da1fde35e.wav  \n",
            "  inflating: /content/data/wavs/d0709016a28f7a31c03bd3695a23bcfd.wav  \n",
            "  inflating: /content/data/wavs/6885014a5bdfda15abdf65ad88f17de8.wav  \n",
            "  inflating: /content/data/wavs/cba6f444ca16082282a99e800f5ba803.wav  \n",
            "  inflating: /content/data/wavs/d7578c3bc446e88aa0b83e96a37feeb6.wav  \n",
            "  inflating: /content/data/wavs/0f374d2ce7e394c9d2dadcdd745de9b4.wav  \n",
            "  inflating: /content/data/wavs/3ce0b3ff1fc728fb3fc042d1ead51d95.wav  \n",
            "  inflating: /content/data/wavs/f45ef1853f09c926747a137cd8235e2d.wav  \n",
            "  inflating: /content/data/wavs/eaa6ecb97c8081a7319c6049d399b3af.wav  \n",
            "  inflating: /content/data/wavs/afad86edb1d164d8e0708bb9b7ebe51d.wav  \n",
            "  inflating: /content/data/wavs/de9f167a7c0585cb3974ef51895542f7.wav  \n",
            "  inflating: /content/data/wavs/bf6d4d5bc63213b0a1fe056028d7149b.wav  \n",
            "  inflating: /content/data/wavs/40bfb2a2ab4da1a2b3e43776cdef8538.wav  \n",
            "  inflating: /content/data/wavs/c37bd1b14a81e56b800590ea488f3d6c.wav  \n",
            "  inflating: /content/data/wavs/8a0a0fffd60524b46d296d8383ebd45a.wav  \n",
            "  inflating: /content/data/wavs/2e4a771782899791dd0e5034e73bfa9e.wav  \n",
            "  inflating: /content/data/wavs/541607211e1820dfda4e2e55df46e71b.wav  \n",
            "  inflating: /content/data/wavs/422b02ab48434d3152129a8d9c3a874b.wav  \n",
            "  inflating: /content/data/wavs/2c6f012cbe9ff095dc06f3bf3f425321.wav  \n",
            "  inflating: /content/data/wavs/8c9c178c510f4a6cb28501ce25491456.wav  \n",
            "  inflating: /content/data/wavs/373a2290e07e136a33a037ad46854661.wav  \n",
            "  inflating: /content/data/wavs/65f2219df14fcb9d7731caaa326bd7c4.wav  \n",
            "  inflating: /content/data/wavs/5883327e461cca91d90c51f2b03ba992.wav  \n",
            "  inflating: /content/data/wavs/3e55ac74422c80f20ff01e7104ee998c.wav  \n",
            "  inflating: /content/data/wavs/5c02b91427a0193ee659e41343817db0.wav  \n",
            "  inflating: /content/data/wavs/3d1fb32b1a820133fb7c780956f479dc.wav  \n",
            "  inflating: /content/data/wavs/c5dfa76a299ccd673ecf9b6d47c6bd52.wav  \n",
            "  inflating: /content/data/wavs/a80405629728868ebfe501c9ca9bc8b7.wav  \n",
            "  inflating: /content/data/wavs/55c8cc740b4fd061e69abb499cf8f896.wav  \n",
            "  inflating: /content/data/wavs/8c755697c911f20ae54765c99c56ea7e.wav  \n",
            "  inflating: /content/data/wavs/925ed97e39d03f3532015e5262b5eb2f.wav  \n",
            "  inflating: /content/data/wavs/acbe2cb5bae5acde15ff43230b3a2ff8.wav  \n",
            "  inflating: /content/data/wavs/bb455f50b704dd57f2574912897358ad.wav  \n",
            "  inflating: /content/data/wavs/ef2656dcb594ea2ebfe159ab019b720b.wav  \n",
            "  inflating: /content/data/wavs/3b4d13428da128efc4d2e76244d5d4c8.wav  \n",
            "  inflating: /content/data/wavs/60e4f15759bdce7c22aff3c2840e8773.wav  \n",
            "  inflating: /content/data/wavs/3f5c07177a8ce3558b9e68b5f807e46f.wav  \n",
            "  inflating: /content/data/wavs/2dfab1f84032d4446326e4dfe54acae3.wav  \n",
            "  inflating: /content/data/wavs/714c478ca1e656d0eee653847e24bbf5.wav  \n",
            "  inflating: /content/data/wavs/a381f130ff5edbb5c045e95c4af3fd74.wav  \n",
            "  inflating: /content/data/wavs/cd10b0bb1de809f72135675da5821ab6.wav  \n",
            "  inflating: /content/data/wavs/946a1b33ce66c13677be09b354503dfb.wav  \n",
            "  inflating: /content/data/wavs/da1fb6beeb3a55f6bcb323ef5d22c828.wav  \n",
            "  inflating: /content/data/wavs/a00d38866ab1d8d64916d0b7f008d5a4.wav  \n",
            "  inflating: /content/data/wavs/dfb0c4077339a61372c17cea5f2f7cda.wav  \n",
            "  inflating: /content/data/wavs/6d984a4b0009ac4b24762331f474829b.wav  \n",
            "  inflating: /content/data/wavs/1b426986540f2b031276a2123312a142.wav  \n",
            "  inflating: /content/data/wavs/4a0fc83dd455d51ce99268b45ef77e02.wav  \n",
            "  inflating: /content/data/wavs/cfa0d5919f9cb6a771788b7a6a65ae86.wav  \n",
            "  inflating: /content/data/wavs/81df2ec1d477f0a11b7436705861d7a7.wav  \n",
            "  inflating: /content/data/wavs/70c2615016f1ce53c985739ec608423b.wav  \n",
            "  inflating: /content/data/wavs/87e5185498a82a06181ecb62beedf616.wav  \n",
            "  inflating: /content/data/wavs/1b68ab964d11867024f7d98d4285d91a.wav  \n",
            "  inflating: /content/data/wavs/63f16c3767f9d84e5c676cfa6a05eb98.wav  \n",
            "  inflating: /content/data/wavs/f6d3e867b6bc46e77cb53631dce4086b.wav  \n",
            "  inflating: /content/data/wavs/289ca8eeac3a504c81728b4d0b113e34.wav  \n",
            "  inflating: /content/data/wavs/668adb85f900ab6f7680d4f17344af65.wav  \n",
            "  inflating: /content/data/wavs/6beea2711f32223b1f2a9e9e8392eade.wav  \n",
            "  inflating: /content/data/wavs/2748e0d28d20e72a058beceb98576f0b.wav  \n",
            "  inflating: /content/data/wavs/2622a2eb6d4aac894ca4bd3eeba70e5b.wav  \n",
            "  inflating: /content/data/wavs/e5597ea2b07e57a8b0a7f3ea27807ef3.wav  \n",
            "  inflating: /content/data/wavs/cba622a745fa0265e5304f5bd7aa69f8.wav  \n",
            "  inflating: /content/data/wavs/5eac5d1b6a3c9605f54e7419aae14cb1.wav  \n",
            "  inflating: /content/data/wavs/9af2519b76a8472187444842e666b544.wav  \n",
            "  inflating: /content/data/wavs/c714638e4f5277d5cb143bf7b44e78a7.wav  \n",
            "  inflating: /content/data/wavs/c5bb0980a550ccce16f839004c63100b.wav  \n",
            "  inflating: /content/data/wavs/7419c76d3fa09c21cbd447b9fe49e94a.wav  \n",
            "  inflating: /content/data/wavs/3485c7c892bd709d71fe4fe0b5b641d7.wav  \n",
            "  inflating: /content/data/wavs/86153adaa7a4bbdf660ffa1d6a1e2351.wav  \n",
            "  inflating: /content/data/wavs/33f28039e35ea99725f1888d210c9afe.wav  \n",
            "  inflating: /content/data/wavs/233e6cf689d04c24b3135b38ed591d06.wav  \n",
            "  inflating: /content/data/wavs/2c09187e2f462fd98752fdb744b94d1c.wav  \n",
            "  inflating: /content/data/wavs/641f7e69750b0f894e3ea0b3cf187314.wav  \n",
            "  inflating: /content/data/wavs/54a1e23b50584fc0646869761435de77.wav  \n",
            "  inflating: /content/data/wavs/4c301a1420e67877c570b4adc5386688.wav  \n",
            "  inflating: /content/data/wavs/d6bc80d2f783330d317cf47b7ec3dc18.wav  \n",
            "  inflating: /content/data/wavs/a7bcd4185106ac80d02930b106109386.wav  \n",
            "  inflating: /content/data/wavs/5de3d8d06f6c3c9eecc50344f3382b63.wav  \n",
            "  inflating: /content/data/wavs/a977c924183b5ce54c70ef3989faa9f6.wav  \n",
            "  inflating: /content/data/wavs/e4e46efc71e84d15d3ed9953c851b718.wav  \n",
            "  inflating: /content/data/wavs/6b15b3345b4b6f8b090985dd652bc178.wav  \n",
            "  inflating: /content/data/wavs/ab1884405c1b9fb265ceb02c2916f454.wav  \n",
            "  inflating: /content/data/wavs/4681c51e8c918761d079cdb738b25130.wav  \n",
            "  inflating: /content/data/wavs/d8c2e28b662f18c96de976e0144f208b.wav  \n",
            "  inflating: /content/data/wavs/94bf12740ce304479a788a35373870b8.wav  \n",
            "  inflating: /content/data/wavs/508c4178227c42c26ee1b80ae224bb71.wav  \n",
            "  inflating: /content/data/wavs/fcf001cfbcb7f9df2decb6a80838c128.wav  \n",
            "  inflating: /content/data/wavs/46c80423b3a0a18021a9f7aaf5f73623.wav  \n",
            "  inflating: /content/data/wavs/c909190f29c7fe3954428c9fd1c8fb2b.wav  \n",
            "  inflating: /content/data/wavs/f43ac8e2c2edbf9ce4ef17b11630efd1.wav  \n",
            "  inflating: /content/data/wavs/426ee33ce0a6bfdaca880a5a4b332623.wav  \n",
            "  inflating: /content/data/wavs/101cab3c2661b9199eb4a5703c3bee20.wav  \n",
            "  inflating: /content/data/wavs/a15b4fc907be021e1d9562bf5016a805.wav  \n",
            "  inflating: /content/data/wavs/fbb83a0f54f7f2e294c1be11ebb22c93.wav  \n",
            "  inflating: /content/data/wavs/bfbdcafee95d2d123d622b70628eb70d.wav  \n",
            "  inflating: /content/data/wavs/8d791449afe7c178c8dadb185a41d743.wav  \n",
            "  inflating: /content/data/wavs/148ce8592709c0a17d01e02252f5e438.wav  \n",
            "  inflating: /content/data/wavs/eb8b9d5ab6be6d2dac8a829447d6ee65.wav  \n",
            "  inflating: /content/data/wavs/6026d62236cce9b21156c7bdb050d203.wav  \n",
            "  inflating: /content/data/wavs/bc5d84dd0b82cfa244aabf847f4be9dc.wav  \n",
            "  inflating: /content/data/wavs/607d6a58ee68a63c4e3931134b375c9d.wav  \n",
            "  inflating: /content/data/wavs/ecf5216abe87bf25cb4c799615bf07f6.wav  \n",
            "  inflating: /content/data/wavs/40086339c6714e33a9807364b17183bb.wav  \n",
            "  inflating: /content/data/wavs/fc4ca58e599e2a03e6185eefd5465fb3.wav  \n",
            "  inflating: /content/data/wavs/1e4d0f0ad47cf5fbfe7c47b868d3a370.wav  \n",
            "  inflating: /content/data/wavs/08f8fcdc310777f11683623868de376d.wav  \n",
            "  inflating: /content/data/wavs/cb8d255bc3ef825c0918ccfb0d44284b.wav  \n",
            "  inflating: /content/data/wavs/3bac6ddee4fcf553b1ddc30ae0d1ece3.wav  \n",
            "  inflating: /content/data/wavs/f90bd0ef98b6e721ee5952f534cbeb5f.wav  \n",
            "  inflating: /content/data/wavs/a7dd1b1c19f2d24b66c6e1e3ce10f29c.wav  \n",
            "  inflating: /content/data/wavs/1209b38db362dd85cd3c66d7d433ce9e.wav  \n",
            "  inflating: /content/data/wavs/248fea009f441d1a794b8279e41ad45d.wav  \n",
            "  inflating: /content/data/wavs/6aca47cb8b487e2f523c8524bbaf0e33.wav  \n",
            "  inflating: /content/data/wavs/fb19d72196e00a0e5c98c3568af4adf2.wav  \n",
            "  inflating: /content/data/wavs/4209e0827166d7f2ed2d0a0b901654e3.wav  \n",
            "  inflating: /content/data/wavs/62569c76e559aed061074a7eabf59c0e.wav  \n",
            "  inflating: /content/data/wavs/f5e3b325a931c512167250d9c7cd46ae.wav  \n",
            "  inflating: /content/data/wavs/842232e3f8a14dcb1084821f1f0e3e44.wav  \n",
            "  inflating: /content/data/wavs/9c8eede5dfc890bd5f5b7ac8e17e152f.wav  \n",
            "  inflating: /content/data/wavs/f599283cd8dcbf0911ec8a22a483d6bb.wav  \n",
            "  inflating: /content/data/wavs/6f3c93a10ab5b6cf68a30cf5e05a5c0a.wav  \n",
            "  inflating: /content/data/wavs/a7f410543e08595e88f2f113df7b32a9.wav  \n",
            "  inflating: /content/data/wavs/d602e83f04f6e6289837b1cd63d6f651.wav  \n",
            "  inflating: /content/data/wavs/423d1b365463f9cdb8b85a7bc9ccc4e9.wav  \n",
            "  inflating: /content/data/wavs/f01645c4d3a1664150262b65b3a922df.wav  \n",
            "  inflating: /content/data/wavs/894965b2fdb72db0a4cde818cb59f046.wav  \n",
            "  inflating: /content/data/wavs/87b107f39be9b75f737bb1077da9c936.wav  \n",
            "  inflating: /content/data/wavs/05f818feda79f10c8c75b77a4230b7cc.wav  \n",
            "  inflating: /content/data/wavs/263bf4684741f6305fe18ac909064f92.wav  \n",
            "  inflating: /content/data/wavs/d56dffa0f8942d281887e27e16896dc7.wav  \n",
            "  inflating: /content/data/wavs/b56bd7642525626b448d2f566c2f9d68.wav  \n",
            "  inflating: /content/data/wavs/65a2701a2e885f5b21c05317e9d030b6.wav  \n",
            "  inflating: /content/data/wavs/9ce1045ca0b13ef1dd0d397e6c7daa3f.wav  \n",
            "  inflating: /content/data/wavs/7211e9b3dca1fc2739ec0dad008d7dd0.wav  \n",
            "  inflating: /content/data/wavs/e99e2b459b83673839ab4aabbd48276f.wav  \n",
            "  inflating: /content/data/wavs/76eb9b8f639f86294202c6ce327e20ef.wav  \n",
            "  inflating: /content/data/wavs/c0ffa376e0b12d791911643120d2b94c.wav  \n",
            "  inflating: /content/data/wavs/83e26dc15e11f6b6fe67414241c12a7a.wav  \n",
            "  inflating: /content/data/wavs/208b41bfb96fe1fbb9f43792ad0d610b.wav  \n",
            "  inflating: /content/data/wavs/a808ee75b9d33642635736f12e3c4a5a.wav  \n",
            "  inflating: /content/data/wavs/8f2b2caa604ca4d4f44ca7fbcc6ddde8.wav  \n",
            "  inflating: /content/data/wavs/b130dd75d8e4802e7947cb83d3edd040.wav  \n",
            "  inflating: /content/data/wavs/60a96039477e28f52bd11edc75dee95f.wav  \n",
            "  inflating: /content/data/wavs/e15a5a01342bca28f72ab0b2d8507afc.wav  \n",
            "  inflating: /content/data/wavs/9d5250a78c851067feb85a103e195711.wav  \n",
            "  inflating: /content/data/wavs/f3b126683038850b68d007893c51bd3e.wav  \n",
            "  inflating: /content/data/wavs/075947833067331279d2896d18fe64ac.wav  \n",
            "  inflating: /content/data/wavs/6c4570b9eaf781710d38026ea4ab5780.wav  \n",
            "  inflating: /content/data/wavs/90482e98af9a0019e182d91baef43894.wav  \n",
            "  inflating: /content/data/wavs/598052d92115ba8c9c15b2be4191aee4.wav  \n",
            "  inflating: /content/data/wavs/ab17c3e15e3addf4d8d2fc32a689a71b.wav  \n",
            "  inflating: /content/data/wavs/cb8f1a8d846581859f3da6b5a6344d53.wav  \n",
            "  inflating: /content/data/wavs/ab0673bdb2bc6377e045c8c8a4411c4e.wav  \n",
            "  inflating: /content/data/wavs/b415b4417b352c0fc5e0b2605b848f2a.wav  \n",
            "  inflating: /content/data/wavs/3a7388f648f2c9da0775836b3d5ce3e0.wav  \n",
            "  inflating: /content/data/wavs/51538e9f4e73a29b7ad97680884296d3.wav  \n",
            "  inflating: /content/data/wavs/677ae68a77d3551ec6e6437c26a5efdf.wav  \n",
            "  inflating: /content/data/wavs/dc3dfa4667c519737810ade87e17c768.wav  \n",
            "  inflating: /content/data/wavs/04cb38bbb047ace22adc2563dd43756e.wav  \n",
            "  inflating: /content/data/wavs/4473976ff716c396a971eb4b3f8f8d60.wav  \n",
            "  inflating: /content/data/wavs/4a5f15ecb7573c5472368dbe3272533f.wav  \n",
            "  inflating: /content/data/wavs/dda0abcb9a2cb06e5910f57f5617b1c6.wav  \n",
            "  inflating: /content/data/wavs/bfc16f3f301b719769ee5a292d8b2273.wav  \n",
            "  inflating: /content/data/wavs/bb320dfc1a52188859f0ba541c1f5081.wav  \n",
            "  inflating: /content/data/wavs/c784e42743c80f2a1ed24ffa44fe2ea7.wav  \n",
            "  inflating: /content/data/wavs/2859ae09f2f1e40bf25e98cca07cac8b.wav  \n",
            "  inflating: /content/data/wavs/19299908a712590390d197c56315912e.wav  \n",
            "  inflating: /content/data/wavs/019c3f53a525a0f0ba62f304f3666609.wav  \n",
            "  inflating: /content/data/wavs/8ee30ff41d08a3b950259fe98d66dfeb.wav  \n",
            "  inflating: /content/data/wavs/f51d4b6535de551fde1a811bdde1511d.wav  \n",
            "  inflating: /content/data/wavs/f11c2b6b0315f4fb507e8cc63a82ae80.wav  \n",
            "  inflating: /content/data/wavs/4508264ae1ab1daa3054f958ffd4a27a.wav  \n",
            "  inflating: /content/data/wavs/2c9683dd81881a1529891225904f6f33.wav  \n",
            "  inflating: /content/data/wavs/7d41cd62cb406ceb404e6c8a6ad71d00.wav  \n",
            "  inflating: /content/data/wavs/b244e099e8a92be557658c051874f814.wav  \n",
            "  inflating: /content/data/wavs/1824bc8367824fca17950d81eb5d4d5f.wav  \n",
            "  inflating: /content/data/wavs/e978dbb090ec6820be5ff3938baf0072.wav  \n",
            "  inflating: /content/data/wavs/e7a94ba9d9550b612debb83819972a90.wav  \n",
            "  inflating: /content/data/wavs/7291b94e475a44ae1ddaf0964b4f9234.wav  \n",
            "  inflating: /content/data/wavs/2779799cc1296e98fd3a39627366e4c5.wav  \n",
            "  inflating: /content/data/wavs/8756bd9e39b227b48aad44a2c83d6a8e.wav  \n",
            "  inflating: /content/data/wavs/e27ff7be1eaad241c1cd248e0ac043b5.wav  \n",
            "  inflating: /content/data/wavs/2c622eb0001f928732fa61327ce829c8.wav  \n",
            "  inflating: /content/data/wavs/d41f74e7417b36fec7760e92606a3f85.wav  \n",
            "  inflating: /content/data/wavs/d7baf6b841f00416237d0e22c3d377d2.wav  \n",
            "  inflating: /content/data/wavs/9f07f22ec7f116446e2350d8ce58ea74.wav  \n",
            "  inflating: /content/data/wavs/60c0582ee34f5a246388d07b8b1ecf99.wav  \n",
            "  inflating: /content/data/wavs/b09c6d2931c0a4e1968a4cfa054bb017.wav  \n",
            "  inflating: /content/data/wavs/0b1135cd7f63606c3189047a074667b1.wav  \n",
            "  inflating: /content/data/wavs/79e1c02d0258f49393edab4448cab23d.wav  \n",
            "  inflating: /content/data/wavs/ef5119401d388c2a3716690791bca106.wav  \n",
            "  inflating: /content/data/wavs/6698d98d7d66e2aa3b00b8801d1767bd.wav  \n",
            "  inflating: /content/data/wavs/7a4bb52cb1fdff5a93fbc39b2e743f65.wav  \n",
            "  inflating: /content/data/wavs/367dea3265cfc3c8b2794bae1a9c7239.wav  \n",
            "  inflating: /content/data/wavs/b1edade0959550726ba70c8f9d1fd159.wav  \n",
            "  inflating: /content/data/wavs/4cc5469ee4c304073f92bcffa18b522b.wav  \n",
            "  inflating: /content/data/wavs/d0819d14a2733aa080a90087f0e94fa5.wav  \n",
            "  inflating: /content/data/wavs/fb610dc1ca1c3fe07057ac1b5bab3bc8.wav  \n",
            "  inflating: /content/data/wavs/9c7c3d93d6ed55ed3e594239ff6cabc3.wav  \n",
            "  inflating: /content/data/wavs/e0597e9aa2b9380ac258e91e31eb7bd6.wav  \n",
            "  inflating: /content/data/wavs/1b7320beadab62ea5f33adfdcfcf6ad9.wav  \n",
            "  inflating: /content/data/wavs/c4d0eb45030b183d1e5de76ac51ad9ff.wav  \n",
            "  inflating: /content/data/wavs/a2bf48ef0168bc3d36aa3e070b38a53c.wav  \n",
            "  inflating: /content/data/wavs/27459f3782a67333e0005ec9cdd3bb3b.wav  \n",
            "  inflating: /content/data/wavs/730c5cdecd7ea11c7662b4feee4d19e2.wav  \n",
            "  inflating: /content/data/wavs/88f64fd017d7f04b68cf57a1d96d3f33.wav  \n",
            "  inflating: /content/data/wavs/cf5c1d2e3e10fc83eb9c395ac2ddaf24.wav  \n",
            "  inflating: /content/data/wavs/9eb9af6115fccc91458f8462853796ef.wav  \n",
            "  inflating: /content/data/wavs/0ec6b48af1cc100f921842d937e213d8.wav  \n",
            "  inflating: /content/data/wavs/e8ba8f1fcff824c0fb0d83eff864b79c.wav  \n",
            "  inflating: /content/data/wavs/f3f29bbeca680d69981563951126472c.wav  \n",
            "  inflating: /content/data/wavs/589508459642d970260b4b8103c6d321.wav  \n",
            "  inflating: /content/data/wavs/58f4de18fb7ba253cedbc53a41305d3a.wav  \n",
            "  inflating: /content/data/wavs/218ae20e0cbbfc621d9dba56955c5e96.wav  \n",
            "  inflating: /content/data/wavs/0c9e5b353b766705162fca651d519894.wav  \n",
            "  inflating: /content/data/wavs/5692a6fd39d682de74eba2496a176a2d.wav  \n",
            "  inflating: /content/data/wavs/2a0fb83bce91ee5ddfe5488eb52025c1.wav  \n",
            "  inflating: /content/data/wavs/1390aed9f1fe371fbe4c6b6641c863fb.wav  \n",
            "  inflating: /content/data/wavs/9a4c207861425d3806fabc33e8202395.wav  \n",
            "  inflating: /content/data/wavs/588aaae6bea565a0ab06a9a14163c54e.wav  \n",
            "  inflating: /content/data/wavs/26369c875465ba4d6782d305e14e16be.wav  \n",
            "  inflating: /content/data/wavs/3fc813a2dd4082bf6df8a310e96bb141.wav  \n",
            "  inflating: /content/data/wavs/0aa57dff927358b1a21584435f3e6f8c.wav  \n",
            "  inflating: /content/data/wavs/598f5feed70caa66a7e74f18910bbead.wav  \n",
            "  inflating: /content/data/wavs/0e0fc27681c412e7914cbe47f29b68ce.wav  \n",
            "  inflating: /content/data/wavs/0e3abfa3ba1944e7ae285bccbb0b3216.wav  \n",
            "  inflating: /content/data/wavs/537d6a5265062d5c2a3b5f8be701823d.wav  \n",
            "  inflating: /content/data/wavs/a08a8623b14109616346ece1d82da15f.wav  \n",
            "  inflating: /content/data/wavs/949a0e1289e9bed87e2516ef13bb0b14.wav  \n",
            "  inflating: /content/data/wavs/4af696b637902a9afcb4d06a229a580f.wav  \n",
            "  inflating: /content/data/wavs/cf6433dfe30abb9ca930f98e694c4b11.wav  \n",
            "  inflating: /content/data/wavs/48d74535092e55e0a2ed2888c914169b.wav  \n",
            "  inflating: /content/data/wavs/16e21f39fdb37dc31a5815d9459ba840.wav  \n",
            "  inflating: /content/data/wavs/6b17631ba4b787f2555b279e761327aa.wav  \n",
            "  inflating: /content/data/wavs/a393726d8d5af429b284eac517842b96.wav  \n",
            "  inflating: /content/data/wavs/3807533b1172f9a58e6a0fee6cb854e8.wav  \n",
            "  inflating: /content/data/wavs/201fa63fa81ba4f06494e84b634fce3b.wav  \n",
            "  inflating: /content/data/wavs/1d92f78c6aa7f4c9e7a4557ce0cdfec6.wav  \n",
            "  inflating: /content/data/wavs/de8ef0388bcbc10569fb1cac76d93089.wav  \n",
            "  inflating: /content/data/wavs/08b96595f55c222b610ae0fc368a0744.wav  \n",
            "  inflating: /content/data/wavs/4956466c3777f1d56a699fe652c4329b.wav  \n",
            "  inflating: /content/data/wavs/9daafb756bc05fbb86cec4923188047c.wav  \n",
            "  inflating: /content/data/wavs/93f6c5ba198f83c4433cec68b5218f2d.wav  \n",
            "  inflating: /content/data/wavs/fd3d94789e9d5229ba5f94dacd089a69.wav  \n",
            "  inflating: /content/data/wavs/01ba6aa39e44a6a749b199f74eae99f5.wav  \n",
            "  inflating: /content/data/wavs/cf73a1ace269e328014f2c73318f042b.wav  \n",
            "  inflating: /content/data/wavs/0d1de7daec8db1ac78403b48a866a917.wav  \n",
            "  inflating: /content/data/wavs/625921ba1a3e7418f7160c22ca272426.wav  \n",
            "  inflating: /content/data/wavs/655b29ad4eb13a7ef5e090983a522b01.wav  \n",
            "  inflating: /content/data/wavs/7911750fcc31b947c56b7f785598e521.wav  \n",
            "  inflating: /content/data/wavs/c5a8752f8a13969639df8acfdca0ff24.wav  \n",
            "  inflating: /content/data/wavs/895e0e092d638b922432fe6c8bd5f83d.wav  \n",
            "  inflating: /content/data/wavs/15d40e55e9d95eb6e0486c4294c50e1f.wav  \n",
            "  inflating: /content/data/wavs/e5c4dc806ccdc86106dd18d28d20a691.wav  \n",
            "  inflating: /content/data/wavs/1e17ea19760d3656475c56a24bfd7d22.wav  \n",
            "  inflating: /content/data/wavs/405ee8af0eb75f53f9c611611c30cd9d.wav  \n",
            "  inflating: /content/data/wavs/8bcf9ac91304396d50b4291e793b7613.wav  \n",
            "  inflating: /content/data/wavs/3394eeeef9d212506b25dc16814dd899.wav  \n",
            "  inflating: /content/data/wavs/2270ed0a504fb4a99f9d0c402d7754df.wav  \n",
            "  inflating: /content/data/wavs/44ec9c03b7638564d34abad3fabbfead.wav  \n",
            "  inflating: /content/data/wavs/47091a4afe2f203163118635c6367e05.wav  \n",
            "  inflating: /content/data/wavs/99d2ce9f5e65732ab8ea0d61b4c3227a.wav  \n",
            "  inflating: /content/data/wavs/bcee91bcc4765a0f9011b31d98f2a593.wav  \n",
            "  inflating: /content/data/wavs/7c8fefc7ec6751f7f04074d209cb68b5.wav  \n",
            "  inflating: /content/data/wavs/ac2b97d02675633a4e69c67d4b262fa2.wav  \n",
            "  inflating: /content/data/wavs/8b7e066643712615256a5821dbdfb58c.wav  \n",
            "  inflating: /content/data/wavs/fdf8bb5d590dcc5795ed0664adb74c9c.wav  \n",
            "  inflating: /content/data/wavs/b945c335d52ea809d7bcfaafbbbb8518.wav  \n",
            "  inflating: /content/data/wavs/13566970691ad7582c44b6d0bc514c03.wav  \n",
            "  inflating: /content/data/wavs/73a09d1f3c366cc515a6ad4406945fa2.wav  \n",
            "  inflating: /content/data/wavs/e985659d19d94b244190f308e42ccbb5.wav  \n",
            "  inflating: /content/data/wavs/076044405d7ffb22a6c8495621d252e7.wav  \n",
            "  inflating: /content/data/wavs/eac2a4fdbbbc7fa5209d820896cb0ddc.wav  \n",
            "  inflating: /content/data/wavs/a1f05a925e8e0b511ebf7debf84ec762.wav  \n",
            "  inflating: /content/data/wavs/f0b6d69c41f6c3829713cf1ff5b99bf0.wav  \n",
            "  inflating: /content/data/wavs/a7a587e832f2d8ff6c600b1563dd3672.wav  \n",
            "  inflating: /content/data/wavs/5faef8d561783703f23eceacb4c37c65.wav  \n",
            "  inflating: /content/data/wavs/24f07867f60b7addc03dc3e89623d372.wav  \n",
            "  inflating: /content/data/wavs/004219a2c3929b5774ff4eda3082113e.wav  \n",
            "  inflating: /content/data/wavs/4642e5d987bfc8aedaeb7ad8b3a7f857.wav  \n",
            "  inflating: /content/data/wavs/2513d73678c1040c60b3f927f4b476d8.wav  \n",
            "  inflating: /content/data/wavs/035e9aa460ac49edc91fe9c8963b2237.wav  \n",
            "  inflating: /content/data/wavs/7dc206b43e5ac25ab08d406621d9ff21.wav  \n",
            "  inflating: /content/data/wavs/c779bb5fd24b0ae4a31894d5531776be.wav  \n",
            "  inflating: /content/data/wavs/52b9aa8d984c24d9a230d1ad97f3f7e9.wav  \n",
            "  inflating: /content/data/wavs/560746c71eac0d3f3c744ae460769019.wav  \n",
            "  inflating: /content/data/wavs/e0ffed0a5bcf0412734b5d5dffac4f21.wav  \n",
            "  inflating: /content/data/wavs/90293cc1e1c2e927971c618f26141e6a.wav  \n",
            "  inflating: /content/data/wavs/b6eee546f75ff98121838b58cb5240c2.wav  \n",
            "  inflating: /content/data/wavs/7b5928021a79434926c625e371d66e60.wav  \n",
            "  inflating: /content/data/wavs/c37c7f0e5d7b5a852874513df4c0647f.wav  \n",
            "  inflating: /content/data/wavs/ced9196fa271c91b23a2e23af94bdb07.wav  \n",
            "  inflating: /content/data/wavs/a81270d376836c9df6efe34e0353138c.wav  \n",
            "  inflating: /content/data/wavs/7f0cb255ef899f29d321b85c6e9020ac.wav  \n",
            "  inflating: /content/data/wavs/76bc993f84ae2792adb90a4122143910.wav  \n",
            "  inflating: /content/data/wavs/af0f8553071e27d402a28147777c8867.wav  \n",
            "  inflating: /content/data/wavs/7112aba6e0f55696a1bcb697c9c20454.wav  \n",
            "  inflating: /content/data/wavs/08c33aca96ec381af1ad3d30c00e8244.wav  \n",
            "  inflating: /content/data/wavs/ae3962fb6c8c1dca134a855dc35a1158.wav  \n",
            "  inflating: /content/data/wavs/e8e784ac5a6db7729dd05fbe2ad82a29.wav  \n",
            "  inflating: /content/data/wavs/370f67a02487fbbe0c17dc2e7da3af74.wav  \n",
            "  inflating: /content/data/wavs/6e6e9b754fb5146dc9ca80898122a064.wav  \n",
            "  inflating: /content/data/wavs/0dd7d8c7a1dddcab1787b6981f64ad0a.wav  \n",
            "  inflating: /content/data/wavs/514c41b426d0e94b1662d67e339a7a5d.wav  \n",
            "  inflating: /content/data/wavs/05bde653cf1e6d4774178258859e5452.wav  \n",
            "  inflating: /content/data/wavs/a614fa3d1fe9acde825ae43bce0534bb.wav  \n",
            "  inflating: /content/data/wavs/bad4f6d1d7a9aa81586e816e76d69998.wav  \n",
            "  inflating: /content/data/wavs/bd0894f98a442ae64400271aebd4a820.wav  \n",
            "  inflating: /content/data/wavs/2ec77675c9b0feba22bcda949b822765.wav  \n",
            "  inflating: /content/data/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav  \n",
            "  inflating: /content/data/wavs/1aeaa6d255f2238c96e0276b20dc5016.wav  \n",
            "  inflating: /content/data/wavs/3fab0b75982061c7a70b6d76541fb37b.wav  \n",
            "  inflating: /content/data/wavs/25ab9b5da4121672d579bef1702a92a3.wav  \n",
            "  inflating: /content/data/wavs/25dadbd7549da22dc672d9f1a88cf9bf.wav  \n",
            "  inflating: /content/data/wavs/4643cefde2401fcc17f0ca19552c5dcf.wav  \n",
            "  inflating: /content/data/wavs/60508e966a508f84970e9ca69dd99d1b.wav  \n",
            "  inflating: /content/data/wavs/4c3a0cea4772d40a335fc470fde75c27.wav  \n",
            "  inflating: /content/data/wavs/90fdaba60b6ed2b6839a7b37cd92308d.wav  \n",
            "  inflating: /content/data/wavs/b71aca0a6bccf6a9ec1c29107989cf0d.wav  \n",
            "  inflating: /content/data/wavs/b0a47a5c8b44f90f40660496233dd9f8.wav  \n",
            "  inflating: /content/data/wavs/759d8704f2cfbc4e9cf0995ce21e1474.wav  \n",
            "  inflating: /content/data/wavs/9f2979bbd4cf4bef585c19d22ba13bfa.wav  \n",
            "  inflating: /content/data/wavs/62c78266fd07d766e0b0c92c769fe48f.wav  \n",
            "  inflating: /content/data/wavs/d164889c3b6d20ecb58f5e4c182ab63a.wav  \n",
            "  inflating: /content/data/wavs/f37bc91e869701c7703980b24246f7d0.wav  \n",
            "  inflating: /content/data/wavs/a26bcdcc02ef71f92d2a0eeda8c0b235.wav  \n",
            "  inflating: /content/data/wavs/67537aef66ca95123080175eb1e97fb2.wav  \n",
            "  inflating: /content/data/wavs/f3e6038180835ad1ca4eaa5b27261c08.wav  \n",
            "  inflating: /content/data/wavs/e3c1a181298ff78c617d1bc78cf98db4.wav  \n",
            "  inflating: /content/data/wavs/0a3890882d381bb57db47e0bafe17354.wav  \n",
            "  inflating: /content/data/wavs/18c377c8f85207f6b6ee6ee9c2809585.wav  \n",
            "  inflating: /content/data/wavs/22749733a1dae738aac958a5a41425dd.wav  \n",
            "  inflating: /content/data/wavs/c026ca6e3655610d9e68f11d2ce3b37d.wav  \n",
            "  inflating: /content/data/wavs/baf5249d46cf5b733d702285ac22c582.wav  \n",
            "  inflating: /content/data/wavs/8ad69a8aeb4fb1892968057f187938b0.wav  \n",
            "  inflating: /content/data/wavs/5e13887b86513510a7bb577976a83edf.wav  \n",
            "  inflating: /content/data/wavs/da660f029dc0a627968973622bfeaeac.wav  \n",
            "  inflating: /content/data/wavs/2792da8213455f45fedbbafc26c5e3ac.wav  \n",
            "  inflating: /content/data/wavs/a937ffbd07ccc2e37adef527480c9523.wav  \n",
            "  inflating: /content/data/wavs/09f2a7ddc3d7f7b62ee9e203ab426513.wav  \n",
            "  inflating: /content/data/wavs/f677829a1169f79e4d8e9fddfb071c9d.wav  \n",
            "  inflating: /content/data/wavs/0e64a6b506e256243801d055e9d50f66.wav  \n",
            "  inflating: /content/data/wavs/639c0f0b37bf33ed0f1566178d7a524d.wav  \n",
            "  inflating: /content/data/wavs/63d33863c8c5467262ba4a95da91dcc5.wav  \n",
            "  inflating: /content/data/wavs/4f1c657316d9f51c42fd18fa89683faf.wav  \n",
            "  inflating: /content/data/wavs/46da7e99431a5e2d4eebdf2736c1d1a2.wav  \n",
            "  inflating: /content/data/wavs/22637b4a02c545fd651ee415eb0063ae.wav  \n",
            "  inflating: /content/data/wavs/d01c516c8f70a20d1510e74b97627086.wav  \n",
            "  inflating: /content/data/wavs/dcceed09a4fa2335c024a4836177471b.wav  \n",
            "  inflating: /content/data/wavs/622e77c4926932aec0227f78043e3cb5.wav  \n",
            "  inflating: /content/data/wavs/6c40cb6793b6b4c615336ccccfea4504.wav  \n",
            "  inflating: /content/data/wavs/7b585057822ebb70eb3352f901da5fd3.wav  \n",
            "  inflating: /content/data/metadata.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/Emergent/recorded_dataset/dataset/total_dataset_as_of_day_15/Luganda_24_LJSpeech.zip -d /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CCv7awTiTuO",
        "outputId": "a086ad4d-4495-412b-c058-fc61e28f85de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting TTS\n",
            "  Downloading TTS-0.4.2.tar.gz (1.4 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 35.5 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 81 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 122 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 133 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 143 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 153 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 163 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 174 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 184 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 194 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 204 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 215 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 225 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 235 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 245 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 256 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 266 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 276 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 286 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 296 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 307 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 317 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 327 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 337 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 348 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 358 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 368 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 378 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 389 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 399 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 409 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 419 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 430 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 440 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 450 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 460 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 471 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 481 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 491 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 501 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 512 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 522 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 532 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 542 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 552 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 563 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 573 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 583 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 593 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 604 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 614 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 624 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 634 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 645 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 655 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 665 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 675 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 686 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 696 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 706 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 716 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 727 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 737 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 747 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 757 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 768 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 778 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 788 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 798 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 808 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 819 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 829 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 839 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 849 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 860 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 870 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 880 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 890 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 901 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 911 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 921 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 931 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 942 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 952 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 962 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 972 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 983 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 993 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.0 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.0 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.0 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.1 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.1 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.1 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.1 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.1 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.2 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.2 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.2 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.3 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.3 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.3 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.3 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.3 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.3 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.3 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4 MB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 8.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (from TTS) (2.4.1)\n",
            "Requirement already satisfied: gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from TTS) (2.0.4)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from TTS) (0.10.3.post1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from TTS) (0.42.1)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from TTS) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from TTS) (1.10.0+cu111)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from TTS) (3.13)\n",
            "Requirement already satisfied: librosa==0.8.0 in /usr/local/lib/python3.7/dist-packages (from TTS) (0.8.0)\n",
            "Requirement already satisfied: coqpit in /usr/local/lib/python3.7/dist-packages (from TTS) (0.0.14)\n",
            "Requirement already satisfied: numba==0.53 in /usr/local/lib/python3.7/dist-packages (from TTS) (0.53.0)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from TTS) (1.4.1)\n",
            "Requirement already satisfied: mecab-python3==1.0.3 in /usr/local/lib/python3.7/dist-packages (from TTS) (1.0.3)\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from TTS) (1.19.5)\n",
            "Requirement already satisfied: unidic-lite==1.0.8 in /usr/local/lib/python3.7/dist-packages (from TTS) (1.0.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from TTS) (4.62.3)\n",
            "Requirement already satisfied: pypinyin in /usr/local/lib/python3.7/dist-packages (from TTS) (0.44.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from TTS) (0.29.24)\n",
            "Requirement already satisfied: fsspec>=2021.04.0 in /usr/local/lib/python3.7/dist-packages (from TTS) (2021.11.1)\n",
            "Requirement already satisfied: pyworld in /usr/local/lib/python3.7/dist-packages (from TTS) (0.3.0)\n",
            "Requirement already satisfied: umap-learn==0.5.1 in /usr/local/lib/python3.7/dist-packages (from TTS) (0.5.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from TTS) (1.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from TTS) (3.2.2)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from TTS) (2.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from TTS) (1.1.5)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.7/dist-packages (from TTS) (0.3.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from TTS) (3.6.4)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (1.5.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (1.0.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (2.1.9)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (0.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (1.1.0)\n",
            "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba==0.53->TTS) (0.36.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.53->TTS) (57.4.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.5.1->TTS) (0.5.5)\n",
            "Requirement already satisfied: num2words<1.0.0,>=0.5.10 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (0.5.10)\n",
            "Requirement already satisfied: jsonlines~=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (1.2.0)\n",
            "Requirement already satisfied: gruut-ipa~=0.10.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (0.10.1)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.9.1)\n",
            "Requirement already satisfied: networkx<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.6.3)\n",
            "Requirement already satisfied: python-crfsuite~=0.9.7 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (0.9.7)\n",
            "Requirement already satisfied: dateparser~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (1.0.0)\n",
            "Requirement already satisfied: gruut-lang-sv~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-es~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-nl~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-ru~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-de~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-pt~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-fr~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-it~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-cs~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from Babel<3.0.0,>=2.8.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2018.9)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.7/dist-packages (from dateparser~=1.0.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2019.12.20)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser~=1.0.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser~=1.0.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from jsonlines~=1.2.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (1.15.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words<1.0.0,>=0.5.10->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (0.6.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS) (21.3)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS) (2.23.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.0->TTS) (3.0.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->TTS) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->TTS) (2.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->TTS) (3.10.0.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->TTS) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->TTS) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->TTS) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->TTS) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->TTS) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS) (1.3.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS) (2.10)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->TTS) (3.17.3)\n",
            "Building wheels for collected packages: TTS\n",
            "  Building wheel for TTS (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for TTS: filename=TTS-0.4.2-cp37-cp37m-linux_x86_64.whl size=572310 sha256=58f1210248992864eb8acedfd4f8d043b9590d5f4c55a889230b6e5ef5387c23\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/a5/d8/ad1b09ee4031f7369d6174a975a8ce30c528f3f8949f4fb70b\n",
            "Successfully built TTS\n",
            "Installing collected packages: TTS\n",
            "Successfully installed TTS-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install TTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize training of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFmiAB1VdIlq",
        "outputId": "b9d00198-b2f8-4a5f-9914-d04d4b9089fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "     | > postnet_loss: 0.63024  (0.63060)\n",
            "     | > stopnet_loss: 0.22705  (0.27516)\n",
            "     | > decoder_coarse_loss: 0.98871  (0.96669)\n",
            "     | > decoder_ddc_loss: 0.02201  (0.02585)\n",
            "     | > ga_loss: 0.00077  (0.00119)\n",
            "     | > decoder_diff_spec_loss: 0.30396  (0.30337)\n",
            "     | > postnet_diff_spec_loss: 0.30378  (0.31038)\n",
            "     | > decoder_ssim_loss: 0.34709  (0.34596)\n",
            "     | > postnet_ssim_loss: 0.32580  (0.32695)\n",
            "     | > loss: 1.14166  (1.18411)\n",
            "     | > align_error: 0.46046  (0.46348)\n",
            "     | > grad_norm: 2.35317  (3.31239)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.94480  (0.95797)\n",
            "     | > loader_time: 0.00570  (0.00585)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 63/361 -- GLOBAL_STEP: 18525\u001b[0m\n",
            "     | > decoder_loss: 0.67952  (0.70568)\n",
            "     | > postnet_loss: 0.60084  (0.63139)\n",
            "     | > stopnet_loss: 0.19218  (0.25945)\n",
            "     | > decoder_coarse_loss: 0.95181  (0.97110)\n",
            "     | > decoder_ddc_loss: 0.01897  (0.02361)\n",
            "     | > ga_loss: 0.00056  (0.00097)\n",
            "     | > decoder_diff_spec_loss: 0.30240  (0.30610)\n",
            "     | > postnet_diff_spec_loss: 0.30288  (0.30969)\n",
            "     | > decoder_ssim_loss: 0.35377  (0.34514)\n",
            "     | > postnet_ssim_loss: 0.33166  (0.32551)\n",
            "     | > loss: 1.08044  (1.16886)\n",
            "     | > align_error: 0.47094  (0.46280)\n",
            "     | > grad_norm: 0.83735  (2.69605)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.10010  (1.00259)\n",
            "     | > loader_time: 0.00570  (0.00589)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/361 -- GLOBAL_STEP: 18550\u001b[0m\n",
            "     | > decoder_loss: 0.70711  (0.70661)\n",
            "     | > postnet_loss: 0.62644  (0.63054)\n",
            "     | > stopnet_loss: 0.17413  (0.24819)\n",
            "     | > decoder_coarse_loss: 0.94966  (0.96904)\n",
            "     | > decoder_ddc_loss: 0.01823  (0.02207)\n",
            "     | > ga_loss: 0.00053  (0.00084)\n",
            "     | > decoder_diff_spec_loss: 0.30867  (0.30714)\n",
            "     | > postnet_diff_spec_loss: 0.30905  (0.30949)\n",
            "     | > decoder_ssim_loss: 0.35716  (0.34621)\n",
            "     | > postnet_ssim_loss: 0.33370  (0.32609)\n",
            "     | > loss: 1.07927  (1.15670)\n",
            "     | > align_error: 0.46561  (0.46369)\n",
            "     | > grad_norm: 0.99556  (2.27936)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.08650  (1.04601)\n",
            "     | > loader_time: 0.00550  (0.00585)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 113/361 -- GLOBAL_STEP: 18575\u001b[0m\n",
            "     | > decoder_loss: 0.72146  (0.70743)\n",
            "     | > postnet_loss: 0.63302  (0.62990)\n",
            "     | > stopnet_loss: 0.21385  (0.23859)\n",
            "     | > decoder_coarse_loss: 0.99230  (0.96791)\n",
            "     | > decoder_ddc_loss: 0.01643  (0.02091)\n",
            "     | > ga_loss: 0.00037  (0.00075)\n",
            "     | > decoder_diff_spec_loss: 0.31614  (0.30821)\n",
            "     | > postnet_diff_spec_loss: 0.30883  (0.30899)\n",
            "     | > decoder_ssim_loss: 0.35212  (0.34710)\n",
            "     | > postnet_ssim_loss: 0.32881  (0.32650)\n",
            "     | > loss: 1.13296  (1.14658)\n",
            "     | > align_error: 0.46267  (0.46301)\n",
            "     | > grad_norm: 2.60383  (2.10541)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.29230  (1.08325)\n",
            "     | > loader_time: 0.00630  (0.00588)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 138/361 -- GLOBAL_STEP: 18600\u001b[0m\n",
            "     | > decoder_loss: 0.69405  (0.70925)\n",
            "     | > postnet_loss: 0.61229  (0.63007)\n",
            "     | > stopnet_loss: 0.18853  (0.23060)\n",
            "     | > decoder_coarse_loss: 0.97303  (0.97012)\n",
            "     | > decoder_ddc_loss: 0.01508  (0.01994)\n",
            "     | > ga_loss: 0.00036  (0.00068)\n",
            "     | > decoder_diff_spec_loss: 0.31799  (0.30984)\n",
            "     | > postnet_diff_spec_loss: 0.31324  (0.30963)\n",
            "     | > decoder_ssim_loss: 0.35602  (0.34777)\n",
            "     | > postnet_ssim_loss: 0.33287  (0.32669)\n",
            "     | > loss: 1.09398  (1.13984)\n",
            "     | > align_error: 0.46437  (0.46327)\n",
            "     | > grad_norm: 1.52615  (1.96814)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.27300  (1.11547)\n",
            "     | > loader_time: 0.00570  (0.00587)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 163/361 -- GLOBAL_STEP: 18625\u001b[0m\n",
            "     | > decoder_loss: 0.72947  (0.70863)\n",
            "     | > postnet_loss: 0.65108  (0.62914)\n",
            "     | > stopnet_loss: 0.21444  (0.22453)\n",
            "     | > decoder_coarse_loss: 0.98561  (0.96961)\n",
            "     | > decoder_ddc_loss: 0.01345  (0.01910)\n",
            "     | > ga_loss: 0.00028  (0.00063)\n",
            "     | > decoder_diff_spec_loss: 0.31712  (0.31000)\n",
            "     | > postnet_diff_spec_loss: 0.30640  (0.30951)\n",
            "     | > decoder_ssim_loss: 0.34289  (0.34836)\n",
            "     | > postnet_ssim_loss: 0.32327  (0.32708)\n",
            "     | > loss: 1.13317  (1.13301)\n",
            "     | > align_error: 0.45779  (0.46333)\n",
            "     | > grad_norm: 0.88302  (1.83425)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.44360  (1.15075)\n",
            "     | > loader_time: 0.00610  (0.00594)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 188/361 -- GLOBAL_STEP: 18650\u001b[0m\n",
            "     | > decoder_loss: 0.68169  (0.70844)\n",
            "     | > postnet_loss: 0.59876  (0.62843)\n",
            "     | > stopnet_loss: 0.14054  (0.22027)\n",
            "     | > decoder_coarse_loss: 0.92954  (0.96972)\n",
            "     | > decoder_ddc_loss: 0.01362  (0.01837)\n",
            "     | > ga_loss: 0.00028  (0.00058)\n",
            "     | > decoder_diff_spec_loss: 0.30756  (0.31047)\n",
            "     | > postnet_diff_spec_loss: 0.31941  (0.31001)\n",
            "     | > decoder_ssim_loss: 0.37178  (0.34856)\n",
            "     | > postnet_ssim_loss: 0.34863  (0.32710)\n",
            "     | > loss: 1.03468  (1.12846)\n",
            "     | > align_error: 0.46522  (0.46297)\n",
            "     | > grad_norm: 0.79627  (1.73482)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.41070  (1.18813)\n",
            "     | > loader_time: 0.00660  (0.00596)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 213/361 -- GLOBAL_STEP: 18675\u001b[0m\n",
            "     | > decoder_loss: 0.75425  (0.70893)\n",
            "     | > postnet_loss: 0.66833  (0.62849)\n",
            "     | > stopnet_loss: 0.22009  (0.21567)\n",
            "     | > decoder_coarse_loss: 1.04201  (0.97133)\n",
            "     | > decoder_ddc_loss: 0.01199  (0.01771)\n",
            "     | > ga_loss: 0.00025  (0.00054)\n",
            "     | > decoder_diff_spec_loss: 0.32377  (0.31104)\n",
            "     | > postnet_diff_spec_loss: 0.31503  (0.30995)\n",
            "     | > decoder_ssim_loss: 0.33317  (0.34892)\n",
            "     | > postnet_ssim_loss: 0.31177  (0.32730)\n",
            "     | > loss: 1.16141  (1.12431)\n",
            "     | > align_error: 0.47319  (0.46310)\n",
            "     | > grad_norm: 1.60283  (1.65489)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.63120  (1.22490)\n",
            "     | > loader_time: 0.00750  (0.00600)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 238/361 -- GLOBAL_STEP: 18700\u001b[0m\n",
            "     | > decoder_loss: 0.75515  (0.71050)\n",
            "     | > postnet_loss: 0.65823  (0.62952)\n",
            "     | > stopnet_loss: 0.15675  (0.21390)\n",
            "     | > decoder_coarse_loss: 1.04124  (0.97361)\n",
            "     | > decoder_ddc_loss: 0.01198  (0.01710)\n",
            "     | > ga_loss: 0.00021  (0.00051)\n",
            "     | > decoder_diff_spec_loss: 0.33721  (0.31173)\n",
            "     | > postnet_diff_spec_loss: 0.33270  (0.31050)\n",
            "     | > decoder_ssim_loss: 0.37074  (0.34870)\n",
            "     | > postnet_ssim_loss: 0.34359  (0.32693)\n",
            "     | > loss: 1.12052  (1.12361)\n",
            "     | > align_error: 0.48059  (0.46357)\n",
            "     | > grad_norm: 0.67211  (1.61093)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.53570  (1.26645)\n",
            "     | > loader_time: 0.00610  (0.00608)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 263/361 -- GLOBAL_STEP: 18725\u001b[0m\n",
            "     | > decoder_loss: 0.72076  (0.71248)\n",
            "     | > postnet_loss: 0.62965  (0.63083)\n",
            "     | > stopnet_loss: 0.18171  (0.21118)\n",
            "     | > decoder_coarse_loss: 0.99304  (0.97654)\n",
            "     | > decoder_ddc_loss: 0.01050  (0.01652)\n",
            "     | > ga_loss: 0.00021  (0.00048)\n",
            "     | > decoder_diff_spec_loss: 0.32274  (0.31256)\n",
            "     | > postnet_diff_spec_loss: 0.31620  (0.31095)\n",
            "     | > decoder_ssim_loss: 0.34682  (0.34879)\n",
            "     | > postnet_ssim_loss: 0.32149  (0.32681)\n",
            "     | > loss: 1.09804  (1.12247)\n",
            "     | > align_error: 0.46644  (0.46437)\n",
            "     | > grad_norm: 1.02226  (1.58665)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.86330  (1.30866)\n",
            "     | > loader_time: 0.00730  (0.00614)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 288/361 -- GLOBAL_STEP: 18750\u001b[0m\n",
            "     | > decoder_loss: 0.76472  (0.71396)\n",
            "     | > postnet_loss: 0.66707  (0.63185)\n",
            "     | > stopnet_loss: 0.15936  (0.20811)\n",
            "     | > decoder_coarse_loss: 1.04771  (0.97865)\n",
            "     | > decoder_ddc_loss: 0.00988  (0.01598)\n",
            "     | > ga_loss: 0.00017  (0.00046)\n",
            "     | > decoder_diff_spec_loss: 0.32293  (0.31313)\n",
            "     | > postnet_diff_spec_loss: 0.32096  (0.31130)\n",
            "     | > decoder_ssim_loss: 0.36430  (0.34908)\n",
            "     | > postnet_ssim_loss: 0.33805  (0.32692)\n",
            "     | > loss: 1.11913  (1.12062)\n",
            "     | > align_error: 0.46464  (0.46442)\n",
            "     | > grad_norm: 2.06575  (1.55728)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.87060  (1.35027)\n",
            "     | > loader_time: 0.00750  (0.00619)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 313/361 -- GLOBAL_STEP: 18775\u001b[0m\n",
            "     | > decoder_loss: 0.72213  (0.71546)\n",
            "     | > postnet_loss: 0.62902  (0.63274)\n",
            "     | > stopnet_loss: 0.14441  (0.20508)\n",
            "     | > decoder_coarse_loss: 0.99948  (0.98117)\n",
            "     | > decoder_ddc_loss: 0.00910  (0.01546)\n",
            "     | > ga_loss: 0.00019  (0.00044)\n",
            "     | > decoder_diff_spec_loss: 0.32131  (0.31374)\n",
            "     | > postnet_diff_spec_loss: 0.32093  (0.31186)\n",
            "     | > decoder_ssim_loss: 0.35665  (0.34948)\n",
            "     | > postnet_ssim_loss: 0.32970  (0.32715)\n",
            "     | > loss: 1.06742  (1.11903)\n",
            "     | > align_error: 0.47952  (0.46508)\n",
            "     | > grad_norm: 1.04922  (1.52662)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.80880  (1.38869)\n",
            "     | > loader_time: 0.00860  (0.00623)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 338/361 -- GLOBAL_STEP: 18800\u001b[0m\n",
            "     | > decoder_loss: 0.72046  (0.71753)\n",
            "     | > postnet_loss: 0.62714  (0.63389)\n",
            "     | > stopnet_loss: 0.15939  (0.20276)\n",
            "     | > decoder_coarse_loss: 0.99056  (0.98455)\n",
            "     | > decoder_ddc_loss: 0.00827  (0.01496)\n",
            "     | > ga_loss: 0.00015  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.31899  (0.31465)\n",
            "     | > postnet_diff_spec_loss: 0.31854  (0.31244)\n",
            "     | > decoder_ssim_loss: 0.36522  (0.34979)\n",
            "     | > postnet_ssim_loss: 0.33893  (0.32725)\n",
            "     | > loss: 1.08218  (1.11862)\n",
            "     | > align_error: 0.46755  (0.46549)\n",
            "     | > grad_norm: 0.90484  (1.48343)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 2.09980  (1.43639)\n",
            "     | > loader_time: 0.00750  (0.00630)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.67064  (0.67064)\n",
            "     | > postnet_loss: 0.58382  (0.58382)\n",
            "     | > stopnet_loss: 0.28543  (0.28543)\n",
            "     | > decoder_coarse_loss: 0.87785  (0.87785)\n",
            "     | > decoder_ddc_loss: 0.01661  (0.01661)\n",
            "     | > ga_loss: 0.00112  (0.00112)\n",
            "     | > decoder_diff_spec_loss: 0.26101  (0.26101)\n",
            "     | > postnet_diff_spec_loss: 0.22060  (0.22060)\n",
            "     | > decoder_ssim_loss: 0.30647  (0.30647)\n",
            "     | > postnet_ssim_loss: 0.28066  (0.28066)\n",
            "     | > loss: 1.09547  (1.09547)\n",
            "     | > align_error: 0.54505  (0.54505)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.63541  (0.63541)\n",
            "     | > postnet_loss: 0.54368  (0.54368)\n",
            "     | > stopnet_loss: 0.14837  (0.14837)\n",
            "     | > decoder_coarse_loss: 0.80395  (0.80395)\n",
            "     | > decoder_ddc_loss: 0.01578  (0.01578)\n",
            "     | > ga_loss: 0.00047  (0.00047)\n",
            "     | > decoder_diff_spec_loss: 0.28074  (0.28074)\n",
            "     | > postnet_diff_spec_loss: 0.23369  (0.23369)\n",
            "     | > decoder_ssim_loss: 0.34782  (0.34782)\n",
            "     | > postnet_ssim_loss: 0.31448  (0.31448)\n",
            "     | > loss: 0.94463  (0.94463)\n",
            "     | > align_error: 0.44031  (0.44031)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.63953  (0.63747)\n",
            "     | > postnet_loss: 0.55248  (0.54808)\n",
            "     | > stopnet_loss: 0.18604  (0.16721)\n",
            "     | > decoder_coarse_loss: 0.83178  (0.81787)\n",
            "     | > decoder_ddc_loss: 0.01326  (0.01452)\n",
            "     | > ga_loss: 0.00040  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.27117  (0.27596)\n",
            "     | > postnet_diff_spec_loss: 0.22587  (0.22978)\n",
            "     | > decoder_ssim_loss: 0.31497  (0.33140)\n",
            "     | > postnet_ssim_loss: 0.28609  (0.30029)\n",
            "     | > loss: 0.97181  (0.95822)\n",
            "     | > align_error: 0.45812  (0.44921)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.66904  (0.64799)\n",
            "     | > postnet_loss: 0.57119  (0.55578)\n",
            "     | > stopnet_loss: 0.16306  (0.16583)\n",
            "     | > decoder_coarse_loss: 0.89360  (0.84311)\n",
            "     | > decoder_ddc_loss: 0.01167  (0.01357)\n",
            "     | > ga_loss: 0.00026  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.28697  (0.27963)\n",
            "     | > postnet_diff_spec_loss: 0.23888  (0.23281)\n",
            "     | > decoder_ssim_loss: 0.33165  (0.33148)\n",
            "     | > postnet_ssim_loss: 0.29917  (0.29991)\n",
            "     | > loss: 0.98988  (0.96877)\n",
            "     | > align_error: 0.44882  (0.44908)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.66537  (0.65233)\n",
            "     | > postnet_loss: 0.56319  (0.55764)\n",
            "     | > stopnet_loss: 0.10826  (0.15143)\n",
            "     | > decoder_coarse_loss: 0.86964  (0.84974)\n",
            "     | > decoder_ddc_loss: 0.01113  (0.01296)\n",
            "     | > ga_loss: 0.00020  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.29771  (0.28415)\n",
            "     | > postnet_diff_spec_loss: 0.24592  (0.23609)\n",
            "     | > decoder_ssim_loss: 0.35841  (0.33821)\n",
            "     | > postnet_ssim_loss: 0.32214  (0.30547)\n",
            "     | > loss: 0.94263  (0.96224)\n",
            "     | > align_error: 0.43725  (0.44613)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.68493  (0.65885)\n",
            "     | > postnet_loss: 0.58432  (0.56297)\n",
            "     | > stopnet_loss: 0.18589  (0.15832)\n",
            "     | > decoder_coarse_loss: 0.89238  (0.85827)\n",
            "     | > decoder_ddc_loss: 0.00952  (0.01227)\n",
            "     | > ga_loss: 0.00015  (0.00030)\n",
            "     | > decoder_diff_spec_loss: 0.28903  (0.28512)\n",
            "     | > postnet_diff_spec_loss: 0.23755  (0.23638)\n",
            "     | > decoder_ssim_loss: 0.32256  (0.33508)\n",
            "     | > postnet_ssim_loss: 0.28941  (0.30226)\n",
            "     | > loss: 1.01409  (0.97261)\n",
            "     | > align_error: 0.44948  (0.44680)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.68849  (0.66379)\n",
            "     | > postnet_loss: 0.58678  (0.56694)\n",
            "     | > stopnet_loss: 0.20990  (0.16692)\n",
            "     | > decoder_coarse_loss: 0.88951  (0.86348)\n",
            "     | > decoder_ddc_loss: 0.00710  (0.01141)\n",
            "     | > ga_loss: 0.00012  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.29581  (0.28691)\n",
            "     | > postnet_diff_spec_loss: 0.24298  (0.23748)\n",
            "     | > decoder_ssim_loss: 0.31253  (0.33132)\n",
            "     | > postnet_ssim_loss: 0.28044  (0.29862)\n",
            "     | > loss: 1.03641  (0.98324)\n",
            "     | > align_error: 0.47299  (0.45116)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.72474  (0.67250)\n",
            "     | > postnet_loss: 0.61389  (0.57365)\n",
            "     | > stopnet_loss: 0.07251  (0.15343)\n",
            "     | > decoder_coarse_loss: 0.98883  (0.88139)\n",
            "     | > decoder_ddc_loss: 0.00732  (0.01083)\n",
            "     | > ga_loss: 0.00012  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.29812  (0.28851)\n",
            "     | > postnet_diff_spec_loss: 0.24770  (0.23894)\n",
            "     | > decoder_ssim_loss: 0.35902  (0.33528)\n",
            "     | > postnet_ssim_loss: 0.32347  (0.30217)\n",
            "     | > loss: 0.96386  (0.98047)\n",
            "     | > align_error: 0.43475  (0.44882)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00613 \u001b[0m(-0.00024)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.67250 \u001b[0m(-0.00755)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.57365 \u001b[0m(-0.00936)\n",
            "     | > avg_stopnet_loss:\u001b[91m 0.15343 \u001b[0m(+0.00001)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.88139 \u001b[0m(-0.04946)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.01083 \u001b[0m(-0.00211)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00024 \u001b[0m(-0.00001)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.28851 \u001b[0m(-0.00125)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.23894 \u001b[0m(-0.00094)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.33528 \u001b[0m(-0.00173)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.30217 \u001b[0m(-0.00122)\n",
            "     | > avg_loss:\u001b[92m 0.98047 \u001b[0m(-0.01845)\n",
            "     | > avg_align_error:\u001b[92m 0.44882 \u001b[0m(-0.01849)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_18824.pth.tar\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 52/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 03:25:02) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 1/361 -- GLOBAL_STEP: 18825\u001b[0m\n",
            "     | > decoder_loss: 0.73355  (0.73355)\n",
            "     | > postnet_loss: 0.65341  (0.65341)\n",
            "     | > stopnet_loss: 0.41168  (0.41168)\n",
            "     | > decoder_coarse_loss: 0.99699  (0.99699)\n",
            "     | > decoder_ddc_loss: 0.03124  (0.03124)\n",
            "     | > ga_loss: 0.00279  (0.00279)\n",
            "     | > decoder_diff_spec_loss: 0.29698  (0.29698)\n",
            "     | > postnet_diff_spec_loss: 0.30177  (0.30177)\n",
            "     | > decoder_ssim_loss: 0.33416  (0.33416)\n",
            "     | > postnet_ssim_loss: 0.31721  (0.31721)\n",
            "     | > loss: 1.34196  (1.34196)\n",
            "     | > align_error: 0.46536  (0.46536)\n",
            "     | > grad_norm: 15.87761  (15.87761)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.73310  (0.73306)\n",
            "     | > loader_time: 0.00220  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 26/361 -- GLOBAL_STEP: 18850\u001b[0m\n",
            "     | > decoder_loss: 0.68867  (0.69352)\n",
            "     | > postnet_loss: 0.61588  (0.62313)\n",
            "     | > stopnet_loss: 0.23650  (0.27496)\n",
            "     | > decoder_coarse_loss: 0.90581  (0.93036)\n",
            "     | > decoder_ddc_loss: 0.02044  (0.02397)\n",
            "     | > ga_loss: 0.00077  (0.00134)\n",
            "     | > decoder_diff_spec_loss: 0.29655  (0.30078)\n",
            "     | > postnet_diff_spec_loss: 0.28924  (0.30755)\n",
            "     | > decoder_ssim_loss: 0.34903  (0.34794)\n",
            "     | > postnet_ssim_loss: 0.32908  (0.32878)\n",
            "     | > loss: 1.11403  (1.17069)\n",
            "     | > align_error: 0.43536  (0.44679)\n",
            "     | > grad_norm: 1.39530  (3.66044)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.01990  (0.92848)\n",
            "     | > loader_time: 0.00480  (0.00580)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 51/361 -- GLOBAL_STEP: 18875\u001b[0m\n",
            "     | > decoder_loss: 0.70807  (0.69803)\n",
            "     | > postnet_loss: 0.62869  (0.62560)\n",
            "     | > stopnet_loss: 0.24090  (0.26064)\n",
            "     | > decoder_coarse_loss: 0.97097  (0.93989)\n",
            "     | > decoder_ddc_loss: 0.01719  (0.02159)\n",
            "     | > ga_loss: 0.00060  (0.00103)\n",
            "     | > decoder_diff_spec_loss: 0.31320  (0.30373)\n",
            "     | > postnet_diff_spec_loss: 0.30273  (0.30792)\n",
            "     | > decoder_ssim_loss: 0.33447  (0.34475)\n",
            "     | > postnet_ssim_loss: 0.31504  (0.32557)\n",
            "     | > loss: 1.14147  (1.15754)\n",
            "     | > align_error: 0.45037  (0.44519)\n",
            "     | > grad_norm: 0.81519  (2.79337)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.10410  (0.97535)\n",
            "     | > loader_time: 0.00530  (0.00557)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 76/361 -- GLOBAL_STEP: 18900\u001b[0m\n",
            "     | > decoder_loss: 0.72651  (0.70174)\n",
            "     | > postnet_loss: 0.64141  (0.62688)\n",
            "     | > stopnet_loss: 0.19956  (0.24838)\n",
            "     | > decoder_coarse_loss: 0.99429  (0.94262)\n",
            "     | > decoder_ddc_loss: 0.01569  (0.01991)\n",
            "     | > ga_loss: 0.00049  (0.00087)\n",
            "     | > decoder_diff_spec_loss: 0.31450  (0.30608)\n",
            "     | > postnet_diff_spec_loss: 0.30748  (0.30800)\n",
            "     | > decoder_ssim_loss: 0.35798  (0.34506)\n",
            "     | > postnet_ssim_loss: 0.33578  (0.32523)\n",
            "     | > loss: 1.12540  (1.14659)\n",
            "     | > align_error: 0.45701  (0.44724)\n",
            "     | > grad_norm: 1.35445  (2.26846)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.13550  (1.02097)\n",
            "     | > loader_time: 0.00550  (0.00560)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 101/361 -- GLOBAL_STEP: 18925\u001b[0m\n",
            "     | > decoder_loss: 0.67915  (0.69986)\n",
            "     | > postnet_loss: 0.60043  (0.62458)\n",
            "     | > stopnet_loss: 0.17432  (0.24113)\n",
            "     | > decoder_coarse_loss: 0.88928  (0.93864)\n",
            "     | > decoder_ddc_loss: 0.01522  (0.01875)\n",
            "     | > ga_loss: 0.00043  (0.00076)\n",
            "     | > decoder_diff_spec_loss: 0.30606  (0.30601)\n",
            "     | > postnet_diff_spec_loss: 0.30406  (0.30737)\n",
            "     | > decoder_ssim_loss: 0.35793  (0.34516)\n",
            "     | > postnet_ssim_loss: 0.33557  (0.32510)\n",
            "     | > loss: 1.04839  (1.13632)\n",
            "     | > align_error: 0.43894  (0.44616)\n",
            "     | > grad_norm: 1.27399  (2.10358)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.22380  (1.05690)\n",
            "     | > loader_time: 0.00590  (0.00566)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 126/361 -- GLOBAL_STEP: 18950\u001b[0m\n",
            "     | > decoder_loss: 0.71353  (0.70225)\n",
            "     | > postnet_loss: 0.62657  (0.62541)\n",
            "     | > stopnet_loss: 0.16170  (0.23347)\n",
            "     | > decoder_coarse_loss: 0.95786  (0.94123)\n",
            "     | > decoder_ddc_loss: 0.01356  (0.01783)\n",
            "     | > ga_loss: 0.00034  (0.00069)\n",
            "     | > decoder_diff_spec_loss: 0.30872  (0.30741)\n",
            "     | > postnet_diff_spec_loss: 0.31049  (0.30737)\n",
            "     | > decoder_ssim_loss: 0.36130  (0.34544)\n",
            "     | > postnet_ssim_loss: 0.33739  (0.32494)\n",
            "     | > loss: 1.07077  (1.12988)\n",
            "     | > align_error: 0.44044  (0.44495)\n",
            "     | > grad_norm: 2.50939  (1.94168)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.23180  (1.09561)\n",
            "     | > loader_time: 0.00630  (0.00570)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 151/361 -- GLOBAL_STEP: 18975\u001b[0m\n",
            "     | > decoder_loss: 0.69689  (0.70211)\n",
            "     | > postnet_loss: 0.61001  (0.62444)\n",
            "     | > stopnet_loss: 0.15077  (0.22720)\n",
            "     | > decoder_coarse_loss: 0.93543  (0.94158)\n",
            "     | > decoder_ddc_loss: 0.01329  (0.01707)\n",
            "     | > ga_loss: 0.00031  (0.00063)\n",
            "     | > decoder_diff_spec_loss: 0.30719  (0.30799)\n",
            "     | > postnet_diff_spec_loss: 0.31062  (0.30747)\n",
            "     | > decoder_ssim_loss: 0.36617  (0.34597)\n",
            "     | > postnet_ssim_loss: 0.34172  (0.32516)\n",
            "     | > loss: 1.04765  (1.12328)\n",
            "     | > align_error: 0.43871  (0.44396)\n",
            "     | > grad_norm: 1.01656  (1.81199)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.27820  (1.13308)\n",
            "     | > loader_time: 0.00530  (0.00578)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 176/361 -- GLOBAL_STEP: 19000\u001b[0m\n",
            "     | > decoder_loss: 0.69237  (0.70180)\n",
            "     | > postnet_loss: 0.62454  (0.62377)\n",
            "     | > stopnet_loss: 0.31853  (0.22123)\n",
            "     | > decoder_coarse_loss: 0.93833  (0.94118)\n",
            "     | > decoder_ddc_loss: 0.01112  (0.01638)\n",
            "     | > ga_loss: 0.00032  (0.00058)\n",
            "     | > decoder_diff_spec_loss: 0.30820  (0.30838)\n",
            "     | > postnet_diff_spec_loss: 0.30728  (0.30737)\n",
            "     | > decoder_ssim_loss: 0.30772  (0.34640)\n",
            "     | > postnet_ssim_loss: 0.29280  (0.32542)\n",
            "     | > loss: 1.19073  (1.11680)\n",
            "     | > align_error: 0.44218  (0.44333)\n",
            "     | > grad_norm: 1.62374  (1.69835)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.49990  (1.16619)\n",
            "     | > loader_time: 0.00560  (0.00583)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/checkpoint_19000.pth.tar\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "\u001b[1m   --> STEP: 201/361 -- GLOBAL_STEP: 19025\u001b[0m\n",
            "     | > decoder_loss: 0.71857  (0.70175)\n",
            "     | > postnet_loss: 0.64026  (0.62343)\n",
            "     | > stopnet_loss: 0.21798  (0.21648)\n",
            "     | > decoder_coarse_loss: 0.95052  (0.94113)\n",
            "     | > decoder_ddc_loss: 0.01110  (0.01578)\n",
            "     | > ga_loss: 0.00025  (0.00054)\n",
            "     | > decoder_diff_spec_loss: 0.32075  (0.30878)\n",
            "     | > postnet_diff_spec_loss: 0.30916  (0.30775)\n",
            "     | > decoder_ssim_loss: 0.32951  (0.34696)\n",
            "     | > postnet_ssim_loss: 0.30963  (0.32583)\n",
            "     | > loss: 1.11658  (1.11203)\n",
            "     | > align_error: 0.43918  (0.44288)\n",
            "     | > grad_norm: 0.74619  (1.61030)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.55040  (1.20688)\n",
            "     | > loader_time: 0.00710  (0.00622)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 226/361 -- GLOBAL_STEP: 19050\u001b[0m\n",
            "     | > decoder_loss: 0.72907  (0.70258)\n",
            "     | > postnet_loss: 0.63713  (0.62374)\n",
            "     | > stopnet_loss: 0.20584  (0.21486)\n",
            "     | > decoder_coarse_loss: 0.96490  (0.94311)\n",
            "     | > decoder_ddc_loss: 0.01041  (0.01523)\n",
            "     | > ga_loss: 0.00022  (0.00050)\n",
            "     | > decoder_diff_spec_loss: 0.32259  (0.30935)\n",
            "     | > postnet_diff_spec_loss: 0.31638  (0.30814)\n",
            "     | > decoder_ssim_loss: 0.34345  (0.34640)\n",
            "     | > postnet_ssim_loss: 0.31912  (0.32514)\n",
            "     | > loss: 1.11768  (1.11080)\n",
            "     | > align_error: 0.44022  (0.44281)\n",
            "     | > grad_norm: 0.66544  (1.55688)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.49500  (1.24615)\n",
            "     | > loader_time: 0.00640  (0.00622)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 251/361 -- GLOBAL_STEP: 19075\u001b[0m\n",
            "     | > decoder_loss: 0.70198  (0.70405)\n",
            "     | > postnet_loss: 0.62496  (0.62450)\n",
            "     | > stopnet_loss: 0.19934  (0.21165)\n",
            "     | > decoder_coarse_loss: 0.96133  (0.94543)\n",
            "     | > decoder_ddc_loss: 0.00979  (0.01472)\n",
            "     | > ga_loss: 0.00022  (0.00048)\n",
            "     | > decoder_diff_spec_loss: 0.30376  (0.31008)\n",
            "     | > postnet_diff_spec_loss: 0.29532  (0.30846)\n",
            "     | > decoder_ssim_loss: 0.34036  (0.34651)\n",
            "     | > postnet_ssim_loss: 0.31880  (0.32504)\n",
            "     | > loss: 1.08949  (1.10873)\n",
            "     | > align_error: 0.45193  (0.44298)\n",
            "     | > grad_norm: 1.69410  (1.52421)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.72030  (1.28678)\n",
            "     | > loader_time: 0.00590  (0.00624)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 276/361 -- GLOBAL_STEP: 19100\u001b[0m\n",
            "     | > decoder_loss: 0.72851  (0.70569)\n",
            "     | > postnet_loss: 0.64664  (0.62570)\n",
            "     | > stopnet_loss: 0.15516  (0.20918)\n",
            "     | > decoder_coarse_loss: 0.97866  (0.94740)\n",
            "     | > decoder_ddc_loss: 0.00951  (0.01424)\n",
            "     | > ga_loss: 0.00019  (0.00045)\n",
            "     | > decoder_diff_spec_loss: 0.32606  (0.31065)\n",
            "     | > postnet_diff_spec_loss: 0.32162  (0.30882)\n",
            "     | > decoder_ssim_loss: 0.35600  (0.34669)\n",
            "     | > postnet_ssim_loss: 0.33228  (0.32505)\n",
            "     | > loss: 1.08091  (1.10749)\n",
            "     | > align_error: 0.44206  (0.44320)\n",
            "     | > grad_norm: 1.45213  (1.50692)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.73010  (1.32860)\n",
            "     | > loader_time: 0.00670  (0.00626)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 301/361 -- GLOBAL_STEP: 19125\u001b[0m\n",
            "     | > decoder_loss: 0.71157  (0.70701)\n",
            "     | > postnet_loss: 0.62490  (0.62637)\n",
            "     | > stopnet_loss: 0.18768  (0.20561)\n",
            "     | > decoder_coarse_loss: 0.96566  (0.94991)\n",
            "     | > decoder_ddc_loss: 0.00853  (0.01379)\n",
            "     | > ga_loss: 0.00016  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.31300  (0.31129)\n",
            "     | > postnet_diff_spec_loss: 0.30192  (0.30935)\n",
            "     | > decoder_ssim_loss: 0.34924  (0.34727)\n",
            "     | > postnet_ssim_loss: 0.32584  (0.32542)\n",
            "     | > loss: 1.08865  (1.10534)\n",
            "     | > align_error: 0.44345  (0.44334)\n",
            "     | > grad_norm: 1.11562  (1.46537)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.82210  (1.36760)\n",
            "     | > loader_time: 0.00740  (0.00630)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 326/361 -- GLOBAL_STEP: 19150\u001b[0m\n",
            "     | > decoder_loss: 0.74896  (0.70856)\n",
            "     | > postnet_loss: 0.65682  (0.62736)\n",
            "     | > stopnet_loss: 0.20445  (0.20367)\n",
            "     | > decoder_coarse_loss: 1.01626  (0.95281)\n",
            "     | > decoder_ddc_loss: 0.00784  (0.01337)\n",
            "     | > ga_loss: 0.00015  (0.00041)\n",
            "     | > decoder_diff_spec_loss: 0.32725  (0.31204)\n",
            "     | > postnet_diff_spec_loss: 0.31509  (0.30983)\n",
            "     | > decoder_ssim_loss: 0.33522  (0.34725)\n",
            "     | > postnet_ssim_loss: 0.31218  (0.32525)\n",
            "     | > loss: 1.13512  (1.10482)\n",
            "     | > align_error: 0.44366  (0.44348)\n",
            "     | > grad_norm: 0.73729  (1.42962)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 2.05890  (1.41115)\n",
            "     | > loader_time: 0.00770  (0.00635)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 351/361 -- GLOBAL_STEP: 19175\u001b[0m\n",
            "     | > decoder_loss: 0.74006  (0.71049)\n",
            "     | > postnet_loss: 0.64900  (0.62854)\n",
            "     | > stopnet_loss: 0.12800  (0.20241)\n",
            "     | > decoder_coarse_loss: 1.00138  (0.95627)\n",
            "     | > decoder_ddc_loss: 0.00667  (0.01293)\n",
            "     | > ga_loss: 0.00014  (0.00039)\n",
            "     | > decoder_diff_spec_loss: 0.31432  (0.31275)\n",
            "     | > postnet_diff_spec_loss: 0.32548  (0.31048)\n",
            "     | > decoder_ssim_loss: 0.37244  (0.34726)\n",
            "     | > postnet_ssim_loss: 0.34678  (0.32511)\n",
            "     | > loss: 1.06775  (1.10531)\n",
            "     | > align_error: 0.45028  (0.44392)\n",
            "     | > grad_norm: 0.83526  (1.39763)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 2.25690  (1.46539)\n",
            "     | > loader_time: 0.00680  (0.00642)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.66116  (0.66116)\n",
            "     | > postnet_loss: 0.57097  (0.57097)\n",
            "     | > stopnet_loss: 0.28830  (0.28830)\n",
            "     | > decoder_coarse_loss: 0.86397  (0.86397)\n",
            "     | > decoder_ddc_loss: 0.01531  (0.01531)\n",
            "     | > ga_loss: 0.00108  (0.00108)\n",
            "     | > decoder_diff_spec_loss: 0.25957  (0.25957)\n",
            "     | > postnet_diff_spec_loss: 0.21917  (0.21917)\n",
            "     | > decoder_ssim_loss: 0.30485  (0.30485)\n",
            "     | > postnet_ssim_loss: 0.27893  (0.27893)\n",
            "     | > loss: 1.08718  (1.08718)\n",
            "     | > align_error: 0.52857  (0.52857)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.62645  (0.62645)\n",
            "     | > postnet_loss: 0.53182  (0.53182)\n",
            "     | > stopnet_loss: 0.15062  (0.15062)\n",
            "     | > decoder_coarse_loss: 0.78594  (0.78594)\n",
            "     | > decoder_ddc_loss: 0.01445  (0.01445)\n",
            "     | > ga_loss: 0.00046  (0.00046)\n",
            "     | > decoder_diff_spec_loss: 0.27887  (0.27887)\n",
            "     | > postnet_diff_spec_loss: 0.23161  (0.23161)\n",
            "     | > decoder_ssim_loss: 0.34621  (0.34621)\n",
            "     | > postnet_ssim_loss: 0.31256  (0.31256)\n",
            "     | > loss: 0.93488  (0.93488)\n",
            "     | > align_error: 0.42293  (0.42293)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.63030  (0.62837)\n",
            "     | > postnet_loss: 0.53959  (0.53571)\n",
            "     | > stopnet_loss: 0.18719  (0.16891)\n",
            "     | > decoder_coarse_loss: 0.80999  (0.79796)\n",
            "     | > decoder_ddc_loss: 0.01207  (0.01326)\n",
            "     | > ga_loss: 0.00038  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.26925  (0.27406)\n",
            "     | > postnet_diff_spec_loss: 0.22391  (0.22776)\n",
            "     | > decoder_ssim_loss: 0.31305  (0.32963)\n",
            "     | > postnet_ssim_loss: 0.28404  (0.29830)\n",
            "     | > loss: 0.95964  (0.94726)\n",
            "     | > align_error: 0.43981  (0.43137)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.65978  (0.63884)\n",
            "     | > postnet_loss: 0.55848  (0.54330)\n",
            "     | > stopnet_loss: 0.16324  (0.16702)\n",
            "     | > decoder_coarse_loss: 0.86954  (0.82182)\n",
            "     | > decoder_ddc_loss: 0.01061  (0.01238)\n",
            "     | > ga_loss: 0.00024  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.28523  (0.27778)\n",
            "     | > postnet_diff_spec_loss: 0.23681  (0.23078)\n",
            "     | > decoder_ssim_loss: 0.32950  (0.32959)\n",
            "     | > postnet_ssim_loss: 0.29666  (0.29775)\n",
            "     | > loss: 0.97609  (0.95687)\n",
            "     | > align_error: 0.43044  (0.43106)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.65770  (0.64356)\n",
            "     | > postnet_loss: 0.55073  (0.54516)\n",
            "     | > stopnet_loss: 0.10836  (0.15235)\n",
            "     | > decoder_coarse_loss: 0.84861  (0.82852)\n",
            "     | > decoder_ddc_loss: 0.01005  (0.01180)\n",
            "     | > ga_loss: 0.00019  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.29566  (0.28225)\n",
            "     | > postnet_diff_spec_loss: 0.24353  (0.23397)\n",
            "     | > decoder_ssim_loss: 0.35610  (0.33622)\n",
            "     | > postnet_ssim_loss: 0.31938  (0.30316)\n",
            "     | > loss: 0.92973  (0.95008)\n",
            "     | > align_error: 0.41728  (0.42761)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.67364  (0.64957)\n",
            "     | > postnet_loss: 0.56902  (0.54993)\n",
            "     | > stopnet_loss: 0.18639  (0.15916)\n",
            "     | > decoder_coarse_loss: 0.85955  (0.83472)\n",
            "     | > decoder_ddc_loss: 0.00851  (0.01114)\n",
            "     | > ga_loss: 0.00014  (0.00028)\n",
            "     | > decoder_diff_spec_loss: 0.28678  (0.28316)\n",
            "     | > postnet_diff_spec_loss: 0.23519  (0.23421)\n",
            "     | > decoder_ssim_loss: 0.32005  (0.33298)\n",
            "     | > postnet_ssim_loss: 0.28663  (0.29985)\n",
            "     | > loss: 0.99695  (0.95946)\n",
            "     | > align_error: 0.42889  (0.42787)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.67929  (0.65453)\n",
            "     | > postnet_loss: 0.57243  (0.55368)\n",
            "     | > stopnet_loss: 0.21112  (0.16782)\n",
            "     | > decoder_coarse_loss: 0.86409  (0.83962)\n",
            "     | > decoder_ddc_loss: 0.00631  (0.01033)\n",
            "     | > ga_loss: 0.00011  (0.00025)\n",
            "     | > decoder_diff_spec_loss: 0.29422  (0.28500)\n",
            "     | > postnet_diff_spec_loss: 0.24068  (0.23529)\n",
            "     | > decoder_ssim_loss: 0.31014  (0.32918)\n",
            "     | > postnet_ssim_loss: 0.27765  (0.29615)\n",
            "     | > loss: 1.02286  (0.97002)\n",
            "     | > align_error: 0.45496  (0.43239)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.72121  (0.66405)\n",
            "     | > postnet_loss: 0.60712  (0.56131)\n",
            "     | > stopnet_loss: 0.07252  (0.15421)\n",
            "     | > decoder_coarse_loss: 0.95666  (0.85634)\n",
            "     | > decoder_ddc_loss: 0.00668  (0.00981)\n",
            "     | > ga_loss: 0.00011  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.29691  (0.28670)\n",
            "     | > postnet_diff_spec_loss: 0.24633  (0.23687)\n",
            "     | > decoder_ssim_loss: 0.35679  (0.33312)\n",
            "     | > postnet_ssim_loss: 0.32105  (0.29971)\n",
            "     | > loss: 0.95126  (0.96734)\n",
            "     | > align_error: 0.41793  (0.43032)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00481 \u001b[0m(-0.00133)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.66405 \u001b[0m(-0.00845)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.56131 \u001b[0m(-0.01234)\n",
            "     | > avg_stopnet_loss:\u001b[91m 0.15421 \u001b[0m(+0.00077)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.85634 \u001b[0m(-0.02505)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00981 \u001b[0m(-0.00102)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00023 \u001b[0m(-0.00001)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.28670 \u001b[0m(-0.00180)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.23687 \u001b[0m(-0.00208)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.33312 \u001b[0m(-0.00216)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.29971 \u001b[0m(-0.00246)\n",
            "     | > avg_loss:\u001b[92m 0.96734 \u001b[0m(-0.01313)\n",
            "     | > avg_align_error:\u001b[92m 0.43032 \u001b[0m(-0.01850)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_19186.pth.tar\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 53/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 03:34:47) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 14/361 -- GLOBAL_STEP: 19200\u001b[0m\n",
            "     | > decoder_loss: 0.68989  (0.68858)\n",
            "     | > postnet_loss: 0.61596  (0.62302)\n",
            "     | > stopnet_loss: 0.28937  (0.31606)\n",
            "     | > decoder_coarse_loss: 0.89938  (0.90933)\n",
            "     | > decoder_ddc_loss: 0.01998  (0.02365)\n",
            "     | > ga_loss: 0.00106  (0.00162)\n",
            "     | > decoder_diff_spec_loss: 0.29300  (0.29778)\n",
            "     | > postnet_diff_spec_loss: 0.28916  (0.31221)\n",
            "     | > decoder_ssim_loss: 0.32971  (0.34340)\n",
            "     | > postnet_ssim_loss: 0.31293  (0.32564)\n",
            "     | > loss: 1.15719  (1.20508)\n",
            "     | > align_error: 0.42440  (0.43109)\n",
            "     | > grad_norm: 1.12853  (5.92820)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.92190  (0.83279)\n",
            "     | > loader_time: 0.00470  (0.00572)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 39/361 -- GLOBAL_STEP: 19225\u001b[0m\n",
            "     | > decoder_loss: 0.65247  (0.68740)\n",
            "     | > postnet_loss: 0.57643  (0.61804)\n",
            "     | > stopnet_loss: 0.19442  (0.26937)\n",
            "     | > decoder_coarse_loss: 0.88718  (0.91302)\n",
            "     | > decoder_ddc_loss: 0.01810  (0.02060)\n",
            "     | > ga_loss: 0.00067  (0.00111)\n",
            "     | > decoder_diff_spec_loss: 0.29247  (0.29970)\n",
            "     | > postnet_diff_spec_loss: 0.30183  (0.30585)\n",
            "     | > decoder_ssim_loss: 0.35329  (0.34242)\n",
            "     | > postnet_ssim_loss: 0.33166  (0.32401)\n",
            "     | > loss: 1.05113  (1.15269)\n",
            "     | > align_error: 0.41077  (0.42705)\n",
            "     | > grad_norm: 4.19288  (3.14121)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.95400  (0.95010)\n",
            "     | > loader_time: 0.00530  (0.00579)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 64/361 -- GLOBAL_STEP: 19250\u001b[0m\n",
            "     | > decoder_loss: 0.70499  (0.69175)\n",
            "     | > postnet_loss: 0.62268  (0.61988)\n",
            "     | > stopnet_loss: 0.19861  (0.25586)\n",
            "     | > decoder_coarse_loss: 0.92291  (0.91813)\n",
            "     | > decoder_ddc_loss: 0.01585  (0.01884)\n",
            "     | > ga_loss: 0.00052  (0.00091)\n",
            "     | > decoder_diff_spec_loss: 0.31216  (0.30248)\n",
            "     | > postnet_diff_spec_loss: 0.30989  (0.30533)\n",
            "     | > decoder_ssim_loss: 0.34873  (0.34151)\n",
            "     | > postnet_ssim_loss: 0.32447  (0.32249)\n",
            "     | > loss: 1.09162  (1.14050)\n",
            "     | > align_error: 0.42487  (0.42477)\n",
            "     | > grad_norm: 1.30893  (2.53478)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.04570  (0.99677)\n",
            "     | > loader_time: 0.00550  (0.00573)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 89/361 -- GLOBAL_STEP: 19275\u001b[0m\n",
            "     | > decoder_loss: 0.69554  (0.69191)\n",
            "     | > postnet_loss: 0.61617  (0.61868)\n",
            "     | > stopnet_loss: 0.20011  (0.24595)\n",
            "     | > decoder_coarse_loss: 0.90983  (0.91620)\n",
            "     | > decoder_ddc_loss: 0.01413  (0.01761)\n",
            "     | > ga_loss: 0.00043  (0.00079)\n",
            "     | > decoder_diff_spec_loss: 0.30686  (0.30319)\n",
            "     | > postnet_diff_spec_loss: 0.30389  (0.30499)\n",
            "     | > decoder_ssim_loss: 0.35603  (0.34234)\n",
            "     | > postnet_ssim_loss: 0.33309  (0.32287)\n",
            "     | > loss: 1.08612  (1.12933)\n",
            "     | > align_error: 0.42357  (0.42491)\n",
            "     | > grad_norm: 1.81519  (2.20191)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.11370  (1.03834)\n",
            "     | > loader_time: 0.00660  (0.00570)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 114/361 -- GLOBAL_STEP: 19300\u001b[0m\n",
            "     | > decoder_loss: 0.68802  (0.69329)\n",
            "     | > postnet_loss: 0.60752  (0.61888)\n",
            "     | > stopnet_loss: 0.19743  (0.23664)\n",
            "     | > decoder_coarse_loss: 0.92301  (0.91607)\n",
            "     | > decoder_ddc_loss: 0.01294  (0.01670)\n",
            "     | > ga_loss: 0.00035  (0.00070)\n",
            "     | > decoder_diff_spec_loss: 0.30717  (0.30403)\n",
            "     | > postnet_diff_spec_loss: 0.29917  (0.30425)\n",
            "     | > decoder_ssim_loss: 0.34103  (0.34292)\n",
            "     | > postnet_ssim_loss: 0.31920  (0.32303)\n",
            "     | > loss: 1.07371  (1.11992)\n",
            "     | > align_error: 0.41945  (0.42407)\n",
            "     | > grad_norm: 0.84498  (2.02296)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.18790  (1.07506)\n",
            "     | > loader_time: 0.00550  (0.00571)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 139/361 -- GLOBAL_STEP: 19325\u001b[0m\n",
            "     | > decoder_loss: 0.67438  (0.69466)\n",
            "     | > postnet_loss: 0.59541  (0.61885)\n",
            "     | > stopnet_loss: 0.20969  (0.22919)\n",
            "     | > decoder_coarse_loss: 0.86132  (0.91787)\n",
            "     | > decoder_ddc_loss: 0.01193  (0.01596)\n",
            "     | > ga_loss: 0.00030  (0.00063)\n",
            "     | > decoder_diff_spec_loss: 0.30550  (0.30552)\n",
            "     | > postnet_diff_spec_loss: 0.30174  (0.30489)\n",
            "     | > decoder_ssim_loss: 0.35331  (0.34351)\n",
            "     | > postnet_ssim_loss: 0.33092  (0.32317)\n",
            "     | > loss: 1.06984  (1.11346)\n",
            "     | > align_error: 0.41000  (0.42349)\n",
            "     | > grad_norm: 1.21811  (1.88459)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.26100  (1.10927)\n",
            "     | > loader_time: 0.00620  (0.00579)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 164/361 -- GLOBAL_STEP: 19350\u001b[0m\n",
            "     | > decoder_loss: 0.70489  (0.69405)\n",
            "     | > postnet_loss: 0.62255  (0.61787)\n",
            "     | > stopnet_loss: 0.19884  (0.22297)\n",
            "     | > decoder_coarse_loss: 0.93688  (0.91751)\n",
            "     | > decoder_ddc_loss: 0.01116  (0.01530)\n",
            "     | > ga_loss: 0.00028  (0.00058)\n",
            "     | > decoder_diff_spec_loss: 0.31815  (0.30573)\n",
            "     | > postnet_diff_spec_loss: 0.30616  (0.30480)\n",
            "     | > decoder_ssim_loss: 0.34544  (0.34395)\n",
            "     | > postnet_ssim_loss: 0.32285  (0.32343)\n",
            "     | > loss: 1.09226  (1.10652)\n",
            "     | > align_error: 0.42208  (0.42344)\n",
            "     | > grad_norm: 1.10622  (1.74350)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.53050  (1.14935)\n",
            "     | > loader_time: 0.00650  (0.00587)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 189/361 -- GLOBAL_STEP: 19375\u001b[0m\n",
            "     | > decoder_loss: 0.65883  (0.69353)\n",
            "     | > postnet_loss: 0.58462  (0.61703)\n",
            "     | > stopnet_loss: 0.17984  (0.21862)\n",
            "     | > decoder_coarse_loss: 0.88020  (0.91726)\n",
            "     | > decoder_ddc_loss: 0.01080  (0.01472)\n",
            "     | > ga_loss: 0.00025  (0.00054)\n",
            "     | > decoder_diff_spec_loss: 0.30966  (0.30613)\n",
            "     | > postnet_diff_spec_loss: 0.29963  (0.30524)\n",
            "     | > decoder_ssim_loss: 0.34217  (0.34410)\n",
            "     | > postnet_ssim_loss: 0.32024  (0.32343)\n",
            "     | > loss: 1.03262  (1.10168)\n",
            "     | > align_error: 0.42055  (0.42298)\n",
            "     | > grad_norm: 0.56761  (1.65056)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.41240  (1.18250)\n",
            "     | > loader_time: 0.00530  (0.00589)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 214/361 -- GLOBAL_STEP: 19400\u001b[0m\n",
            "     | > decoder_loss: 0.67093  (0.69429)\n",
            "     | > postnet_loss: 0.59504  (0.61725)\n",
            "     | > stopnet_loss: 0.21401  (0.21426)\n",
            "     | > decoder_coarse_loss: 0.89170  (0.91817)\n",
            "     | > decoder_ddc_loss: 0.01013  (0.01421)\n",
            "     | > ga_loss: 0.00025  (0.00050)\n",
            "     | > decoder_diff_spec_loss: 0.29483  (0.30664)\n",
            "     | > postnet_diff_spec_loss: 0.28774  (0.30516)\n",
            "     | > decoder_ssim_loss: 0.33176  (0.34438)\n",
            "     | > postnet_ssim_loss: 0.31094  (0.32353)\n",
            "     | > loss: 1.06351  (1.09768)\n",
            "     | > align_error: 0.42708  (0.42272)\n",
            "     | > grad_norm: 1.18701  (1.56943)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.47290  (1.22052)\n",
            "     | > loader_time: 0.00560  (0.00592)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 239/361 -- GLOBAL_STEP: 19425\u001b[0m\n",
            "     | > decoder_loss: 0.71692  (0.69601)\n",
            "     | > postnet_loss: 0.62276  (0.61849)\n",
            "     | > stopnet_loss: 0.15276  (0.21260)\n",
            "     | > decoder_coarse_loss: 0.95073  (0.92021)\n",
            "     | > decoder_ddc_loss: 0.00925  (0.01373)\n",
            "     | > ga_loss: 0.00020  (0.00047)\n",
            "     | > decoder_diff_spec_loss: 0.31613  (0.30742)\n",
            "     | > postnet_diff_spec_loss: 0.31962  (0.30584)\n",
            "     | > decoder_ssim_loss: 0.36298  (0.34429)\n",
            "     | > postnet_ssim_loss: 0.33760  (0.32327)\n",
            "     | > loss: 1.06275  (1.09727)\n",
            "     | > align_error: 0.42531  (0.42285)\n",
            "     | > grad_norm: 1.67854  (1.54410)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.58040  (1.26043)\n",
            "     | > loader_time: 0.00580  (0.00597)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 264/361 -- GLOBAL_STEP: 19450\u001b[0m\n",
            "     | > decoder_loss: 0.71356  (0.69752)\n",
            "     | > postnet_loss: 0.62810  (0.61938)\n",
            "     | > stopnet_loss: 0.18352  (0.21010)\n",
            "     | > decoder_coarse_loss: 0.95274  (0.92229)\n",
            "     | > decoder_ddc_loss: 0.00886  (0.01328)\n",
            "     | > ga_loss: 0.00018  (0.00044)\n",
            "     | > decoder_diff_spec_loss: 0.31845  (0.30818)\n",
            "     | > postnet_diff_spec_loss: 0.31278  (0.30618)\n",
            "     | > decoder_ssim_loss: 0.34362  (0.34423)\n",
            "     | > postnet_ssim_loss: 0.32029  (0.32302)\n",
            "     | > loss: 1.08401  (1.09584)\n",
            "     | > align_error: 0.42868  (0.42292)\n",
            "     | > grad_norm: 1.02507  (1.51339)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.68130  (1.30127)\n",
            "     | > loader_time: 0.00680  (0.00603)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 289/361 -- GLOBAL_STEP: 19475\u001b[0m\n",
            "     | > decoder_loss: 0.70851  (0.69897)\n",
            "     | > postnet_loss: 0.62228  (0.62035)\n",
            "     | > stopnet_loss: 0.16369  (0.20702)\n",
            "     | > decoder_coarse_loss: 0.93938  (0.92418)\n",
            "     | > decoder_ddc_loss: 0.00795  (0.01287)\n",
            "     | > ga_loss: 0.00016  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.30904  (0.30874)\n",
            "     | > postnet_diff_spec_loss: 0.30589  (0.30656)\n",
            "     | > decoder_ssim_loss: 0.36054  (0.34459)\n",
            "     | > postnet_ssim_loss: 0.33566  (0.32319)\n",
            "     | > loss: 1.06181  (1.09398)\n",
            "     | > align_error: 0.43649  (0.42302)\n",
            "     | > grad_norm: 1.30921  (1.48884)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.79580  (1.34211)\n",
            "     | > loader_time: 0.00620  (0.00610)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 314/361 -- GLOBAL_STEP: 19500\u001b[0m\n",
            "     | > decoder_loss: 0.72314  (0.70015)\n",
            "     | > postnet_loss: 0.63291  (0.62093)\n",
            "     | > stopnet_loss: 0.14238  (0.20396)\n",
            "     | > decoder_coarse_loss: 0.95598  (0.92640)\n",
            "     | > decoder_ddc_loss: 0.00740  (0.01247)\n",
            "     | > ga_loss: 0.00016  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.31751  (0.30932)\n",
            "     | > postnet_diff_spec_loss: 0.31700  (0.30710)\n",
            "     | > decoder_ssim_loss: 0.36113  (0.34493)\n",
            "     | > postnet_ssim_loss: 0.33660  (0.32337)\n",
            "     | > loss: 1.05609  (1.09212)\n",
            "     | > align_error: 0.42706  (0.42340)\n",
            "     | > grad_norm: 0.47298  (1.45110)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.90330  (1.38150)\n",
            "     | > loader_time: 0.00750  (0.00615)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 339/361 -- GLOBAL_STEP: 19525\u001b[0m\n",
            "     | > decoder_loss: 0.72202  (0.70222)\n",
            "     | > postnet_loss: 0.63829  (0.62211)\n",
            "     | > stopnet_loss: 0.26438  (0.20207)\n",
            "     | > decoder_coarse_loss: 0.99018  (0.92960)\n",
            "     | > decoder_ddc_loss: 0.00670  (0.01208)\n",
            "     | > ga_loss: 0.00014  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.31995  (0.31019)\n",
            "     | > postnet_diff_spec_loss: 0.31526  (0.30765)\n",
            "     | > decoder_ssim_loss: 0.31274  (0.34510)\n",
            "     | > postnet_ssim_loss: 0.29396  (0.32334)\n",
            "     | > loss: 1.16488  (1.09204)\n",
            "     | > align_error: 0.42952  (0.42365)\n",
            "     | > grad_norm: 1.07206  (1.41403)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 2.47480  (1.42940)\n",
            "     | > loader_time: 0.00610  (0.00622)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.65550  (0.65550)\n",
            "     | > postnet_loss: 0.56925  (0.56925)\n",
            "     | > stopnet_loss: 0.28792  (0.28792)\n",
            "     | > decoder_coarse_loss: 0.84826  (0.84826)\n",
            "     | > decoder_ddc_loss: 0.01403  (0.01403)\n",
            "     | > ga_loss: 0.00104  (0.00104)\n",
            "     | > decoder_diff_spec_loss: 0.25778  (0.25778)\n",
            "     | > postnet_diff_spec_loss: 0.21819  (0.21819)\n",
            "     | > decoder_ssim_loss: 0.30284  (0.30284)\n",
            "     | > postnet_ssim_loss: 0.27725  (0.27725)\n",
            "     | > loss: 1.07888  (1.07888)\n",
            "     | > align_error: 0.51501  (0.51501)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.62169  (0.62169)\n",
            "     | > postnet_loss: 0.53034  (0.53034)\n",
            "     | > stopnet_loss: 0.15008  (0.15008)\n",
            "     | > decoder_coarse_loss: 0.77307  (0.77307)\n",
            "     | > decoder_ddc_loss: 0.01315  (0.01315)\n",
            "     | > ga_loss: 0.00043  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.27732  (0.27732)\n",
            "     | > postnet_diff_spec_loss: 0.23080  (0.23080)\n",
            "     | > decoder_ssim_loss: 0.34380  (0.34380)\n",
            "     | > postnet_ssim_loss: 0.31067  (0.31067)\n",
            "     | > loss: 0.92747  (0.92747)\n",
            "     | > align_error: 0.40593  (0.40593)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.62740  (0.62455)\n",
            "     | > postnet_loss: 0.53945  (0.53489)\n",
            "     | > stopnet_loss: 0.18686  (0.16847)\n",
            "     | > decoder_coarse_loss: 0.78921  (0.78114)\n",
            "     | > decoder_ddc_loss: 0.01081  (0.01198)\n",
            "     | > ga_loss: 0.00036  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.26832  (0.27282)\n",
            "     | > postnet_diff_spec_loss: 0.22354  (0.22717)\n",
            "     | > decoder_ssim_loss: 0.31106  (0.32743)\n",
            "     | > postnet_ssim_loss: 0.28235  (0.29651)\n",
            "     | > loss: 0.95172  (0.93959)\n",
            "     | > align_error: 0.42326  (0.41459)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.65314  (0.63408)\n",
            "     | > postnet_loss: 0.55606  (0.54195)\n",
            "     | > stopnet_loss: 0.16392  (0.16696)\n",
            "     | > decoder_coarse_loss: 0.85941  (0.80723)\n",
            "     | > decoder_ddc_loss: 0.00984  (0.01127)\n",
            "     | > ga_loss: 0.00023  (0.00034)\n",
            "     | > decoder_diff_spec_loss: 0.28316  (0.27627)\n",
            "     | > postnet_diff_spec_loss: 0.23580  (0.23005)\n",
            "     | > decoder_ssim_loss: 0.32689  (0.32725)\n",
            "     | > postnet_ssim_loss: 0.29476  (0.29593)\n",
            "     | > loss: 0.96984  (0.94967)\n",
            "     | > align_error: 0.41437  (0.41452)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.65272  (0.63874)\n",
            "     | > postnet_loss: 0.54984  (0.54392)\n",
            "     | > stopnet_loss: 0.10784  (0.15218)\n",
            "     | > decoder_coarse_loss: 0.83146  (0.81329)\n",
            "     | > decoder_ddc_loss: 0.00917  (0.01075)\n",
            "     | > ga_loss: 0.00018  (0.00030)\n",
            "     | > decoder_diff_spec_loss: 0.29392  (0.28068)\n",
            "     | > postnet_diff_spec_loss: 0.24265  (0.23320)\n",
            "     | > decoder_ssim_loss: 0.35369  (0.33386)\n",
            "     | > postnet_ssim_loss: 0.31759  (0.30134)\n",
            "     | > loss: 0.92149  (0.94263)\n",
            "     | > align_error: 0.40038  (0.41098)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.67101  (0.64519)\n",
            "     | > postnet_loss: 0.57143  (0.54943)\n",
            "     | > stopnet_loss: 0.18692  (0.15912)\n",
            "     | > decoder_coarse_loss: 0.84007  (0.81865)\n",
            "     | > decoder_ddc_loss: 0.00793  (0.01018)\n",
            "     | > ga_loss: 0.00014  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.28452  (0.28145)\n",
            "     | > postnet_diff_spec_loss: 0.23401  (0.23336)\n",
            "     | > decoder_ssim_loss: 0.31811  (0.33071)\n",
            "     | > postnet_ssim_loss: 0.28525  (0.29812)\n",
            "     | > loss: 0.99068  (0.95224)\n",
            "     | > align_error: 0.41256  (0.41130)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.67267  (0.64977)\n",
            "     | > postnet_loss: 0.57100  (0.55302)\n",
            "     | > stopnet_loss: 0.20979  (0.16757)\n",
            "     | > decoder_coarse_loss: 0.84132  (0.82243)\n",
            "     | > decoder_ddc_loss: 0.00576  (0.00945)\n",
            "     | > ga_loss: 0.00010  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.29168  (0.28315)\n",
            "     | > postnet_diff_spec_loss: 0.23940  (0.23437)\n",
            "     | > decoder_ssim_loss: 0.30807  (0.32694)\n",
            "     | > postnet_ssim_loss: 0.27613  (0.29446)\n",
            "     | > loss: 1.01181  (0.96217)\n",
            "     | > align_error: 0.43764  (0.41569)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.71406  (0.65896)\n",
            "     | > postnet_loss: 0.60487  (0.56043)\n",
            "     | > stopnet_loss: 0.07173  (0.15388)\n",
            "     | > decoder_coarse_loss: 0.93662  (0.83874)\n",
            "     | > decoder_ddc_loss: 0.00607  (0.00896)\n",
            "     | > ga_loss: 0.00010  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.29481  (0.28482)\n",
            "     | > postnet_diff_spec_loss: 0.24499  (0.23589)\n",
            "     | > decoder_ssim_loss: 0.35478  (0.33092)\n",
            "     | > postnet_ssim_loss: 0.31939  (0.29802)\n",
            "     | > loss: 0.94113  (0.95916)\n",
            "     | > align_error: 0.40560  (0.41425)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00611 \u001b[0m(+0.00131)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.65896 \u001b[0m(-0.00510)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.56043 \u001b[0m(-0.00089)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.15388 \u001b[0m(-0.00033)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.83874 \u001b[0m(-0.01760)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00896 \u001b[0m(-0.00085)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00022 \u001b[0m(-0.00001)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.28482 \u001b[0m(-0.00188)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.23589 \u001b[0m(-0.00098)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.33092 \u001b[0m(-0.00221)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.29802 \u001b[0m(-0.00169)\n",
            "     | > avg_loss:\u001b[92m 0.95916 \u001b[0m(-0.00818)\n",
            "     | > avg_align_error:\u001b[92m 0.41425 \u001b[0m(-0.01607)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_19548.pth.tar\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 54/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 03:44:24) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/361 -- GLOBAL_STEP: 19550\u001b[0m\n",
            "     | > decoder_loss: 0.66919  (0.69622)\n",
            "     | > postnet_loss: 0.59772  (0.62413)\n",
            "     | > stopnet_loss: 0.34592  (0.40606)\n",
            "     | > decoder_coarse_loss: 0.85444  (0.90322)\n",
            "     | > decoder_ddc_loss: 0.02667  (0.02635)\n",
            "     | > ga_loss: 0.00213  (0.00240)\n",
            "     | > decoder_diff_spec_loss: 0.30400  (0.29944)\n",
            "     | > postnet_diff_spec_loss: 0.30184  (0.29847)\n",
            "     | > decoder_ssim_loss: 0.33722  (0.33428)\n",
            "     | > postnet_ssim_loss: 0.31875  (0.31673)\n",
            "     | > loss: 1.20905  (1.29275)\n",
            "     | > align_error: 0.41001  (0.42279)\n",
            "     | > grad_norm: 15.49199  (18.40006)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.76750  (0.72645)\n",
            "     | > loader_time: 0.00460  (0.01219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 27/361 -- GLOBAL_STEP: 19575\u001b[0m\n",
            "     | > decoder_loss: 0.70716  (0.68108)\n",
            "     | > postnet_loss: 0.64424  (0.61334)\n",
            "     | > stopnet_loss: 0.28742  (0.28149)\n",
            "     | > decoder_coarse_loss: 0.95644  (0.89921)\n",
            "     | > decoder_ddc_loss: 0.01668  (0.02070)\n",
            "     | > ga_loss: 0.00076  (0.00125)\n",
            "     | > decoder_diff_spec_loss: 0.30644  (0.29701)\n",
            "     | > postnet_diff_spec_loss: 0.30145  (0.30240)\n",
            "     | > decoder_ssim_loss: 0.31690  (0.34300)\n",
            "     | > postnet_ssim_loss: 0.30176  (0.32448)\n",
            "     | > loss: 1.17901  (1.15803)\n",
            "     | > align_error: 0.40767  (0.41011)\n",
            "     | > grad_norm: 1.26308  (4.18993)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.03560  (0.94390)\n",
            "     | > loader_time: 0.00620  (0.00595)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 52/361 -- GLOBAL_STEP: 19600\u001b[0m\n",
            "     | > decoder_loss: 0.67287  (0.68611)\n",
            "     | > postnet_loss: 0.59590  (0.61573)\n",
            "     | > stopnet_loss: 0.18732  (0.26204)\n",
            "     | > decoder_coarse_loss: 0.86683  (0.90318)\n",
            "     | > decoder_ddc_loss: 0.01533  (0.01852)\n",
            "     | > ga_loss: 0.00054  (0.00096)\n",
            "     | > decoder_diff_spec_loss: 0.29984  (0.30002)\n",
            "     | > postnet_diff_spec_loss: 0.29592  (0.30306)\n",
            "     | > decoder_ssim_loss: 0.34761  (0.34115)\n",
            "     | > postnet_ssim_loss: 0.32648  (0.32244)\n",
            "     | > loss: 1.04522  (1.13939)\n",
            "     | > align_error: 0.40953  (0.40948)\n",
            "     | > grad_norm: 1.36146  (2.97052)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.01750  (0.98348)\n",
            "     | > loader_time: 0.00520  (0.00580)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 77/361 -- GLOBAL_STEP: 19625\u001b[0m\n",
            "     | > decoder_loss: 0.67886  (0.68807)\n",
            "     | > postnet_loss: 0.60200  (0.61595)\n",
            "     | > stopnet_loss: 0.22693  (0.24938)\n",
            "     | > decoder_coarse_loss: 0.85348  (0.90326)\n",
            "     | > decoder_ddc_loss: 0.01341  (0.01712)\n",
            "     | > ga_loss: 0.00045  (0.00081)\n",
            "     | > decoder_diff_spec_loss: 0.29805  (0.30186)\n",
            "     | > postnet_diff_spec_loss: 0.28860  (0.30294)\n",
            "     | > decoder_ssim_loss: 0.33844  (0.34092)\n",
            "     | > postnet_ssim_loss: 0.31798  (0.32170)\n",
            "     | > loss: 1.07690  (1.12638)\n",
            "     | > align_error: 0.40270  (0.40814)\n",
            "     | > grad_norm: 0.77410  (2.43178)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.15950  (1.02271)\n",
            "     | > loader_time: 0.00610  (0.00574)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 102/361 -- GLOBAL_STEP: 19650\u001b[0m\n",
            "     | > decoder_loss: 0.67003  (0.68613)\n",
            "     | > postnet_loss: 0.59747  (0.61376)\n",
            "     | > stopnet_loss: 0.22632  (0.24165)\n",
            "     | > decoder_coarse_loss: 0.90438  (0.89814)\n",
            "     | > decoder_ddc_loss: 0.01226  (0.01613)\n",
            "     | > ga_loss: 0.00039  (0.00071)\n",
            "     | > decoder_diff_spec_loss: 0.30559  (0.30201)\n",
            "     | > postnet_diff_spec_loss: 0.29603  (0.30268)\n",
            "     | > decoder_ssim_loss: 0.32826  (0.34087)\n",
            "     | > postnet_ssim_loss: 0.30823  (0.32148)\n",
            "     | > loss: 1.08382  (1.11551)\n",
            "     | > align_error: 0.41137  (0.40686)\n",
            "     | > grad_norm: 1.60515  (2.27230)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.26230  (1.06363)\n",
            "     | > loader_time: 0.00590  (0.00577)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 127/361 -- GLOBAL_STEP: 19675\u001b[0m\n",
            "     | > decoder_loss: 0.69436  (0.68810)\n",
            "     | > postnet_loss: 0.61349  (0.61421)\n",
            "     | > stopnet_loss: 0.18281  (0.23367)\n",
            "     | > decoder_coarse_loss: 0.94651  (0.90616)\n",
            "     | > decoder_ddc_loss: 0.01410  (0.01563)\n",
            "     | > ga_loss: 0.00035  (0.00064)\n",
            "     | > decoder_diff_spec_loss: 0.31091  (0.30346)\n",
            "     | > postnet_diff_spec_loss: 0.30628  (0.30288)\n",
            "     | > decoder_ssim_loss: 0.34260  (0.34129)\n",
            "     | > postnet_ssim_loss: 0.32001  (0.32144)\n",
            "     | > loss: 1.07160  (1.11016)\n",
            "     | > align_error: 0.40225  (0.40572)\n",
            "     | > grad_norm: 2.26063  (2.19173)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.22120  (1.09897)\n",
            "     | > loader_time: 0.00510  (0.00577)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 152/361 -- GLOBAL_STEP: 19700\u001b[0m\n",
            "     | > decoder_loss: 0.67791  (0.68759)\n",
            "     | > postnet_loss: 0.60834  (0.61297)\n",
            "     | > stopnet_loss: 0.24414  (0.22734)\n",
            "     | > decoder_coarse_loss: 0.90114  (0.90937)\n",
            "     | > decoder_ddc_loss: 0.01107  (0.01514)\n",
            "     | > ga_loss: 0.00027  (0.00058)\n",
            "     | > decoder_diff_spec_loss: 0.30011  (0.30393)\n",
            "     | > postnet_diff_spec_loss: 0.29137  (0.30287)\n",
            "     | > decoder_ssim_loss: 0.33002  (0.34171)\n",
            "     | > postnet_ssim_loss: 0.31050  (0.32155)\n",
            "     | > loss: 1.10311  (1.10404)\n",
            "     | > align_error: 0.40275  (0.40515)\n",
            "     | > grad_norm: 0.79939  (2.01691)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.43320  (1.13389)\n",
            "     | > loader_time: 0.00530  (0.00580)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 177/361 -- GLOBAL_STEP: 19725\u001b[0m\n",
            "     | > decoder_loss: 0.67506  (0.68735)\n",
            "     | > postnet_loss: 0.59784  (0.61231)\n",
            "     | > stopnet_loss: 0.21449  (0.22132)\n",
            "     | > decoder_coarse_loss: 0.89479  (0.90910)\n",
            "     | > decoder_ddc_loss: 0.01013  (0.01460)\n",
            "     | > ga_loss: 0.00022  (0.00054)\n",
            "     | > decoder_diff_spec_loss: 0.30665  (0.30437)\n",
            "     | > postnet_diff_spec_loss: 0.29664  (0.30288)\n",
            "     | > decoder_ssim_loss: 0.33291  (0.34213)\n",
            "     | > postnet_ssim_loss: 0.31253  (0.32177)\n",
            "     | > loss: 1.07224  (1.09764)\n",
            "     | > align_error: 0.40597  (0.40493)\n",
            "     | > grad_norm: 0.97635  (1.87101)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.42870  (1.16934)\n",
            "     | > loader_time: 0.00620  (0.00583)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 202/361 -- GLOBAL_STEP: 19750\u001b[0m\n",
            "     | > decoder_loss: 0.69084  (0.68728)\n",
            "     | > postnet_loss: 0.61261  (0.61195)\n",
            "     | > stopnet_loss: 0.18644  (0.21638)\n",
            "     | > decoder_coarse_loss: 0.91958  (0.90873)\n",
            "     | > decoder_ddc_loss: 0.01023  (0.01410)\n",
            "     | > ga_loss: 0.00022  (0.00050)\n",
            "     | > decoder_diff_spec_loss: 0.31451  (0.30474)\n",
            "     | > postnet_diff_spec_loss: 0.30465  (0.30326)\n",
            "     | > decoder_ssim_loss: 0.34610  (0.34266)\n",
            "     | > postnet_ssim_loss: 0.32427  (0.32216)\n",
            "     | > loss: 1.06823  (1.09261)\n",
            "     | > align_error: 0.40137  (0.40466)\n",
            "     | > grad_norm: 1.30303  (1.75794)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 1.51130  (1.20770)\n",
            "     | > loader_time: 0.00700  (0.00589)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 227/361 -- GLOBAL_STEP: 19775\u001b[0m\n",
            "     | > decoder_loss: 0.70552  (0.68819)\n",
            "     | > postnet_loss: 0.62098  (0.61241)\n",
            "     | > stopnet_loss: 0.19607  (0.21477)\n",
            "     | > decoder_coarse_loss: 0.93660  (0.90974)\n",
            "     | > decoder_ddc_loss: 0.00900  (0.01360)\n",
            "     | > ga_loss: 0.00018  (0.00047)\n",
            "     | > decoder_diff_spec_loss: 0.31113  (0.30533)\n",
            "     | > postnet_diff_spec_loss: 0.30767  (0.30368)\n",
            "     | > decoder_ssim_loss: 0.35422  (0.34217)\n",
            "     | > postnet_ssim_loss: 0.33099  (0.32152)\n",
            "     | > loss: 1.09101  (1.09127)\n",
            "     | > align_error: 0.40549  (0.40495)\n",
            "     | > grad_norm: 2.10717  (1.68480)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.67740  (1.25324)\n",
            "     | > loader_time: 0.00640  (0.00596)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 252/361 -- GLOBAL_STEP: 19800\u001b[0m\n",
            "     | > decoder_loss: 0.71944  (0.68973)\n",
            "     | > postnet_loss: 0.63530  (0.61317)\n",
            "     | > stopnet_loss: 0.19231  (0.21164)\n",
            "     | > decoder_coarse_loss: 0.95124  (0.91137)\n",
            "     | > decoder_ddc_loss: 0.00860  (0.01314)\n",
            "     | > ga_loss: 0.00018  (0.00044)\n",
            "     | > decoder_diff_spec_loss: 0.31665  (0.30608)\n",
            "     | > postnet_diff_spec_loss: 0.30721  (0.30402)\n",
            "     | > decoder_ssim_loss: 0.34215  (0.34222)\n",
            "     | > postnet_ssim_loss: 0.31942  (0.32136)\n",
            "     | > loss: 1.09323  (1.08912)\n",
            "     | > align_error: 0.41232  (0.40511)\n",
            "     | > grad_norm: 1.18156  (1.64601)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.80090  (1.29284)\n",
            "     | > loader_time: 0.00770  (0.00602)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 277/361 -- GLOBAL_STEP: 19825\u001b[0m\n",
            "     | > decoder_loss: 0.72048  (0.69151)\n",
            "     | > postnet_loss: 0.63660  (0.61451)\n",
            "     | > stopnet_loss: 0.14832  (0.20898)\n",
            "     | > decoder_coarse_loss: 0.95249  (0.91292)\n",
            "     | > decoder_ddc_loss: 0.00856  (0.01271)\n",
            "     | > ga_loss: 0.00016  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.33170  (0.30669)\n",
            "     | > postnet_diff_spec_loss: 0.32708  (0.30448)\n",
            "     | > decoder_ssim_loss: 0.35279  (0.34244)\n",
            "     | > postnet_ssim_loss: 0.32773  (0.32141)\n",
            "     | > loss: 1.06348  (1.08773)\n",
            "     | > align_error: 0.40348  (0.40517)\n",
            "     | > grad_norm: 0.91686  (1.61105)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.67650  (1.33438)\n",
            "     | > loader_time: 0.00720  (0.00608)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 302/361 -- GLOBAL_STEP: 19850\u001b[0m\n",
            "     | > decoder_loss: 0.70439  (0.69258)\n",
            "     | > postnet_loss: 0.62096  (0.61498)\n",
            "     | > stopnet_loss: 0.16436  (0.20552)\n",
            "     | > decoder_coarse_loss: 0.94228  (0.91441)\n",
            "     | > decoder_ddc_loss: 0.00762  (0.01231)\n",
            "     | > ga_loss: 0.00015  (0.00039)\n",
            "     | > decoder_diff_spec_loss: 0.31612  (0.30722)\n",
            "     | > postnet_diff_spec_loss: 0.30584  (0.30489)\n",
            "     | > decoder_ssim_loss: 0.34457  (0.34293)\n",
            "     | > postnet_ssim_loss: 0.32162  (0.32172)\n",
            "     | > loss: 1.05597  (1.08526)\n",
            "     | > align_error: 0.41240  (0.40534)\n",
            "     | > grad_norm: 0.64495  (1.56235)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.80960  (1.37423)\n",
            "     | > loader_time: 0.00680  (0.00614)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 327/361 -- GLOBAL_STEP: 19875\u001b[0m\n",
            "     | > decoder_loss: 0.71714  (0.69411)\n",
            "     | > postnet_loss: 0.62664  (0.61591)\n",
            "     | > stopnet_loss: 0.16962  (0.20344)\n",
            "     | > decoder_coarse_loss: 0.92761  (0.91645)\n",
            "     | > decoder_ddc_loss: 0.00666  (0.01192)\n",
            "     | > ga_loss: 0.00013  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.31684  (0.30797)\n",
            "     | > postnet_diff_spec_loss: 0.30822  (0.30538)\n",
            "     | > decoder_ssim_loss: 0.34643  (0.34292)\n",
            "     | > postnet_ssim_loss: 0.32238  (0.32155)\n",
            "     | > loss: 1.06326  (1.08437)\n",
            "     | > align_error: 0.39958  (0.40534)\n",
            "     | > grad_norm: 0.66663  (1.51670)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.07640  (1.41779)\n",
            "     | > loader_time: 0.00620  (0.00619)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 352/361 -- GLOBAL_STEP: 19900\u001b[0m\n",
            "     | > decoder_loss: 0.71880  (0.69614)\n",
            "     | > postnet_loss: 0.65947  (0.61729)\n",
            "     | > stopnet_loss: 0.31805  (0.20247)\n",
            "     | > decoder_coarse_loss: 0.95999  (0.91929)\n",
            "     | > decoder_ddc_loss: 0.00574  (0.01153)\n",
            "     | > ga_loss: 0.00018  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.31396  (0.30870)\n",
            "     | > postnet_diff_spec_loss: 0.32060  (0.30609)\n",
            "     | > decoder_ssim_loss: 0.29742  (0.34283)\n",
            "     | > postnet_ssim_loss: 0.28325  (0.32135)\n",
            "     | > loss: 1.20874  (1.08507)\n",
            "     | > align_error: 0.42152  (0.40596)\n",
            "     | > grad_norm: 1.13474  (1.47688)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.80240  (1.47494)\n",
            "     | > loader_time: 0.00680  (0.00628)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.65690  (0.65690)\n",
            "     | > postnet_loss: 0.56556  (0.56556)\n",
            "     | > stopnet_loss: 0.28090  (0.28090)\n",
            "     | > decoder_coarse_loss: 0.83941  (0.83941)\n",
            "     | > decoder_ddc_loss: 0.01339  (0.01339)\n",
            "     | > ga_loss: 0.00102  (0.00102)\n",
            "     | > decoder_diff_spec_loss: 0.25618  (0.25618)\n",
            "     | > postnet_diff_spec_loss: 0.21673  (0.21673)\n",
            "     | > decoder_ssim_loss: 0.30138  (0.30138)\n",
            "     | > postnet_ssim_loss: 0.27599  (0.27599)\n",
            "     | > loss: 1.06736  (1.06736)\n",
            "     | > align_error: 0.49810  (0.49810)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.61701  (0.61701)\n",
            "     | > postnet_loss: 0.52271  (0.52271)\n",
            "     | > stopnet_loss: 0.14494  (0.14494)\n",
            "     | > decoder_coarse_loss: 0.75670  (0.75670)\n",
            "     | > decoder_ddc_loss: 0.01295  (0.01295)\n",
            "     | > ga_loss: 0.00043  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.27461  (0.27461)\n",
            "     | > postnet_diff_spec_loss: 0.22882  (0.22882)\n",
            "     | > decoder_ssim_loss: 0.34096  (0.34096)\n",
            "     | > postnet_ssim_loss: 0.30844  (0.30844)\n",
            "     | > loss: 0.91262  (0.91262)\n",
            "     | > align_error: 0.38666  (0.38666)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.62555  (0.62128)\n",
            "     | > postnet_loss: 0.53251  (0.52761)\n",
            "     | > stopnet_loss: 0.18331  (0.16413)\n",
            "     | > decoder_coarse_loss: 0.77517  (0.76594)\n",
            "     | > decoder_ddc_loss: 0.01068  (0.01182)\n",
            "     | > ga_loss: 0.00036  (0.00039)\n",
            "     | > decoder_diff_spec_loss: 0.26589  (0.27025)\n",
            "     | > postnet_diff_spec_loss: 0.22169  (0.22526)\n",
            "     | > decoder_ssim_loss: 0.30917  (0.32507)\n",
            "     | > postnet_ssim_loss: 0.28081  (0.29463)\n",
            "     | > loss: 0.94046  (0.92654)\n",
            "     | > align_error: 0.40350  (0.39508)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.65044  (0.63100)\n",
            "     | > postnet_loss: 0.55054  (0.53525)\n",
            "     | > stopnet_loss: 0.16108  (0.16311)\n",
            "     | > decoder_coarse_loss: 0.84233  (0.79140)\n",
            "     | > decoder_ddc_loss: 0.00957  (0.01107)\n",
            "     | > ga_loss: 0.00022  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.28148  (0.27399)\n",
            "     | > postnet_diff_spec_loss: 0.23421  (0.22824)\n",
            "     | > decoder_ssim_loss: 0.32524  (0.32513)\n",
            "     | > postnet_ssim_loss: 0.29312  (0.29412)\n",
            "     | > loss: 0.95893  (0.93734)\n",
            "     | > align_error: 0.39592  (0.39536)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.64869  (0.63542)\n",
            "     | > postnet_loss: 0.54109  (0.53671)\n",
            "     | > stopnet_loss: 0.10613  (0.14887)\n",
            "     | > decoder_coarse_loss: 0.80988  (0.79602)\n",
            "     | > decoder_ddc_loss: 0.00889  (0.01052)\n",
            "     | > ga_loss: 0.00017  (0.00029)\n",
            "     | > decoder_diff_spec_loss: 0.29154  (0.27838)\n",
            "     | > postnet_diff_spec_loss: 0.24059  (0.23133)\n",
            "     | > decoder_ssim_loss: 0.35090  (0.33157)\n",
            "     | > postnet_ssim_loss: 0.31510  (0.29937)\n",
            "     | > loss: 0.90866  (0.93017)\n",
            "     | > align_error: 0.38082  (0.39173)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.66488  (0.64131)\n",
            "     | > postnet_loss: 0.55987  (0.54134)\n",
            "     | > stopnet_loss: 0.18268  (0.15563)\n",
            "     | > decoder_coarse_loss: 0.81539  (0.79989)\n",
            "     | > decoder_ddc_loss: 0.00738  (0.00989)\n",
            "     | > ga_loss: 0.00013  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.28228  (0.27916)\n",
            "     | > postnet_diff_spec_loss: 0.23200  (0.23146)\n",
            "     | > decoder_ssim_loss: 0.31593  (0.32844)\n",
            "     | > postnet_ssim_loss: 0.28324  (0.29614)\n",
            "     | > loss: 0.97357  (0.93885)\n",
            "     | > align_error: 0.39305  (0.39199)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.67155  (0.64635)\n",
            "     | > postnet_loss: 0.56380  (0.54509)\n",
            "     | > stopnet_loss: 0.20849  (0.16444)\n",
            "     | > decoder_coarse_loss: 0.82769  (0.80453)\n",
            "     | > decoder_ddc_loss: 0.00556  (0.00917)\n",
            "     | > ga_loss: 0.00010  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.28973  (0.28092)\n",
            "     | > postnet_diff_spec_loss: 0.23749  (0.23247)\n",
            "     | > decoder_ssim_loss: 0.30625  (0.32474)\n",
            "     | > postnet_ssim_loss: 0.27434  (0.29251)\n",
            "     | > loss: 1.00308  (0.94955)\n",
            "     | > align_error: 0.41791  (0.39631)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.70923  (0.65533)\n",
            "     | > postnet_loss: 0.59813  (0.55266)\n",
            "     | > stopnet_loss: 0.07219  (0.15126)\n",
            "     | > decoder_coarse_loss: 0.92459  (0.82168)\n",
            "     | > decoder_ddc_loss: 0.00607  (0.00873)\n",
            "     | > ga_loss: 0.00009  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.29249  (0.28257)\n",
            "     | > postnet_diff_spec_loss: 0.24304  (0.23398)\n",
            "     | > decoder_ssim_loss: 0.35288  (0.32876)\n",
            "     | > postnet_ssim_loss: 0.31782  (0.29612)\n",
            "     | > loss: 0.93372  (0.94729)\n",
            "     | > align_error: 0.38794  (0.39512)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00666 \u001b[0m(+0.00055)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.65533 \u001b[0m(-0.00362)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.55266 \u001b[0m(-0.00776)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.15126 \u001b[0m(-0.00262)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.82168 \u001b[0m(-0.01706)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00873 \u001b[0m(-0.00023)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00021 \u001b[0m(-0.00001)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.28257 \u001b[0m(-0.00225)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.23398 \u001b[0m(-0.00191)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.32876 \u001b[0m(-0.00215)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.29612 \u001b[0m(-0.00190)\n",
            "     | > avg_loss:\u001b[92m 0.94729 \u001b[0m(-0.01187)\n",
            "     | > avg_align_error:\u001b[92m 0.39512 \u001b[0m(-0.01913)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_19910.pth.tar\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 55/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 03:54:04) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/361 -- GLOBAL_STEP: 19925\u001b[0m\n",
            "     | > decoder_loss: 0.70373  (0.67791)\n",
            "     | > postnet_loss: 0.62467  (0.61569)\n",
            "     | > stopnet_loss: 0.23525  (0.30799)\n",
            "     | > decoder_coarse_loss: 0.94398  (0.88570)\n",
            "     | > decoder_ddc_loss: 0.02032  (0.02160)\n",
            "     | > ga_loss: 0.00100  (0.00152)\n",
            "     | > decoder_diff_spec_loss: 0.31304  (0.29549)\n",
            "     | > postnet_diff_spec_loss: 0.30673  (0.30733)\n",
            "     | > decoder_ssim_loss: 0.34035  (0.33965)\n",
            "     | > postnet_ssim_loss: 0.32117  (0.32243)\n",
            "     | > loss: 1.13375  (1.18205)\n",
            "     | > align_error: 0.39082  (0.39231)\n",
            "     | > grad_norm: 1.45112  (4.63610)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.90980  (0.86652)\n",
            "     | > loader_time: 0.00660  (0.00534)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 40/361 -- GLOBAL_STEP: 19950\u001b[0m\n",
            "     | > decoder_loss: 0.69334  (0.67620)\n",
            "     | > postnet_loss: 0.63025  (0.61020)\n",
            "     | > stopnet_loss: 0.32261  (0.27030)\n",
            "     | > decoder_coarse_loss: 0.88222  (0.88502)\n",
            "     | > decoder_ddc_loss: 0.01501  (0.01881)\n",
            "     | > ga_loss: 0.00063  (0.00105)\n",
            "     | > decoder_diff_spec_loss: 0.29911  (0.29643)\n",
            "     | > postnet_diff_spec_loss: 0.29628  (0.30194)\n",
            "     | > decoder_ssim_loss: 0.31457  (0.33838)\n",
            "     | > postnet_ssim_loss: 0.29711  (0.32057)\n",
            "     | > loss: 1.18271  (1.13743)\n",
            "     | > align_error: 0.38648  (0.38738)\n",
            "     | > grad_norm: 3.39551  (2.89665)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.97540  (0.95633)\n",
            "     | > loader_time: 0.00510  (0.00556)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 65/361 -- GLOBAL_STEP: 19975\u001b[0m\n",
            "     | > decoder_loss: 0.70066  (0.67962)\n",
            "     | > postnet_loss: 0.61657  (0.61079)\n",
            "     | > stopnet_loss: 0.17945  (0.25402)\n",
            "     | > decoder_coarse_loss: 0.89961  (0.88760)\n",
            "     | > decoder_ddc_loss: 0.01418  (0.01716)\n",
            "     | > ga_loss: 0.00046  (0.00086)\n",
            "     | > decoder_diff_spec_loss: 0.31611  (0.29894)\n",
            "     | > postnet_diff_spec_loss: 0.31477  (0.30130)\n",
            "     | > decoder_ssim_loss: 0.34842  (0.33775)\n",
            "     | > postnet_ssim_loss: 0.32445  (0.31927)\n",
            "     | > loss: 1.06544  (1.12140)\n",
            "     | > align_error: 0.37992  (0.38767)\n",
            "     | > grad_norm: 0.64030  (2.28866)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.09690  (1.00127)\n",
            "     | > loader_time: 0.00480  (0.00555)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/361 -- GLOBAL_STEP: 20000\u001b[0m\n",
            "     | > decoder_loss: 0.65739  (0.67886)\n",
            "     | > postnet_loss: 0.58486  (0.60872)\n",
            "     | > stopnet_loss: 0.16698  (0.24389)\n",
            "     | > decoder_coarse_loss: 0.84433  (0.88440)\n",
            "     | > decoder_ddc_loss: 0.01295  (0.01606)\n",
            "     | > ga_loss: 0.00036  (0.00074)\n",
            "     | > decoder_diff_spec_loss: 0.28852  (0.29911)\n",
            "     | > postnet_diff_spec_loss: 0.29326  (0.30056)\n",
            "     | > decoder_ssim_loss: 0.36165  (0.33859)\n",
            "     | > postnet_ssim_loss: 0.34112  (0.31973)\n",
            "     | > loss: 1.01478  (1.10909)\n",
            "     | > align_error: 0.38667  (0.38716)\n",
            "     | > grad_norm: 1.62567  (2.01189)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.44810  (1.04269)\n",
            "     | > loader_time: 0.00510  (0.00558)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/checkpoint_20000.pth.tar\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "\u001b[1m   --> STEP: 115/361 -- GLOBAL_STEP: 20025\u001b[0m\n",
            "     | > decoder_loss: 0.67651  (0.67918)\n",
            "     | > postnet_loss: 0.59842  (0.60802)\n",
            "     | > stopnet_loss: 0.20066  (0.23543)\n",
            "     | > decoder_coarse_loss: 0.88195  (0.88444)\n",
            "     | > decoder_ddc_loss: 0.01173  (0.01524)\n",
            "     | > ga_loss: 0.00033  (0.00066)\n",
            "     | > decoder_diff_spec_loss: 0.31039  (0.30010)\n",
            "     | > postnet_diff_spec_loss: 0.30184  (0.29995)\n",
            "     | > decoder_ssim_loss: 0.33799  (0.33879)\n",
            "     | > postnet_ssim_loss: 0.31559  (0.31953)\n",
            "     | > loss: 1.06092  (1.10003)\n",
            "     | > align_error: 0.37987  (0.38695)\n",
            "     | > grad_norm: 0.80218  (1.88009)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.18500  (1.08700)\n",
            "     | > loader_time: 0.00540  (0.00606)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 140/361 -- GLOBAL_STEP: 20050\u001b[0m\n",
            "     | > decoder_loss: 0.67193  (0.68071)\n",
            "     | > postnet_loss: 0.59546  (0.60804)\n",
            "     | > stopnet_loss: 0.21840  (0.22845)\n",
            "     | > decoder_coarse_loss: 0.88453  (0.88550)\n",
            "     | > decoder_ddc_loss: 0.01080  (0.01456)\n",
            "     | > ga_loss: 0.00028  (0.00060)\n",
            "     | > decoder_diff_spec_loss: 0.29547  (0.30145)\n",
            "     | > postnet_diff_spec_loss: 0.28687  (0.30048)\n",
            "     | > decoder_ssim_loss: 0.33227  (0.33943)\n",
            "     | > postnet_ssim_loss: 0.31222  (0.31971)\n",
            "     | > loss: 1.06717  (1.09389)\n",
            "     | > align_error: 0.38220  (0.38665)\n",
            "     | > grad_norm: 0.78331  (1.75500)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.36060  (1.12337)\n",
            "     | > loader_time: 0.00530  (0.00604)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 165/361 -- GLOBAL_STEP: 20075\u001b[0m\n",
            "     | > decoder_loss: 0.66809  (0.68035)\n",
            "     | > postnet_loss: 0.59020  (0.60725)\n",
            "     | > stopnet_loss: 0.16485  (0.22236)\n",
            "     | > decoder_coarse_loss: 0.85499  (0.88515)\n",
            "     | > decoder_ddc_loss: 0.01075  (0.01397)\n",
            "     | > ga_loss: 0.00023  (0.00054)\n",
            "     | > decoder_diff_spec_loss: 0.30285  (0.30179)\n",
            "     | > postnet_diff_spec_loss: 0.29601  (0.30053)\n",
            "     | > decoder_ssim_loss: 0.34637  (0.34002)\n",
            "     | > postnet_ssim_loss: 0.32367  (0.32010)\n",
            "     | > loss: 1.01422  (1.08737)\n",
            "     | > align_error: 0.38457  (0.38658)\n",
            "     | > grad_norm: 0.59898  (1.63940)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.36600  (1.15836)\n",
            "     | > loader_time: 0.00660  (0.00608)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 190/361 -- GLOBAL_STEP: 20100\u001b[0m\n",
            "     | > decoder_loss: 0.67565  (0.68028)\n",
            "     | > postnet_loss: 0.59717  (0.60665)\n",
            "     | > stopnet_loss: 0.14258  (0.21823)\n",
            "     | > decoder_coarse_loss: 0.88081  (0.88508)\n",
            "     | > decoder_ddc_loss: 0.00964  (0.01347)\n",
            "     | > ga_loss: 0.00024  (0.00051)\n",
            "     | > decoder_diff_spec_loss: 0.31019  (0.30238)\n",
            "     | > postnet_diff_spec_loss: 0.30911  (0.30118)\n",
            "     | > decoder_ssim_loss: 0.36481  (0.34031)\n",
            "     | > postnet_ssim_loss: 0.34060  (0.32021)\n",
            "     | > loss: 1.01575  (1.08315)\n",
            "     | > align_error: 0.40214  (0.38692)\n",
            "     | > grad_norm: 0.94339  (1.56089)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.45940  (1.19538)\n",
            "     | > loader_time: 0.00610  (0.00608)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 215/361 -- GLOBAL_STEP: 20125\u001b[0m\n",
            "     | > decoder_loss: 0.69371  (0.68110)\n",
            "     | > postnet_loss: 0.61421  (0.60688)\n",
            "     | > stopnet_loss: 0.16633  (0.21407)\n",
            "     | > decoder_coarse_loss: 0.89159  (0.88611)\n",
            "     | > decoder_ddc_loss: 0.00911  (0.01301)\n",
            "     | > ga_loss: 0.00020  (0.00047)\n",
            "     | > decoder_diff_spec_loss: 0.30694  (0.30301)\n",
            "     | > postnet_diff_spec_loss: 0.30327  (0.30118)\n",
            "     | > decoder_ssim_loss: 0.35051  (0.34055)\n",
            "     | > postnet_ssim_loss: 0.32739  (0.32026)\n",
            "     | > loss: 1.04151  (1.07946)\n",
            "     | > align_error: 0.40036  (0.38777)\n",
            "     | > grad_norm: 0.88187  (1.50063)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.52920  (1.23646)\n",
            "     | > loader_time: 0.00720  (0.00613)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 240/361 -- GLOBAL_STEP: 20150\u001b[0m\n",
            "     | > decoder_loss: 0.67952  (0.68281)\n",
            "     | > postnet_loss: 0.59172  (0.60792)\n",
            "     | > stopnet_loss: 0.14555  (0.21224)\n",
            "     | > decoder_coarse_loss: 0.86831  (0.88822)\n",
            "     | > decoder_ddc_loss: 0.00860  (0.01257)\n",
            "     | > ga_loss: 0.00018  (0.00044)\n",
            "     | > decoder_diff_spec_loss: 0.30414  (0.30380)\n",
            "     | > postnet_diff_spec_loss: 0.30314  (0.30192)\n",
            "     | > decoder_ssim_loss: 0.35058  (0.34048)\n",
            "     | > postnet_ssim_loss: 0.32670  (0.32001)\n",
            "     | > loss: 1.00461  (1.07889)\n",
            "     | > align_error: 0.39401  (0.38837)\n",
            "     | > grad_norm: 1.03856  (1.47404)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.61800  (1.27903)\n",
            "     | > loader_time: 0.00570  (0.00619)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 265/361 -- GLOBAL_STEP: 20175\u001b[0m\n",
            "     | > decoder_loss: 0.69101  (0.68448)\n",
            "     | > postnet_loss: 0.61408  (0.60894)\n",
            "     | > stopnet_loss: 0.14947  (0.20991)\n",
            "     | > decoder_coarse_loss: 0.88746  (0.89045)\n",
            "     | > decoder_ddc_loss: 0.00783  (0.01217)\n",
            "     | > ga_loss: 0.00016  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.29769  (0.30461)\n",
            "     | > postnet_diff_spec_loss: 0.29975  (0.30234)\n",
            "     | > decoder_ssim_loss: 0.35256  (0.34046)\n",
            "     | > postnet_ssim_loss: 0.32847  (0.31980)\n",
            "     | > loss: 1.02001  (1.07782)\n",
            "     | > align_error: 0.38099  (0.38822)\n",
            "     | > grad_norm: 2.13376  (1.45477)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.65800  (1.32214)\n",
            "     | > loader_time: 0.00650  (0.00627)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 290/361 -- GLOBAL_STEP: 20200\u001b[0m\n",
            "     | > decoder_loss: 0.70730  (0.68596)\n",
            "     | > postnet_loss: 0.62037  (0.60987)\n",
            "     | > stopnet_loss: 0.12794  (0.20674)\n",
            "     | > decoder_coarse_loss: 0.91940  (0.89275)\n",
            "     | > decoder_ddc_loss: 0.00817  (0.01181)\n",
            "     | > ga_loss: 0.00015  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.30270  (0.30517)\n",
            "     | > postnet_diff_spec_loss: 0.30982  (0.30275)\n",
            "     | > decoder_ssim_loss: 0.36189  (0.34083)\n",
            "     | > postnet_ssim_loss: 0.33756  (0.31999)\n",
            "     | > loss: 1.02047  (1.07600)\n",
            "     | > align_error: 0.39560  (0.38797)\n",
            "     | > grad_norm: 1.37249  (1.42851)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.83610  (1.36615)\n",
            "     | > loader_time: 0.00750  (0.00632)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 315/361 -- GLOBAL_STEP: 20225\u001b[0m\n",
            "     | > decoder_loss: 0.70033  (0.68721)\n",
            "     | > postnet_loss: 0.61084  (0.61052)\n",
            "     | > stopnet_loss: 0.17713  (0.20383)\n",
            "     | > decoder_coarse_loss: 0.90644  (0.89507)\n",
            "     | > decoder_ddc_loss: 0.00708  (0.01147)\n",
            "     | > ga_loss: 0.00014  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.32079  (0.30584)\n",
            "     | > postnet_diff_spec_loss: 0.31019  (0.30331)\n",
            "     | > decoder_ssim_loss: 0.33811  (0.34111)\n",
            "     | > postnet_ssim_loss: 0.31346  (0.32011)\n",
            "     | > loss: 1.05462  (1.07436)\n",
            "     | > align_error: 0.39045  (0.38832)\n",
            "     | > grad_norm: 0.52808  (1.38441)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.98780  (1.40840)\n",
            "     | > loader_time: 0.00720  (0.00638)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 340/361 -- GLOBAL_STEP: 20250\u001b[0m\n",
            "     | > decoder_loss: 0.73421  (0.68916)\n",
            "     | > postnet_loss: 0.64732  (0.61170)\n",
            "     | > stopnet_loss: 0.21691  (0.20192)\n",
            "     | > decoder_coarse_loss: 0.96757  (0.89824)\n",
            "     | > decoder_ddc_loss: 0.00644  (0.01112)\n",
            "     | > ga_loss: 0.00012  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.31148  (0.30660)\n",
            "     | > postnet_diff_spec_loss: 0.30220  (0.30378)\n",
            "     | > decoder_ssim_loss: 0.33141  (0.34121)\n",
            "     | > postnet_ssim_loss: 0.30954  (0.32002)\n",
            "     | > loss: 1.12004  (1.07416)\n",
            "     | > align_error: 0.39996  (0.38859)\n",
            "     | > grad_norm: 1.20022  (1.35252)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.34510  (1.45553)\n",
            "     | > loader_time: 0.00720  (0.00644)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.65169  (0.65169)\n",
            "     | > postnet_loss: 0.56313  (0.56313)\n",
            "     | > stopnet_loss: 0.28112  (0.28112)\n",
            "     | > decoder_coarse_loss: 0.83332  (0.83332)\n",
            "     | > decoder_ddc_loss: 0.01311  (0.01311)\n",
            "     | > ga_loss: 0.00101  (0.00101)\n",
            "     | > decoder_diff_spec_loss: 0.25552  (0.25552)\n",
            "     | > postnet_diff_spec_loss: 0.21654  (0.21654)\n",
            "     | > decoder_ssim_loss: 0.30041  (0.30041)\n",
            "     | > postnet_ssim_loss: 0.27554  (0.27554)\n",
            "     | > loss: 1.06347  (1.06347)\n",
            "     | > align_error: 0.48813  (0.48813)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.61406  (0.61406)\n",
            "     | > postnet_loss: 0.52050  (0.52050)\n",
            "     | > stopnet_loss: 0.14580  (0.14580)\n",
            "     | > decoder_coarse_loss: 0.75918  (0.75918)\n",
            "     | > decoder_ddc_loss: 0.01299  (0.01299)\n",
            "     | > ga_loss: 0.00042  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.27454  (0.27454)\n",
            "     | > postnet_diff_spec_loss: 0.22856  (0.22856)\n",
            "     | > decoder_ssim_loss: 0.33972  (0.33972)\n",
            "     | > postnet_ssim_loss: 0.30762  (0.30762)\n",
            "     | > loss: 0.91219  (0.91219)\n",
            "     | > align_error: 0.38073  (0.38073)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.62085  (0.61746)\n",
            "     | > postnet_loss: 0.53000  (0.52525)\n",
            "     | > stopnet_loss: 0.18383  (0.16481)\n",
            "     | > decoder_coarse_loss: 0.77389  (0.76653)\n",
            "     | > decoder_ddc_loss: 0.01079  (0.01189)\n",
            "     | > ga_loss: 0.00036  (0.00039)\n",
            "     | > decoder_diff_spec_loss: 0.26617  (0.27035)\n",
            "     | > postnet_diff_spec_loss: 0.22173  (0.22515)\n",
            "     | > decoder_ssim_loss: 0.30837  (0.32404)\n",
            "     | > postnet_ssim_loss: 0.28051  (0.29407)\n",
            "     | > loss: 0.93869  (0.92544)\n",
            "     | > align_error: 0.39595  (0.38834)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.64774  (0.62755)\n",
            "     | > postnet_loss: 0.54936  (0.53329)\n",
            "     | > stopnet_loss: 0.16003  (0.16322)\n",
            "     | > decoder_coarse_loss: 0.83365  (0.78890)\n",
            "     | > decoder_ddc_loss: 0.00946  (0.01108)\n",
            "     | > ga_loss: 0.00022  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.28075  (0.27382)\n",
            "     | > postnet_diff_spec_loss: 0.23361  (0.22797)\n",
            "     | > decoder_ssim_loss: 0.32388  (0.32399)\n",
            "     | > postnet_ssim_loss: 0.29244  (0.29353)\n",
            "     | > loss: 0.95385  (0.93491)\n",
            "     | > align_error: 0.38927  (0.38865)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.64415  (0.63170)\n",
            "     | > postnet_loss: 0.53913  (0.53475)\n",
            "     | > stopnet_loss: 0.10733  (0.14924)\n",
            "     | > decoder_coarse_loss: 0.80451  (0.79281)\n",
            "     | > decoder_ddc_loss: 0.00882  (0.01052)\n",
            "     | > ga_loss: 0.00017  (0.00029)\n",
            "     | > decoder_diff_spec_loss: 0.29109  (0.27814)\n",
            "     | > postnet_diff_spec_loss: 0.24020  (0.23102)\n",
            "     | > decoder_ssim_loss: 0.34970  (0.33042)\n",
            "     | > postnet_ssim_loss: 0.31443  (0.29875)\n",
            "     | > loss: 0.90619  (0.92773)\n",
            "     | > align_error: 0.37589  (0.38546)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.65602  (0.63656)\n",
            "     | > postnet_loss: 0.55384  (0.53856)\n",
            "     | > stopnet_loss: 0.18319  (0.15603)\n",
            "     | > decoder_coarse_loss: 0.81373  (0.79699)\n",
            "     | > decoder_ddc_loss: 0.00715  (0.00984)\n",
            "     | > ga_loss: 0.00013  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.28177  (0.27886)\n",
            "     | > postnet_diff_spec_loss: 0.23164  (0.23115)\n",
            "     | > decoder_ssim_loss: 0.31437  (0.32721)\n",
            "     | > postnet_ssim_loss: 0.28223  (0.29545)\n",
            "     | > loss: 0.96902  (0.93599)\n",
            "     | > align_error: 0.38776  (0.38592)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.66520  (0.64134)\n",
            "     | > postnet_loss: 0.56016  (0.54216)\n",
            "     | > stopnet_loss: 0.20911  (0.16488)\n",
            "     | > decoder_coarse_loss: 0.82241  (0.80123)\n",
            "     | > decoder_ddc_loss: 0.00544  (0.00911)\n",
            "     | > ga_loss: 0.00010  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.28893  (0.28054)\n",
            "     | > postnet_diff_spec_loss: 0.23696  (0.23212)\n",
            "     | > decoder_ssim_loss: 0.30483  (0.32348)\n",
            "     | > postnet_ssim_loss: 0.27367  (0.29182)\n",
            "     | > loss: 0.99900  (0.94649)\n",
            "     | > align_error: 0.41187  (0.39025)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.70785  (0.65084)\n",
            "     | > postnet_loss: 0.59575  (0.54982)\n",
            "     | > stopnet_loss: 0.07228  (0.15165)\n",
            "     | > decoder_coarse_loss: 0.90743  (0.81640)\n",
            "     | > decoder_ddc_loss: 0.00601  (0.00867)\n",
            "     | > ga_loss: 0.00010  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.29350  (0.28239)\n",
            "     | > postnet_diff_spec_loss: 0.24365  (0.23376)\n",
            "     | > decoder_ssim_loss: 0.35232  (0.32760)\n",
            "     | > postnet_ssim_loss: 0.31767  (0.29551)\n",
            "     | > loss: 0.92882  (0.94397)\n",
            "     | > align_error: 0.38356  (0.38929)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00486 \u001b[0m(-0.00180)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.65084 \u001b[0m(-0.00450)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.54982 \u001b[0m(-0.00284)\n",
            "     | > avg_stopnet_loss:\u001b[91m 0.15165 \u001b[0m(+0.00039)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.81640 \u001b[0m(-0.00528)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00867 \u001b[0m(-0.00006)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00021 \u001b[0m(-0.00000)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.28239 \u001b[0m(-0.00018)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.23376 \u001b[0m(-0.00021)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.32760 \u001b[0m(-0.00116)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.29551 \u001b[0m(-0.00061)\n",
            "     | > avg_loss:\u001b[92m 0.94397 \u001b[0m(-0.00333)\n",
            "     | > avg_align_error:\u001b[92m 0.38929 \u001b[0m(-0.00583)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_20272.pth.tar\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 56/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 04:03:56) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 3/361 -- GLOBAL_STEP: 20275\u001b[0m\n",
            "     | > decoder_loss: 0.73824  (0.70148)\n",
            "     | > postnet_loss: 0.64233  (0.62330)\n",
            "     | > stopnet_loss: 0.29740  (0.35010)\n",
            "     | > decoder_coarse_loss: 0.97601  (0.91055)\n",
            "     | > decoder_ddc_loss: 0.02391  (0.02474)\n",
            "     | > ga_loss: 0.00190  (0.00220)\n",
            "     | > decoder_diff_spec_loss: 0.31377  (0.29980)\n",
            "     | > postnet_diff_spec_loss: 0.31397  (0.30060)\n",
            "     | > decoder_ssim_loss: 0.35499  (0.33755)\n",
            "     | > postnet_ssim_loss: 0.33334  (0.31906)\n",
            "     | > loss: 1.23101  (1.24039)\n",
            "     | > align_error: 0.37296  (0.38585)\n",
            "     | > grad_norm: 11.76057  (14.22557)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.65420  (0.67899)\n",
            "     | > loader_time: 0.00480  (0.01137)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 28/361 -- GLOBAL_STEP: 20300\u001b[0m\n",
            "     | > decoder_loss: 0.66805  (0.66936)\n",
            "     | > postnet_loss: 0.59649  (0.60363)\n",
            "     | > stopnet_loss: 0.22427  (0.27675)\n",
            "     | > decoder_coarse_loss: 0.87208  (0.86506)\n",
            "     | > decoder_ddc_loss: 0.01726  (0.01923)\n",
            "     | > ga_loss: 0.00076  (0.00119)\n",
            "     | > decoder_diff_spec_loss: 0.28959  (0.29336)\n",
            "     | > postnet_diff_spec_loss: 0.28365  (0.29859)\n",
            "     | > decoder_ssim_loss: 0.34680  (0.33936)\n",
            "     | > postnet_ssim_loss: 0.32785  (0.32146)\n",
            "     | > loss: 1.07849  (1.13520)\n",
            "     | > align_error: 0.37014  (0.37777)\n",
            "     | > grad_norm: 2.48521  (3.82485)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.00230  (0.90932)\n",
            "     | > loader_time: 0.00660  (0.00638)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 53/361 -- GLOBAL_STEP: 20325\u001b[0m\n",
            "     | > decoder_loss: 0.69259  (0.67272)\n",
            "     | > postnet_loss: 0.61724  (0.60491)\n",
            "     | > stopnet_loss: 0.18973  (0.25929)\n",
            "     | > decoder_coarse_loss: 0.88407  (0.87014)\n",
            "     | > decoder_ddc_loss: 0.01475  (0.01732)\n",
            "     | > ga_loss: 0.00053  (0.00092)\n",
            "     | > decoder_diff_spec_loss: 0.30077  (0.29619)\n",
            "     | > postnet_diff_spec_loss: 0.30087  (0.29918)\n",
            "     | > decoder_ssim_loss: 0.35469  (0.33734)\n",
            "     | > postnet_ssim_loss: 0.33403  (0.31913)\n",
            "     | > loss: 1.06715  (1.11810)\n",
            "     | > align_error: 0.37480  (0.37235)\n",
            "     | > grad_norm: 0.73517  (2.84757)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.20960  (0.98099)\n",
            "     | > loader_time: 0.00540  (0.00615)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 78/361 -- GLOBAL_STEP: 20350\u001b[0m\n",
            "     | > decoder_loss: 0.62744  (0.67353)\n",
            "     | > postnet_loss: 0.56226  (0.60394)\n",
            "     | > stopnet_loss: 0.23904  (0.24842)\n",
            "     | > decoder_coarse_loss: 0.81254  (0.87162)\n",
            "     | > decoder_ddc_loss: 0.01238  (0.01607)\n",
            "     | > ga_loss: 0.00042  (0.00077)\n",
            "     | > decoder_diff_spec_loss: 0.28459  (0.29781)\n",
            "     | > postnet_diff_spec_loss: 0.27796  (0.29871)\n",
            "     | > decoder_ssim_loss: 0.32844  (0.33673)\n",
            "     | > postnet_ssim_loss: 0.31035  (0.31802)\n",
            "     | > loss: 1.04512  (1.10640)\n",
            "     | > align_error: 0.36649  (0.37050)\n",
            "     | > grad_norm: 1.83493  (2.37000)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.19420  (1.02926)\n",
            "     | > loader_time: 0.00660  (0.00606)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 103/361 -- GLOBAL_STEP: 20375\u001b[0m\n",
            "     | > decoder_loss: 0.72296  (0.67351)\n",
            "     | > postnet_loss: 0.63499  (0.60321)\n",
            "     | > stopnet_loss: 0.17637  (0.23999)\n",
            "     | > decoder_coarse_loss: 0.92125  (0.86999)\n",
            "     | > decoder_ddc_loss: 0.01227  (0.01521)\n",
            "     | > ga_loss: 0.00037  (0.00068)\n",
            "     | > decoder_diff_spec_loss: 0.31374  (0.29846)\n",
            "     | > postnet_diff_spec_loss: 0.31265  (0.29902)\n",
            "     | > decoder_ssim_loss: 0.35543  (0.33724)\n",
            "     | > postnet_ssim_loss: 0.33195  (0.31822)\n",
            "     | > loss: 1.07952  (1.09710)\n",
            "     | > align_error: 0.36698  (0.36983)\n",
            "     | > grad_norm: 0.77019  (2.12966)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.26540  (1.07296)\n",
            "     | > loader_time: 0.00590  (0.00601)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 128/361 -- GLOBAL_STEP: 20400\u001b[0m\n",
            "     | > decoder_loss: 0.66832  (0.67510)\n",
            "     | > postnet_loss: 0.59713  (0.60360)\n",
            "     | > stopnet_loss: 0.20883  (0.23276)\n",
            "     | > decoder_coarse_loss: 0.87755  (0.87121)\n",
            "     | > decoder_ddc_loss: 0.01085  (0.01451)\n",
            "     | > ga_loss: 0.00035  (0.00061)\n",
            "     | > decoder_diff_spec_loss: 0.29329  (0.29971)\n",
            "     | > postnet_diff_spec_loss: 0.28788  (0.29899)\n",
            "     | > decoder_ssim_loss: 0.32220  (0.33736)\n",
            "     | > postnet_ssim_loss: 0.30351  (0.31796)\n",
            "     | > loss: 1.05075  (1.09043)\n",
            "     | > align_error: 0.36051  (0.36932)\n",
            "     | > grad_norm: 1.13981  (1.97501)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.22130  (1.11098)\n",
            "     | > loader_time: 0.00490  (0.00596)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 153/361 -- GLOBAL_STEP: 20425\u001b[0m\n",
            "     | > decoder_loss: 0.68515  (0.67509)\n",
            "     | > postnet_loss: 0.60476  (0.60288)\n",
            "     | > stopnet_loss: 0.17068  (0.22668)\n",
            "     | > decoder_coarse_loss: 0.87236  (0.87171)\n",
            "     | > decoder_ddc_loss: 0.01030  (0.01393)\n",
            "     | > ga_loss: 0.00027  (0.00056)\n",
            "     | > decoder_diff_spec_loss: 0.30941  (0.30027)\n",
            "     | > postnet_diff_spec_loss: 0.30282  (0.29909)\n",
            "     | > decoder_ssim_loss: 0.34617  (0.33790)\n",
            "     | > postnet_ssim_loss: 0.32420  (0.31822)\n",
            "     | > loss: 1.03581  (1.08424)\n",
            "     | > align_error: 0.37292  (0.36925)\n",
            "     | > grad_norm: 1.07955  (1.83772)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.39590  (1.14137)\n",
            "     | > loader_time: 0.00540  (0.00599)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 178/361 -- GLOBAL_STEP: 20450\u001b[0m\n",
            "     | > decoder_loss: 0.67613  (0.67483)\n",
            "     | > postnet_loss: 0.59671  (0.60215)\n",
            "     | > stopnet_loss: 0.15250  (0.22074)\n",
            "     | > decoder_coarse_loss: 0.85321  (0.87143)\n",
            "     | > decoder_ddc_loss: 0.00967  (0.01339)\n",
            "     | > ga_loss: 0.00023  (0.00051)\n",
            "     | > decoder_diff_spec_loss: 0.30305  (0.30068)\n",
            "     | > postnet_diff_spec_loss: 0.30535  (0.29911)\n",
            "     | > decoder_ssim_loss: 0.35486  (0.33842)\n",
            "     | > postnet_ssim_loss: 0.33184  (0.31853)\n",
            "     | > loss: 1.01134  (1.07794)\n",
            "     | > align_error: 0.36780  (0.36936)\n",
            "     | > grad_norm: 0.94550  (1.72592)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.35860  (1.17673)\n",
            "     | > loader_time: 0.00680  (0.00603)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 203/361 -- GLOBAL_STEP: 20475\u001b[0m\n",
            "     | > decoder_loss: 0.66423  (0.67506)\n",
            "     | > postnet_loss: 0.59122  (0.60211)\n",
            "     | > stopnet_loss: 0.18896  (0.21606)\n",
            "     | > decoder_coarse_loss: 0.87599  (0.87208)\n",
            "     | > decoder_ddc_loss: 0.00955  (0.01293)\n",
            "     | > ga_loss: 0.00020  (0.00048)\n",
            "     | > decoder_diff_spec_loss: 0.29833  (0.30115)\n",
            "     | > postnet_diff_spec_loss: 0.28907  (0.29949)\n",
            "     | > decoder_ssim_loss: 0.33518  (0.33892)\n",
            "     | > postnet_ssim_loss: 0.31476  (0.31891)\n",
            "     | > loss: 1.03456  (1.07361)\n",
            "     | > align_error: 0.36692  (0.36981)\n",
            "     | > grad_norm: 1.16570  (1.63782)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.56230  (1.21104)\n",
            "     | > loader_time: 0.00690  (0.00610)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 228/361 -- GLOBAL_STEP: 20500\u001b[0m\n",
            "     | > decoder_loss: 0.67145  (0.67585)\n",
            "     | > postnet_loss: 0.59071  (0.60241)\n",
            "     | > stopnet_loss: 0.14071  (0.21419)\n",
            "     | > decoder_coarse_loss: 0.88096  (0.87379)\n",
            "     | > decoder_ddc_loss: 0.00907  (0.01250)\n",
            "     | > ga_loss: 0.00016  (0.00045)\n",
            "     | > decoder_diff_spec_loss: 0.30618  (0.30178)\n",
            "     | > postnet_diff_spec_loss: 0.31005  (0.29998)\n",
            "     | > decoder_ssim_loss: 0.35201  (0.33850)\n",
            "     | > postnet_ssim_loss: 0.32838  (0.31833)\n",
            "     | > loss: 1.00370  (1.07221)\n",
            "     | > align_error: 0.37492  (0.37048)\n",
            "     | > grad_norm: 0.54023  (1.57357)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.51750  (1.25157)\n",
            "     | > loader_time: 0.00680  (0.00616)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 253/361 -- GLOBAL_STEP: 20525\u001b[0m\n",
            "     | > decoder_loss: 0.68151  (0.67732)\n",
            "     | > postnet_loss: 0.60422  (0.60320)\n",
            "     | > stopnet_loss: 0.13501  (0.21106)\n",
            "     | > decoder_coarse_loss: 0.87470  (0.87583)\n",
            "     | > decoder_ddc_loss: 0.00823  (0.01209)\n",
            "     | > ga_loss: 0.00016  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.30816  (0.30259)\n",
            "     | > postnet_diff_spec_loss: 0.31830  (0.30038)\n",
            "     | > decoder_ssim_loss: 0.36308  (0.33862)\n",
            "     | > postnet_ssim_loss: 0.34042  (0.31826)\n",
            "     | > loss: 1.01049  (1.07023)\n",
            "     | > align_error: 0.38559  (0.37130)\n",
            "     | > grad_norm: 1.01441  (1.54699)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.72740  (1.29060)\n",
            "     | > loader_time: 0.00710  (0.00620)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 278/361 -- GLOBAL_STEP: 20550\u001b[0m\n",
            "     | > decoder_loss: 0.67103  (0.67874)\n",
            "     | > postnet_loss: 0.59008  (0.60421)\n",
            "     | > stopnet_loss: 0.15041  (0.20844)\n",
            "     | > decoder_coarse_loss: 0.86679  (0.87788)\n",
            "     | > decoder_ddc_loss: 0.00785  (0.01172)\n",
            "     | > ga_loss: 0.00015  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.30470  (0.30317)\n",
            "     | > postnet_diff_spec_loss: 0.29925  (0.30073)\n",
            "     | > decoder_ssim_loss: 0.34307  (0.33875)\n",
            "     | > postnet_ssim_loss: 0.31994  (0.31821)\n",
            "     | > loss: 1.00184  (1.06877)\n",
            "     | > align_error: 0.38170  (0.37144)\n",
            "     | > grad_norm: 1.03696  (1.51620)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.77990  (1.33186)\n",
            "     | > loader_time: 0.00700  (0.00624)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 303/361 -- GLOBAL_STEP: 20575\u001b[0m\n",
            "     | > decoder_loss: 0.71843  (0.68017)\n",
            "     | > postnet_loss: 0.67459  (0.60517)\n",
            "     | > stopnet_loss: 0.35185  (0.20566)\n",
            "     | > decoder_coarse_loss: 0.92888  (0.88018)\n",
            "     | > decoder_ddc_loss: 0.00687  (0.01137)\n",
            "     | > ga_loss: 0.00013  (0.00037)\n",
            "     | > decoder_diff_spec_loss: 0.31052  (0.30373)\n",
            "     | > postnet_diff_spec_loss: 0.32183  (0.30125)\n",
            "     | > decoder_ssim_loss: 0.28929  (0.33908)\n",
            "     | > postnet_ssim_loss: 0.27894  (0.31841)\n",
            "     | > loss: 1.23483  (1.06738)\n",
            "     | > align_error: 0.38360  (0.37249)\n",
            "     | > grad_norm: 1.10018  (1.46774)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.17850  (1.37442)\n",
            "     | > loader_time: 0.00800  (0.00630)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 328/361 -- GLOBAL_STEP: 20600\u001b[0m\n",
            "     | > decoder_loss: 0.70084  (0.68180)\n",
            "     | > postnet_loss: 0.61726  (0.60599)\n",
            "     | > stopnet_loss: 0.17411  (0.20307)\n",
            "     | > decoder_coarse_loss: 0.89083  (0.88269)\n",
            "     | > decoder_ddc_loss: 0.00613  (0.01103)\n",
            "     | > ga_loss: 0.00012  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.30838  (0.30453)\n",
            "     | > postnet_diff_spec_loss: 0.29894  (0.30173)\n",
            "     | > decoder_ssim_loss: 0.34058  (0.33929)\n",
            "     | > postnet_ssim_loss: 0.31834  (0.31841)\n",
            "     | > loss: 1.04501  (1.06622)\n",
            "     | > align_error: 0.38799  (0.37344)\n",
            "     | > grad_norm: 0.60213  (1.42469)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.23560  (1.41791)\n",
            "     | > loader_time: 0.00760  (0.00636)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 353/361 -- GLOBAL_STEP: 20625\u001b[0m\n",
            "     | > decoder_loss: 0.76865  (0.68410)\n",
            "     | > postnet_loss: 0.67380  (0.60763)\n",
            "     | > stopnet_loss: 0.22586  (0.20223)\n",
            "     | > decoder_coarse_loss: 1.01500  (0.88624)\n",
            "     | > decoder_ddc_loss: 0.00542  (0.01068)\n",
            "     | > ga_loss: 0.00011  (0.00034)\n",
            "     | > decoder_diff_spec_loss: 0.33783  (0.30534)\n",
            "     | > postnet_diff_spec_loss: 0.32603  (0.30252)\n",
            "     | > decoder_ssim_loss: 0.32293  (0.33914)\n",
            "     | > postnet_ssim_loss: 0.30149  (0.31815)\n",
            "     | > loss: 1.16421  (1.06738)\n",
            "     | > align_error: 0.38995  (0.37437)\n",
            "     | > grad_norm: 0.49594  (1.38961)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.21960  (1.47285)\n",
            "     | > loader_time: 0.00750  (0.00645)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.65056  (0.65056)\n",
            "     | > postnet_loss: 0.56353  (0.56353)\n",
            "     | > stopnet_loss: 0.28056  (0.28056)\n",
            "     | > decoder_coarse_loss: 0.81904  (0.81904)\n",
            "     | > decoder_ddc_loss: 0.01255  (0.01255)\n",
            "     | > ga_loss: 0.00098  (0.00098)\n",
            "     | > decoder_diff_spec_loss: 0.25427  (0.25427)\n",
            "     | > postnet_diff_spec_loss: 0.21553  (0.21553)\n",
            "     | > decoder_ssim_loss: 0.29890  (0.29890)\n",
            "     | > postnet_ssim_loss: 0.27403  (0.27403)\n",
            "     | > loss: 1.05756  (1.05756)\n",
            "     | > align_error: 0.47621  (0.47621)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.61057  (0.61057)\n",
            "     | > postnet_loss: 0.51920  (0.51920)\n",
            "     | > stopnet_loss: 0.14686  (0.14686)\n",
            "     | > decoder_coarse_loss: 0.74435  (0.74435)\n",
            "     | > decoder_ddc_loss: 0.01240  (0.01240)\n",
            "     | > ga_loss: 0.00041  (0.00041)\n",
            "     | > decoder_diff_spec_loss: 0.27264  (0.27264)\n",
            "     | > postnet_diff_spec_loss: 0.22731  (0.22731)\n",
            "     | > decoder_ssim_loss: 0.33821  (0.33821)\n",
            "     | > postnet_ssim_loss: 0.30600  (0.30600)\n",
            "     | > loss: 0.90657  (0.90657)\n",
            "     | > align_error: 0.36384  (0.36384)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.61896  (0.61477)\n",
            "     | > postnet_loss: 0.53011  (0.52466)\n",
            "     | > stopnet_loss: 0.18383  (0.16534)\n",
            "     | > decoder_coarse_loss: 0.75763  (0.75099)\n",
            "     | > decoder_ddc_loss: 0.01027  (0.01134)\n",
            "     | > ga_loss: 0.00034  (0.00037)\n",
            "     | > decoder_diff_spec_loss: 0.26434  (0.26849)\n",
            "     | > postnet_diff_spec_loss: 0.22069  (0.22400)\n",
            "     | > decoder_ssim_loss: 0.30674  (0.32248)\n",
            "     | > postnet_ssim_loss: 0.27897  (0.29249)\n",
            "     | > loss: 0.93246  (0.91951)\n",
            "     | > align_error: 0.38016  (0.37200)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.64174  (0.62376)\n",
            "     | > postnet_loss: 0.54599  (0.53177)\n",
            "     | > stopnet_loss: 0.16137  (0.16402)\n",
            "     | > decoder_coarse_loss: 0.82021  (0.77406)\n",
            "     | > decoder_ddc_loss: 0.00909  (0.01059)\n",
            "     | > ga_loss: 0.00021  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.27826  (0.27175)\n",
            "     | > postnet_diff_spec_loss: 0.23211  (0.22670)\n",
            "     | > decoder_ssim_loss: 0.32188  (0.32228)\n",
            "     | > postnet_ssim_loss: 0.29064  (0.29187)\n",
            "     | > loss: 0.94741  (0.92881)\n",
            "     | > align_error: 0.37288  (0.37229)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.64115  (0.62811)\n",
            "     | > postnet_loss: 0.53803  (0.53333)\n",
            "     | > stopnet_loss: 0.10716  (0.14980)\n",
            "     | > decoder_coarse_loss: 0.79145  (0.77841)\n",
            "     | > decoder_ddc_loss: 0.00848  (0.01006)\n",
            "     | > ga_loss: 0.00016  (0.00028)\n",
            "     | > decoder_diff_spec_loss: 0.28896  (0.27605)\n",
            "     | > postnet_diff_spec_loss: 0.23877  (0.22972)\n",
            "     | > decoder_ssim_loss: 0.34736  (0.32855)\n",
            "     | > postnet_ssim_loss: 0.31218  (0.29695)\n",
            "     | > loss: 0.89956  (0.92150)\n",
            "     | > align_error: 0.35748  (0.36859)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.65704  (0.63389)\n",
            "     | > postnet_loss: 0.55684  (0.53804)\n",
            "     | > stopnet_loss: 0.18321  (0.15648)\n",
            "     | > decoder_coarse_loss: 0.79429  (0.78158)\n",
            "     | > decoder_ddc_loss: 0.00692  (0.00943)\n",
            "     | > ga_loss: 0.00012  (0.00025)\n",
            "     | > decoder_diff_spec_loss: 0.27950  (0.27674)\n",
            "     | > postnet_diff_spec_loss: 0.23015  (0.22981)\n",
            "     | > decoder_ssim_loss: 0.31288  (0.32542)\n",
            "     | > postnet_ssim_loss: 0.28076  (0.29371)\n",
            "     | > loss: 0.96341  (0.92988)\n",
            "     | > align_error: 0.37052  (0.36898)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.66126  (0.63845)\n",
            "     | > postnet_loss: 0.55880  (0.54150)\n",
            "     | > stopnet_loss: 0.20875  (0.16520)\n",
            "     | > decoder_coarse_loss: 0.80699  (0.78582)\n",
            "     | > decoder_ddc_loss: 0.00527  (0.00874)\n",
            "     | > ga_loss: 0.00009  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.28667  (0.27839)\n",
            "     | > postnet_diff_spec_loss: 0.23546  (0.23075)\n",
            "     | > decoder_ssim_loss: 0.30309  (0.32169)\n",
            "     | > postnet_ssim_loss: 0.27193  (0.29008)\n",
            "     | > loss: 0.99156  (0.94016)\n",
            "     | > align_error: 0.39564  (0.37342)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.69604  (0.64668)\n",
            "     | > postnet_loss: 0.58957  (0.54836)\n",
            "     | > stopnet_loss: 0.07185  (0.15186)\n",
            "     | > decoder_coarse_loss: 0.89598  (0.80156)\n",
            "     | > decoder_ddc_loss: 0.00579  (0.00832)\n",
            "     | > ga_loss: 0.00009  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.29022  (0.28008)\n",
            "     | > postnet_diff_spec_loss: 0.24173  (0.23232)\n",
            "     | > decoder_ssim_loss: 0.34973  (0.32570)\n",
            "     | > postnet_ssim_loss: 0.31533  (0.29369)\n",
            "     | > loss: 0.91838  (0.93705)\n",
            "     | > align_error: 0.36636  (0.37241)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00506 \u001b[0m(+0.00020)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.64668 \u001b[0m(-0.00416)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.54836 \u001b[0m(-0.00145)\n",
            "     | > avg_stopnet_loss:\u001b[91m 0.15186 \u001b[0m(+0.00021)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.80156 \u001b[0m(-0.01485)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00832 \u001b[0m(-0.00035)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00020 \u001b[0m(-0.00001)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.28008 \u001b[0m(-0.00231)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.23232 \u001b[0m(-0.00145)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.32570 \u001b[0m(-0.00190)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.29369 \u001b[0m(-0.00182)\n",
            "     | > avg_loss:\u001b[92m 0.93705 \u001b[0m(-0.00692)\n",
            "     | > avg_align_error:\u001b[92m 0.37241 \u001b[0m(-0.01688)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_20634.pth.tar\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 57/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 04:13:34) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/361 -- GLOBAL_STEP: 20650\u001b[0m\n",
            "     | > decoder_loss: 0.63165  (0.66252)\n",
            "     | > postnet_loss: 0.56858  (0.60229)\n",
            "     | > stopnet_loss: 0.25535  (0.30415)\n",
            "     | > decoder_coarse_loss: 0.83384  (0.84635)\n",
            "     | > decoder_ddc_loss: 0.01797  (0.02013)\n",
            "     | > ga_loss: 0.00092  (0.00144)\n",
            "     | > decoder_diff_spec_loss: 0.27921  (0.29180)\n",
            "     | > postnet_diff_spec_loss: 0.27730  (0.30249)\n",
            "     | > decoder_ssim_loss: 0.34155  (0.33636)\n",
            "     | > postnet_ssim_loss: 0.32378  (0.31944)\n",
            "     | > loss: 1.07840  (1.15669)\n",
            "     | > align_error: 0.37022  (0.37025)\n",
            "     | > grad_norm: 1.15670  (4.50255)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.99440  (0.87667)\n",
            "     | > loader_time: 0.00540  (0.00706)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 41/361 -- GLOBAL_STEP: 20675\u001b[0m\n",
            "     | > decoder_loss: 0.66478  (0.66417)\n",
            "     | > postnet_loss: 0.59459  (0.60002)\n",
            "     | > stopnet_loss: 0.20467  (0.27024)\n",
            "     | > decoder_coarse_loss: 0.86726  (0.85024)\n",
            "     | > decoder_ddc_loss: 0.01542  (0.01769)\n",
            "     | > ga_loss: 0.00054  (0.00100)\n",
            "     | > decoder_diff_spec_loss: 0.28954  (0.29346)\n",
            "     | > postnet_diff_spec_loss: 0.28776  (0.29847)\n",
            "     | > decoder_ssim_loss: 0.33501  (0.33480)\n",
            "     | > postnet_ssim_loss: 0.31548  (0.31733)\n",
            "     | > loss: 1.04983  (1.11930)\n",
            "     | > align_error: 0.36379  (0.36333)\n",
            "     | > grad_norm: 1.51085  (2.89581)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.09030  (0.95698)\n",
            "     | > loader_time: 0.00630  (0.00641)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 66/361 -- GLOBAL_STEP: 20700\u001b[0m\n",
            "     | > decoder_loss: 0.67646  (0.66828)\n",
            "     | > postnet_loss: 0.60105  (0.60116)\n",
            "     | > stopnet_loss: 0.21875  (0.25428)\n",
            "     | > decoder_coarse_loss: 0.84080  (0.85708)\n",
            "     | > decoder_ddc_loss: 0.01339  (0.01626)\n",
            "     | > ga_loss: 0.00046  (0.00082)\n",
            "     | > decoder_diff_spec_loss: 0.29703  (0.29630)\n",
            "     | > postnet_diff_spec_loss: 0.29315  (0.29825)\n",
            "     | > decoder_ssim_loss: 0.34515  (0.33453)\n",
            "     | > postnet_ssim_loss: 0.32388  (0.31642)\n",
            "     | > loss: 1.06876  (1.10544)\n",
            "     | > align_error: 0.35698  (0.35984)\n",
            "     | > grad_norm: 2.06336  (2.32141)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.12080  (1.00060)\n",
            "     | > loader_time: 0.00640  (0.00621)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 91/361 -- GLOBAL_STEP: 20725\u001b[0m\n",
            "     | > decoder_loss: 0.66756  (0.66809)\n",
            "     | > postnet_loss: 0.59526  (0.59954)\n",
            "     | > stopnet_loss: 0.18289  (0.24386)\n",
            "     | > decoder_coarse_loss: 0.88257  (0.85618)\n",
            "     | > decoder_ddc_loss: 0.01203  (0.01526)\n",
            "     | > ga_loss: 0.00038  (0.00070)\n",
            "     | > decoder_diff_spec_loss: 0.29633  (0.29658)\n",
            "     | > postnet_diff_spec_loss: 0.29672  (0.29773)\n",
            "     | > decoder_ssim_loss: 0.34718  (0.33555)\n",
            "     | > postnet_ssim_loss: 0.32672  (0.31702)\n",
            "     | > loss: 1.04086  (1.09387)\n",
            "     | > align_error: 0.35921  (0.36014)\n",
            "     | > grad_norm: 2.08607  (2.05542)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.06950  (1.03628)\n",
            "     | > loader_time: 0.00590  (0.00614)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 116/361 -- GLOBAL_STEP: 20750\u001b[0m\n",
            "     | > decoder_loss: 0.67781  (0.66899)\n",
            "     | > postnet_loss: 0.59428  (0.59930)\n",
            "     | > stopnet_loss: 0.17902  (0.23581)\n",
            "     | > decoder_coarse_loss: 0.87930  (0.85641)\n",
            "     | > decoder_ddc_loss: 0.01135  (0.01452)\n",
            "     | > ga_loss: 0.00033  (0.00063)\n",
            "     | > decoder_diff_spec_loss: 0.31240  (0.29767)\n",
            "     | > postnet_diff_spec_loss: 0.30743  (0.29721)\n",
            "     | > decoder_ssim_loss: 0.33794  (0.33568)\n",
            "     | > postnet_ssim_loss: 0.31573  (0.31675)\n",
            "     | > loss: 1.03973  (1.08557)\n",
            "     | > align_error: 0.35221  (0.35950)\n",
            "     | > grad_norm: 1.82864  (1.97065)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.20980  (1.07436)\n",
            "     | > loader_time: 0.00650  (0.00607)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 141/361 -- GLOBAL_STEP: 20775\u001b[0m\n",
            "     | > decoder_loss: 0.66631  (0.67027)\n",
            "     | > postnet_loss: 0.59215  (0.59948)\n",
            "     | > stopnet_loss: 0.16522  (0.22865)\n",
            "     | > decoder_coarse_loss: 0.85489  (0.85780)\n",
            "     | > decoder_ddc_loss: 0.01127  (0.01393)\n",
            "     | > ga_loss: 0.00026  (0.00057)\n",
            "     | > decoder_diff_spec_loss: 0.29935  (0.29877)\n",
            "     | > postnet_diff_spec_loss: 0.29532  (0.29752)\n",
            "     | > decoder_ssim_loss: 0.34809  (0.33633)\n",
            "     | > postnet_ssim_loss: 0.32647  (0.31703)\n",
            "     | > loss: 1.01498  (1.07927)\n",
            "     | > align_error: 0.35134  (0.35857)\n",
            "     | > grad_norm: 0.68303  (1.83470)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.29120  (1.10424)\n",
            "     | > loader_time: 0.00560  (0.00604)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 166/361 -- GLOBAL_STEP: 20800\u001b[0m\n",
            "     | > decoder_loss: 0.64930  (0.67012)\n",
            "     | > postnet_loss: 0.57607  (0.59892)\n",
            "     | > stopnet_loss: 0.15390  (0.22226)\n",
            "     | > decoder_coarse_loss: 0.83405  (0.85815)\n",
            "     | > decoder_ddc_loss: 0.01062  (0.01338)\n",
            "     | > ga_loss: 0.00024  (0.00052)\n",
            "     | > decoder_diff_spec_loss: 0.29834  (0.29907)\n",
            "     | > postnet_diff_spec_loss: 0.29547  (0.29760)\n",
            "     | > decoder_ssim_loss: 0.34835  (0.33691)\n",
            "     | > postnet_ssim_loss: 0.32648  (0.31740)\n",
            "     | > loss: 0.98977  (1.07274)\n",
            "     | > align_error: 0.35604  (0.35872)\n",
            "     | > grad_norm: 0.72965  (1.70052)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.28550  (1.13979)\n",
            "     | > loader_time: 0.00670  (0.00605)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 191/361 -- GLOBAL_STEP: 20825\u001b[0m\n",
            "     | > decoder_loss: 0.67766  (0.67011)\n",
            "     | > postnet_loss: 0.59838  (0.59838)\n",
            "     | > stopnet_loss: 0.13267  (0.21768)\n",
            "     | > decoder_coarse_loss: 0.88632  (0.85874)\n",
            "     | > decoder_ddc_loss: 0.00939  (0.01290)\n",
            "     | > ga_loss: 0.00023  (0.00048)\n",
            "     | > decoder_diff_spec_loss: 0.30947  (0.29958)\n",
            "     | > postnet_diff_spec_loss: 0.30874  (0.29819)\n",
            "     | > decoder_ssim_loss: 0.35761  (0.33722)\n",
            "     | > postnet_ssim_loss: 0.33458  (0.31752)\n",
            "     | > loss: 1.00435  (1.06825)\n",
            "     | > align_error: 0.36299  (0.35920)\n",
            "     | > grad_norm: 0.91675  (1.62149)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.37600  (1.17860)\n",
            "     | > loader_time: 0.00590  (0.00608)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 216/361 -- GLOBAL_STEP: 20850\u001b[0m\n",
            "     | > decoder_loss: 0.65947  (0.67049)\n",
            "     | > postnet_loss: 0.58639  (0.59830)\n",
            "     | > stopnet_loss: 0.20734  (0.21386)\n",
            "     | > decoder_coarse_loss: 0.85794  (0.86001)\n",
            "     | > decoder_ddc_loss: 0.00835  (0.01247)\n",
            "     | > ga_loss: 0.00019  (0.00045)\n",
            "     | > decoder_diff_spec_loss: 0.30021  (0.30006)\n",
            "     | > postnet_diff_spec_loss: 0.29205  (0.29804)\n",
            "     | > decoder_ssim_loss: 0.32452  (0.33722)\n",
            "     | > postnet_ssim_loss: 0.30383  (0.31737)\n",
            "     | > loss: 1.04147  (1.06459)\n",
            "     | > align_error: 0.36532  (0.36011)\n",
            "     | > grad_norm: 1.24461  (1.54842)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.62550  (1.21403)\n",
            "     | > loader_time: 0.00570  (0.00613)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 241/361 -- GLOBAL_STEP: 20875\u001b[0m\n",
            "     | > decoder_loss: 0.67801  (0.67194)\n",
            "     | > postnet_loss: 0.60167  (0.59921)\n",
            "     | > stopnet_loss: 0.20455  (0.21206)\n",
            "     | > decoder_coarse_loss: 0.89085  (0.86248)\n",
            "     | > decoder_ddc_loss: 0.00816  (0.01206)\n",
            "     | > ga_loss: 0.00018  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.30765  (0.30078)\n",
            "     | > postnet_diff_spec_loss: 0.29622  (0.29868)\n",
            "     | > decoder_ssim_loss: 0.33321  (0.33710)\n",
            "     | > postnet_ssim_loss: 0.31154  (0.31708)\n",
            "     | > loss: 1.06228  (1.06400)\n",
            "     | > align_error: 0.37517  (0.36097)\n",
            "     | > grad_norm: 1.12102  (1.52436)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.60910  (1.25772)\n",
            "     | > loader_time: 0.00600  (0.00618)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 266/361 -- GLOBAL_STEP: 20900\u001b[0m\n",
            "     | > decoder_loss: 0.70094  (0.67363)\n",
            "     | > postnet_loss: 0.63146  (0.60036)\n",
            "     | > stopnet_loss: 0.15339  (0.20950)\n",
            "     | > decoder_coarse_loss: 0.86784  (0.86493)\n",
            "     | > decoder_ddc_loss: 0.00762  (0.01168)\n",
            "     | > ga_loss: 0.00016  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.30328  (0.30148)\n",
            "     | > postnet_diff_spec_loss: 0.31038  (0.29906)\n",
            "     | > decoder_ssim_loss: 0.35108  (0.33713)\n",
            "     | > postnet_ssim_loss: 0.32724  (0.31691)\n",
            "     | > loss: 1.02913  (1.06277)\n",
            "     | > align_error: 0.36162  (0.36135)\n",
            "     | > grad_norm: 1.87099  (1.50501)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.66750  (1.30215)\n",
            "     | > loader_time: 0.00730  (0.00625)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 291/361 -- GLOBAL_STEP: 20925\u001b[0m\n",
            "     | > decoder_loss: 0.71957  (0.67516)\n",
            "     | > postnet_loss: 0.63183  (0.60129)\n",
            "     | > stopnet_loss: 0.22216  (0.20665)\n",
            "     | > decoder_coarse_loss: 0.92731  (0.86699)\n",
            "     | > decoder_ddc_loss: 0.00741  (0.01134)\n",
            "     | > ga_loss: 0.00013  (0.00037)\n",
            "     | > decoder_diff_spec_loss: 0.32077  (0.30209)\n",
            "     | > postnet_diff_spec_loss: 0.30735  (0.29944)\n",
            "     | > decoder_ssim_loss: 0.32747  (0.33739)\n",
            "     | > postnet_ssim_loss: 0.30575  (0.31699)\n",
            "     | > loss: 1.10968  (1.06119)\n",
            "     | > align_error: 0.37188  (0.36200)\n",
            "     | > grad_norm: 0.85479  (1.47402)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.02160  (1.34687)\n",
            "     | > loader_time: 0.00720  (0.00632)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 316/361 -- GLOBAL_STEP: 20950\u001b[0m\n",
            "     | > decoder_loss: 0.73708  (0.67658)\n",
            "     | > postnet_loss: 0.64624  (0.60210)\n",
            "     | > stopnet_loss: 0.21726  (0.20375)\n",
            "     | > decoder_coarse_loss: 0.93997  (0.87002)\n",
            "     | > decoder_ddc_loss: 0.00687  (0.01103)\n",
            "     | > ga_loss: 0.00012  (0.00035)\n",
            "     | > decoder_diff_spec_loss: 0.32573  (0.30274)\n",
            "     | > postnet_diff_spec_loss: 0.31392  (0.29998)\n",
            "     | > decoder_ssim_loss: 0.33375  (0.33767)\n",
            "     | > postnet_ssim_loss: 0.31112  (0.31711)\n",
            "     | > loss: 1.12154  (1.05982)\n",
            "     | > align_error: 0.36251  (0.36266)\n",
            "     | > grad_norm: 1.02501  (1.43785)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.88050  (1.38808)\n",
            "     | > loader_time: 0.00720  (0.00641)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 341/361 -- GLOBAL_STEP: 20975\u001b[0m\n",
            "     | > decoder_loss: 0.69701  (0.67832)\n",
            "     | > postnet_loss: 0.61715  (0.60308)\n",
            "     | > stopnet_loss: 0.12909  (0.20164)\n",
            "     | > decoder_coarse_loss: 0.90390  (0.87318)\n",
            "     | > decoder_ddc_loss: 0.00658  (0.01070)\n",
            "     | > ga_loss: 0.00011  (0.00034)\n",
            "     | > decoder_diff_spec_loss: 0.31202  (0.30349)\n",
            "     | > postnet_diff_spec_loss: 0.32126  (0.30050)\n",
            "     | > decoder_ssim_loss: 0.35810  (0.33785)\n",
            "     | > postnet_ssim_loss: 0.33341  (0.31710)\n",
            "     | > loss: 1.01701  (1.05938)\n",
            "     | > align_error: 0.37591  (0.36367)\n",
            "     | > grad_norm: 1.15981  (1.40237)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.94950  (1.43462)\n",
            "     | > loader_time: 0.00670  (0.00644)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.64949  (0.64949)\n",
            "     | > postnet_loss: 0.57511  (0.57511)\n",
            "     | > stopnet_loss: 0.27991  (0.27991)\n",
            "     | > decoder_coarse_loss: 0.79767  (0.79767)\n",
            "     | > decoder_ddc_loss: 0.01211  (0.01211)\n",
            "     | > ga_loss: 0.00095  (0.00095)\n",
            "     | > decoder_diff_spec_loss: 0.25399  (0.25399)\n",
            "     | > postnet_diff_spec_loss: 0.21572  (0.21572)\n",
            "     | > decoder_ssim_loss: 0.29769  (0.29769)\n",
            "     | > postnet_ssim_loss: 0.27366  (0.27366)\n",
            "     | > loss: 1.05352  (1.05352)\n",
            "     | > align_error: 0.46914  (0.46914)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.60891  (0.60891)\n",
            "     | > postnet_loss: 0.52909  (0.52909)\n",
            "     | > stopnet_loss: 0.14611  (0.14611)\n",
            "     | > decoder_coarse_loss: 0.73635  (0.73635)\n",
            "     | > decoder_ddc_loss: 0.01217  (0.01217)\n",
            "     | > ga_loss: 0.00040  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.27203  (0.27203)\n",
            "     | > postnet_diff_spec_loss: 0.22735  (0.22735)\n",
            "     | > decoder_ssim_loss: 0.33710  (0.33710)\n",
            "     | > postnet_ssim_loss: 0.30589  (0.30589)\n",
            "     | > loss: 0.90531  (0.90531)\n",
            "     | > align_error: 0.35908  (0.35908)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.61714  (0.61302)\n",
            "     | > postnet_loss: 0.54247  (0.53578)\n",
            "     | > stopnet_loss: 0.18438  (0.16524)\n",
            "     | > decoder_coarse_loss: 0.75051  (0.74343)\n",
            "     | > decoder_ddc_loss: 0.01021  (0.01119)\n",
            "     | > ga_loss: 0.00033  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.26345  (0.26774)\n",
            "     | > postnet_diff_spec_loss: 0.22038  (0.22387)\n",
            "     | > decoder_ssim_loss: 0.30546  (0.32128)\n",
            "     | > postnet_ssim_loss: 0.27870  (0.29229)\n",
            "     | > loss: 0.93310  (0.91920)\n",
            "     | > align_error: 0.37428  (0.36668)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.64160  (0.62255)\n",
            "     | > postnet_loss: 0.55351  (0.54169)\n",
            "     | > stopnet_loss: 0.16253  (0.16434)\n",
            "     | > decoder_coarse_loss: 0.80859  (0.76515)\n",
            "     | > decoder_ddc_loss: 0.00892  (0.01043)\n",
            "     | > ga_loss: 0.00020  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.27813  (0.27120)\n",
            "     | > postnet_diff_spec_loss: 0.23244  (0.22672)\n",
            "     | > decoder_ssim_loss: 0.32106  (0.32121)\n",
            "     | > postnet_ssim_loss: 0.29082  (0.29180)\n",
            "     | > loss: 0.94732  (0.92857)\n",
            "     | > align_error: 0.36593  (0.36643)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.64124  (0.62722)\n",
            "     | > postnet_loss: 0.55135  (0.54411)\n",
            "     | > stopnet_loss: 0.10675  (0.14994)\n",
            "     | > decoder_coarse_loss: 0.78231  (0.76944)\n",
            "     | > decoder_ddc_loss: 0.00836  (0.00991)\n",
            "     | > ga_loss: 0.00016  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.28832  (0.27548)\n",
            "     | > postnet_diff_spec_loss: 0.23901  (0.22980)\n",
            "     | > decoder_ssim_loss: 0.34641  (0.32751)\n",
            "     | > postnet_ssim_loss: 0.31260  (0.29700)\n",
            "     | > loss: 0.89993  (0.92141)\n",
            "     | > align_error: 0.35242  (0.36293)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.65814  (0.63340)\n",
            "     | > postnet_loss: 0.57129  (0.54954)\n",
            "     | > stopnet_loss: 0.18284  (0.15652)\n",
            "     | > decoder_coarse_loss: 0.78736  (0.77302)\n",
            "     | > decoder_ddc_loss: 0.00676  (0.00928)\n",
            "     | > ga_loss: 0.00012  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.27922  (0.27623)\n",
            "     | > postnet_diff_spec_loss: 0.23045  (0.22993)\n",
            "     | > decoder_ssim_loss: 0.31216  (0.32444)\n",
            "     | > postnet_ssim_loss: 0.28107  (0.29382)\n",
            "     | > loss: 0.96504  (0.93014)\n",
            "     | > align_error: 0.36405  (0.36315)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.66475  (0.63863)\n",
            "     | > postnet_loss: 0.57457  (0.55371)\n",
            "     | > stopnet_loss: 0.20868  (0.16521)\n",
            "     | > decoder_coarse_loss: 0.79661  (0.77696)\n",
            "     | > decoder_ddc_loss: 0.00518  (0.00860)\n",
            "     | > ga_loss: 0.00009  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.28696  (0.27802)\n",
            "     | > postnet_diff_spec_loss: 0.23640  (0.23101)\n",
            "     | > decoder_ssim_loss: 0.30259  (0.32080)\n",
            "     | > postnet_ssim_loss: 0.27265  (0.29029)\n",
            "     | > loss: 0.99404  (0.94079)\n",
            "     | > align_error: 0.39049  (0.36771)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.69532  (0.64673)\n",
            "     | > postnet_loss: 0.59450  (0.55954)\n",
            "     | > stopnet_loss: 0.07219  (0.15193)\n",
            "     | > decoder_coarse_loss: 0.88653  (0.79261)\n",
            "     | > decoder_ddc_loss: 0.00563  (0.00818)\n",
            "     | > ga_loss: 0.00008  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.29045  (0.27979)\n",
            "     | > postnet_diff_spec_loss: 0.24220  (0.23260)\n",
            "     | > decoder_ssim_loss: 0.34906  (0.32483)\n",
            "     | > postnet_ssim_loss: 0.31570  (0.29392)\n",
            "     | > loss: 0.91745  (0.93745)\n",
            "     | > align_error: 0.36138  (0.36680)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00494 \u001b[0m(-0.00013)\n",
            "     | > avg_decoder_loss:\u001b[91m 0.64673 \u001b[0m(+0.00005)\n",
            "     | > avg_postnet_loss:\u001b[91m 0.55954 \u001b[0m(+0.01118)\n",
            "     | > avg_stopnet_loss:\u001b[91m 0.15193 \u001b[0m(+0.00007)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.79261 \u001b[0m(-0.00895)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00818 \u001b[0m(-0.00014)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00020 \u001b[0m(-0.00001)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.27979 \u001b[0m(-0.00029)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[91m 0.23260 \u001b[0m(+0.00029)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.32483 \u001b[0m(-0.00086)\n",
            "     | > avg_postnet_ssim_loss:\u001b[91m 0.29392 \u001b[0m(+0.00023)\n",
            "     | > avg_loss:\u001b[91m 0.93745 \u001b[0m(+0.00041)\n",
            "     | > avg_align_error:\u001b[92m 0.36680 \u001b[0m(-0.00561)\n",
            "\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 58/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 04:23:05) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 4/361 -- GLOBAL_STEP: 21000\u001b[0m\n",
            "     | > decoder_loss: 0.66112  (0.68684)\n",
            "     | > postnet_loss: 0.58765  (0.60937)\n",
            "     | > stopnet_loss: 0.23215  (0.31087)\n",
            "     | > decoder_coarse_loss: 0.82998  (0.87140)\n",
            "     | > decoder_ddc_loss: 0.02140  (0.02292)\n",
            "     | > ga_loss: 0.00156  (0.00197)\n",
            "     | > decoder_diff_spec_loss: 0.28493  (0.29437)\n",
            "     | > postnet_diff_spec_loss: 0.29077  (0.29693)\n",
            "     | > decoder_ssim_loss: 0.35409  (0.33982)\n",
            "     | > postnet_ssim_loss: 0.33373  (0.32130)\n",
            "     | > loss: 1.08087  (1.18145)\n",
            "     | > align_error: 0.35392  (0.36834)\n",
            "     | > grad_norm: 5.39525  (10.61075)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.66100  (0.73303)\n",
            "     | > loader_time: 0.00870  (0.00859)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/checkpoint_21000.pth.tar\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "\u001b[1m   --> STEP: 29/361 -- GLOBAL_STEP: 21025\u001b[0m\n",
            "     | > decoder_loss: 0.66985  (0.65595)\n",
            "     | > postnet_loss: 0.59632  (0.59219)\n",
            "     | > stopnet_loss: 0.15643  (0.26953)\n",
            "     | > decoder_coarse_loss: 0.82629  (0.84134)\n",
            "     | > decoder_ddc_loss: 0.01704  (0.01853)\n",
            "     | > ga_loss: 0.00069  (0.00112)\n",
            "     | > decoder_diff_spec_loss: 0.30403  (0.29049)\n",
            "     | > postnet_diff_spec_loss: 0.30975  (0.29541)\n",
            "     | > decoder_ssim_loss: 0.35938  (0.33677)\n",
            "     | > postnet_ssim_loss: 0.33886  (0.31910)\n",
            "     | > loss: 1.01525  (1.11259)\n",
            "     | > align_error: 0.36248  (0.35640)\n",
            "     | > grad_norm: 1.75218  (3.12501)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.89710  (0.90154)\n",
            "     | > loader_time: 0.00480  (0.00655)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 54/361 -- GLOBAL_STEP: 21050\u001b[0m\n",
            "     | > decoder_loss: 0.68417  (0.66079)\n",
            "     | > postnet_loss: 0.61608  (0.59512)\n",
            "     | > stopnet_loss: 0.27905  (0.25721)\n",
            "     | > decoder_coarse_loss: 0.86105  (0.84932)\n",
            "     | > decoder_ddc_loss: 0.01363  (0.01666)\n",
            "     | > ga_loss: 0.00054  (0.00087)\n",
            "     | > decoder_diff_spec_loss: 0.30850  (0.29359)\n",
            "     | > postnet_diff_spec_loss: 0.30280  (0.29609)\n",
            "     | > decoder_ssim_loss: 0.30726  (0.33358)\n",
            "     | > postnet_ssim_loss: 0.29140  (0.31581)\n",
            "     | > loss: 1.12798  (1.10180)\n",
            "     | > align_error: 0.34567  (0.35318)\n",
            "     | > grad_norm: 1.42085  (2.47904)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.09780  (0.96501)\n",
            "     | > loader_time: 0.00550  (0.00610)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 79/361 -- GLOBAL_STEP: 21075\u001b[0m\n",
            "     | > decoder_loss: 0.60447  (0.66146)\n",
            "     | > postnet_loss: 0.54311  (0.59429)\n",
            "     | > stopnet_loss: 0.21601  (0.24617)\n",
            "     | > decoder_coarse_loss: 0.77485  (0.84962)\n",
            "     | > decoder_ddc_loss: 0.01312  (0.01550)\n",
            "     | > ga_loss: 0.00042  (0.00073)\n",
            "     | > decoder_diff_spec_loss: 0.27527  (0.29476)\n",
            "     | > postnet_diff_spec_loss: 0.26915  (0.29537)\n",
            "     | > decoder_ssim_loss: 0.32648  (0.33349)\n",
            "     | > postnet_ssim_loss: 0.30864  (0.31523)\n",
            "     | > loss: 0.99687  (1.08978)\n",
            "     | > align_error: 0.34947  (0.35299)\n",
            "     | > grad_norm: 0.56209  (2.06569)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.14330  (1.01676)\n",
            "     | > loader_time: 0.00590  (0.00611)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 104/361 -- GLOBAL_STEP: 21100\u001b[0m\n",
            "     | > decoder_loss: 0.65946  (0.66230)\n",
            "     | > postnet_loss: 0.58744  (0.59450)\n",
            "     | > stopnet_loss: 0.19558  (0.23802)\n",
            "     | > decoder_coarse_loss: 0.80932  (0.84810)\n",
            "     | > decoder_ddc_loss: 0.01207  (0.01469)\n",
            "     | > ga_loss: 0.00035  (0.00065)\n",
            "     | > decoder_diff_spec_loss: 0.28827  (0.29553)\n",
            "     | > postnet_diff_spec_loss: 0.28207  (0.29579)\n",
            "     | > decoder_ssim_loss: 0.34724  (0.33414)\n",
            "     | > postnet_ssim_loss: 0.32761  (0.31560)\n",
            "     | > loss: 1.02569  (1.08142)\n",
            "     | > align_error: 0.35389  (0.35241)\n",
            "     | > grad_norm: 3.51342  (1.94636)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.28440  (1.05355)\n",
            "     | > loader_time: 0.00650  (0.00609)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 129/361 -- GLOBAL_STEP: 21125\u001b[0m\n",
            "     | > decoder_loss: 0.66054  (0.66432)\n",
            "     | > postnet_loss: 0.58123  (0.59517)\n",
            "     | > stopnet_loss: 0.17015  (0.23102)\n",
            "     | > decoder_coarse_loss: 0.85001  (0.85023)\n",
            "     | > decoder_ddc_loss: 0.01098  (0.01402)\n",
            "     | > ga_loss: 0.00028  (0.00058)\n",
            "     | > decoder_diff_spec_loss: 0.30083  (0.29684)\n",
            "     | > postnet_diff_spec_loss: 0.30068  (0.29590)\n",
            "     | > decoder_ssim_loss: 0.35090  (0.33425)\n",
            "     | > postnet_ssim_loss: 0.32787  (0.31532)\n",
            "     | > loss: 1.01730  (1.07545)\n",
            "     | > align_error: 0.36301  (0.35161)\n",
            "     | > grad_norm: 0.79620  (1.83333)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.22080  (1.09300)\n",
            "     | > loader_time: 0.00590  (0.00617)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 154/361 -- GLOBAL_STEP: 21150\u001b[0m\n",
            "     | > decoder_loss: 0.66546  (0.66445)\n",
            "     | > postnet_loss: 0.60502  (0.59460)\n",
            "     | > stopnet_loss: 0.12740  (0.22499)\n",
            "     | > decoder_coarse_loss: 0.84733  (0.85055)\n",
            "     | > decoder_ddc_loss: 0.01049  (0.01345)\n",
            "     | > ga_loss: 0.00025  (0.00053)\n",
            "     | > decoder_diff_spec_loss: 0.29463  (0.29754)\n",
            "     | > postnet_diff_spec_loss: 0.31155  (0.29623)\n",
            "     | > decoder_ssim_loss: 0.36529  (0.33500)\n",
            "     | > postnet_ssim_loss: 0.34438  (0.31577)\n",
            "     | > loss: 0.98969  (1.06955)\n",
            "     | > align_error: 0.36965  (0.35544)\n",
            "     | > grad_norm: 1.41099  (1.75048)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.31820  (1.13045)\n",
            "     | > loader_time: 0.00660  (0.00615)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 179/361 -- GLOBAL_STEP: 21175\u001b[0m\n",
            "     | > decoder_loss: 0.69130  (0.66453)\n",
            "     | > postnet_loss: 0.63958  (0.59424)\n",
            "     | > stopnet_loss: 0.44538  (0.22100)\n",
            "     | > decoder_coarse_loss: 0.90498  (0.85135)\n",
            "     | > decoder_ddc_loss: 0.00880  (0.01297)\n",
            "     | > ga_loss: 0.00030  (0.00049)\n",
            "     | > decoder_diff_spec_loss: 0.31134  (0.29806)\n",
            "     | > postnet_diff_spec_loss: 0.33389  (0.29636)\n",
            "     | > decoder_ssim_loss: 0.27241  (0.33493)\n",
            "     | > postnet_ssim_loss: 0.26360  (0.31559)\n",
            "     | > loss: 1.30334  (1.06546)\n",
            "     | > align_error: 0.34527  (0.35505)\n",
            "     | > grad_norm: 2.08549  (1.63721)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.67790  (1.16872)\n",
            "     | > loader_time: 0.00580  (0.00617)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 204/361 -- GLOBAL_STEP: 21200\u001b[0m\n",
            "     | > decoder_loss: 0.65957  (0.66473)\n",
            "     | > postnet_loss: 0.58669  (0.59401)\n",
            "     | > stopnet_loss: 0.15370  (0.21502)\n",
            "     | > decoder_coarse_loss: 0.84615  (0.85126)\n",
            "     | > decoder_ddc_loss: 0.00954  (0.01256)\n",
            "     | > ga_loss: 0.00019  (0.00046)\n",
            "     | > decoder_diff_spec_loss: 0.30247  (0.29852)\n",
            "     | > postnet_diff_spec_loss: 0.30156  (0.29659)\n",
            "     | > decoder_ssim_loss: 0.34771  (0.33587)\n",
            "     | > postnet_ssim_loss: 0.32673  (0.31632)\n",
            "     | > loss: 0.99977  (1.05976)\n",
            "     | > align_error: 0.35114  (0.35454)\n",
            "     | > grad_norm: 1.18340  (1.56777)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.53880  (1.20677)\n",
            "     | > loader_time: 0.00700  (0.00617)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 229/361 -- GLOBAL_STEP: 21225\u001b[0m\n",
            "     | > decoder_loss: 0.67279  (0.66573)\n",
            "     | > postnet_loss: 0.58991  (0.59444)\n",
            "     | > stopnet_loss: 0.16443  (0.21305)\n",
            "     | > decoder_coarse_loss: 0.87406  (0.85279)\n",
            "     | > decoder_ddc_loss: 0.00878  (0.01214)\n",
            "     | > ga_loss: 0.00018  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.31049  (0.29912)\n",
            "     | > postnet_diff_spec_loss: 0.30461  (0.29706)\n",
            "     | > decoder_ssim_loss: 0.33727  (0.33539)\n",
            "     | > postnet_ssim_loss: 0.31421  (0.31568)\n",
            "     | > loss: 1.01839  (1.05827)\n",
            "     | > align_error: 0.35622  (0.35481)\n",
            "     | > grad_norm: 0.83794  (1.49860)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.53200  (1.24689)\n",
            "     | > loader_time: 0.00670  (0.00622)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 254/361 -- GLOBAL_STEP: 21250\u001b[0m\n",
            "     | > decoder_loss: 0.67810  (0.66698)\n",
            "     | > postnet_loss: 0.60077  (0.59514)\n",
            "     | > stopnet_loss: 0.14383  (0.20994)\n",
            "     | > decoder_coarse_loss: 0.89859  (0.85477)\n",
            "     | > decoder_ddc_loss: 0.00792  (0.01176)\n",
            "     | > ga_loss: 0.00016  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.31498  (0.29985)\n",
            "     | > postnet_diff_spec_loss: 0.31329  (0.29741)\n",
            "     | > decoder_ssim_loss: 0.34799  (0.33548)\n",
            "     | > postnet_ssim_loss: 0.32468  (0.31560)\n",
            "     | > loss: 1.01619  (1.05619)\n",
            "     | > align_error: 0.35524  (0.35522)\n",
            "     | > grad_norm: 1.27696  (1.47979)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.61840  (1.28713)\n",
            "     | > loader_time: 0.00640  (0.00624)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 279/361 -- GLOBAL_STEP: 21275\u001b[0m\n",
            "     | > decoder_loss: 0.67163  (0.66862)\n",
            "     | > postnet_loss: 0.59693  (0.59636)\n",
            "     | > stopnet_loss: 0.15195  (0.20749)\n",
            "     | > decoder_coarse_loss: 0.83180  (0.85698)\n",
            "     | > decoder_ddc_loss: 0.00809  (0.01141)\n",
            "     | > ga_loss: 0.00014  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.29273  (0.30033)\n",
            "     | > postnet_diff_spec_loss: 0.28964  (0.29766)\n",
            "     | > decoder_ssim_loss: 0.35233  (0.33560)\n",
            "     | > postnet_ssim_loss: 0.33005  (0.31554)\n",
            "     | > loss: 0.99598  (1.05500)\n",
            "     | > align_error: 0.36614  (0.35520)\n",
            "     | > grad_norm: 0.92737  (1.46534)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.80660  (1.33025)\n",
            "     | > loader_time: 0.00640  (0.00632)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 304/361 -- GLOBAL_STEP: 21300\u001b[0m\n",
            "     | > decoder_loss: 0.70669  (0.67041)\n",
            "     | > postnet_loss: 0.62428  (0.59762)\n",
            "     | > stopnet_loss: 0.16364  (0.20479)\n",
            "     | > decoder_coarse_loss: 0.91852  (0.85968)\n",
            "     | > decoder_ddc_loss: 0.00712  (0.01108)\n",
            "     | > ga_loss: 0.00012  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.31919  (0.30103)\n",
            "     | > postnet_diff_spec_loss: 0.30785  (0.29828)\n",
            "     | > decoder_ssim_loss: 0.35044  (0.33597)\n",
            "     | > postnet_ssim_loss: 0.32784  (0.31576)\n",
            "     | > loss: 1.05473  (1.05404)\n",
            "     | > align_error: 0.36318  (0.35576)\n",
            "     | > grad_norm: 1.06680  (1.43897)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.82960  (1.37217)\n",
            "     | > loader_time: 0.00630  (0.00638)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 329/361 -- GLOBAL_STEP: 21325\u001b[0m\n",
            "     | > decoder_loss: 0.70472  (0.67206)\n",
            "     | > postnet_loss: 0.62079  (0.59849)\n",
            "     | > stopnet_loss: 0.18890  (0.20230)\n",
            "     | > decoder_coarse_loss: 0.92258  (0.86196)\n",
            "     | > decoder_ddc_loss: 0.00658  (0.01075)\n",
            "     | > ga_loss: 0.00012  (0.00034)\n",
            "     | > decoder_diff_spec_loss: 0.31512  (0.30178)\n",
            "     | > postnet_diff_spec_loss: 0.30305  (0.29872)\n",
            "     | > decoder_ssim_loss: 0.33618  (0.33610)\n",
            "     | > postnet_ssim_loss: 0.31364  (0.31570)\n",
            "     | > loss: 1.07018  (1.05289)\n",
            "     | > align_error: 0.36948  (0.35621)\n",
            "     | > grad_norm: 0.92785  (1.39326)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.06580  (1.41431)\n",
            "     | > loader_time: 0.00870  (0.00647)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 354/361 -- GLOBAL_STEP: 21350\u001b[0m\n",
            "     | > decoder_loss: 0.72820  (0.67430)\n",
            "     | > postnet_loss: 0.63472  (0.60002)\n",
            "     | > stopnet_loss: 0.15290  (0.20146)\n",
            "     | > decoder_coarse_loss: 0.93736  (0.86546)\n",
            "     | > decoder_ddc_loss: 0.00525  (0.01042)\n",
            "     | > ga_loss: 0.00010  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.32297  (0.30260)\n",
            "     | > postnet_diff_spec_loss: 0.31637  (0.29952)\n",
            "     | > decoder_ssim_loss: 0.34663  (0.33598)\n",
            "     | > postnet_ssim_loss: 0.32180  (0.31545)\n",
            "     | > loss: 1.05674  (1.05402)\n",
            "     | > align_error: 0.36918  (0.35695)\n",
            "     | > grad_norm: 0.73101  (1.35769)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.13360  (1.47046)\n",
            "     | > loader_time: 0.00660  (0.00654)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.63223  (0.63223)\n",
            "     | > postnet_loss: 0.55017  (0.55017)\n",
            "     | > stopnet_loss: 0.28133  (0.28133)\n",
            "     | > decoder_coarse_loss: 0.78799  (0.78799)\n",
            "     | > decoder_ddc_loss: 0.01243  (0.01243)\n",
            "     | > ga_loss: 0.00093  (0.00093)\n",
            "     | > decoder_diff_spec_loss: 0.25172  (0.25172)\n",
            "     | > postnet_diff_spec_loss: 0.21381  (0.21381)\n",
            "     | > decoder_ssim_loss: 0.29611  (0.29611)\n",
            "     | > postnet_ssim_loss: 0.27188  (0.27188)\n",
            "     | > loss: 1.04007  (1.04007)\n",
            "     | > align_error: 0.45848  (0.45848)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.59973  (0.59973)\n",
            "     | > postnet_loss: 0.51088  (0.51088)\n",
            "     | > stopnet_loss: 0.14788  (0.14788)\n",
            "     | > decoder_coarse_loss: 0.72573  (0.72573)\n",
            "     | > decoder_ddc_loss: 0.01237  (0.01237)\n",
            "     | > ga_loss: 0.00039  (0.00039)\n",
            "     | > decoder_diff_spec_loss: 0.27114  (0.27114)\n",
            "     | > postnet_diff_spec_loss: 0.22614  (0.22614)\n",
            "     | > decoder_ssim_loss: 0.33599  (0.33599)\n",
            "     | > postnet_ssim_loss: 0.30437  (0.30437)\n",
            "     | > loss: 0.89641  (0.89641)\n",
            "     | > align_error: 0.34993  (0.34993)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.60330  (0.60151)\n",
            "     | > postnet_loss: 0.51935  (0.51511)\n",
            "     | > stopnet_loss: 0.18479  (0.16633)\n",
            "     | > decoder_coarse_loss: 0.73818  (0.73195)\n",
            "     | > decoder_ddc_loss: 0.01041  (0.01139)\n",
            "     | > ga_loss: 0.00032  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.26171  (0.26643)\n",
            "     | > postnet_diff_spec_loss: 0.21873  (0.22243)\n",
            "     | > decoder_ssim_loss: 0.30383  (0.31991)\n",
            "     | > postnet_ssim_loss: 0.27674  (0.29056)\n",
            "     | > loss: 0.91947  (0.90794)\n",
            "     | > align_error: 0.36401  (0.35697)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.63080  (0.61128)\n",
            "     | > postnet_loss: 0.53692  (0.52238)\n",
            "     | > stopnet_loss: 0.16278  (0.16515)\n",
            "     | > decoder_coarse_loss: 0.79958  (0.75450)\n",
            "     | > decoder_ddc_loss: 0.00900  (0.01059)\n",
            "     | > ga_loss: 0.00020  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.27604  (0.26963)\n",
            "     | > postnet_diff_spec_loss: 0.23035  (0.22507)\n",
            "     | > decoder_ssim_loss: 0.31912  (0.31965)\n",
            "     | > postnet_ssim_loss: 0.28858  (0.28990)\n",
            "     | > loss: 0.93639  (0.91743)\n",
            "     | > align_error: 0.35679  (0.35691)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.62491  (0.61468)\n",
            "     | > postnet_loss: 0.52715  (0.52358)\n",
            "     | > stopnet_loss: 0.10685  (0.15057)\n",
            "     | > decoder_coarse_loss: 0.76942  (0.75823)\n",
            "     | > decoder_ddc_loss: 0.00852  (0.01008)\n",
            "     | > ga_loss: 0.00015  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.28604  (0.27374)\n",
            "     | > postnet_diff_spec_loss: 0.23684  (0.22802)\n",
            "     | > decoder_ssim_loss: 0.34394  (0.32572)\n",
            "     | > postnet_ssim_loss: 0.30989  (0.29490)\n",
            "     | > loss: 0.88429  (0.90914)\n",
            "     | > align_error: 0.34240  (0.35328)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.64022  (0.61979)\n",
            "     | > postnet_loss: 0.54410  (0.52768)\n",
            "     | > stopnet_loss: 0.18337  (0.15713)\n",
            "     | > decoder_coarse_loss: 0.77387  (0.76136)\n",
            "     | > decoder_ddc_loss: 0.00675  (0.00941)\n",
            "     | > ga_loss: 0.00012  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.27761  (0.27451)\n",
            "     | > postnet_diff_spec_loss: 0.22887  (0.22819)\n",
            "     | > decoder_ssim_loss: 0.31051  (0.32268)\n",
            "     | > postnet_ssim_loss: 0.27923  (0.29176)\n",
            "     | > loss: 0.94924  (0.91716)\n",
            "     | > align_error: 0.35464  (0.35355)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.64797  (0.62449)\n",
            "     | > postnet_loss: 0.54954  (0.53132)\n",
            "     | > stopnet_loss: 0.20906  (0.16579)\n",
            "     | > decoder_coarse_loss: 0.78566  (0.76541)\n",
            "     | > decoder_ddc_loss: 0.00514  (0.00870)\n",
            "     | > ga_loss: 0.00008  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.28450  (0.27617)\n",
            "     | > postnet_diff_spec_loss: 0.23393  (0.22914)\n",
            "     | > decoder_ssim_loss: 0.30042  (0.31897)\n",
            "     | > postnet_ssim_loss: 0.27013  (0.28816)\n",
            "     | > loss: 0.97879  (0.92743)\n",
            "     | > align_error: 0.38148  (0.35821)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.68318  (0.63287)\n",
            "     | > postnet_loss: 0.57931  (0.53818)\n",
            "     | > stopnet_loss: 0.07185  (0.15237)\n",
            "     | > decoder_coarse_loss: 0.87469  (0.78102)\n",
            "     | > decoder_ddc_loss: 0.00570  (0.00827)\n",
            "     | > ga_loss: 0.00008  (0.00019)\n",
            "     | > decoder_diff_spec_loss: 0.28794  (0.27785)\n",
            "     | > postnet_diff_spec_loss: 0.24000  (0.23069)\n",
            "     | > decoder_ssim_loss: 0.34692  (0.32296)\n",
            "     | > postnet_ssim_loss: 0.31333  (0.29175)\n",
            "     | > loss: 0.90501  (0.92423)\n",
            "     | > align_error: 0.35518  (0.35777)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00457 \u001b[0m(-0.00036)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.63287 \u001b[0m(-0.01385)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.53818 \u001b[0m(-0.02136)\n",
            "     | > avg_stopnet_loss:\u001b[91m 0.15237 \u001b[0m(+0.00044)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.78102 \u001b[0m(-0.01159)\n",
            "     | > avg_decoder_ddc_loss:\u001b[91m 0.00827 \u001b[0m(+0.00009)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00019 \u001b[0m(-0.00000)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.27785 \u001b[0m(-0.00194)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.23069 \u001b[0m(-0.00191)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.32296 \u001b[0m(-0.00187)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.29175 \u001b[0m(-0.00217)\n",
            "     | > avg_loss:\u001b[92m 0.92423 \u001b[0m(-0.01322)\n",
            "     | > avg_align_error:\u001b[92m 0.35777 \u001b[0m(-0.00903)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_21358.pth.tar\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 59/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 04:32:47) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 17/361 -- GLOBAL_STEP: 21375\u001b[0m\n",
            "     | > decoder_loss: 0.61289  (0.65111)\n",
            "     | > postnet_loss: 0.55395  (0.59215)\n",
            "     | > stopnet_loss: 0.27222  (0.29873)\n",
            "     | > decoder_coarse_loss: 0.78491  (0.82678)\n",
            "     | > decoder_ddc_loss: 0.01745  (0.01959)\n",
            "     | > ga_loss: 0.00088  (0.00135)\n",
            "     | > decoder_diff_spec_loss: 0.28560  (0.28912)\n",
            "     | > postnet_diff_spec_loss: 0.28025  (0.29784)\n",
            "     | > decoder_ssim_loss: 0.31992  (0.33289)\n",
            "     | > postnet_ssim_loss: 0.30311  (0.31595)\n",
            "     | > loss: 1.06614  (1.13685)\n",
            "     | > align_error: 0.33971  (0.35055)\n",
            "     | > grad_norm: 1.24749  (4.43578)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.03410  (0.87494)\n",
            "     | > loader_time: 0.01540  (0.00733)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 42/361 -- GLOBAL_STEP: 21400\u001b[0m\n",
            "     | > decoder_loss: 0.72241  (0.65421)\n",
            "     | > postnet_loss: 0.64106  (0.59057)\n",
            "     | > stopnet_loss: 0.27926  (0.26712)\n",
            "     | > decoder_coarse_loss: 0.91040  (0.83520)\n",
            "     | > decoder_ddc_loss: 0.01391  (0.01733)\n",
            "     | > ga_loss: 0.00051  (0.00096)\n",
            "     | > decoder_diff_spec_loss: 0.31705  (0.29155)\n",
            "     | > postnet_diff_spec_loss: 0.30819  (0.29539)\n",
            "     | > decoder_ssim_loss: 0.32497  (0.33169)\n",
            "     | > postnet_ssim_loss: 0.30420  (0.31430)\n",
            "     | > loss: 1.16738  (1.10447)\n",
            "     | > align_error: 0.34986  (0.34745)\n",
            "     | > grad_norm: 2.23829  (2.89326)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.02670  (0.95138)\n",
            "     | > loader_time: 0.00470  (0.00648)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 67/361 -- GLOBAL_STEP: 21425\u001b[0m\n",
            "     | > decoder_loss: 0.66789  (0.65729)\n",
            "     | > postnet_loss: 0.59255  (0.59104)\n",
            "     | > stopnet_loss: 0.20421  (0.25139)\n",
            "     | > decoder_coarse_loss: 0.85800  (0.83994)\n",
            "     | > decoder_ddc_loss: 0.01304  (0.01586)\n",
            "     | > ga_loss: 0.00046  (0.00079)\n",
            "     | > decoder_diff_spec_loss: 0.30645  (0.29379)\n",
            "     | > postnet_diff_spec_loss: 0.30103  (0.29509)\n",
            "     | > decoder_ssim_loss: 0.34054  (0.33178)\n",
            "     | > postnet_ssim_loss: 0.31960  (0.31369)\n",
            "     | > loss: 1.05630  (1.08996)\n",
            "     | > align_error: 0.34932  (0.34828)\n",
            "     | > grad_norm: 1.04927  (2.34283)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.16680  (0.99871)\n",
            "     | > loader_time: 0.00520  (0.00612)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 92/361 -- GLOBAL_STEP: 21450\u001b[0m\n",
            "     | > decoder_loss: 0.64564  (0.65701)\n",
            "     | > postnet_loss: 0.57221  (0.58991)\n",
            "     | > stopnet_loss: 0.18193  (0.24204)\n",
            "     | > decoder_coarse_loss: 0.80310  (0.83898)\n",
            "     | > decoder_ddc_loss: 0.01247  (0.01487)\n",
            "     | > ga_loss: 0.00033  (0.00068)\n",
            "     | > decoder_diff_spec_loss: 0.29141  (0.29388)\n",
            "     | > postnet_diff_spec_loss: 0.29127  (0.29444)\n",
            "     | > decoder_ssim_loss: 0.34807  (0.33288)\n",
            "     | > postnet_ssim_loss: 0.32649  (0.31449)\n",
            "     | > loss: 1.00627  (1.07956)\n",
            "     | > align_error: 0.33818  (0.34715)\n",
            "     | > grad_norm: 1.77296  (2.11160)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.14680  (1.03880)\n",
            "     | > loader_time: 0.00630  (0.00603)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 117/361 -- GLOBAL_STEP: 21475\u001b[0m\n",
            "     | > decoder_loss: 0.65701  (0.65889)\n",
            "     | > postnet_loss: 0.58452  (0.59085)\n",
            "     | > stopnet_loss: 0.19184  (0.23401)\n",
            "     | > decoder_coarse_loss: 0.80628  (0.83949)\n",
            "     | > decoder_ddc_loss: 0.01051  (0.01417)\n",
            "     | > ga_loss: 0.00030  (0.00061)\n",
            "     | > decoder_diff_spec_loss: 0.29729  (0.29492)\n",
            "     | > postnet_diff_spec_loss: 0.29059  (0.29395)\n",
            "     | > decoder_ssim_loss: 0.33484  (0.33282)\n",
            "     | > postnet_ssim_loss: 0.31445  (0.31410)\n",
            "     | > loss: 1.01719  (1.07184)\n",
            "     | > align_error: 0.33846  (0.34589)\n",
            "     | > grad_norm: 1.28369  (1.96936)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.27260  (1.07603)\n",
            "     | > loader_time: 0.00650  (0.00599)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 142/361 -- GLOBAL_STEP: 21500\u001b[0m\n",
            "     | > decoder_loss: 0.63960  (0.66007)\n",
            "     | > postnet_loss: 0.57189  (0.59078)\n",
            "     | > stopnet_loss: 0.26686  (0.22774)\n",
            "     | > decoder_coarse_loss: 0.81521  (0.84101)\n",
            "     | > decoder_ddc_loss: 0.01019  (0.01357)\n",
            "     | > ga_loss: 0.00025  (0.00055)\n",
            "     | > decoder_diff_spec_loss: 0.28891  (0.29593)\n",
            "     | > postnet_diff_spec_loss: 0.28101  (0.29422)\n",
            "     | > decoder_ssim_loss: 0.30895  (0.33315)\n",
            "     | > postnet_ssim_loss: 0.29194  (0.31410)\n",
            "     | > loss: 1.07005  (1.06619)\n",
            "     | > align_error: 0.35035  (0.34529)\n",
            "     | > grad_norm: 1.87379  (1.88087)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.40820  (1.11191)\n",
            "     | > loader_time: 0.00640  (0.00596)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 167/361 -- GLOBAL_STEP: 21525\u001b[0m\n",
            "     | > decoder_loss: 0.64691  (0.65954)\n",
            "     | > postnet_loss: 0.57314  (0.58995)\n",
            "     | > stopnet_loss: 0.19377  (0.22122)\n",
            "     | > decoder_coarse_loss: 0.83092  (0.84089)\n",
            "     | > decoder_ddc_loss: 0.00978  (0.01306)\n",
            "     | > ga_loss: 0.00021  (0.00050)\n",
            "     | > decoder_diff_spec_loss: 0.29164  (0.29613)\n",
            "     | > postnet_diff_spec_loss: 0.28292  (0.29424)\n",
            "     | > decoder_ssim_loss: 0.33024  (0.33374)\n",
            "     | > postnet_ssim_loss: 0.31040  (0.31450)\n",
            "     | > loss: 1.01383  (1.05924)\n",
            "     | > align_error: 0.33827  (0.34503)\n",
            "     | > grad_norm: 0.95813  (1.76365)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.40640  (1.14714)\n",
            "     | > loader_time: 0.00570  (0.00600)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 192/361 -- GLOBAL_STEP: 21550\u001b[0m\n",
            "     | > decoder_loss: 0.65974  (0.65951)\n",
            "     | > postnet_loss: 0.58747  (0.58954)\n",
            "     | > stopnet_loss: 0.17314  (0.21681)\n",
            "     | > decoder_coarse_loss: 0.85776  (0.84088)\n",
            "     | > decoder_ddc_loss: 0.00941  (0.01258)\n",
            "     | > ga_loss: 0.00022  (0.00046)\n",
            "     | > decoder_diff_spec_loss: 0.29936  (0.29677)\n",
            "     | > postnet_diff_spec_loss: 0.29032  (0.29494)\n",
            "     | > decoder_ssim_loss: 0.34655  (0.33416)\n",
            "     | > postnet_ssim_loss: 0.32596  (0.31476)\n",
            "     | > loss: 1.01839  (1.05492)\n",
            "     | > align_error: 0.34746  (0.34558)\n",
            "     | > grad_norm: 0.98731  (1.68861)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.46790  (1.18384)\n",
            "     | > loader_time: 0.00590  (0.00602)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 217/361 -- GLOBAL_STEP: 21575\u001b[0m\n",
            "     | > decoder_loss: 0.66205  (0.66009)\n",
            "     | > postnet_loss: 0.60155  (0.58971)\n",
            "     | > stopnet_loss: 0.29643  (0.21368)\n",
            "     | > decoder_coarse_loss: 0.84358  (0.84181)\n",
            "     | > decoder_ddc_loss: 0.00784  (0.01217)\n",
            "     | > ga_loss: 0.00021  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.29473  (0.29722)\n",
            "     | > postnet_diff_spec_loss: 0.29653  (0.29484)\n",
            "     | > decoder_ssim_loss: 0.29524  (0.33390)\n",
            "     | > postnet_ssim_loss: 0.28032  (0.31440)\n",
            "     | > loss: 1.11794  (1.05188)\n",
            "     | > align_error: 0.34701  (0.34585)\n",
            "     | > grad_norm: 0.81469  (1.60967)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.66780  (1.22375)\n",
            "     | > loader_time: 0.00590  (0.00604)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 242/361 -- GLOBAL_STEP: 21600\u001b[0m\n",
            "     | > decoder_loss: 0.66532  (0.66160)\n",
            "     | > postnet_loss: 0.58847  (0.59063)\n",
            "     | > stopnet_loss: 0.18769  (0.21131)\n",
            "     | > decoder_coarse_loss: 0.85630  (0.84446)\n",
            "     | > decoder_ddc_loss: 0.00823  (0.01178)\n",
            "     | > ga_loss: 0.00015  (0.00041)\n",
            "     | > decoder_diff_spec_loss: 0.30589  (0.29801)\n",
            "     | > postnet_diff_spec_loss: 0.29650  (0.29550)\n",
            "     | > decoder_ssim_loss: 0.33207  (0.33397)\n",
            "     | > postnet_ssim_loss: 0.31054  (0.31427)\n",
            "     | > loss: 1.02929  (1.05090)\n",
            "     | > align_error: 0.35669  (0.34631)\n",
            "     | > grad_norm: 1.23047  (1.56268)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.64940  (1.26106)\n",
            "     | > loader_time: 0.00850  (0.00610)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 267/361 -- GLOBAL_STEP: 21625\u001b[0m\n",
            "     | > decoder_loss: 0.68643  (0.66325)\n",
            "     | > postnet_loss: 0.60857  (0.59176)\n",
            "     | > stopnet_loss: 0.14345  (0.20868)\n",
            "     | > decoder_coarse_loss: 0.87399  (0.84685)\n",
            "     | > decoder_ddc_loss: 0.00823  (0.01142)\n",
            "     | > ga_loss: 0.00014  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.30964  (0.29872)\n",
            "     | > postnet_diff_spec_loss: 0.30859  (0.29595)\n",
            "     | > decoder_ssim_loss: 0.34566  (0.33403)\n",
            "     | > postnet_ssim_loss: 0.32192  (0.31414)\n",
            "     | > loss: 1.00991  (1.04962)\n",
            "     | > align_error: 0.34067  (0.34643)\n",
            "     | > grad_norm: 1.10613  (1.53059)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.61600  (1.30311)\n",
            "     | > loader_time: 0.00650  (0.00613)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 292/361 -- GLOBAL_STEP: 21650\u001b[0m\n",
            "     | > decoder_loss: 0.67718  (0.66479)\n",
            "     | > postnet_loss: 0.59680  (0.59274)\n",
            "     | > stopnet_loss: 0.13196  (0.20579)\n",
            "     | > decoder_coarse_loss: 0.86971  (0.84888)\n",
            "     | > decoder_ddc_loss: 0.00760  (0.01108)\n",
            "     | > ga_loss: 0.00011  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.30880  (0.29932)\n",
            "     | > postnet_diff_spec_loss: 0.31077  (0.29634)\n",
            "     | > decoder_ssim_loss: 0.34998  (0.33430)\n",
            "     | > postnet_ssim_loss: 0.32607  (0.31425)\n",
            "     | > loss: 0.99426  (1.04801)\n",
            "     | > align_error: 0.35129  (0.34668)\n",
            "     | > grad_norm: 0.79050  (1.49360)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.67700  (1.34446)\n",
            "     | > loader_time: 0.00670  (0.00621)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 317/361 -- GLOBAL_STEP: 21675\u001b[0m\n",
            "     | > decoder_loss: 0.69870  (0.66613)\n",
            "     | > postnet_loss: 0.61950  (0.59353)\n",
            "     | > stopnet_loss: 0.20551  (0.20322)\n",
            "     | > decoder_coarse_loss: 0.89655  (0.85103)\n",
            "     | > decoder_ddc_loss: 0.00668  (0.01076)\n",
            "     | > ga_loss: 0.00013  (0.00034)\n",
            "     | > decoder_diff_spec_loss: 0.30368  (0.29994)\n",
            "     | > postnet_diff_spec_loss: 0.29440  (0.29682)\n",
            "     | > decoder_ssim_loss: 0.32413  (0.33451)\n",
            "     | > postnet_ssim_loss: 0.30354  (0.31430)\n",
            "     | > loss: 1.06794  (1.04669)\n",
            "     | > align_error: 0.34598  (0.34694)\n",
            "     | > grad_norm: 0.97648  (1.45016)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.98650  (1.38577)\n",
            "     | > loader_time: 0.00710  (0.00627)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 342/361 -- GLOBAL_STEP: 21700\u001b[0m\n",
            "     | > decoder_loss: 0.69555  (0.66813)\n",
            "     | > postnet_loss: 0.63118  (0.59482)\n",
            "     | > stopnet_loss: 0.32157  (0.20142)\n",
            "     | > decoder_coarse_loss: 0.88222  (0.85383)\n",
            "     | > decoder_ddc_loss: 0.00568  (0.01044)\n",
            "     | > ga_loss: 0.00011  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.31593  (0.30074)\n",
            "     | > postnet_diff_spec_loss: 0.32176  (0.29744)\n",
            "     | > decoder_ssim_loss: 0.28541  (0.33461)\n",
            "     | > postnet_ssim_loss: 0.27158  (0.31424)\n",
            "     | > loss: 1.17444  (1.04661)\n",
            "     | > align_error: 0.36285  (0.34776)\n",
            "     | > grad_norm: 0.58342  (1.41007)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.60910  (1.43457)\n",
            "     | > loader_time: 0.00690  (0.00631)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.64391  (0.64391)\n",
            "     | > postnet_loss: 0.55596  (0.55596)\n",
            "     | > stopnet_loss: 0.28167  (0.28167)\n",
            "     | > decoder_coarse_loss: 0.78240  (0.78240)\n",
            "     | > decoder_ddc_loss: 0.01245  (0.01245)\n",
            "     | > ga_loss: 0.00095  (0.00095)\n",
            "     | > decoder_diff_spec_loss: 0.25337  (0.25337)\n",
            "     | > postnet_diff_spec_loss: 0.21388  (0.21388)\n",
            "     | > decoder_ssim_loss: 0.29652  (0.29652)\n",
            "     | > postnet_ssim_loss: 0.27160  (0.27160)\n",
            "     | > loss: 1.04395  (1.04395)\n",
            "     | > align_error: 0.47528  (0.47528)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.60434  (0.60434)\n",
            "     | > postnet_loss: 0.51065  (0.51065)\n",
            "     | > stopnet_loss: 0.14754  (0.14754)\n",
            "     | > decoder_coarse_loss: 0.72106  (0.72106)\n",
            "     | > decoder_ddc_loss: 0.01233  (0.01233)\n",
            "     | > ga_loss: 0.00040  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.27252  (0.27252)\n",
            "     | > postnet_diff_spec_loss: 0.22603  (0.22603)\n",
            "     | > decoder_ssim_loss: 0.33551  (0.33551)\n",
            "     | > postnet_ssim_loss: 0.30302  (0.30302)\n",
            "     | > loss: 0.89588  (0.89588)\n",
            "     | > align_error: 0.36276  (0.36276)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.61223  (0.60829)\n",
            "     | > postnet_loss: 0.52118  (0.51592)\n",
            "     | > stopnet_loss: 0.18339  (0.16546)\n",
            "     | > decoder_coarse_loss: 0.73376  (0.72741)\n",
            "     | > decoder_ddc_loss: 0.01023  (0.01128)\n",
            "     | > ga_loss: 0.00033  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.26311  (0.26782)\n",
            "     | > postnet_diff_spec_loss: 0.21860  (0.22231)\n",
            "     | > decoder_ssim_loss: 0.30342  (0.31946)\n",
            "     | > postnet_ssim_loss: 0.27565  (0.28934)\n",
            "     | > loss: 0.91959  (0.90774)\n",
            "     | > align_error: 0.38361  (0.37319)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.63040  (0.61566)\n",
            "     | > postnet_loss: 0.53338  (0.52174)\n",
            "     | > stopnet_loss: 0.16324  (0.16472)\n",
            "     | > decoder_coarse_loss: 0.79149  (0.74877)\n",
            "     | > decoder_ddc_loss: 0.00882  (0.01046)\n",
            "     | > ga_loss: 0.00021  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.27636  (0.27066)\n",
            "     | > postnet_diff_spec_loss: 0.22977  (0.22480)\n",
            "     | > decoder_ssim_loss: 0.31845  (0.31913)\n",
            "     | > postnet_ssim_loss: 0.28720  (0.28862)\n",
            "     | > loss: 0.93324  (0.91624)\n",
            "     | > align_error: 0.37870  (0.37503)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.63417  (0.62029)\n",
            "     | > postnet_loss: 0.52943  (0.52366)\n",
            "     | > stopnet_loss: 0.10488  (0.14976)\n",
            "     | > decoder_coarse_loss: 0.76644  (0.75319)\n",
            "     | > decoder_ddc_loss: 0.00836  (0.00994)\n",
            "     | > ga_loss: 0.00016  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.28741  (0.27485)\n",
            "     | > postnet_diff_spec_loss: 0.23689  (0.22782)\n",
            "     | > decoder_ssim_loss: 0.34441  (0.32545)\n",
            "     | > postnet_ssim_loss: 0.30925  (0.29378)\n",
            "     | > loss: 0.88477  (0.90837)\n",
            "     | > align_error: 0.35552  (0.37015)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.64564  (0.62536)\n",
            "     | > postnet_loss: 0.54344  (0.52762)\n",
            "     | > stopnet_loss: 0.18341  (0.15649)\n",
            "     | > decoder_coarse_loss: 0.76571  (0.75569)\n",
            "     | > decoder_ddc_loss: 0.00680  (0.00931)\n",
            "     | > ga_loss: 0.00012  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.27716  (0.27531)\n",
            "     | > postnet_diff_spec_loss: 0.22771  (0.22780)\n",
            "     | > decoder_ssim_loss: 0.30984  (0.32233)\n",
            "     | > postnet_ssim_loss: 0.27775  (0.29057)\n",
            "     | > loss: 0.94752  (0.91620)\n",
            "     | > align_error: 0.37668  (0.37145)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.65280  (0.62993)\n",
            "     | > postnet_loss: 0.54822  (0.53105)\n",
            "     | > stopnet_loss: 0.20739  (0.16497)\n",
            "     | > decoder_coarse_loss: 0.77717  (0.75927)\n",
            "     | > decoder_ddc_loss: 0.00506  (0.00860)\n",
            "     | > ga_loss: 0.00009  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.28507  (0.27694)\n",
            "     | > postnet_diff_spec_loss: 0.23294  (0.22866)\n",
            "     | > decoder_ssim_loss: 0.30036  (0.31866)\n",
            "     | > postnet_ssim_loss: 0.26899  (0.28698)\n",
            "     | > loss: 0.97548  (0.92608)\n",
            "     | > align_error: 0.39526  (0.37542)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.68709  (0.63810)\n",
            "     | > postnet_loss: 0.58029  (0.53808)\n",
            "     | > stopnet_loss: 0.07092  (0.15154)\n",
            "     | > decoder_coarse_loss: 0.86608  (0.77453)\n",
            "     | > decoder_ddc_loss: 0.00550  (0.00816)\n",
            "     | > ga_loss: 0.00008  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.28857  (0.27860)\n",
            "     | > postnet_diff_spec_loss: 0.23862  (0.23008)\n",
            "     | > decoder_ssim_loss: 0.34736  (0.32276)\n",
            "     | > postnet_ssim_loss: 0.31246  (0.29062)\n",
            "     | > loss: 0.90283  (0.92276)\n",
            "     | > align_error: 0.36991  (0.37464)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00626 \u001b[0m(+0.00168)\n",
            "     | > avg_decoder_loss:\u001b[91m 0.63810 \u001b[0m(+0.00522)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.53808 \u001b[0m(-0.00010)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.15154 \u001b[0m(-0.00083)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.77453 \u001b[0m(-0.00649)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00816 \u001b[0m(-0.00011)\n",
            "     | > avg_ga_loss:\u001b[91m 0.00020 \u001b[0m(+0.00001)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[91m 0.27860 \u001b[0m(+0.00074)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.23008 \u001b[0m(-0.00061)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.32276 \u001b[0m(-0.00020)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.29062 \u001b[0m(-0.00113)\n",
            "     | > avg_loss:\u001b[92m 0.92276 \u001b[0m(-0.00147)\n",
            "     | > avg_align_error:\u001b[91m 0.37464 \u001b[0m(+0.01686)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_21720.pth.tar\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 60/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 04:42:22) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/361 -- GLOBAL_STEP: 21725\u001b[0m\n",
            "     | > decoder_loss: 0.65624  (0.67511)\n",
            "     | > postnet_loss: 0.58721  (0.59952)\n",
            "     | > stopnet_loss: 0.28260  (0.31174)\n",
            "     | > decoder_coarse_loss: 0.85188  (0.85969)\n",
            "     | > decoder_ddc_loss: 0.02128  (0.02308)\n",
            "     | > ga_loss: 0.00147  (0.00191)\n",
            "     | > decoder_diff_spec_loss: 0.29080  (0.29221)\n",
            "     | > postnet_diff_spec_loss: 0.28897  (0.29222)\n",
            "     | > decoder_ssim_loss: 0.33759  (0.33718)\n",
            "     | > postnet_ssim_loss: 0.31949  (0.31897)\n",
            "     | > loss: 1.12831  (1.17078)\n",
            "     | > align_error: 0.36803  (0.37050)\n",
            "     | > grad_norm: 5.61235  (10.67971)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.74450  (0.72598)\n",
            "     | > loader_time: 0.00220  (0.00633)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/361 -- GLOBAL_STEP: 21750\u001b[0m\n",
            "     | > decoder_loss: 0.67544  (0.65357)\n",
            "     | > postnet_loss: 0.60623  (0.58940)\n",
            "     | > stopnet_loss: 0.24063  (0.27011)\n",
            "     | > decoder_coarse_loss: 0.84432  (0.82522)\n",
            "     | > decoder_ddc_loss: 0.01538  (0.01847)\n",
            "     | > ga_loss: 0.00069  (0.00111)\n",
            "     | > decoder_diff_spec_loss: 0.29353  (0.29005)\n",
            "     | > postnet_diff_spec_loss: 0.28474  (0.29321)\n",
            "     | > decoder_ssim_loss: 0.32692  (0.33478)\n",
            "     | > postnet_ssim_loss: 0.30975  (0.31731)\n",
            "     | > loss: 1.08314  (1.10619)\n",
            "     | > align_error: 0.34058  (0.35754)\n",
            "     | > grad_norm: 1.27682  (3.96058)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.04070  (0.91253)\n",
            "     | > loader_time: 0.00510  (0.00537)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/361 -- GLOBAL_STEP: 21775\u001b[0m\n",
            "     | > decoder_loss: 0.65949  (0.65681)\n",
            "     | > postnet_loss: 0.59675  (0.59077)\n",
            "     | > stopnet_loss: 0.29361  (0.25922)\n",
            "     | > decoder_coarse_loss: 0.84989  (0.83317)\n",
            "     | > decoder_ddc_loss: 0.01338  (0.01654)\n",
            "     | > ga_loss: 0.00050  (0.00087)\n",
            "     | > decoder_diff_spec_loss: 0.29079  (0.29258)\n",
            "     | > postnet_diff_spec_loss: 0.28449  (0.29405)\n",
            "     | > decoder_ssim_loss: 0.31265  (0.33142)\n",
            "     | > postnet_ssim_loss: 0.29595  (0.31381)\n",
            "     | > loss: 1.12197  (1.09586)\n",
            "     | > align_error: 0.33542  (0.34736)\n",
            "     | > grad_norm: 1.55189  (2.89376)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.06250  (0.98342)\n",
            "     | > loader_time: 0.00540  (0.00557)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/361 -- GLOBAL_STEP: 21800\u001b[0m\n",
            "     | > decoder_loss: 0.62151  (0.65601)\n",
            "     | > postnet_loss: 0.55737  (0.58863)\n",
            "     | > stopnet_loss: 0.14355  (0.24567)\n",
            "     | > decoder_coarse_loss: 0.77025  (0.83200)\n",
            "     | > decoder_ddc_loss: 0.01310  (0.01532)\n",
            "     | > ga_loss: 0.00036  (0.00073)\n",
            "     | > decoder_diff_spec_loss: 0.28505  (0.29347)\n",
            "     | > postnet_diff_spec_loss: 0.29542  (0.29345)\n",
            "     | > decoder_ssim_loss: 0.35930  (0.33199)\n",
            "     | > postnet_ssim_loss: 0.33875  (0.31384)\n",
            "     | > loss: 0.95553  (1.08050)\n",
            "     | > align_error: 0.33425  (0.34319)\n",
            "     | > grad_norm: 1.57332  (2.37678)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.02680  (1.02686)\n",
            "     | > loader_time: 0.00550  (0.00571)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/361 -- GLOBAL_STEP: 21825\u001b[0m\n",
            "     | > decoder_loss: 0.63892  (0.65583)\n",
            "     | > postnet_loss: 0.57833  (0.58842)\n",
            "     | > stopnet_loss: 0.22594  (0.23861)\n",
            "     | > decoder_coarse_loss: 0.80361  (0.83142)\n",
            "     | > decoder_ddc_loss: 0.01162  (0.01448)\n",
            "     | > ga_loss: 0.00030  (0.00064)\n",
            "     | > decoder_diff_spec_loss: 0.29465  (0.29399)\n",
            "     | > postnet_diff_spec_loss: 0.28507  (0.29350)\n",
            "     | > decoder_ssim_loss: 0.32063  (0.33192)\n",
            "     | > postnet_ssim_loss: 0.30268  (0.31356)\n",
            "     | > loss: 1.03634  (1.07259)\n",
            "     | > align_error: 0.32537  (0.34050)\n",
            "     | > grad_norm: 1.04527  (2.15872)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.29110  (1.06306)\n",
            "     | > loader_time: 0.00630  (0.00572)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/361 -- GLOBAL_STEP: 21850\u001b[0m\n",
            "     | > decoder_loss: 0.66864  (0.65811)\n",
            "     | > postnet_loss: 0.58989  (0.58934)\n",
            "     | > stopnet_loss: 0.20706  (0.23111)\n",
            "     | > decoder_coarse_loss: 0.87318  (0.83433)\n",
            "     | > decoder_ddc_loss: 0.01047  (0.01384)\n",
            "     | > ga_loss: 0.00025  (0.00057)\n",
            "     | > decoder_diff_spec_loss: 0.30351  (0.29532)\n",
            "     | > postnet_diff_spec_loss: 0.29472  (0.29376)\n",
            "     | > decoder_ssim_loss: 0.32550  (0.33206)\n",
            "     | > postnet_ssim_loss: 0.30485  (0.31329)\n",
            "     | > loss: 1.05100  (1.06650)\n",
            "     | > align_error: 0.33240  (0.33859)\n",
            "     | > grad_norm: 0.55756  (1.97153)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.38740  (1.09867)\n",
            "     | > loader_time: 0.00600  (0.00576)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 155/361 -- GLOBAL_STEP: 21875\u001b[0m\n",
            "     | > decoder_loss: 0.68728  (0.65792)\n",
            "     | > postnet_loss: 0.61048  (0.58873)\n",
            "     | > stopnet_loss: 0.19988  (0.22469)\n",
            "     | > decoder_coarse_loss: 0.86646  (0.83474)\n",
            "     | > decoder_ddc_loss: 0.00992  (0.01327)\n",
            "     | > ga_loss: 0.00023  (0.00052)\n",
            "     | > decoder_diff_spec_loss: 0.31012  (0.29586)\n",
            "     | > postnet_diff_spec_loss: 0.29738  (0.29400)\n",
            "     | > decoder_ssim_loss: 0.32427  (0.33260)\n",
            "     | > postnet_ssim_loss: 0.30352  (0.31358)\n",
            "     | > loss: 1.05337  (1.05997)\n",
            "     | > align_error: 0.34570  (0.33802)\n",
            "     | > grad_norm: 0.97572  (1.82500)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.43080  (1.13591)\n",
            "     | > loader_time: 0.00630  (0.00582)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 180/361 -- GLOBAL_STEP: 21900\u001b[0m\n",
            "     | > decoder_loss: 0.66522  (0.65739)\n",
            "     | > postnet_loss: 0.58826  (0.58795)\n",
            "     | > stopnet_loss: 0.18854  (0.22054)\n",
            "     | > decoder_coarse_loss: 0.83943  (0.83476)\n",
            "     | > decoder_ddc_loss: 0.00927  (0.01276)\n",
            "     | > ga_loss: 0.00021  (0.00048)\n",
            "     | > decoder_diff_spec_loss: 0.29259  (0.29610)\n",
            "     | > postnet_diff_spec_loss: 0.28709  (0.29395)\n",
            "     | > decoder_ssim_loss: 0.33035  (0.33249)\n",
            "     | > postnet_ssim_loss: 0.30888  (0.31338)\n",
            "     | > loss: 1.01987  (1.05513)\n",
            "     | > align_error: 0.33573  (0.33752)\n",
            "     | > grad_norm: 0.84118  (1.71930)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.38060  (1.17050)\n",
            "     | > loader_time: 0.00660  (0.00586)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 205/361 -- GLOBAL_STEP: 21925\u001b[0m\n",
            "     | > decoder_loss: 0.66536  (0.65750)\n",
            "     | > postnet_loss: 0.59289  (0.58764)\n",
            "     | > stopnet_loss: 0.22789  (0.21452)\n",
            "     | > decoder_coarse_loss: 0.83479  (0.83477)\n",
            "     | > decoder_ddc_loss: 0.00856  (0.01234)\n",
            "     | > ga_loss: 0.00018  (0.00044)\n",
            "     | > decoder_diff_spec_loss: 0.30198  (0.29645)\n",
            "     | > postnet_diff_spec_loss: 0.29241  (0.29407)\n",
            "     | > decoder_ssim_loss: 0.32057  (0.33335)\n",
            "     | > postnet_ssim_loss: 0.30162  (0.31404)\n",
            "     | > loss: 1.05836  (1.04928)\n",
            "     | > align_error: 0.33213  (0.33703)\n",
            "     | > grad_norm: 1.40110  (1.63686)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.58360  (1.20432)\n",
            "     | > loader_time: 0.00700  (0.00588)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/361 -- GLOBAL_STEP: 21950\u001b[0m\n",
            "     | > decoder_loss: 0.68669  (0.65846)\n",
            "     | > postnet_loss: 0.60735  (0.58811)\n",
            "     | > stopnet_loss: 0.17296  (0.21261)\n",
            "     | > decoder_coarse_loss: 0.88240  (0.83664)\n",
            "     | > decoder_ddc_loss: 0.00829  (0.01193)\n",
            "     | > ga_loss: 0.00016  (0.00041)\n",
            "     | > decoder_diff_spec_loss: 0.30853  (0.29709)\n",
            "     | > postnet_diff_spec_loss: 0.30741  (0.29464)\n",
            "     | > decoder_ssim_loss: 0.35398  (0.33301)\n",
            "     | > postnet_ssim_loss: 0.33111  (0.31354)\n",
            "     | > loss: 1.04521  (1.04803)\n",
            "     | > align_error: 0.35086  (0.33783)\n",
            "     | > grad_norm: 0.82695  (1.57107)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.52300  (1.24704)\n",
            "     | > loader_time: 0.00770  (0.00603)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 255/361 -- GLOBAL_STEP: 21975\u001b[0m\n",
            "     | > decoder_loss: 0.66867  (0.65968)\n",
            "     | > postnet_loss: 0.59347  (0.58882)\n",
            "     | > stopnet_loss: 0.19576  (0.20964)\n",
            "     | > decoder_coarse_loss: 0.86727  (0.83862)\n",
            "     | > decoder_ddc_loss: 0.00747  (0.01155)\n",
            "     | > ga_loss: 0.00014  (0.00039)\n",
            "     | > decoder_diff_spec_loss: 0.30575  (0.29781)\n",
            "     | > postnet_diff_spec_loss: 0.29544  (0.29497)\n",
            "     | > decoder_ssim_loss: 0.33227  (0.33307)\n",
            "     | > postnet_ssim_loss: 0.31184  (0.31342)\n",
            "     | > loss: 1.04199  (1.04607)\n",
            "     | > align_error: 0.34882  (0.33924)\n",
            "     | > grad_norm: 0.88919  (1.54128)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.70510  (1.28960)\n",
            "     | > loader_time: 0.00670  (0.00612)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 280/361 -- GLOBAL_STEP: 22000\u001b[0m\n",
            "     | > decoder_loss: 0.65679  (0.66088)\n",
            "     | > postnet_loss: 0.58234  (0.58963)\n",
            "     | > stopnet_loss: 0.15344  (0.20706)\n",
            "     | > decoder_coarse_loss: 0.84648  (0.84020)\n",
            "     | > decoder_ddc_loss: 0.00736  (0.01119)\n",
            "     | > ga_loss: 0.00013  (0.00037)\n",
            "     | > decoder_diff_spec_loss: 0.29980  (0.29823)\n",
            "     | > postnet_diff_spec_loss: 0.29349  (0.29520)\n",
            "     | > decoder_ssim_loss: 0.34592  (0.33320)\n",
            "     | > postnet_ssim_loss: 0.32401  (0.31338)\n",
            "     | > loss: 0.99313  (1.04437)\n",
            "     | > align_error: 0.33839  (0.33969)\n",
            "     | > grad_norm: 0.54346  (1.50916)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.74320  (1.33136)\n",
            "     | > loader_time: 0.00720  (0.00617)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/checkpoint_22000.pth.tar\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "\u001b[1m   --> STEP: 305/361 -- GLOBAL_STEP: 22025\u001b[0m\n",
            "     | > decoder_loss: 0.66504  (0.66244)\n",
            "     | > postnet_loss: 0.58802  (0.59061)\n",
            "     | > stopnet_loss: 0.15357  (0.20436)\n",
            "     | > decoder_coarse_loss: 0.84565  (0.84260)\n",
            "     | > decoder_ddc_loss: 0.00660  (0.01086)\n",
            "     | > ga_loss: 0.00010  (0.00035)\n",
            "     | > decoder_diff_spec_loss: 0.30357  (0.29890)\n",
            "     | > postnet_diff_spec_loss: 0.29763  (0.29581)\n",
            "     | > decoder_ssim_loss: 0.33750  (0.33346)\n",
            "     | > postnet_ssim_loss: 0.31503  (0.31350)\n",
            "     | > loss: 0.99385  (1.04314)\n",
            "     | > align_error: 0.33544  (0.34010)\n",
            "     | > grad_norm: 0.62471  (1.46601)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.88250  (1.37631)\n",
            "     | > loader_time: 0.00630  (0.00649)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 330/361 -- GLOBAL_STEP: 22050\u001b[0m\n",
            "     | > decoder_loss: 0.69101  (0.66406)\n",
            "     | > postnet_loss: 0.60660  (0.59147)\n",
            "     | > stopnet_loss: 0.15968  (0.20192)\n",
            "     | > decoder_coarse_loss: 0.87410  (0.84504)\n",
            "     | > decoder_ddc_loss: 0.00633  (0.01054)\n",
            "     | > ga_loss: 0.00010  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.30176  (0.29963)\n",
            "     | > postnet_diff_spec_loss: 0.29436  (0.29625)\n",
            "     | > decoder_ssim_loss: 0.34528  (0.33361)\n",
            "     | > postnet_ssim_loss: 0.32304  (0.31346)\n",
            "     | > loss: 1.02081  (1.04208)\n",
            "     | > align_error: 0.35736  (0.34081)\n",
            "     | > grad_norm: 1.03169  (1.42296)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.01990  (1.41995)\n",
            "     | > loader_time: 0.00770  (0.00656)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 355/361 -- GLOBAL_STEP: 22075\u001b[0m\n",
            "     | > decoder_loss: 0.74098  (0.66719)\n",
            "     | > postnet_loss: 0.64285  (0.59367)\n",
            "     | > stopnet_loss: 0.12730  (0.20096)\n",
            "     | > decoder_coarse_loss: 0.93275  (0.84860)\n",
            "     | > decoder_ddc_loss: 0.00552  (0.01021)\n",
            "     | > ga_loss: 0.00009  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.32633  (0.30067)\n",
            "     | > postnet_diff_spec_loss: 0.32791  (0.29731)\n",
            "     | > decoder_ssim_loss: 0.36119  (0.33368)\n",
            "     | > postnet_ssim_loss: 0.33488  (0.31337)\n",
            "     | > loss: 1.04584  (1.04370)\n",
            "     | > align_error: 0.35988  (0.34279)\n",
            "     | > grad_norm: 0.72054  (1.39999)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.94700  (1.47538)\n",
            "     | > loader_time: 0.00700  (0.00663)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.63836  (0.63836)\n",
            "     | > postnet_loss: 0.56267  (0.56267)\n",
            "     | > stopnet_loss: 0.27141  (0.27141)\n",
            "     | > decoder_coarse_loss: 0.77712  (0.77712)\n",
            "     | > decoder_ddc_loss: 0.01200  (0.01200)\n",
            "     | > ga_loss: 0.00091  (0.00091)\n",
            "     | > decoder_diff_spec_loss: 0.25044  (0.25044)\n",
            "     | > postnet_diff_spec_loss: 0.21323  (0.21323)\n",
            "     | > decoder_ssim_loss: 0.29397  (0.29397)\n",
            "     | > postnet_ssim_loss: 0.27050  (0.27050)\n",
            "     | > loss: 1.03053  (1.03053)\n",
            "     | > align_error: 0.44878  (0.44878)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.60593  (0.60593)\n",
            "     | > postnet_loss: 0.52266  (0.52266)\n",
            "     | > stopnet_loss: 0.14153  (0.14153)\n",
            "     | > decoder_coarse_loss: 0.72158  (0.72158)\n",
            "     | > decoder_ddc_loss: 0.01178  (0.01178)\n",
            "     | > ga_loss: 0.00037  (0.00037)\n",
            "     | > decoder_diff_spec_loss: 0.27089  (0.27089)\n",
            "     | > postnet_diff_spec_loss: 0.22649  (0.22649)\n",
            "     | > decoder_ssim_loss: 0.33424  (0.33424)\n",
            "     | > postnet_ssim_loss: 0.30339  (0.30339)\n",
            "     | > loss: 0.89264  (0.89264)\n",
            "     | > align_error: 0.34063  (0.34063)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.60922  (0.60757)\n",
            "     | > postnet_loss: 0.53173  (0.52719)\n",
            "     | > stopnet_loss: 0.17977  (0.16065)\n",
            "     | > decoder_coarse_loss: 0.72956  (0.72557)\n",
            "     | > decoder_ddc_loss: 0.01002  (0.01090)\n",
            "     | > ga_loss: 0.00031  (0.00034)\n",
            "     | > decoder_diff_spec_loss: 0.26135  (0.26612)\n",
            "     | > postnet_diff_spec_loss: 0.21890  (0.22269)\n",
            "     | > decoder_ssim_loss: 0.30156  (0.31790)\n",
            "     | > postnet_ssim_loss: 0.27524  (0.28931)\n",
            "     | > loss: 0.91573  (0.90418)\n",
            "     | > align_error: 0.35575  (0.34819)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.63084  (0.61533)\n",
            "     | > postnet_loss: 0.54246  (0.53228)\n",
            "     | > stopnet_loss: 0.15809  (0.15980)\n",
            "     | > decoder_coarse_loss: 0.78444  (0.74519)\n",
            "     | > decoder_ddc_loss: 0.00849  (0.01010)\n",
            "     | > ga_loss: 0.00019  (0.00029)\n",
            "     | > decoder_diff_spec_loss: 0.27480  (0.26901)\n",
            "     | > postnet_diff_spec_loss: 0.23031  (0.22523)\n",
            "     | > decoder_ssim_loss: 0.31665  (0.31748)\n",
            "     | > postnet_ssim_loss: 0.28717  (0.28860)\n",
            "     | > loss: 0.92784  (0.91207)\n",
            "     | > align_error: 0.35078  (0.34905)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.63180  (0.61945)\n",
            "     | > postnet_loss: 0.53948  (0.53408)\n",
            "     | > stopnet_loss: 0.10358  (0.14574)\n",
            "     | > decoder_coarse_loss: 0.76239  (0.74949)\n",
            "     | > decoder_ddc_loss: 0.00813  (0.00961)\n",
            "     | > ga_loss: 0.00015  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.28608  (0.27328)\n",
            "     | > postnet_diff_spec_loss: 0.23739  (0.22827)\n",
            "     | > decoder_ssim_loss: 0.34223  (0.32367)\n",
            "     | > postnet_ssim_loss: 0.30886  (0.29366)\n",
            "     | > loss: 0.88340  (0.90490)\n",
            "     | > align_error: 0.33242  (0.34490)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.64426  (0.62441)\n",
            "     | > postnet_loss: 0.55600  (0.53847)\n",
            "     | > stopnet_loss: 0.17947  (0.15249)\n",
            "     | > decoder_coarse_loss: 0.76440  (0.75247)\n",
            "     | > decoder_ddc_loss: 0.00651  (0.00899)\n",
            "     | > ga_loss: 0.00011  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.27622  (0.27387)\n",
            "     | > postnet_diff_spec_loss: 0.22857  (0.22833)\n",
            "     | > decoder_ssim_loss: 0.30850  (0.32064)\n",
            "     | > postnet_ssim_loss: 0.27796  (0.29052)\n",
            "     | > loss: 0.94561  (0.91304)\n",
            "     | > align_error: 0.34701  (0.34532)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.64820  (0.62837)\n",
            "     | > postnet_loss: 0.55668  (0.54150)\n",
            "     | > stopnet_loss: 0.20587  (0.16138)\n",
            "     | > decoder_coarse_loss: 0.77275  (0.75585)\n",
            "     | > decoder_ddc_loss: 0.00489  (0.00830)\n",
            "     | > ga_loss: 0.00008  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.28344  (0.27546)\n",
            "     | > postnet_diff_spec_loss: 0.23368  (0.22922)\n",
            "     | > decoder_ssim_loss: 0.29806  (0.31687)\n",
            "     | > postnet_ssim_loss: 0.26855  (0.28686)\n",
            "     | > loss: 0.97282  (0.92300)\n",
            "     | > align_error: 0.37164  (0.34971)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.67422  (0.63492)\n",
            "     | > postnet_loss: 0.57593  (0.54642)\n",
            "     | > stopnet_loss: 0.07096  (0.14847)\n",
            "     | > decoder_coarse_loss: 0.85574  (0.77012)\n",
            "     | > decoder_ddc_loss: 0.00535  (0.00788)\n",
            "     | > ga_loss: 0.00007  (0.00018)\n",
            "     | > decoder_diff_spec_loss: 0.28613  (0.27699)\n",
            "     | > postnet_diff_spec_loss: 0.23916  (0.23064)\n",
            "     | > decoder_ssim_loss: 0.34407  (0.32076)\n",
            "     | > postnet_ssim_loss: 0.31142  (0.29037)\n",
            "     | > loss: 0.89432  (0.91891)\n",
            "     | > align_error: 0.34820  (0.34949)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00723 \u001b[0m(+0.00098)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.63492 \u001b[0m(-0.00318)\n",
            "     | > avg_postnet_loss:\u001b[91m 0.54642 \u001b[0m(+0.00834)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.14847 \u001b[0m(-0.00307)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.77012 \u001b[0m(-0.00441)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00788 \u001b[0m(-0.00028)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00018 \u001b[0m(-0.00001)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.27699 \u001b[0m(-0.00161)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[91m 0.23064 \u001b[0m(+0.00056)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.32076 \u001b[0m(-0.00200)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.29037 \u001b[0m(-0.00025)\n",
            "     | > avg_loss:\u001b[92m 0.91891 \u001b[0m(-0.00385)\n",
            "     | > avg_align_error:\u001b[92m 0.34949 \u001b[0m(-0.02514)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_22082.pth.tar\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 61/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 04:52:08) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 18/361 -- GLOBAL_STEP: 22100\u001b[0m\n",
            "     | > decoder_loss: 0.61490  (0.64205)\n",
            "     | > postnet_loss: 0.54763  (0.58360)\n",
            "     | > stopnet_loss: 0.23304  (0.29672)\n",
            "     | > decoder_coarse_loss: 0.78508  (0.81277)\n",
            "     | > decoder_ddc_loss: 0.01678  (0.01911)\n",
            "     | > ga_loss: 0.00083  (0.00130)\n",
            "     | > decoder_diff_spec_loss: 0.28580  (0.28721)\n",
            "     | > postnet_diff_spec_loss: 0.28117  (0.29462)\n",
            "     | > decoder_ssim_loss: 0.32792  (0.33037)\n",
            "     | > postnet_ssim_loss: 0.31002  (0.31390)\n",
            "     | > loss: 1.02952  (1.12412)\n",
            "     | > align_error: 0.33416  (0.33820)\n",
            "     | > grad_norm: 3.42029  (3.71301)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.99160  (0.85463)\n",
            "     | > loader_time: 0.00850  (0.00644)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 43/361 -- GLOBAL_STEP: 22125\u001b[0m\n",
            "     | > decoder_loss: 0.63327  (0.64846)\n",
            "     | > postnet_loss: 0.56307  (0.58477)\n",
            "     | > stopnet_loss: 0.19690  (0.26501)\n",
            "     | > decoder_coarse_loss: 0.82981  (0.82427)\n",
            "     | > decoder_ddc_loss: 0.01415  (0.01688)\n",
            "     | > ga_loss: 0.00052  (0.00092)\n",
            "     | > decoder_diff_spec_loss: 0.29069  (0.28971)\n",
            "     | > postnet_diff_spec_loss: 0.28626  (0.29313)\n",
            "     | > decoder_ssim_loss: 0.33883  (0.32974)\n",
            "     | > postnet_ssim_loss: 0.31914  (0.31256)\n",
            "     | > loss: 1.01832  (1.09450)\n",
            "     | > align_error: 0.31671  (0.33029)\n",
            "     | > grad_norm: 0.81977  (2.62893)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.05400  (0.97392)\n",
            "     | > loader_time: 0.00580  (0.00608)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 68/361 -- GLOBAL_STEP: 22150\u001b[0m\n",
            "     | > decoder_loss: 0.63919  (0.64997)\n",
            "     | > postnet_loss: 0.56861  (0.58468)\n",
            "     | > stopnet_loss: 0.19656  (0.25035)\n",
            "     | > decoder_coarse_loss: 0.79692  (0.82623)\n",
            "     | > decoder_ddc_loss: 0.01216  (0.01554)\n",
            "     | > ga_loss: 0.00038  (0.00076)\n",
            "     | > decoder_diff_spec_loss: 0.28655  (0.29170)\n",
            "     | > postnet_diff_spec_loss: 0.28255  (0.29261)\n",
            "     | > decoder_ssim_loss: 0.33646  (0.32949)\n",
            "     | > postnet_ssim_loss: 0.31709  (0.31177)\n",
            "     | > loss: 1.00833  (1.07964)\n",
            "     | > align_error: 0.31601  (0.32699)\n",
            "     | > grad_norm: 1.01513  (2.09085)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.09620  (1.01329)\n",
            "     | > loader_time: 0.00500  (0.00592)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 93/361 -- GLOBAL_STEP: 22175\u001b[0m\n",
            "     | > decoder_loss: 0.62068  (0.64923)\n",
            "     | > postnet_loss: 0.55665  (0.58351)\n",
            "     | > stopnet_loss: 0.18068  (0.24088)\n",
            "     | > decoder_coarse_loss: 0.77712  (0.82339)\n",
            "     | > decoder_ddc_loss: 0.01203  (0.01464)\n",
            "     | > ga_loss: 0.00035  (0.00065)\n",
            "     | > decoder_diff_spec_loss: 0.29039  (0.29168)\n",
            "     | > postnet_diff_spec_loss: 0.28636  (0.29200)\n",
            "     | > decoder_ssim_loss: 0.33495  (0.33030)\n",
            "     | > postnet_ssim_loss: 0.31580  (0.31235)\n",
            "     | > loss: 0.98090  (1.06843)\n",
            "     | > align_error: 0.32340  (0.32608)\n",
            "     | > grad_norm: 1.55577  (1.90522)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.14310  (1.04785)\n",
            "     | > loader_time: 0.00560  (0.00595)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 118/361 -- GLOBAL_STEP: 22200\u001b[0m\n",
            "     | > decoder_loss: 0.70719  (0.65154)\n",
            "     | > postnet_loss: 0.62039  (0.58478)\n",
            "     | > stopnet_loss: 0.14623  (0.23278)\n",
            "     | > decoder_coarse_loss: 0.87429  (0.82459)\n",
            "     | > decoder_ddc_loss: 0.01124  (0.01395)\n",
            "     | > ga_loss: 0.00031  (0.00058)\n",
            "     | > decoder_diff_spec_loss: 0.30910  (0.29284)\n",
            "     | > postnet_diff_spec_loss: 0.31347  (0.29173)\n",
            "     | > decoder_ssim_loss: 0.35569  (0.33043)\n",
            "     | > postnet_ssim_loss: 0.33308  (0.31210)\n",
            "     | > loss: 1.02887  (1.06119)\n",
            "     | > align_error: 0.32886  (0.32529)\n",
            "     | > grad_norm: 0.76385  (1.81714)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.18610  (1.08547)\n",
            "     | > loader_time: 0.00540  (0.00593)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 143/361 -- GLOBAL_STEP: 22225\u001b[0m\n",
            "     | > decoder_loss: 0.64960  (0.65239)\n",
            "     | > postnet_loss: 0.57896  (0.58460)\n",
            "     | > stopnet_loss: 0.25476  (0.22739)\n",
            "     | > decoder_coarse_loss: 0.82981  (0.82585)\n",
            "     | > decoder_ddc_loss: 0.01021  (0.01337)\n",
            "     | > ga_loss: 0.00025  (0.00053)\n",
            "     | > decoder_diff_spec_loss: 0.28979  (0.29374)\n",
            "     | > postnet_diff_spec_loss: 0.28169  (0.29181)\n",
            "     | > decoder_ssim_loss: 0.32723  (0.33062)\n",
            "     | > postnet_ssim_loss: 0.30793  (0.31196)\n",
            "     | > loss: 1.07483  (1.05612)\n",
            "     | > align_error: 0.32289  (0.32483)\n",
            "     | > grad_norm: 1.84615  (1.73097)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.42850  (1.12090)\n",
            "     | > loader_time: 0.00570  (0.00599)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 168/361 -- GLOBAL_STEP: 22250\u001b[0m\n",
            "     | > decoder_loss: 0.62953  (0.65170)\n",
            "     | > postnet_loss: 0.56861  (0.58363)\n",
            "     | > stopnet_loss: 0.13489  (0.22011)\n",
            "     | > decoder_coarse_loss: 0.79781  (0.82533)\n",
            "     | > decoder_ddc_loss: 0.01029  (0.01285)\n",
            "     | > ga_loss: 0.00025  (0.00048)\n",
            "     | > decoder_diff_spec_loss: 0.29303  (0.29398)\n",
            "     | > postnet_diff_spec_loss: 0.29835  (0.29192)\n",
            "     | > decoder_ssim_loss: 0.34641  (0.33129)\n",
            "     | > postnet_ssim_loss: 0.32569  (0.31243)\n",
            "     | > loss: 0.95356  (1.04830)\n",
            "     | > align_error: 0.31879  (0.32483)\n",
            "     | > grad_norm: 0.85709  (1.61048)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.32760  (1.15345)\n",
            "     | > loader_time: 0.00640  (0.00603)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 193/361 -- GLOBAL_STEP: 22275\u001b[0m\n",
            "     | > decoder_loss: 0.65733  (0.65186)\n",
            "     | > postnet_loss: 0.58536  (0.58329)\n",
            "     | > stopnet_loss: 0.18066  (0.21598)\n",
            "     | > decoder_coarse_loss: 0.85626  (0.82577)\n",
            "     | > decoder_ddc_loss: 0.00962  (0.01239)\n",
            "     | > ga_loss: 0.00019  (0.00045)\n",
            "     | > decoder_diff_spec_loss: 0.30540  (0.29457)\n",
            "     | > postnet_diff_spec_loss: 0.29784  (0.29254)\n",
            "     | > decoder_ssim_loss: 0.34021  (0.33159)\n",
            "     | > postnet_ssim_loss: 0.31932  (0.31257)\n",
            "     | > loss: 1.02445  (1.04436)\n",
            "     | > align_error: 0.32614  (0.32544)\n",
            "     | > grad_norm: 0.98955  (1.54901)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.38420  (1.19071)\n",
            "     | > loader_time: 0.00690  (0.00609)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 218/361 -- GLOBAL_STEP: 22300\u001b[0m\n",
            "     | > decoder_loss: 0.68174  (0.65260)\n",
            "     | > postnet_loss: 0.62290  (0.58371)\n",
            "     | > stopnet_loss: 0.13597  (0.21280)\n",
            "     | > decoder_coarse_loss: 0.85129  (0.82642)\n",
            "     | > decoder_ddc_loss: 0.00900  (0.01198)\n",
            "     | > ga_loss: 0.00017  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.30275  (0.29507)\n",
            "     | > postnet_diff_spec_loss: 0.31681  (0.29256)\n",
            "     | > decoder_ssim_loss: 0.35853  (0.33145)\n",
            "     | > postnet_ssim_loss: 0.33585  (0.31231)\n",
            "     | > loss: 1.00652  (1.04141)\n",
            "     | > align_error: 0.34391  (0.32634)\n",
            "     | > grad_norm: 2.24793  (1.49349)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.51250  (1.22778)\n",
            "     | > loader_time: 0.00640  (0.00614)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 243/361 -- GLOBAL_STEP: 22325\u001b[0m\n",
            "     | > decoder_loss: 0.70024  (0.65412)\n",
            "     | > postnet_loss: 0.61577  (0.58455)\n",
            "     | > stopnet_loss: 0.18852  (0.21069)\n",
            "     | > decoder_coarse_loss: 0.88078  (0.82864)\n",
            "     | > decoder_ddc_loss: 0.00827  (0.01160)\n",
            "     | > ga_loss: 0.00014  (0.00039)\n",
            "     | > decoder_diff_spec_loss: 0.32457  (0.29591)\n",
            "     | > postnet_diff_spec_loss: 0.31139  (0.29319)\n",
            "     | > decoder_ssim_loss: 0.32678  (0.33140)\n",
            "     | > postnet_ssim_loss: 0.30380  (0.31206)\n",
            "     | > loss: 1.05714  (1.04050)\n",
            "     | > align_error: 0.32764  (0.32677)\n",
            "     | > grad_norm: 1.94881  (1.45824)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.88970  (1.26861)\n",
            "     | > loader_time: 0.00580  (0.00619)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 268/361 -- GLOBAL_STEP: 22350\u001b[0m\n",
            "     | > decoder_loss: 0.68028  (0.65617)\n",
            "     | > postnet_loss: 0.60008  (0.58613)\n",
            "     | > stopnet_loss: 0.19933  (0.20798)\n",
            "     | > decoder_coarse_loss: 0.86792  (0.83088)\n",
            "     | > decoder_ddc_loss: 0.00726  (0.01124)\n",
            "     | > ga_loss: 0.00011  (0.00037)\n",
            "     | > decoder_diff_spec_loss: 0.30649  (0.29658)\n",
            "     | > postnet_diff_spec_loss: 0.29653  (0.29361)\n",
            "     | > decoder_ssim_loss: 0.33142  (0.33153)\n",
            "     | > postnet_ssim_loss: 0.30973  (0.31199)\n",
            "     | > loss: 1.04980  (1.03935)\n",
            "     | > align_error: 0.33699  (0.32743)\n",
            "     | > grad_norm: 0.71599  (1.44515)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.78580  (1.30990)\n",
            "     | > loader_time: 0.00750  (0.00624)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 293/361 -- GLOBAL_STEP: 22375\u001b[0m\n",
            "     | > decoder_loss: 0.68260  (0.65783)\n",
            "     | > postnet_loss: 0.59156  (0.58720)\n",
            "     | > stopnet_loss: 0.14003  (0.20487)\n",
            "     | > decoder_coarse_loss: 0.87326  (0.83275)\n",
            "     | > decoder_ddc_loss: 0.00704  (0.01091)\n",
            "     | > ga_loss: 0.00012  (0.00035)\n",
            "     | > decoder_diff_spec_loss: 0.30835  (0.29721)\n",
            "     | > postnet_diff_spec_loss: 0.30926  (0.29406)\n",
            "     | > decoder_ssim_loss: 0.35051  (0.33189)\n",
            "     | > postnet_ssim_loss: 0.32633  (0.31218)\n",
            "     | > loss: 1.00285  (1.03760)\n",
            "     | > align_error: 0.34109  (0.32809)\n",
            "     | > grad_norm: 1.01360  (1.41127)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.69650  (1.35408)\n",
            "     | > loader_time: 0.00720  (0.00628)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 318/361 -- GLOBAL_STEP: 22400\u001b[0m\n",
            "     | > decoder_loss: 0.68359  (0.65921)\n",
            "     | > postnet_loss: 0.59735  (0.58801)\n",
            "     | > stopnet_loss: 0.18638  (0.20259)\n",
            "     | > decoder_coarse_loss: 0.87468  (0.83525)\n",
            "     | > decoder_ddc_loss: 0.00662  (0.01059)\n",
            "     | > ga_loss: 0.00011  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.31941  (0.29787)\n",
            "     | > postnet_diff_spec_loss: 0.31006  (0.29455)\n",
            "     | > decoder_ssim_loss: 0.32899  (0.33202)\n",
            "     | > postnet_ssim_loss: 0.30540  (0.31216)\n",
            "     | > loss: 1.04345  (1.03665)\n",
            "     | > align_error: 0.34800  (0.32915)\n",
            "     | > grad_norm: 1.30950  (1.38247)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.95020  (1.39379)\n",
            "     | > loader_time: 0.00660  (0.00635)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 343/361 -- GLOBAL_STEP: 22425\u001b[0m\n",
            "     | > decoder_loss: 0.70270  (0.66112)\n",
            "     | > postnet_loss: 0.61498  (0.58917)\n",
            "     | > stopnet_loss: 0.14487  (0.20070)\n",
            "     | > decoder_coarse_loss: 0.90638  (0.83829)\n",
            "     | > decoder_ddc_loss: 0.00557  (0.01028)\n",
            "     | > ga_loss: 0.00010  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.31613  (0.29868)\n",
            "     | > postnet_diff_spec_loss: 0.31109  (0.29520)\n",
            "     | > decoder_ssim_loss: 0.34277  (0.33216)\n",
            "     | > postnet_ssim_loss: 0.31910  (0.31213)\n",
            "     | > loss: 1.02506  (1.03652)\n",
            "     | > align_error: 0.35792  (0.33097)\n",
            "     | > grad_norm: 0.84058  (1.34959)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 2.14490  (1.44127)\n",
            "     | > loader_time: 0.00680  (0.00642)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.63424  (0.63424)\n",
            "     | > postnet_loss: 0.55350  (0.55350)\n",
            "     | > stopnet_loss: 0.27566  (0.27566)\n",
            "     | > decoder_coarse_loss: 0.76871  (0.76871)\n",
            "     | > decoder_ddc_loss: 0.01189  (0.01189)\n",
            "     | > ga_loss: 0.00088  (0.00088)\n",
            "     | > decoder_diff_spec_loss: 0.24859  (0.24859)\n",
            "     | > postnet_diff_spec_loss: 0.21162  (0.21162)\n",
            "     | > decoder_ssim_loss: 0.29351  (0.29351)\n",
            "     | > postnet_ssim_loss: 0.26974  (0.26974)\n",
            "     | > loss: 1.02798  (1.02798)\n",
            "     | > align_error: 0.44091  (0.44091)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.59842  (0.59842)\n",
            "     | > postnet_loss: 0.51173  (0.51173)\n",
            "     | > stopnet_loss: 0.14511  (0.14511)\n",
            "     | > decoder_coarse_loss: 0.71096  (0.71096)\n",
            "     | > decoder_ddc_loss: 0.01176  (0.01176)\n",
            "     | > ga_loss: 0.00036  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.26839  (0.26839)\n",
            "     | > postnet_diff_spec_loss: 0.22439  (0.22439)\n",
            "     | > decoder_ssim_loss: 0.33318  (0.33318)\n",
            "     | > postnet_ssim_loss: 0.30225  (0.30225)\n",
            "     | > loss: 0.88719  (0.88719)\n",
            "     | > align_error: 0.33165  (0.33165)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.60448  (0.60145)\n",
            "     | > postnet_loss: 0.52236  (0.51704)\n",
            "     | > stopnet_loss: 0.18354  (0.16432)\n",
            "     | > decoder_coarse_loss: 0.72183  (0.71640)\n",
            "     | > decoder_ddc_loss: 0.01001  (0.01088)\n",
            "     | > ga_loss: 0.00031  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.25909  (0.26374)\n",
            "     | > postnet_diff_spec_loss: 0.21686  (0.22063)\n",
            "     | > decoder_ssim_loss: 0.30160  (0.31739)\n",
            "     | > postnet_ssim_loss: 0.27498  (0.28861)\n",
            "     | > loss: 0.91287  (0.90003)\n",
            "     | > align_error: 0.34393  (0.33779)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.62735  (0.61008)\n",
            "     | > postnet_loss: 0.53627  (0.52345)\n",
            "     | > stopnet_loss: 0.16144  (0.16336)\n",
            "     | > decoder_coarse_loss: 0.77769  (0.73683)\n",
            "     | > decoder_ddc_loss: 0.00856  (0.01011)\n",
            "     | > ga_loss: 0.00019  (0.00028)\n",
            "     | > decoder_diff_spec_loss: 0.27279  (0.26675)\n",
            "     | > postnet_diff_spec_loss: 0.22828  (0.22318)\n",
            "     | > decoder_ssim_loss: 0.31652  (0.31710)\n",
            "     | > postnet_ssim_loss: 0.28654  (0.28792)\n",
            "     | > loss: 0.92587  (0.90864)\n",
            "     | > align_error: 0.34102  (0.33887)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.62617  (0.61410)\n",
            "     | > postnet_loss: 0.53041  (0.52519)\n",
            "     | > stopnet_loss: 0.10490  (0.14874)\n",
            "     | > decoder_coarse_loss: 0.75491  (0.74135)\n",
            "     | > decoder_ddc_loss: 0.00815  (0.00962)\n",
            "     | > ga_loss: 0.00014  (0.00025)\n",
            "     | > decoder_diff_spec_loss: 0.28318  (0.27086)\n",
            "     | > postnet_diff_spec_loss: 0.23492  (0.22611)\n",
            "     | > decoder_ssim_loss: 0.34145  (0.32319)\n",
            "     | > postnet_ssim_loss: 0.30790  (0.29292)\n",
            "     | > loss: 0.87738  (0.90083)\n",
            "     | > align_error: 0.32480  (0.33535)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.63903  (0.61909)\n",
            "     | > postnet_loss: 0.54635  (0.52942)\n",
            "     | > stopnet_loss: 0.18082  (0.15516)\n",
            "     | > decoder_coarse_loss: 0.75392  (0.74386)\n",
            "     | > decoder_ddc_loss: 0.00644  (0.00898)\n",
            "     | > ga_loss: 0.00011  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.27397  (0.27148)\n",
            "     | > postnet_diff_spec_loss: 0.22628  (0.22615)\n",
            "     | > decoder_ssim_loss: 0.30791  (0.32013)\n",
            "     | > postnet_ssim_loss: 0.27705  (0.28974)\n",
            "     | > loss: 0.93909  (0.90848)\n",
            "     | > align_error: 0.33950  (0.33618)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.64554  (0.62350)\n",
            "     | > postnet_loss: 0.55064  (0.53296)\n",
            "     | > stopnet_loss: 0.20610  (0.16365)\n",
            "     | > decoder_coarse_loss: 0.76268  (0.74700)\n",
            "     | > decoder_ddc_loss: 0.00488  (0.00830)\n",
            "     | > ga_loss: 0.00008  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.28052  (0.27299)\n",
            "     | > postnet_diff_spec_loss: 0.23126  (0.22700)\n",
            "     | > decoder_ssim_loss: 0.29787  (0.31642)\n",
            "     | > postnet_ssim_loss: 0.26827  (0.28616)\n",
            "     | > loss: 0.96690  (0.91822)\n",
            "     | > align_error: 0.36353  (0.34074)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.67823  (0.63132)\n",
            "     | > postnet_loss: 0.57745  (0.53932)\n",
            "     | > stopnet_loss: 0.07163  (0.15050)\n",
            "     | > decoder_coarse_loss: 0.84848  (0.76149)\n",
            "     | > decoder_ddc_loss: 0.00535  (0.00788)\n",
            "     | > ga_loss: 0.00007  (0.00018)\n",
            "     | > decoder_diff_spec_loss: 0.28378  (0.27453)\n",
            "     | > postnet_diff_spec_loss: 0.23710  (0.22844)\n",
            "     | > decoder_ssim_loss: 0.34387  (0.32034)\n",
            "     | > postnet_ssim_loss: 0.31091  (0.28970)\n",
            "     | > loss: 0.89328  (0.91465)\n",
            "     | > align_error: 0.33881  (0.34046)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00586 \u001b[0m(-0.00137)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.63132 \u001b[0m(-0.00361)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.53932 \u001b[0m(-0.00710)\n",
            "     | > avg_stopnet_loss:\u001b[91m 0.15050 \u001b[0m(+0.00204)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.76149 \u001b[0m(-0.00863)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00788 \u001b[0m(-0.00000)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00018 \u001b[0m(-0.00000)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.27453 \u001b[0m(-0.00245)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.22844 \u001b[0m(-0.00220)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.32034 \u001b[0m(-0.00042)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.28970 \u001b[0m(-0.00067)\n",
            "     | > avg_loss:\u001b[92m 0.91465 \u001b[0m(-0.00425)\n",
            "     | > avg_align_error:\u001b[92m 0.34046 \u001b[0m(-0.00903)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_22444.pth.tar\n",
            "\n",
            " > Number of output frames: 4\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 62/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-23 05:01:44) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 6/361 -- GLOBAL_STEP: 22450\u001b[0m\n",
            "     | > decoder_loss: 0.66411  (0.65388)\n",
            "     | > postnet_loss: 0.58959  (0.58303)\n",
            "     | > stopnet_loss: 0.23742  (0.28742)\n",
            "     | > decoder_coarse_loss: 0.83338  (0.83305)\n",
            "     | > decoder_ddc_loss: 0.01990  (0.02154)\n",
            "     | > ga_loss: 0.00130  (0.00170)\n",
            "     | > decoder_diff_spec_loss: 0.29191  (0.28920)\n",
            "     | > postnet_diff_spec_loss: 0.28931  (0.28834)\n",
            "     | > decoder_ssim_loss: 0.34748  (0.33665)\n",
            "     | > postnet_ssim_loss: 0.32819  (0.31855)\n",
            "     | > loss: 1.08490  (1.12696)\n",
            "     | > align_error: 0.33970  (0.33378)\n",
            "     | > grad_norm: 1.21948  (6.75995)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.73840  (0.73212)\n",
            "     | > loader_time: 0.00790  (0.00488)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 31/361 -- GLOBAL_STEP: 22475\u001b[0m\n",
            "     | > decoder_loss: 0.64941  (0.63738)\n",
            "     | > postnet_loss: 0.57655  (0.57618)\n",
            "     | > stopnet_loss: 0.15889  (0.26305)\n",
            "     | > decoder_coarse_loss: 0.81498  (0.80795)\n",
            "     | > decoder_ddc_loss: 0.01520  (0.01764)\n",
            "     | > ga_loss: 0.00062  (0.00102)\n",
            "     | > decoder_diff_spec_loss: 0.29911  (0.28621)\n",
            "     | > postnet_diff_spec_loss: 0.30406  (0.28929)\n",
            "     | > decoder_ssim_loss: 0.36152  (0.33208)\n",
            "     | > postnet_ssim_loss: 0.34166  (0.31491)\n",
            "     | > loss: 1.00262  (1.08359)\n",
            "     | > align_error: 0.32249  (0.32684)\n",
            "     | > grad_norm: 2.37852  (3.06639)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.97120  (0.92374)\n",
            "     | > loader_time: 0.00520  (0.00558)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 56/361 -- GLOBAL_STEP: 22500\u001b[0m\n",
            "     | > decoder_loss: 0.63298  (0.64294)\n",
            "     | > postnet_loss: 0.56801  (0.58029)\n",
            "     | > stopnet_loss: 0.17814  (0.25529)\n",
            "     | > decoder_coarse_loss: 0.80033  (0.81604)\n",
            "     | > decoder_ddc_loss: 0.01305  (0.01593)\n",
            "     | > ga_loss: 0.00043  (0.00081)\n",
            "     | > decoder_diff_spec_loss: 0.28854  (0.28866)\n",
            "     | > postnet_diff_spec_loss: 0.29275  (0.28996)\n",
            "     | > decoder_ssim_loss: 0.35054  (0.32828)\n",
            "     | > postnet_ssim_loss: 0.33086  (0.31105)\n",
            "     | > loss: 0.99953  (1.07762)\n",
            "     | > align_error: 0.31486  (0.32346)\n",
            "     | > grad_norm: 1.29277  (2.45686)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 0.97420  (0.98570)\n",
            "     | > loader_time: 0.00580  (0.00558)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 81/361 -- GLOBAL_STEP: 22525\u001b[0m\n",
            "     | > decoder_loss: 0.61263  (0.64375)\n",
            "     | > postnet_loss: 0.54431  (0.57903)\n",
            "     | > stopnet_loss: 0.16820  (0.24288)\n",
            "     | > decoder_coarse_loss: 0.78341  (0.81692)\n",
            "     | > decoder_ddc_loss: 0.01280  (0.01489)\n",
            "     | > ga_loss: 0.00037  (0.00068)\n",
            "     | > decoder_diff_spec_loss: 0.28165  (0.28982)\n",
            "     | > postnet_diff_spec_loss: 0.28837  (0.28976)\n",
            "     | > decoder_ssim_loss: 0.34588  (0.32876)\n",
            "     | > postnet_ssim_loss: 0.32597  (0.31102)\n",
            "     | > loss: 0.96880  (1.06479)\n",
            "     | > align_error: 0.32159  (0.32202)\n",
            "     | > grad_norm: 1.22408  (2.06545)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.05500  (1.02242)\n",
            "     | > loader_time: 0.00600  (0.00568)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 106/361 -- GLOBAL_STEP: 22550\u001b[0m\n",
            "     | > decoder_loss: 0.67651  (0.64510)\n",
            "     | > postnet_loss: 0.60055  (0.57981)\n",
            "     | > stopnet_loss: 0.19744  (0.23673)\n",
            "     | > decoder_coarse_loss: 0.84410  (0.81687)\n",
            "     | > decoder_ddc_loss: 0.01168  (0.01414)\n",
            "     | > ga_loss: 0.00030  (0.00060)\n",
            "     | > decoder_diff_spec_loss: 0.30577  (0.29080)\n",
            "     | > postnet_diff_spec_loss: 0.29552  (0.29010)\n",
            "     | > decoder_ssim_loss: 0.33373  (0.32866)\n",
            "     | > postnet_ssim_loss: 0.31359  (0.31071)\n",
            "     | > loss: 1.04430  (1.05879)\n",
            "     | > align_error: 0.32228  (0.32191)\n",
            "     | > grad_norm: 0.92015  (1.95451)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.16150  (1.06349)\n",
            "     | > loader_time: 0.00610  (0.00578)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 131/361 -- GLOBAL_STEP: 22575\u001b[0m\n",
            "     | > decoder_loss: 0.67111  (0.64735)\n",
            "     | > postnet_loss: 0.59597  (0.58073)\n",
            "     | > stopnet_loss: 0.22462  (0.22982)\n",
            "     | > decoder_coarse_loss: 0.87106  (0.81958)\n",
            "     | > decoder_ddc_loss: 0.01104  (0.01353)\n",
            "     | > ga_loss: 0.00025  (0.00054)\n",
            "     | > decoder_diff_spec_loss: 0.30489  (0.29213)\n",
            "     | > postnet_diff_spec_loss: 0.29915  (0.29048)\n",
            "     | > decoder_ssim_loss: 0.32996  (0.32879)\n",
            "     | > postnet_ssim_loss: 0.30916  (0.31048)\n",
            "     | > loss: 1.07397  (1.05330)\n",
            "     | > align_error: 0.32095  (0.32174)\n",
            "     | > grad_norm: 1.75782  (1.81812)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.21870  (1.10084)\n",
            "     | > loader_time: 0.00550  (0.00579)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 156/361 -- GLOBAL_STEP: 22600\u001b[0m\n",
            "     | > decoder_loss: 0.62277  (0.64708)\n",
            "     | > postnet_loss: 0.55983  (0.58003)\n",
            "     | > stopnet_loss: 0.22620  (0.22376)\n",
            "     | > decoder_coarse_loss: 0.80852  (0.81930)\n",
            "     | > decoder_ddc_loss: 0.00902  (0.01297)\n",
            "     | > ga_loss: 0.00020  (0.00049)\n",
            "     | > decoder_diff_spec_loss: 0.27973  (0.29253)\n",
            "     | > postnet_diff_spec_loss: 0.27177  (0.29049)\n",
            "     | > decoder_ssim_loss: 0.31729  (0.32935)\n",
            "     | > postnet_ssim_loss: 0.29988  (0.31080)\n",
            "     | > loss: 1.01941  (1.04687)\n",
            "     | > align_error: 0.31520  (0.32208)\n",
            "     | > grad_norm: 1.15876  (1.71291)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.47020  (1.13697)\n",
            "     | > loader_time: 0.00550  (0.00583)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 181/361 -- GLOBAL_STEP: 22625\u001b[0m\n",
            "     | > decoder_loss: 0.66116  (0.64727)\n",
            "     | > postnet_loss: 0.58135  (0.57969)\n",
            "     | > stopnet_loss: 0.14054  (0.21948)\n",
            "     | > decoder_coarse_loss: 0.82327  (0.81947)\n",
            "     | > decoder_ddc_loss: 0.00971  (0.01248)\n",
            "     | > ga_loss: 0.00018  (0.00046)\n",
            "     | > decoder_diff_spec_loss: 0.30081  (0.29315)\n",
            "     | > postnet_diff_spec_loss: 0.30875  (0.29094)\n",
            "     | > decoder_ssim_loss: 0.34149  (0.32955)\n",
            "     | > postnet_ssim_loss: 0.31896  (0.31081)\n",
            "     | > loss: 0.97784  (1.04260)\n",
            "     | > align_error: 0.33075  (0.32251)\n",
            "     | > grad_norm: 0.58116  (1.60912)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.44560  (1.17217)\n",
            "     | > loader_time: 0.00590  (0.00590)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 206/361 -- GLOBAL_STEP: 22650\u001b[0m\n",
            "     | > decoder_loss: 0.67580  (0.64760)\n",
            "     | > postnet_loss: 0.60390  (0.57967)\n",
            "     | > stopnet_loss: 0.22691  (0.21399)\n",
            "     | > decoder_coarse_loss: 0.84269  (0.81952)\n",
            "     | > decoder_ddc_loss: 0.00845  (0.01208)\n",
            "     | > ga_loss: 0.00016  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.30117  (0.29357)\n",
            "     | > postnet_diff_spec_loss: 0.29225  (0.29104)\n",
            "     | > decoder_ssim_loss: 0.31519  (0.33026)\n",
            "     | > postnet_ssim_loss: 0.29664  (0.31136)\n",
            "     | > loss: 1.06171  (1.03738)\n",
            "     | > align_error: 0.33227  (0.32308)\n",
            "     | > grad_norm: 0.72447  (1.54280)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.51690  (1.20506)\n",
            "     | > loader_time: 0.00580  (0.00596)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 231/361 -- GLOBAL_STEP: 22675\u001b[0m\n",
            "     | > decoder_loss: 0.65940  (0.64879)\n",
            "     | > postnet_loss: 0.59970  (0.58035)\n",
            "     | > stopnet_loss: 0.22966  (0.21209)\n",
            "     | > decoder_coarse_loss: 0.83407  (0.82123)\n",
            "     | > decoder_ddc_loss: 0.00814  (0.01168)\n",
            "     | > ga_loss: 0.00014  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.29884  (0.29425)\n",
            "     | > postnet_diff_spec_loss: 0.28698  (0.29164)\n",
            "     | > decoder_ssim_loss: 0.31366  (0.32999)\n",
            "     | > postnet_ssim_loss: 0.29509  (0.31092)\n",
            "     | > loss: 1.05433  (1.03628)\n",
            "     | > align_error: 0.32505  (0.32378)\n",
            "     | > grad_norm: 1.51467  (1.49558)\n",
            "     | > current_lr: 0.00004 \n",
            "     | > step_time: 1.73930  (1.24510)\n",
            "     | > loader_time: 0.00660  (0.00603)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Emergent/train/24dBtrain_tacotron_ddc.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Continue training of the model from previous checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg-KhJT4eLoo",
        "outputId": "4ef317b0-dd45-49ec-e338-708c4096bb0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "     | > align_error: 0.22849  (0.21336)\n",
            "     | > grad_norm: 0.79598  (0.83312)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.92830  (2.72944)\n",
            "     | > loader_time: 0.00550  (0.00517)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 651/722 -- GLOBAL_STEP: 270050\u001b[0m\n",
            "     | > decoder_loss: 0.30063  (0.28861)\n",
            "     | > postnet_loss: 0.27751  (0.26741)\n",
            "     | > stopnet_loss: 0.07039  (0.14346)\n",
            "     | > decoder_coarse_loss: 0.51617  (0.48974)\n",
            "     | > decoder_ddc_loss: 0.00513  (0.00764)\n",
            "     | > ga_loss: 0.00005  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.20445  (0.20153)\n",
            "     | > postnet_diff_spec_loss: 0.18380  (0.18204)\n",
            "     | > decoder_ssim_loss: 0.23020  (0.22027)\n",
            "     | > postnet_ssim_loss: 0.21925  (0.21045)\n",
            "     | > loss: 0.55494  (0.61146)\n",
            "     | > align_error: 0.23722  (0.21406)\n",
            "     | > grad_norm: 0.95814  (0.83097)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.71830  (2.77799)\n",
            "     | > loader_time: 0.00560  (0.00519)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 676/722 -- GLOBAL_STEP: 270075\u001b[0m\n",
            "     | > decoder_loss: 0.30514  (0.28912)\n",
            "     | > postnet_loss: 0.28054  (0.26780)\n",
            "     | > stopnet_loss: 0.13390  (0.14278)\n",
            "     | > decoder_coarse_loss: 0.52375  (0.49135)\n",
            "     | > decoder_ddc_loss: 0.00461  (0.00753)\n",
            "     | > ga_loss: 0.00004  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.20996  (0.20176)\n",
            "     | > postnet_diff_spec_loss: 0.18762  (0.18218)\n",
            "     | > decoder_ssim_loss: 0.22244  (0.22022)\n",
            "     | > postnet_ssim_loss: 0.21108  (0.21036)\n",
            "     | > loss: 0.62038  (0.61140)\n",
            "     | > align_error: 0.22911  (0.21482)\n",
            "     | > grad_norm: 0.55973  (0.82607)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.67200  (2.83231)\n",
            "     | > loader_time: 0.00560  (0.00521)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 701/722 -- GLOBAL_STEP: 270100\u001b[0m\n",
            "     | > decoder_loss: 0.30249  (0.28956)\n",
            "     | > postnet_loss: 0.27661  (0.26813)\n",
            "     | > stopnet_loss: 0.16480  (0.14265)\n",
            "     | > decoder_coarse_loss: 0.52890  (0.49280)\n",
            "     | > decoder_ddc_loss: 0.00407  (0.00741)\n",
            "     | > ga_loss: 0.00003  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.21156  (0.20199)\n",
            "     | > postnet_diff_spec_loss: 0.18770  (0.18232)\n",
            "     | > decoder_ssim_loss: 0.21836  (0.22004)\n",
            "     | > postnet_ssim_loss: 0.20751  (0.21017)\n",
            "     | > loss: 0.64925  (0.61177)\n",
            "     | > align_error: 0.23878  (0.21566)\n",
            "     | > grad_norm: 0.51929  (0.82008)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 5.40330  (2.89921)\n",
            "     | > loader_time: 0.00660  (0.00525)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.41055  (0.41055)\n",
            "     | > postnet_loss: 0.36927  (0.36927)\n",
            "     | > stopnet_loss: 0.22606  (0.22606)\n",
            "     | > decoder_coarse_loss: 0.63035  (0.63035)\n",
            "     | > decoder_ddc_loss: 0.00837  (0.00837)\n",
            "     | > ga_loss: 0.00062  (0.00062)\n",
            "     | > decoder_diff_spec_loss: 0.18752  (0.18752)\n",
            "     | > postnet_diff_spec_loss: 0.15535  (0.15535)\n",
            "     | > decoder_ssim_loss: 0.21521  (0.21521)\n",
            "     | > postnet_ssim_loss: 0.20082  (0.20082)\n",
            "     | > loss: 0.77352  (0.77352)\n",
            "     | > align_error: 0.36232  (0.36232)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.35585  (0.35585)\n",
            "     | > postnet_loss: 0.31648  (0.31648)\n",
            "     | > stopnet_loss: 0.10741  (0.10741)\n",
            "     | > decoder_coarse_loss: 0.57672  (0.57672)\n",
            "     | > decoder_ddc_loss: 0.00849  (0.00849)\n",
            "     | > ga_loss: 0.00027  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.19353  (0.19353)\n",
            "     | > postnet_diff_spec_loss: 0.15895  (0.15895)\n",
            "     | > decoder_ssim_loss: 0.23542  (0.23542)\n",
            "     | > postnet_ssim_loss: 0.21815  (0.21815)\n",
            "     | > loss: 0.62465  (0.62465)\n",
            "     | > align_error: 0.26379  (0.26379)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.37698  (0.36641)\n",
            "     | > postnet_loss: 0.33728  (0.32688)\n",
            "     | > stopnet_loss: 0.14715  (0.12728)\n",
            "     | > decoder_coarse_loss: 0.59475  (0.58574)\n",
            "     | > decoder_ddc_loss: 0.00788  (0.00819)\n",
            "     | > ga_loss: 0.00020  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.18916  (0.19134)\n",
            "     | > postnet_diff_spec_loss: 0.15566  (0.15730)\n",
            "     | > decoder_ssim_loss: 0.21569  (0.22556)\n",
            "     | > postnet_ssim_loss: 0.20030  (0.20923)\n",
            "     | > loss: 0.66759  (0.64612)\n",
            "     | > align_error: 0.27671  (0.27025)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.39917  (0.37733)\n",
            "     | > postnet_loss: 0.35238  (0.33538)\n",
            "     | > stopnet_loss: 0.13176  (0.12877)\n",
            "     | > decoder_coarse_loss: 0.65691  (0.60946)\n",
            "     | > decoder_ddc_loss: 0.00627  (0.00755)\n",
            "     | > ga_loss: 0.00012  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.20587  (0.19618)\n",
            "     | > postnet_diff_spec_loss: 0.16634  (0.16031)\n",
            "     | > decoder_ssim_loss: 0.22205  (0.22439)\n",
            "     | > postnet_ssim_loss: 0.20528  (0.20791)\n",
            "     | > loss: 0.68592  (0.65939)\n",
            "     | > align_error: 0.27013  (0.27021)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.36440  (0.37410)\n",
            "     | > postnet_loss: 0.31995  (0.33152)\n",
            "     | > stopnet_loss: 0.07607  (0.11560)\n",
            "     | > decoder_coarse_loss: 0.61364  (0.61051)\n",
            "     | > decoder_ddc_loss: 0.00603  (0.00717)\n",
            "     | > ga_loss: 0.00010  (0.00017)\n",
            "     | > decoder_diff_spec_loss: 0.20067  (0.19731)\n",
            "     | > postnet_diff_spec_loss: 0.16273  (0.16092)\n",
            "     | > decoder_ssim_loss: 0.23388  (0.22676)\n",
            "     | > postnet_ssim_loss: 0.21582  (0.20989)\n",
            "     | > loss: 0.60583  (0.64600)\n",
            "     | > align_error: 0.25019  (0.26521)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.36141  (0.37156)\n",
            "     | > postnet_loss: 0.32021  (0.32926)\n",
            "     | > stopnet_loss: 0.14602  (0.12168)\n",
            "     | > decoder_coarse_loss: 0.60285  (0.60897)\n",
            "     | > decoder_ddc_loss: 0.00565  (0.00686)\n",
            "     | > ga_loss: 0.00007  (0.00015)\n",
            "     | > decoder_diff_spec_loss: 0.19043  (0.19593)\n",
            "     | > postnet_diff_spec_loss: 0.15412  (0.15956)\n",
            "     | > decoder_ssim_loss: 0.20926  (0.22326)\n",
            "     | > postnet_ssim_loss: 0.19280  (0.20647)\n",
            "     | > loss: 0.65553  (0.64791)\n",
            "     | > align_error: 0.27249  (0.26666)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.35081  (0.36810)\n",
            "     | > postnet_loss: 0.30759  (0.32565)\n",
            "     | > stopnet_loss: 0.16948  (0.12965)\n",
            "     | > decoder_coarse_loss: 0.59905  (0.60732)\n",
            "     | > decoder_ddc_loss: 0.00392  (0.00637)\n",
            "     | > ga_loss: 0.00004  (0.00013)\n",
            "     | > decoder_diff_spec_loss: 0.19166  (0.19522)\n",
            "     | > postnet_diff_spec_loss: 0.15515  (0.15882)\n",
            "     | > decoder_ssim_loss: 0.20019  (0.21941)\n",
            "     | > postnet_ssim_loss: 0.18378  (0.20269)\n",
            "     | > loss: 0.66772  (0.65121)\n",
            "     | > align_error: 0.30110  (0.27240)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.40760  (0.37375)\n",
            "     | > postnet_loss: 0.35396  (0.32969)\n",
            "     | > stopnet_loss: 0.05438  (0.11890)\n",
            "     | > decoder_coarse_loss: 0.71111  (0.62215)\n",
            "     | > decoder_ddc_loss: 0.00397  (0.00603)\n",
            "     | > ga_loss: 0.00003  (0.00012)\n",
            "     | > decoder_diff_spec_loss: 0.20608  (0.19677)\n",
            "     | > postnet_diff_spec_loss: 0.16574  (0.15981)\n",
            "     | > decoder_ssim_loss: 0.24433  (0.22297)\n",
            "     | > postnet_ssim_loss: 0.22538  (0.20593)\n",
            "     | > loss: 0.63406  (0.64876)\n",
            "     | > align_error: 0.26138  (0.27083)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00542 \u001b[0m(+0.00019)\n",
            "     | > avg_decoder_loss:\u001b[91m 0.37375 \u001b[0m(+0.00239)\n",
            "     | > avg_postnet_loss:\u001b[91m 0.32969 \u001b[0m(+0.00180)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.11890 \u001b[0m(-0.00003)\n",
            "     | > avg_decoder_coarse_loss:\u001b[91m 0.62215 \u001b[0m(+0.00197)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00603 \u001b[0m(-0.00000)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00012 \u001b[0m(-0.00000)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[91m 0.19677 \u001b[0m(+0.00008)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.15981 \u001b[0m(-0.00011)\n",
            "     | > avg_decoder_ssim_loss:\u001b[91m 0.22297 \u001b[0m(+0.00007)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.20593 \u001b[0m(-0.00003)\n",
            "     | > avg_loss:\u001b[91m 0.64876 \u001b[0m(+0.00152)\n",
            "     | > avg_align_error:\u001b[92m 0.27083 \u001b[0m(-0.00094)\n",
            "\n",
            "\n",
            " > Number of output frames: 1\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 14/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2022-01-01 02:04:56) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 3/722 -- GLOBAL_STEP: 270125\u001b[0m\n",
            "     | > decoder_loss: 0.27042  (0.26932)\n",
            "     | > postnet_loss: 0.25236  (0.25055)\n",
            "     | > stopnet_loss: 0.18398  (0.24355)\n",
            "     | > decoder_coarse_loss: 0.48882  (0.46692)\n",
            "     | > decoder_ddc_loss: 0.01635  (0.01557)\n",
            "     | > ga_loss: 0.00143  (0.00158)\n",
            "     | > decoder_diff_spec_loss: 0.18856  (0.19408)\n",
            "     | > postnet_diff_spec_loss: 0.17308  (0.17673)\n",
            "     | > decoder_ssim_loss: 0.22923  (0.22009)\n",
            "     | > postnet_ssim_loss: 0.22059  (0.21178)\n",
            "     | > loss: 0.65099  (0.70272)\n",
            "     | > align_error: 0.20550  (0.19983)\n",
            "     | > grad_norm: 1.68236  (1.56616)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.23880  (1.29409)\n",
            "     | > loader_time: 0.00170  (0.00558)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 28/722 -- GLOBAL_STEP: 270150\u001b[0m\n",
            "     | > decoder_loss: 0.27740  (0.27300)\n",
            "     | > postnet_loss: 0.25875  (0.25465)\n",
            "     | > stopnet_loss: 0.16181  (0.20500)\n",
            "     | > decoder_coarse_loss: 0.45491  (0.45510)\n",
            "     | > decoder_ddc_loss: 0.01424  (0.01399)\n",
            "     | > ga_loss: 0.00060  (0.00096)\n",
            "     | > decoder_diff_spec_loss: 0.19309  (0.19511)\n",
            "     | > postnet_diff_spec_loss: 0.17595  (0.17791)\n",
            "     | > decoder_ssim_loss: 0.23337  (0.22837)\n",
            "     | > postnet_ssim_loss: 0.22384  (0.21947)\n",
            "     | > loss: 0.62270  (0.66421)\n",
            "     | > align_error: 0.18809  (0.18813)\n",
            "     | > grad_norm: 2.01929  (1.61107)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.54630  (1.53194)\n",
            "     | > loader_time: 0.00440  (0.00477)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 53/722 -- GLOBAL_STEP: 270175\u001b[0m\n",
            "     | > decoder_loss: 0.27062  (0.27557)\n",
            "     | > postnet_loss: 0.25274  (0.25692)\n",
            "     | > stopnet_loss: 0.16663  (0.18985)\n",
            "     | > decoder_coarse_loss: 0.44073  (0.46103)\n",
            "     | > decoder_ddc_loss: 0.01091  (0.01268)\n",
            "     | > ga_loss: 0.00049  (0.00076)\n",
            "     | > decoder_diff_spec_loss: 0.18957  (0.19564)\n",
            "     | > postnet_diff_spec_loss: 0.17347  (0.17826)\n",
            "     | > decoder_ssim_loss: 0.22694  (0.22635)\n",
            "     | > postnet_ssim_loss: 0.21777  (0.21735)\n",
            "     | > loss: 0.61475  (0.64958)\n",
            "     | > align_error: 0.19864  (0.19028)\n",
            "     | > grad_norm: 1.12567  (1.32517)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.67730  (1.62035)\n",
            "     | > loader_time: 0.00430  (0.00467)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 78/722 -- GLOBAL_STEP: 270200\u001b[0m\n",
            "     | > decoder_loss: 0.28202  (0.27768)\n",
            "     | > postnet_loss: 0.26171  (0.25872)\n",
            "     | > stopnet_loss: 0.15510  (0.18754)\n",
            "     | > decoder_coarse_loss: 0.50235  (0.46609)\n",
            "     | > decoder_ddc_loss: 0.01045  (0.01199)\n",
            "     | > ga_loss: 0.00039  (0.00066)\n",
            "     | > decoder_diff_spec_loss: 0.20076  (0.19688)\n",
            "     | > postnet_diff_spec_loss: 0.18068  (0.17915)\n",
            "     | > decoder_ssim_loss: 0.21572  (0.22448)\n",
            "     | > postnet_ssim_loss: 0.20621  (0.21541)\n",
            "     | > loss: 0.62204  (0.64846)\n",
            "     | > align_error: 0.19742  (0.19237)\n",
            "     | > grad_norm: 0.54004  (1.25806)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.92830  (1.71232)\n",
            "     | > loader_time: 0.00440  (0.00464)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 103/722 -- GLOBAL_STEP: 270225\u001b[0m\n",
            "     | > decoder_loss: 0.29094  (0.27900)\n",
            "     | > postnet_loss: 0.27077  (0.25980)\n",
            "     | > stopnet_loss: 0.19470  (0.18026)\n",
            "     | > decoder_coarse_loss: 0.46304  (0.46949)\n",
            "     | > decoder_ddc_loss: 0.00952  (0.01153)\n",
            "     | > ga_loss: 0.00030  (0.00059)\n",
            "     | > decoder_diff_spec_loss: 0.20307  (0.19762)\n",
            "     | > postnet_diff_spec_loss: 0.18455  (0.17959)\n",
            "     | > decoder_ssim_loss: 0.21841  (0.22395)\n",
            "     | > postnet_ssim_loss: 0.20978  (0.21481)\n",
            "     | > loss: 0.65873  (0.64215)\n",
            "     | > align_error: 0.21571  (0.19424)\n",
            "     | > grad_norm: 0.67244  (1.18577)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.18710  (1.80047)\n",
            "     | > loader_time: 0.00550  (0.00473)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 128/722 -- GLOBAL_STEP: 270250\u001b[0m\n",
            "     | > decoder_loss: 0.28120  (0.27988)\n",
            "     | > postnet_loss: 0.26130  (0.26054)\n",
            "     | > stopnet_loss: 0.12976  (0.17704)\n",
            "     | > decoder_coarse_loss: 0.50069  (0.47054)\n",
            "     | > decoder_ddc_loss: 0.00959  (0.01112)\n",
            "     | > ga_loss: 0.00032  (0.00054)\n",
            "     | > decoder_diff_spec_loss: 0.19904  (0.19795)\n",
            "     | > postnet_diff_spec_loss: 0.18031  (0.17985)\n",
            "     | > decoder_ssim_loss: 0.22124  (0.22317)\n",
            "     | > postnet_ssim_loss: 0.21168  (0.21401)\n",
            "     | > loss: 0.59763  (0.63898)\n",
            "     | > align_error: 0.20778  (0.19524)\n",
            "     | > grad_norm: 0.63670  (1.10493)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.10670  (1.88113)\n",
            "     | > loader_time: 0.00460  (0.00482)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 153/722 -- GLOBAL_STEP: 270275\u001b[0m\n",
            "     | > decoder_loss: 0.28137  (0.28084)\n",
            "     | > postnet_loss: 0.26164  (0.26135)\n",
            "     | > stopnet_loss: 0.14807  (0.17319)\n",
            "     | > decoder_coarse_loss: 0.47137  (0.47243)\n",
            "     | > decoder_ddc_loss: 0.00850  (0.01077)\n",
            "     | > ga_loss: 0.00024  (0.00049)\n",
            "     | > decoder_diff_spec_loss: 0.19768  (0.19830)\n",
            "     | > postnet_diff_spec_loss: 0.17892  (0.18008)\n",
            "     | > decoder_ssim_loss: 0.22464  (0.22295)\n",
            "     | > postnet_ssim_loss: 0.21468  (0.21370)\n",
            "     | > loss: 0.60898  (0.63576)\n",
            "     | > align_error: 0.21074  (0.19638)\n",
            "     | > grad_norm: 0.68773  (1.07245)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.29800  (1.94934)\n",
            "     | > loader_time: 0.00450  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 178/722 -- GLOBAL_STEP: 270300\u001b[0m\n",
            "     | > decoder_loss: 0.29359  (0.28124)\n",
            "     | > postnet_loss: 0.27356  (0.26170)\n",
            "     | > stopnet_loss: 0.13353  (0.17127)\n",
            "     | > decoder_coarse_loss: 0.49172  (0.47218)\n",
            "     | > decoder_ddc_loss: 0.00867  (0.01047)\n",
            "     | > ga_loss: 0.00022  (0.00046)\n",
            "     | > decoder_diff_spec_loss: 0.20065  (0.19830)\n",
            "     | > postnet_diff_spec_loss: 0.18190  (0.18004)\n",
            "     | > decoder_ssim_loss: 0.22823  (0.22276)\n",
            "     | > postnet_ssim_loss: 0.21826  (0.21347)\n",
            "     | > loss: 0.60877  (0.63361)\n",
            "     | > align_error: 0.20881  (0.19754)\n",
            "     | > grad_norm: 0.84732  (1.07544)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.63580  (2.00840)\n",
            "     | > loader_time: 0.00480  (0.00492)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 203/722 -- GLOBAL_STEP: 270325\u001b[0m\n",
            "     | > decoder_loss: 0.27915  (0.28166)\n",
            "     | > postnet_loss: 0.25982  (0.26202)\n",
            "     | > stopnet_loss: 0.12693  (0.16793)\n",
            "     | > decoder_coarse_loss: 0.45556  (0.47288)\n",
            "     | > decoder_ddc_loss: 0.00865  (0.01023)\n",
            "     | > ga_loss: 0.00023  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.19497  (0.19841)\n",
            "     | > postnet_diff_spec_loss: 0.17785  (0.18008)\n",
            "     | > decoder_ssim_loss: 0.22925  (0.22258)\n",
            "     | > postnet_ssim_loss: 0.21949  (0.21325)\n",
            "     | > loss: 0.58427  (0.63038)\n",
            "     | > align_error: 0.21191  (0.19886)\n",
            "     | > grad_norm: 0.63495  (1.04048)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.49460  (2.05453)\n",
            "     | > loader_time: 0.00540  (0.00492)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 228/722 -- GLOBAL_STEP: 270350\u001b[0m\n",
            "     | > decoder_loss: 0.28690  (0.28219)\n",
            "     | > postnet_loss: 0.26598  (0.26246)\n",
            "     | > stopnet_loss: 0.15859  (0.16557)\n",
            "     | > decoder_coarse_loss: 0.48475  (0.47377)\n",
            "     | > decoder_ddc_loss: 0.00752  (0.01001)\n",
            "     | > ga_loss: 0.00021  (0.00041)\n",
            "     | > decoder_diff_spec_loss: 0.19846  (0.19851)\n",
            "     | > postnet_diff_spec_loss: 0.17983  (0.18015)\n",
            "     | > decoder_ssim_loss: 0.21378  (0.22228)\n",
            "     | > postnet_ssim_loss: 0.20416  (0.21292)\n",
            "     | > loss: 0.61997  (0.62819)\n",
            "     | > align_error: 0.20758  (0.20017)\n",
            "     | > grad_norm: 0.73819  (1.01526)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.53680  (2.10275)\n",
            "     | > loader_time: 0.00460  (0.00492)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 253/722 -- GLOBAL_STEP: 270375\u001b[0m\n",
            "     | > decoder_loss: 0.29482  (0.28289)\n",
            "     | > postnet_loss: 0.27311  (0.26303)\n",
            "     | > stopnet_loss: 0.12202  (0.16307)\n",
            "     | > decoder_coarse_loss: 0.50041  (0.47514)\n",
            "     | > decoder_ddc_loss: 0.00848  (0.00981)\n",
            "     | > ga_loss: 0.00019  (0.00039)\n",
            "     | > decoder_diff_spec_loss: 0.20179  (0.19895)\n",
            "     | > postnet_diff_spec_loss: 0.18171  (0.18047)\n",
            "     | > decoder_ssim_loss: 0.22496  (0.22198)\n",
            "     | > postnet_ssim_loss: 0.21495  (0.21259)\n",
            "     | > loss: 0.59801  (0.62622)\n",
            "     | > align_error: 0.22464  (0.20148)\n",
            "     | > grad_norm: 1.31503  (0.98829)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.53310  (2.14700)\n",
            "     | > loader_time: 0.00460  (0.00498)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 278/722 -- GLOBAL_STEP: 270400\u001b[0m\n",
            "     | > decoder_loss: 0.28972  (0.28341)\n",
            "     | > postnet_loss: 0.26875  (0.26345)\n",
            "     | > stopnet_loss: 0.12498  (0.16042)\n",
            "     | > decoder_coarse_loss: 0.48698  (0.47638)\n",
            "     | > decoder_ddc_loss: 0.00790  (0.00963)\n",
            "     | > ga_loss: 0.00017  (0.00037)\n",
            "     | > decoder_diff_spec_loss: 0.20093  (0.19926)\n",
            "     | > postnet_diff_spec_loss: 0.18297  (0.18067)\n",
            "     | > decoder_ssim_loss: 0.23249  (0.22194)\n",
            "     | > postnet_ssim_loss: 0.22256  (0.21251)\n",
            "     | > loss: 0.59891  (0.62407)\n",
            "     | > align_error: 0.20426  (0.20228)\n",
            "     | > grad_norm: 0.52819  (0.97071)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.62220  (2.18749)\n",
            "     | > loader_time: 0.00470  (0.00499)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 303/722 -- GLOBAL_STEP: 270425\u001b[0m\n",
            "     | > decoder_loss: 0.27632  (0.28358)\n",
            "     | > postnet_loss: 0.25620  (0.26357)\n",
            "     | > stopnet_loss: 0.09824  (0.15904)\n",
            "     | > decoder_coarse_loss: 0.45625  (0.47684)\n",
            "     | > decoder_ddc_loss: 0.00804  (0.00945)\n",
            "     | > ga_loss: 0.00015  (0.00035)\n",
            "     | > decoder_diff_spec_loss: 0.19516  (0.19927)\n",
            "     | > postnet_diff_spec_loss: 0.17657  (0.18065)\n",
            "     | > decoder_ssim_loss: 0.23170  (0.22153)\n",
            "     | > postnet_ssim_loss: 0.22164  (0.21209)\n",
            "     | > loss: 0.55444  (0.62254)\n",
            "     | > align_error: 0.21431  (0.20319)\n",
            "     | > grad_norm: 1.02199  (0.95601)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.55100  (2.23327)\n",
            "     | > loader_time: 0.00460  (0.00502)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 328/722 -- GLOBAL_STEP: 270450\u001b[0m\n",
            "     | > decoder_loss: 0.29486  (0.28402)\n",
            "     | > postnet_loss: 0.27421  (0.26392)\n",
            "     | > stopnet_loss: 0.15115  (0.15718)\n",
            "     | > decoder_coarse_loss: 0.47334  (0.47753)\n",
            "     | > decoder_ddc_loss: 0.00683  (0.00928)\n",
            "     | > ga_loss: 0.00014  (0.00034)\n",
            "     | > decoder_diff_spec_loss: 0.20343  (0.19939)\n",
            "     | > postnet_diff_spec_loss: 0.18413  (0.18071)\n",
            "     | > decoder_ssim_loss: 0.22292  (0.22156)\n",
            "     | > postnet_ssim_loss: 0.21326  (0.21208)\n",
            "     | > loss: 0.62008  (0.62098)\n",
            "     | > align_error: 0.22044  (0.20431)\n",
            "     | > grad_norm: 0.74458  (0.93771)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.86420  (2.27506)\n",
            "     | > loader_time: 0.00580  (0.00505)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 353/722 -- GLOBAL_STEP: 270475\u001b[0m\n",
            "     | > decoder_loss: 0.29702  (0.28436)\n",
            "     | > postnet_loss: 0.27537  (0.26417)\n",
            "     | > stopnet_loss: 0.20014  (0.15617)\n",
            "     | > decoder_coarse_loss: 0.50025  (0.47843)\n",
            "     | > decoder_ddc_loss: 0.00619  (0.00912)\n",
            "     | > ga_loss: 0.00013  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.20680  (0.19954)\n",
            "     | > postnet_diff_spec_loss: 0.18703  (0.18082)\n",
            "     | > decoder_ssim_loss: 0.19451  (0.22123)\n",
            "     | > postnet_ssim_loss: 0.18569  (0.21173)\n",
            "     | > loss: 0.66400  (0.62013)\n",
            "     | > align_error: 0.22943  (0.20546)\n",
            "     | > grad_norm: 1.13356  (0.93456)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.11320  (2.31731)\n",
            "     | > loader_time: 0.00550  (0.00507)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 378/722 -- GLOBAL_STEP: 270500\u001b[0m\n",
            "     | > decoder_loss: 0.28344  (0.28464)\n",
            "     | > postnet_loss: 0.26203  (0.26437)\n",
            "     | > stopnet_loss: 0.15763  (0.15475)\n",
            "     | > decoder_coarse_loss: 0.47438  (0.47890)\n",
            "     | > decoder_ddc_loss: 0.00650  (0.00897)\n",
            "     | > ga_loss: 0.00013  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.20291  (0.19966)\n",
            "     | > postnet_diff_spec_loss: 0.18197  (0.18086)\n",
            "     | > decoder_ssim_loss: 0.21671  (0.22123)\n",
            "     | > postnet_ssim_loss: 0.20685  (0.21170)\n",
            "     | > loss: 0.61697  (0.61888)\n",
            "     | > align_error: 0.22784  (0.20658)\n",
            "     | > grad_norm: 0.70793  (0.93059)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.92450  (2.35892)\n",
            "     | > loader_time: 0.00490  (0.00509)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 403/722 -- GLOBAL_STEP: 270525\u001b[0m\n",
            "     | > decoder_loss: 0.28808  (0.28511)\n",
            "     | > postnet_loss: 0.26662  (0.26475)\n",
            "     | > stopnet_loss: 0.17138  (0.15253)\n",
            "     | > decoder_coarse_loss: 0.48480  (0.48010)\n",
            "     | > decoder_ddc_loss: 0.00609  (0.00883)\n",
            "     | > ga_loss: 0.00013  (0.00030)\n",
            "     | > decoder_diff_spec_loss: 0.20528  (0.19987)\n",
            "     | > postnet_diff_spec_loss: 0.18420  (0.18100)\n",
            "     | > decoder_ssim_loss: 0.20479  (0.22137)\n",
            "     | > postnet_ssim_loss: 0.19491  (0.21180)\n",
            "     | > loss: 0.63073  (0.61723)\n",
            "     | > align_error: 0.21809  (0.20761)\n",
            "     | > grad_norm: 0.78396  (0.91830)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.21750  (2.39560)\n",
            "     | > loader_time: 0.00580  (0.00512)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 428/722 -- GLOBAL_STEP: 270550\u001b[0m\n",
            "     | > decoder_loss: 0.29178  (0.28540)\n",
            "     | > postnet_loss: 0.26958  (0.26495)\n",
            "     | > stopnet_loss: 0.09448  (0.15160)\n",
            "     | > decoder_coarse_loss: 0.48795  (0.48100)\n",
            "     | > decoder_ddc_loss: 0.00659  (0.00869)\n",
            "     | > ga_loss: 0.00011  (0.00029)\n",
            "     | > decoder_diff_spec_loss: 0.20228  (0.20000)\n",
            "     | > postnet_diff_spec_loss: 0.18127  (0.18107)\n",
            "     | > decoder_ssim_loss: 0.23778  (0.22117)\n",
            "     | > postnet_ssim_loss: 0.22690  (0.21159)\n",
            "     | > loss: 0.57106  (0.61650)\n",
            "     | > align_error: 0.22622  (0.20846)\n",
            "     | > grad_norm: 0.72298  (0.90537)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.03600  (2.43843)\n",
            "     | > loader_time: 0.00570  (0.00514)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 453/722 -- GLOBAL_STEP: 270575\u001b[0m\n",
            "     | > decoder_loss: 0.29681  (0.28576)\n",
            "     | > postnet_loss: 0.27529  (0.26523)\n",
            "     | > stopnet_loss: 0.09244  (0.15125)\n",
            "     | > decoder_coarse_loss: 0.50025  (0.48187)\n",
            "     | > decoder_ddc_loss: 0.00642  (0.00856)\n",
            "     | > ga_loss: 0.00010  (0.00028)\n",
            "     | > decoder_diff_spec_loss: 0.20685  (0.20018)\n",
            "     | > postnet_diff_spec_loss: 0.18569  (0.18118)\n",
            "     | > decoder_ssim_loss: 0.23060  (0.22092)\n",
            "     | > postnet_ssim_loss: 0.22024  (0.21131)\n",
            "     | > loss: 0.57348  (0.61639)\n",
            "     | > align_error: 0.22088  (0.20923)\n",
            "     | > grad_norm: 1.23287  (0.90399)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.93770  (2.48316)\n",
            "     | > loader_time: 0.00510  (0.00517)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 478/722 -- GLOBAL_STEP: 270600\u001b[0m\n",
            "     | > decoder_loss: 0.29424  (0.28619)\n",
            "     | > postnet_loss: 0.27153  (0.26557)\n",
            "     | > stopnet_loss: 0.10202  (0.15038)\n",
            "     | > decoder_coarse_loss: 0.50214  (0.48286)\n",
            "     | > decoder_ddc_loss: 0.00598  (0.00843)\n",
            "     | > ga_loss: 0.00009  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.20410  (0.20037)\n",
            "     | > postnet_diff_spec_loss: 0.18346  (0.18131)\n",
            "     | > decoder_ssim_loss: 0.22293  (0.22076)\n",
            "     | > postnet_ssim_loss: 0.21241  (0.21113)\n",
            "     | > loss: 0.57666  (0.61588)\n",
            "     | > align_error: 0.22424  (0.21006)\n",
            "     | > grad_norm: 0.62584  (0.89417)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.21540  (2.52585)\n",
            "     | > loader_time: 0.00460  (0.00518)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 503/722 -- GLOBAL_STEP: 270625\u001b[0m\n",
            "     | > decoder_loss: 0.29867  (0.28649)\n",
            "     | > postnet_loss: 0.27608  (0.26580)\n",
            "     | > stopnet_loss: 0.18045  (0.14964)\n",
            "     | > decoder_coarse_loss: 0.51099  (0.48370)\n",
            "     | > decoder_ddc_loss: 0.00528  (0.00830)\n",
            "     | > ga_loss: 0.00010  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.19941  (0.20056)\n",
            "     | > postnet_diff_spec_loss: 0.18021  (0.18143)\n",
            "     | > decoder_ssim_loss: 0.20546  (0.22043)\n",
            "     | > postnet_ssim_loss: 0.19594  (0.21078)\n",
            "     | > loss: 0.64896  (0.61531)\n",
            "     | > align_error: 0.23148  (0.21082)\n",
            "     | > grad_norm: 0.75403  (0.88075)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.73050  (2.57052)\n",
            "     | > loader_time: 0.00650  (0.00521)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 528/722 -- GLOBAL_STEP: 270650\u001b[0m\n",
            "     | > decoder_loss: 0.29357  (0.28683)\n",
            "     | > postnet_loss: 0.27068  (0.26605)\n",
            "     | > stopnet_loss: 0.14859  (0.14885)\n",
            "     | > decoder_coarse_loss: 0.50282  (0.48480)\n",
            "     | > decoder_ddc_loss: 0.00597  (0.00818)\n",
            "     | > ga_loss: 0.00009  (0.00025)\n",
            "     | > decoder_diff_spec_loss: 0.20272  (0.20073)\n",
            "     | > postnet_diff_spec_loss: 0.18176  (0.18153)\n",
            "     | > decoder_ssim_loss: 0.21428  (0.22035)\n",
            "     | > postnet_ssim_loss: 0.20406  (0.21068)\n",
            "     | > loss: 0.61799  (0.61488)\n",
            "     | > align_error: 0.22496  (0.21140)\n",
            "     | > grad_norm: 0.62767  (0.86839)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.67370  (2.61506)\n",
            "     | > loader_time: 0.00780  (0.00523)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 553/722 -- GLOBAL_STEP: 270675\u001b[0m\n",
            "     | > decoder_loss: 0.29730  (0.28709)\n",
            "     | > postnet_loss: 0.27467  (0.26623)\n",
            "     | > stopnet_loss: 0.12743  (0.14761)\n",
            "     | > decoder_coarse_loss: 0.51660  (0.48571)\n",
            "     | > decoder_ddc_loss: 0.00579  (0.00807)\n",
            "     | > ga_loss: 0.00007  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.21005  (0.20086)\n",
            "     | > postnet_diff_spec_loss: 0.18827  (0.18161)\n",
            "     | > decoder_ssim_loss: 0.21785  (0.22032)\n",
            "     | > postnet_ssim_loss: 0.20690  (0.21061)\n",
            "     | > loss: 0.60716  (0.61395)\n",
            "     | > align_error: 0.22904  (0.21210)\n",
            "     | > grad_norm: 0.52839  (0.85752)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.55000  (2.65777)\n",
            "     | > loader_time: 0.00590  (0.00524)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 578/722 -- GLOBAL_STEP: 270700\u001b[0m\n",
            "     | > decoder_loss: 0.29874  (0.28742)\n",
            "     | > postnet_loss: 0.27542  (0.26647)\n",
            "     | > stopnet_loss: 0.09832  (0.14646)\n",
            "     | > decoder_coarse_loss: 0.50986  (0.48676)\n",
            "     | > decoder_ddc_loss: 0.00561  (0.00796)\n",
            "     | > ga_loss: 0.00006  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.20367  (0.20106)\n",
            "     | > postnet_diff_spec_loss: 0.18303  (0.18173)\n",
            "     | > decoder_ssim_loss: 0.23345  (0.22030)\n",
            "     | > postnet_ssim_loss: 0.22273  (0.21057)\n",
            "     | > loss: 0.58176  (0.61320)\n",
            "     | > align_error: 0.21501  (0.21275)\n",
            "     | > grad_norm: 0.61200  (0.84733)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.83760  (2.70345)\n",
            "     | > loader_time: 0.00580  (0.00527)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 603/722 -- GLOBAL_STEP: 270725\u001b[0m\n",
            "     | > decoder_loss: 0.29221  (0.28768)\n",
            "     | > postnet_loss: 0.27006  (0.26666)\n",
            "     | > stopnet_loss: 0.15152  (0.14509)\n",
            "     | > decoder_coarse_loss: 0.50959  (0.48765)\n",
            "     | > decoder_ddc_loss: 0.00512  (0.00785)\n",
            "     | > ga_loss: 0.00005  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.19727  (0.20118)\n",
            "     | > postnet_diff_spec_loss: 0.17708  (0.18179)\n",
            "     | > decoder_ssim_loss: 0.21919  (0.22034)\n",
            "     | > postnet_ssim_loss: 0.20928  (0.21058)\n",
            "     | > loss: 0.62172  (0.61216)\n",
            "     | > align_error: 0.23626  (0.21340)\n",
            "     | > grad_norm: 0.88763  (0.83848)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.93240  (2.74674)\n",
            "     | > loader_time: 0.00510  (0.00529)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 628/722 -- GLOBAL_STEP: 270750\u001b[0m\n",
            "     | > decoder_loss: 0.30566  (0.28814)\n",
            "     | > postnet_loss: 0.28152  (0.26701)\n",
            "     | > stopnet_loss: 0.11371  (0.14441)\n",
            "     | > decoder_coarse_loss: 0.51766  (0.48875)\n",
            "     | > decoder_ddc_loss: 0.00508  (0.00774)\n",
            "     | > ga_loss: 0.00005  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.21059  (0.20139)\n",
            "     | > postnet_diff_spec_loss: 0.18770  (0.18193)\n",
            "     | > decoder_ssim_loss: 0.23284  (0.22028)\n",
            "     | > postnet_ssim_loss: 0.22161  (0.21049)\n",
            "     | > loss: 0.60465  (0.61194)\n",
            "     | > align_error: 0.22328  (0.21403)\n",
            "     | > grad_norm: 0.95913  (0.83502)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.03360  (2.79631)\n",
            "     | > loader_time: 0.00570  (0.00531)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 653/722 -- GLOBAL_STEP: 270775\u001b[0m\n",
            "     | > decoder_loss: 0.29263  (0.28858)\n",
            "     | > postnet_loss: 0.26952  (0.26736)\n",
            "     | > stopnet_loss: 0.10859  (0.14339)\n",
            "     | > decoder_coarse_loss: 0.52658  (0.49003)\n",
            "     | > decoder_ddc_loss: 0.00499  (0.00763)\n",
            "     | > ga_loss: 0.00006  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.20596  (0.20161)\n",
            "     | > postnet_diff_spec_loss: 0.18464  (0.18207)\n",
            "     | > decoder_ssim_loss: 0.21628  (0.22019)\n",
            "     | > postnet_ssim_loss: 0.20568  (0.21038)\n",
            "     | > loss: 0.58546  (0.61143)\n",
            "     | > align_error: 0.23058  (0.21474)\n",
            "     | > grad_norm: 0.61513  (0.83259)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.15130  (2.84362)\n",
            "     | > loader_time: 0.00710  (0.00533)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 678/722 -- GLOBAL_STEP: 270800\u001b[0m\n",
            "     | > decoder_loss: 0.29663  (0.28905)\n",
            "     | > postnet_loss: 0.27219  (0.26773)\n",
            "     | > stopnet_loss: 0.12575  (0.14264)\n",
            "     | > decoder_coarse_loss: 0.51094  (0.49158)\n",
            "     | > decoder_ddc_loss: 0.00432  (0.00752)\n",
            "     | > ga_loss: 0.00004  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.20590  (0.20183)\n",
            "     | > postnet_diff_spec_loss: 0.18388  (0.18220)\n",
            "     | > decoder_ssim_loss: 0.21384  (0.22016)\n",
            "     | > postnet_ssim_loss: 0.20306  (0.21032)\n",
            "     | > loss: 0.59865  (0.61128)\n",
            "     | > align_error: 0.23450  (0.21546)\n",
            "     | > grad_norm: 0.68365  (0.82781)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.42210  (2.89693)\n",
            "     | > loader_time: 0.01120  (0.00537)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 703/722 -- GLOBAL_STEP: 270825\u001b[0m\n",
            "     | > decoder_loss: 0.29617  (0.28950)\n",
            "     | > postnet_loss: 0.27232  (0.26806)\n",
            "     | > stopnet_loss: 0.09536  (0.14244)\n",
            "     | > decoder_coarse_loss: 0.52802  (0.49310)\n",
            "     | > decoder_ddc_loss: 0.00468  (0.00740)\n",
            "     | > ga_loss: 0.00003  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.20406  (0.20207)\n",
            "     | > postnet_diff_spec_loss: 0.18302  (0.18235)\n",
            "     | > decoder_ssim_loss: 0.22341  (0.22003)\n",
            "     | > postnet_ssim_loss: 0.21251  (0.21016)\n",
            "     | > loss: 0.57655  (0.61161)\n",
            "     | > align_error: 0.24115  (0.21623)\n",
            "     | > grad_norm: 0.63228  (0.81947)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.71180  (2.96164)\n",
            "     | > loader_time: 0.00650  (0.00540)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.40979  (0.40979)\n",
            "     | > postnet_loss: 0.36937  (0.36937)\n",
            "     | > stopnet_loss: 0.22601  (0.22601)\n",
            "     | > decoder_coarse_loss: 0.62383  (0.62383)\n",
            "     | > decoder_ddc_loss: 0.00839  (0.00839)\n",
            "     | > ga_loss: 0.00062  (0.00062)\n",
            "     | > decoder_diff_spec_loss: 0.18606  (0.18606)\n",
            "     | > postnet_diff_spec_loss: 0.15473  (0.15473)\n",
            "     | > decoder_ssim_loss: 0.21493  (0.21493)\n",
            "     | > postnet_ssim_loss: 0.20078  (0.20078)\n",
            "     | > loss: 0.77108  (0.77108)\n",
            "     | > align_error: 0.36332  (0.36332)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.35381  (0.35381)\n",
            "     | > postnet_loss: 0.31487  (0.31487)\n",
            "     | > stopnet_loss: 0.10642  (0.10642)\n",
            "     | > decoder_coarse_loss: 0.57629  (0.57629)\n",
            "     | > decoder_ddc_loss: 0.00850  (0.00850)\n",
            "     | > ga_loss: 0.00027  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.19320  (0.19320)\n",
            "     | > postnet_diff_spec_loss: 0.15882  (0.15882)\n",
            "     | > decoder_ssim_loss: 0.23538  (0.23538)\n",
            "     | > postnet_ssim_loss: 0.21825  (0.21825)\n",
            "     | > loss: 0.62253  (0.62253)\n",
            "     | > align_error: 0.26476  (0.26476)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.36907  (0.36144)\n",
            "     | > postnet_loss: 0.33061  (0.32274)\n",
            "     | > stopnet_loss: 0.14698  (0.12670)\n",
            "     | > decoder_coarse_loss: 0.59376  (0.58503)\n",
            "     | > decoder_ddc_loss: 0.00785  (0.00818)\n",
            "     | > ga_loss: 0.00020  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.18855  (0.19088)\n",
            "     | > postnet_diff_spec_loss: 0.15541  (0.15712)\n",
            "     | > decoder_ssim_loss: 0.21506  (0.22522)\n",
            "     | > postnet_ssim_loss: 0.19995  (0.20910)\n",
            "     | > loss: 0.66307  (0.64280)\n",
            "     | > align_error: 0.27749  (0.27112)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.39478  (0.37255)\n",
            "     | > postnet_loss: 0.34874  (0.33141)\n",
            "     | > stopnet_loss: 0.13213  (0.12851)\n",
            "     | > decoder_coarse_loss: 0.66077  (0.61027)\n",
            "     | > decoder_ddc_loss: 0.00630  (0.00755)\n",
            "     | > ga_loss: 0.00012  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.20443  (0.19540)\n",
            "     | > postnet_diff_spec_loss: 0.16537  (0.15987)\n",
            "     | > decoder_ssim_loss: 0.22152  (0.22399)\n",
            "     | > postnet_ssim_loss: 0.20499  (0.20773)\n",
            "     | > loss: 0.68446  (0.65669)\n",
            "     | > align_error: 0.27158  (0.27128)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.36647  (0.37103)\n",
            "     | > postnet_loss: 0.32179  (0.32900)\n",
            "     | > stopnet_loss: 0.07607  (0.11540)\n",
            "     | > decoder_coarse_loss: 0.62153  (0.61309)\n",
            "     | > decoder_ddc_loss: 0.00602  (0.00717)\n",
            "     | > ga_loss: 0.00010  (0.00017)\n",
            "     | > decoder_diff_spec_loss: 0.20195  (0.19704)\n",
            "     | > postnet_diff_spec_loss: 0.16371  (0.16083)\n",
            "     | > decoder_ssim_loss: 0.23407  (0.22651)\n",
            "     | > postnet_ssim_loss: 0.21612  (0.20983)\n",
            "     | > loss: 0.60947  (0.64488)\n",
            "     | > align_error: 0.25104  (0.26622)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.36072  (0.36897)\n",
            "     | > postnet_loss: 0.31973  (0.32715)\n",
            "     | > stopnet_loss: 0.14587  (0.12149)\n",
            "     | > decoder_coarse_loss: 0.60687  (0.61185)\n",
            "     | > decoder_ddc_loss: 0.00559  (0.00686)\n",
            "     | > ga_loss: 0.00007  (0.00015)\n",
            "     | > decoder_diff_spec_loss: 0.19049  (0.19573)\n",
            "     | > postnet_diff_spec_loss: 0.15437  (0.15953)\n",
            "     | > decoder_ssim_loss: 0.20931  (0.22307)\n",
            "     | > postnet_ssim_loss: 0.19302  (0.20647)\n",
            "     | > loss: 0.65622  (0.64715)\n",
            "     | > align_error: 0.27407  (0.26779)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.35016  (0.36584)\n",
            "     | > postnet_loss: 0.30765  (0.32390)\n",
            "     | > stopnet_loss: 0.16974  (0.12953)\n",
            "     | > decoder_coarse_loss: 0.60350  (0.61045)\n",
            "     | > decoder_ddc_loss: 0.00391  (0.00636)\n",
            "     | > ga_loss: 0.00004  (0.00013)\n",
            "     | > decoder_diff_spec_loss: 0.19102  (0.19494)\n",
            "     | > postnet_diff_spec_loss: 0.15496  (0.15877)\n",
            "     | > decoder_ssim_loss: 0.20007  (0.21924)\n",
            "     | > postnet_ssim_loss: 0.18389  (0.20270)\n",
            "     | > loss: 0.66872  (0.65075)\n",
            "     | > align_error: 0.30227  (0.27353)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.40922  (0.37203)\n",
            "     | > postnet_loss: 0.35575  (0.32845)\n",
            "     | > stopnet_loss: 0.05458  (0.11883)\n",
            "     | > decoder_coarse_loss: 0.70234  (0.62358)\n",
            "     | > decoder_ddc_loss: 0.00396  (0.00602)\n",
            "     | > ga_loss: 0.00003  (0.00012)\n",
            "     | > decoder_diff_spec_loss: 0.20710  (0.19668)\n",
            "     | > postnet_diff_spec_loss: 0.16641  (0.15986)\n",
            "     | > decoder_ssim_loss: 0.24395  (0.22277)\n",
            "     | > postnet_ssim_loss: 0.22520  (0.20592)\n",
            "     | > loss: 0.63320  (0.64824)\n",
            "     | > align_error: 0.26310  (0.27204)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00559 \u001b[0m(+0.00017)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.37203 \u001b[0m(-0.00171)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.32845 \u001b[0m(-0.00124)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.11883 \u001b[0m(-0.00007)\n",
            "     | > avg_decoder_coarse_loss:\u001b[91m 0.62358 \u001b[0m(+0.00143)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00602 \u001b[0m(-0.00001)\n",
            "     | > avg_ga_loss:\u001b[91m 0.00012 \u001b[0m(+0.00000)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.19668 \u001b[0m(-0.00009)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[91m 0.15986 \u001b[0m(+0.00005)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.22277 \u001b[0m(-0.00021)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.20592 \u001b[0m(-0.00001)\n",
            "     | > avg_loss:\u001b[92m 0.64824 \u001b[0m(-0.00052)\n",
            "     | > avg_align_error:\u001b[91m 0.27204 \u001b[0m(+0.00121)\n",
            "\n",
            "\n",
            " > Number of output frames: 1\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 15/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2022-01-01 02:42:11) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/722 -- GLOBAL_STEP: 270850\u001b[0m\n",
            "     | > decoder_loss: 0.26694  (0.26715)\n",
            "     | > postnet_loss: 0.24887  (0.24897)\n",
            "     | > stopnet_loss: 0.17193  (0.23864)\n",
            "     | > decoder_coarse_loss: 0.44945  (0.45362)\n",
            "     | > decoder_ddc_loss: 0.01564  (0.01535)\n",
            "     | > ga_loss: 0.00115  (0.00144)\n",
            "     | > decoder_diff_spec_loss: 0.19414  (0.19257)\n",
            "     | > postnet_diff_spec_loss: 0.17836  (0.17660)\n",
            "     | > decoder_ssim_loss: 0.22657  (0.22162)\n",
            "     | > postnet_ssim_loss: 0.21767  (0.21310)\n",
            "     | > loss: 0.62709  (0.69309)\n",
            "     | > align_error: 0.16739  (0.18808)\n",
            "     | > grad_norm: 1.76157  (1.52520)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.35090  (1.33954)\n",
            "     | > loader_time: 0.00440  (0.00401)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/722 -- GLOBAL_STEP: 270875\u001b[0m\n",
            "     | > decoder_loss: 0.27842  (0.27308)\n",
            "     | > postnet_loss: 0.25940  (0.25488)\n",
            "     | > stopnet_loss: 0.17732  (0.20362)\n",
            "     | > decoder_coarse_loss: 0.49121  (0.45946)\n",
            "     | > decoder_ddc_loss: 0.01084  (0.01372)\n",
            "     | > ga_loss: 0.00058  (0.00094)\n",
            "     | > decoder_diff_spec_loss: 0.19563  (0.19483)\n",
            "     | > postnet_diff_spec_loss: 0.17839  (0.17786)\n",
            "     | > decoder_ssim_loss: 0.21614  (0.22753)\n",
            "     | > postnet_ssim_loss: 0.20772  (0.21867)\n",
            "     | > loss: 0.63964  (0.66333)\n",
            "     | > align_error: 0.19676  (0.18882)\n",
            "     | > grad_norm: 1.31852  (1.52341)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.95540  (1.57878)\n",
            "     | > loader_time: 0.00460  (0.00465)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/722 -- GLOBAL_STEP: 270900\u001b[0m\n",
            "     | > decoder_loss: 0.28812  (0.27518)\n",
            "     | > postnet_loss: 0.26874  (0.25680)\n",
            "     | > stopnet_loss: 0.24102  (0.19008)\n",
            "     | > decoder_coarse_loss: 0.48787  (0.46389)\n",
            "     | > decoder_ddc_loss: 0.00989  (0.01254)\n",
            "     | > ga_loss: 0.00044  (0.00075)\n",
            "     | > decoder_diff_spec_loss: 0.19746  (0.19527)\n",
            "     | > postnet_diff_spec_loss: 0.18083  (0.17814)\n",
            "     | > decoder_ssim_loss: 0.20294  (0.22558)\n",
            "     | > postnet_ssim_loss: 0.19453  (0.21666)\n",
            "     | > loss: 0.70082  (0.64984)\n",
            "     | > align_error: 0.19206  (0.19016)\n",
            "     | > grad_norm: 0.73786  (1.26375)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.01520  (1.67539)\n",
            "     | > loader_time: 0.00460  (0.00469)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/722 -- GLOBAL_STEP: 270925\u001b[0m\n",
            "     | > decoder_loss: 0.29694  (0.27731)\n",
            "     | > postnet_loss: 0.27512  (0.25860)\n",
            "     | > stopnet_loss: 0.22736  (0.18694)\n",
            "     | > decoder_coarse_loss: 0.50347  (0.46711)\n",
            "     | > decoder_ddc_loss: 0.00928  (0.01191)\n",
            "     | > ga_loss: 0.00035  (0.00066)\n",
            "     | > decoder_diff_spec_loss: 0.20582  (0.19606)\n",
            "     | > postnet_diff_spec_loss: 0.18583  (0.17870)\n",
            "     | > decoder_ssim_loss: 0.20803  (0.22412)\n",
            "     | > postnet_ssim_loss: 0.19833  (0.21515)\n",
            "     | > loss: 0.69984  (0.64746)\n",
            "     | > align_error: 0.20617  (0.19234)\n",
            "     | > grad_norm: 1.95411  (1.26875)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.10380  (1.76647)\n",
            "     | > loader_time: 0.00460  (0.00476)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/722 -- GLOBAL_STEP: 270950\u001b[0m\n",
            "     | > decoder_loss: 0.27646  (0.27846)\n",
            "     | > postnet_loss: 0.25674  (0.25944)\n",
            "     | > stopnet_loss: 0.16604  (0.17925)\n",
            "     | > decoder_coarse_loss: 0.45153  (0.46990)\n",
            "     | > decoder_ddc_loss: 0.00918  (0.01145)\n",
            "     | > ga_loss: 0.00032  (0.00058)\n",
            "     | > decoder_diff_spec_loss: 0.19516  (0.19691)\n",
            "     | > postnet_diff_spec_loss: 0.17785  (0.17915)\n",
            "     | > decoder_ssim_loss: 0.21577  (0.22364)\n",
            "     | > postnet_ssim_loss: 0.20680  (0.21457)\n",
            "     | > loss: 0.61500  (0.64055)\n",
            "     | > align_error: 0.20215  (0.19426)\n",
            "     | > grad_norm: 0.54575  (1.16494)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.18440  (1.83672)\n",
            "     | > loader_time: 0.00610  (0.00473)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/722 -- GLOBAL_STEP: 270975\u001b[0m\n",
            "     | > decoder_loss: 0.28049  (0.27954)\n",
            "     | > postnet_loss: 0.26057  (0.26035)\n",
            "     | > stopnet_loss: 0.11665  (0.17582)\n",
            "     | > decoder_coarse_loss: 0.46905  (0.47152)\n",
            "     | > decoder_ddc_loss: 0.00933  (0.01106)\n",
            "     | > ga_loss: 0.00028  (0.00053)\n",
            "     | > decoder_diff_spec_loss: 0.19953  (0.19735)\n",
            "     | > postnet_diff_spec_loss: 0.18069  (0.17947)\n",
            "     | > decoder_ssim_loss: 0.22408  (0.22299)\n",
            "     | > postnet_ssim_loss: 0.21388  (0.21388)\n",
            "     | > loss: 0.57745  (0.63752)\n",
            "     | > align_error: 0.19506  (0.19571)\n",
            "     | > grad_norm: 0.60137  (1.09060)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.10360  (1.90388)\n",
            "     | > loader_time: 0.00440  (0.00476)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 155/722 -- GLOBAL_STEP: 271000\u001b[0m\n",
            "     | > decoder_loss: 0.28150  (0.28059)\n",
            "     | > postnet_loss: 0.26000  (0.26120)\n",
            "     | > stopnet_loss: 0.10608  (0.17248)\n",
            "     | > decoder_coarse_loss: 0.46275  (0.47277)\n",
            "     | > decoder_ddc_loss: 0.00921  (0.01072)\n",
            "     | > ga_loss: 0.00030  (0.00049)\n",
            "     | > decoder_diff_spec_loss: 0.20366  (0.19778)\n",
            "     | > postnet_diff_spec_loss: 0.18172  (0.17974)\n",
            "     | > decoder_ssim_loss: 0.23187  (0.22282)\n",
            "     | > postnet_ssim_loss: 0.22171  (0.21362)\n",
            "     | > loss: 0.57071  (0.63475)\n",
            "     | > align_error: 0.20056  (0.19729)\n",
            "     | > grad_norm: 0.52698  (1.04948)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.03370  (1.96092)\n",
            "     | > loader_time: 0.00510  (0.00483)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/checkpoint_271000.pth.tar\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "\u001b[1m   --> STEP: 180/722 -- GLOBAL_STEP: 271025\u001b[0m\n",
            "     | > decoder_loss: 0.28861  (0.28137)\n",
            "     | > postnet_loss: 0.26814  (0.26186)\n",
            "     | > stopnet_loss: 0.13437  (0.17075)\n",
            "     | > decoder_coarse_loss: 0.46006  (0.47288)\n",
            "     | > decoder_ddc_loss: 0.00857  (0.01043)\n",
            "     | > ga_loss: 0.00022  (0.00046)\n",
            "     | > decoder_diff_spec_loss: 0.19654  (0.19803)\n",
            "     | > postnet_diff_spec_loss: 0.17791  (0.17990)\n",
            "     | > decoder_ssim_loss: 0.23131  (0.22278)\n",
            "     | > postnet_ssim_loss: 0.22109  (0.21353)\n",
            "     | > loss: 0.59852  (0.63324)\n",
            "     | > align_error: 0.20118  (0.19867)\n",
            "     | > grad_norm: 0.89955  (1.04987)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.36390  (2.02115)\n",
            "     | > loader_time: 0.00550  (0.00510)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 205/722 -- GLOBAL_STEP: 271050\u001b[0m\n",
            "     | > decoder_loss: 0.29039  (0.28189)\n",
            "     | > postnet_loss: 0.26947  (0.26227)\n",
            "     | > stopnet_loss: 0.15765  (0.16766)\n",
            "     | > decoder_coarse_loss: 0.49218  (0.47390)\n",
            "     | > decoder_ddc_loss: 0.00772  (0.01019)\n",
            "     | > ga_loss: 0.00018  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.20159  (0.19830)\n",
            "     | > postnet_diff_spec_loss: 0.18176  (0.18009)\n",
            "     | > decoder_ssim_loss: 0.20755  (0.22246)\n",
            "     | > postnet_ssim_loss: 0.19823  (0.21316)\n",
            "     | > loss: 0.62077  (0.63039)\n",
            "     | > align_error: 0.20967  (0.19989)\n",
            "     | > grad_norm: 0.77901  (1.01968)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.77180  (2.06907)\n",
            "     | > loader_time: 0.00510  (0.00511)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/722 -- GLOBAL_STEP: 271075\u001b[0m\n",
            "     | > decoder_loss: 0.28901  (0.28243)\n",
            "     | > postnet_loss: 0.26756  (0.26267)\n",
            "     | > stopnet_loss: 0.12967  (0.16526)\n",
            "     | > decoder_coarse_loss: 0.48257  (0.47510)\n",
            "     | > decoder_ddc_loss: 0.00842  (0.00998)\n",
            "     | > ga_loss: 0.00023  (0.00041)\n",
            "     | > decoder_diff_spec_loss: 0.20284  (0.19855)\n",
            "     | > postnet_diff_spec_loss: 0.18380  (0.18025)\n",
            "     | > decoder_ssim_loss: 0.22191  (0.22226)\n",
            "     | > postnet_ssim_loss: 0.21200  (0.21292)\n",
            "     | > loss: 0.59787  (0.62834)\n",
            "     | > align_error: 0.21573  (0.20115)\n",
            "     | > grad_norm: 0.91860  (1.00510)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.38960  (2.11486)\n",
            "     | > loader_time: 0.00490  (0.00511)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 255/722 -- GLOBAL_STEP: 271100\u001b[0m\n",
            "     | > decoder_loss: 0.28658  (0.28316)\n",
            "     | > postnet_loss: 0.26579  (0.26325)\n",
            "     | > stopnet_loss: 0.09980  (0.16254)\n",
            "     | > decoder_coarse_loss: 0.47201  (0.47657)\n",
            "     | > decoder_ddc_loss: 0.00916  (0.00978)\n",
            "     | > ga_loss: 0.00019  (0.00039)\n",
            "     | > decoder_diff_spec_loss: 0.20449  (0.19905)\n",
            "     | > postnet_diff_spec_loss: 0.18516  (0.18060)\n",
            "     | > decoder_ssim_loss: 0.23066  (0.22206)\n",
            "     | > postnet_ssim_loss: 0.22062  (0.21267)\n",
            "     | > loss: 0.56938  (0.62626)\n",
            "     | > align_error: 0.21550  (0.20236)\n",
            "     | > grad_norm: 1.18627  (0.98338)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.39140  (2.15890)\n",
            "     | > loader_time: 0.00470  (0.00511)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 280/722 -- GLOBAL_STEP: 271125\u001b[0m\n",
            "     | > decoder_loss: 0.29088  (0.28378)\n",
            "     | > postnet_loss: 0.26893  (0.26373)\n",
            "     | > stopnet_loss: 0.18112  (0.16045)\n",
            "     | > decoder_coarse_loss: 0.48788  (0.47734)\n",
            "     | > decoder_ddc_loss: 0.00711  (0.00959)\n",
            "     | > ga_loss: 0.00016  (0.00037)\n",
            "     | > decoder_diff_spec_loss: 0.20392  (0.19947)\n",
            "     | > postnet_diff_spec_loss: 0.18430  (0.18087)\n",
            "     | > decoder_ssim_loss: 0.21519  (0.22198)\n",
            "     | > postnet_ssim_loss: 0.20620  (0.21254)\n",
            "     | > loss: 0.64801  (0.62461)\n",
            "     | > align_error: 0.21516  (0.20335)\n",
            "     | > grad_norm: 0.64716  (0.96826)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.83140  (2.20100)\n",
            "     | > loader_time: 0.00480  (0.00512)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 305/722 -- GLOBAL_STEP: 271150\u001b[0m\n",
            "     | > decoder_loss: 0.29344  (0.28413)\n",
            "     | > postnet_loss: 0.27148  (0.26401)\n",
            "     | > stopnet_loss: 0.14745  (0.15914)\n",
            "     | > decoder_coarse_loss: 0.49918  (0.47805)\n",
            "     | > decoder_ddc_loss: 0.00701  (0.00941)\n",
            "     | > ga_loss: 0.00013  (0.00035)\n",
            "     | > decoder_diff_spec_loss: 0.19854  (0.19954)\n",
            "     | > postnet_diff_spec_loss: 0.18026  (0.18090)\n",
            "     | > decoder_ssim_loss: 0.21742  (0.22164)\n",
            "     | > postnet_ssim_loss: 0.20774  (0.21219)\n",
            "     | > loss: 0.61688  (0.62336)\n",
            "     | > align_error: 0.20965  (0.20443)\n",
            "     | > grad_norm: 0.55285  (0.95118)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.91200  (2.24662)\n",
            "     | > loader_time: 0.00520  (0.00515)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 330/722 -- GLOBAL_STEP: 271175\u001b[0m\n",
            "     | > decoder_loss: 0.28524  (0.28457)\n",
            "     | > postnet_loss: 0.26508  (0.26435)\n",
            "     | > stopnet_loss: 0.11391  (0.15693)\n",
            "     | > decoder_coarse_loss: 0.47141  (0.47888)\n",
            "     | > decoder_ddc_loss: 0.00709  (0.00925)\n",
            "     | > ga_loss: 0.00012  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.19667  (0.19969)\n",
            "     | > postnet_diff_spec_loss: 0.17914  (0.18098)\n",
            "     | > decoder_ssim_loss: 0.21674  (0.22166)\n",
            "     | > postnet_ssim_loss: 0.20735  (0.21218)\n",
            "     | > loss: 0.57168  (0.62150)\n",
            "     | > align_error: 0.20725  (0.20537)\n",
            "     | > grad_norm: 0.58145  (0.93254)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.70340  (2.28767)\n",
            "     | > loader_time: 0.00480  (0.00518)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 355/722 -- GLOBAL_STEP: 271200\u001b[0m\n",
            "     | > decoder_loss: 0.28868  (0.28498)\n",
            "     | > postnet_loss: 0.26762  (0.26466)\n",
            "     | > stopnet_loss: 0.09211  (0.15583)\n",
            "     | > decoder_coarse_loss: 0.47857  (0.47999)\n",
            "     | > decoder_ddc_loss: 0.00752  (0.00909)\n",
            "     | > ga_loss: 0.00012  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.20665  (0.19995)\n",
            "     | > postnet_diff_spec_loss: 0.18561  (0.18115)\n",
            "     | > decoder_ssim_loss: 0.23560  (0.22141)\n",
            "     | > postnet_ssim_loss: 0.22479  (0.21191)\n",
            "     | > loss: 0.56645  (0.62072)\n",
            "     | > align_error: 0.22095  (0.20629)\n",
            "     | > grad_norm: 0.81781  (0.92993)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.73300  (2.32895)\n",
            "     | > loader_time: 0.00520  (0.00518)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 380/722 -- GLOBAL_STEP: 271225\u001b[0m\n",
            "     | > decoder_loss: 0.28849  (0.28527)\n",
            "     | > postnet_loss: 0.26712  (0.26488)\n",
            "     | > stopnet_loss: 0.08913  (0.15420)\n",
            "     | > decoder_coarse_loss: 0.49276  (0.48061)\n",
            "     | > decoder_ddc_loss: 0.00693  (0.00894)\n",
            "     | > ga_loss: 0.00013  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.20141  (0.20009)\n",
            "     | > postnet_diff_spec_loss: 0.18226  (0.18122)\n",
            "     | > decoder_ssim_loss: 0.24048  (0.22146)\n",
            "     | > postnet_ssim_loss: 0.23000  (0.21192)\n",
            "     | > loss: 0.56712  (0.61934)\n",
            "     | > align_error: 0.22299  (0.20716)\n",
            "     | > grad_norm: 0.79620  (0.92750)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.79610  (2.36926)\n",
            "     | > loader_time: 0.00530  (0.00519)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 405/722 -- GLOBAL_STEP: 271250\u001b[0m\n",
            "     | > decoder_loss: 0.29146  (0.28584)\n",
            "     | > postnet_loss: 0.27028  (0.26534)\n",
            "     | > stopnet_loss: 0.12623  (0.15229)\n",
            "     | > decoder_coarse_loss: 0.51078  (0.48184)\n",
            "     | > decoder_ddc_loss: 0.00724  (0.00881)\n",
            "     | > ga_loss: 0.00011  (0.00030)\n",
            "     | > decoder_diff_spec_loss: 0.20293  (0.20036)\n",
            "     | > postnet_diff_spec_loss: 0.18464  (0.18140)\n",
            "     | > decoder_ssim_loss: 0.22056  (0.22158)\n",
            "     | > postnet_ssim_loss: 0.21037  (0.21200)\n",
            "     | > loss: 0.60136  (0.61808)\n",
            "     | > align_error: 0.22132  (0.20806)\n",
            "     | > grad_norm: 0.86168  (0.91578)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.98090  (2.40593)\n",
            "     | > loader_time: 0.00460  (0.00520)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 430/722 -- GLOBAL_STEP: 271275\u001b[0m\n",
            "     | > decoder_loss: 0.29411  (0.28617)\n",
            "     | > postnet_loss: 0.27238  (0.26558)\n",
            "     | > stopnet_loss: 0.11680  (0.15143)\n",
            "     | > decoder_coarse_loss: 0.49534  (0.48288)\n",
            "     | > decoder_ddc_loss: 0.00633  (0.00867)\n",
            "     | > ga_loss: 0.00009  (0.00029)\n",
            "     | > decoder_diff_spec_loss: 0.20353  (0.20051)\n",
            "     | > postnet_diff_spec_loss: 0.18317  (0.18148)\n",
            "     | > decoder_ssim_loss: 0.22385  (0.22138)\n",
            "     | > postnet_ssim_loss: 0.21345  (0.21178)\n",
            "     | > loss: 0.59027  (0.61748)\n",
            "     | > align_error: 0.21610  (0.20889)\n",
            "     | > grad_norm: 0.62433  (0.90438)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.03810  (2.44787)\n",
            "     | > loader_time: 0.00470  (0.00521)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 455/722 -- GLOBAL_STEP: 271300\u001b[0m\n",
            "     | > decoder_loss: 0.28931  (0.28658)\n",
            "     | > postnet_loss: 0.26736  (0.26590)\n",
            "     | > stopnet_loss: 0.12500  (0.15108)\n",
            "     | > decoder_coarse_loss: 0.50737  (0.48403)\n",
            "     | > decoder_ddc_loss: 0.00593  (0.00854)\n",
            "     | > ga_loss: 0.00009  (0.00028)\n",
            "     | > decoder_diff_spec_loss: 0.20539  (0.20073)\n",
            "     | > postnet_diff_spec_loss: 0.18412  (0.18161)\n",
            "     | > decoder_ssim_loss: 0.21859  (0.22115)\n",
            "     | > postnet_ssim_loss: 0.20785  (0.21152)\n",
            "     | > loss: 0.59690  (0.61747)\n",
            "     | > align_error: 0.22071  (0.20961)\n",
            "     | > grad_norm: 1.04819  (0.90427)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.27840  (2.48703)\n",
            "     | > loader_time: 0.00570  (0.00521)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 480/722 -- GLOBAL_STEP: 271325\u001b[0m\n",
            "     | > decoder_loss: 0.28002  (0.28704)\n",
            "     | > postnet_loss: 0.25860  (0.26628)\n",
            "     | > stopnet_loss: 0.11462  (0.15009)\n",
            "     | > decoder_coarse_loss: 0.48021  (0.48503)\n",
            "     | > decoder_ddc_loss: 0.00575  (0.00841)\n",
            "     | > ga_loss: 0.00009  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.20108  (0.20093)\n",
            "     | > postnet_diff_spec_loss: 0.18024  (0.18175)\n",
            "     | > decoder_ssim_loss: 0.21560  (0.22100)\n",
            "     | > postnet_ssim_loss: 0.20537  (0.21135)\n",
            "     | > loss: 0.57178  (0.61687)\n",
            "     | > align_error: 0.21817  (0.21041)\n",
            "     | > grad_norm: 0.87696  (0.89736)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.26510  (2.52542)\n",
            "     | > loader_time: 0.00560  (0.00522)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 505/722 -- GLOBAL_STEP: 271350\u001b[0m\n",
            "     | > decoder_loss: 0.30173  (0.28744)\n",
            "     | > postnet_loss: 0.27818  (0.26656)\n",
            "     | > stopnet_loss: 0.14975  (0.14943)\n",
            "     | > decoder_coarse_loss: 0.52434  (0.48608)\n",
            "     | > decoder_ddc_loss: 0.00531  (0.00828)\n",
            "     | > ga_loss: 0.00008  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.20601  (0.20119)\n",
            "     | > postnet_diff_spec_loss: 0.18529  (0.18192)\n",
            "     | > decoder_ssim_loss: 0.21754  (0.22070)\n",
            "     | > postnet_ssim_loss: 0.20743  (0.21103)\n",
            "     | > loss: 0.63159  (0.61652)\n",
            "     | > align_error: 0.21715  (0.21118)\n",
            "     | > grad_norm: 0.56283  (0.88544)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.68120  (2.56722)\n",
            "     | > loader_time: 0.00470  (0.00523)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 530/722 -- GLOBAL_STEP: 271375\u001b[0m\n",
            "     | > decoder_loss: 0.28500  (0.28778)\n",
            "     | > postnet_loss: 0.26236  (0.26682)\n",
            "     | > stopnet_loss: 0.12218  (0.14864)\n",
            "     | > decoder_coarse_loss: 0.49047  (0.48725)\n",
            "     | > decoder_ddc_loss: 0.00540  (0.00816)\n",
            "     | > ga_loss: 0.00008  (0.00025)\n",
            "     | > decoder_diff_spec_loss: 0.19843  (0.20137)\n",
            "     | > postnet_diff_spec_loss: 0.17804  (0.18203)\n",
            "     | > decoder_ssim_loss: 0.22410  (0.22062)\n",
            "     | > postnet_ssim_loss: 0.21410  (0.21091)\n",
            "     | > loss: 0.58706  (0.61613)\n",
            "     | > align_error: 0.22322  (0.21174)\n",
            "     | > grad_norm: 0.71385  (0.87127)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.50240  (2.61282)\n",
            "     | > loader_time: 0.00600  (0.00525)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 555/722 -- GLOBAL_STEP: 271400\u001b[0m\n",
            "     | > decoder_loss: 0.30118  (0.28808)\n",
            "     | > postnet_loss: 0.27742  (0.26703)\n",
            "     | > stopnet_loss: 0.11319  (0.14733)\n",
            "     | > decoder_coarse_loss: 0.51728  (0.48831)\n",
            "     | > decoder_ddc_loss: 0.00530  (0.00805)\n",
            "     | > ga_loss: 0.00007  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.21519  (0.20158)\n",
            "     | > postnet_diff_spec_loss: 0.19161  (0.18215)\n",
            "     | > decoder_ssim_loss: 0.21448  (0.22057)\n",
            "     | > postnet_ssim_loss: 0.20409  (0.21084)\n",
            "     | > loss: 0.59519  (0.61519)\n",
            "     | > align_error: 0.22844  (0.21248)\n",
            "     | > grad_norm: 0.60143  (0.85969)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.60160  (2.65543)\n",
            "     | > loader_time: 0.00510  (0.00526)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 580/722 -- GLOBAL_STEP: 271425\u001b[0m\n",
            "     | > decoder_loss: 0.29523  (0.28844)\n",
            "     | > postnet_loss: 0.27255  (0.26730)\n",
            "     | > stopnet_loss: 0.10060  (0.14626)\n",
            "     | > decoder_coarse_loss: 0.50240  (0.48935)\n",
            "     | > decoder_ddc_loss: 0.00519  (0.00795)\n",
            "     | > ga_loss: 0.00007  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.19773  (0.20175)\n",
            "     | > postnet_diff_spec_loss: 0.17858  (0.18224)\n",
            "     | > decoder_ssim_loss: 0.22316  (0.22059)\n",
            "     | > postnet_ssim_loss: 0.21277  (0.21083)\n",
            "     | > loss: 0.57284  (0.61454)\n",
            "     | > align_error: 0.23689  (0.21314)\n",
            "     | > grad_norm: 0.78123  (0.85296)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.52520  (2.70095)\n",
            "     | > loader_time: 0.00570  (0.00528)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 605/722 -- GLOBAL_STEP: 271450\u001b[0m\n",
            "     | > decoder_loss: 0.30555  (0.28874)\n",
            "     | > postnet_loss: 0.28118  (0.26751)\n",
            "     | > stopnet_loss: 0.12976  (0.14498)\n",
            "     | > decoder_coarse_loss: 0.51567  (0.49030)\n",
            "     | > decoder_ddc_loss: 0.00537  (0.00784)\n",
            "     | > ga_loss: 0.00006  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.21627  (0.20193)\n",
            "     | > postnet_diff_spec_loss: 0.19273  (0.18234)\n",
            "     | > decoder_ssim_loss: 0.21706  (0.22061)\n",
            "     | > postnet_ssim_loss: 0.20670  (0.21081)\n",
            "     | > loss: 0.61520  (0.61364)\n",
            "     | > align_error: 0.23394  (0.21386)\n",
            "     | > grad_norm: 0.49906  (0.84632)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.83280  (2.74359)\n",
            "     | > loader_time: 0.00590  (0.00529)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 630/722 -- GLOBAL_STEP: 271475\u001b[0m\n",
            "     | > decoder_loss: 0.29270  (0.28917)\n",
            "     | > postnet_loss: 0.26960  (0.26783)\n",
            "     | > stopnet_loss: 0.16718  (0.14434)\n",
            "     | > decoder_coarse_loss: 0.50748  (0.49146)\n",
            "     | > decoder_ddc_loss: 0.00457  (0.00773)\n",
            "     | > ga_loss: 0.00006  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.20451  (0.20217)\n",
            "     | > postnet_diff_spec_loss: 0.18384  (0.18248)\n",
            "     | > decoder_ssim_loss: 0.20457  (0.22055)\n",
            "     | > postnet_ssim_loss: 0.19465  (0.21073)\n",
            "     | > loss: 0.63295  (0.61347)\n",
            "     | > align_error: 0.22813  (0.21449)\n",
            "     | > grad_norm: 0.57077  (0.84112)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.10020  (2.79212)\n",
            "     | > loader_time: 0.00590  (0.00532)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 655/722 -- GLOBAL_STEP: 271500\u001b[0m\n",
            "     | > decoder_loss: 0.30170  (0.28965)\n",
            "     | > postnet_loss: 0.27832  (0.26821)\n",
            "     | > stopnet_loss: 0.12665  (0.14321)\n",
            "     | > decoder_coarse_loss: 0.53418  (0.49282)\n",
            "     | > decoder_ddc_loss: 0.00447  (0.00762)\n",
            "     | > ga_loss: 0.00005  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.20301  (0.20242)\n",
            "     | > postnet_diff_spec_loss: 0.18211  (0.18265)\n",
            "     | > decoder_ssim_loss: 0.21984  (0.22050)\n",
            "     | > postnet_ssim_loss: 0.20916  (0.21064)\n",
            "     | > loss: 0.61011  (0.61291)\n",
            "     | > align_error: 0.23908  (0.21517)\n",
            "     | > grad_norm: 0.80402  (0.83798)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.12240  (2.83615)\n",
            "     | > loader_time: 0.00550  (0.00533)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 680/722 -- GLOBAL_STEP: 271525\u001b[0m\n",
            "     | > decoder_loss: 0.29666  (0.29011)\n",
            "     | > postnet_loss: 0.27201  (0.26856)\n",
            "     | > stopnet_loss: 0.20069  (0.14278)\n",
            "     | > decoder_coarse_loss: 0.54012  (0.49437)\n",
            "     | > decoder_ddc_loss: 0.00387  (0.00751)\n",
            "     | > ga_loss: 0.00004  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.20212  (0.20264)\n",
            "     | > postnet_diff_spec_loss: 0.18113  (0.18278)\n",
            "     | > decoder_ssim_loss: 0.20066  (0.22040)\n",
            "     | > postnet_ssim_loss: 0.19053  (0.21052)\n",
            "     | > loss: 0.67267  (0.61304)\n",
            "     | > align_error: 0.23967  (0.21596)\n",
            "     | > grad_norm: 0.53533  (0.83231)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.70030  (2.88662)\n",
            "     | > loader_time: 0.00680  (0.00535)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 705/722 -- GLOBAL_STEP: 271550\u001b[0m\n",
            "     | > decoder_loss: 0.30234  (0.29058)\n",
            "     | > postnet_loss: 0.27768  (0.26891)\n",
            "     | > stopnet_loss: 0.27276  (0.14250)\n",
            "     | > decoder_coarse_loss: 0.54563  (0.49591)\n",
            "     | > decoder_ddc_loss: 0.00336  (0.00739)\n",
            "     | > ga_loss: 0.00014  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.20712  (0.20288)\n",
            "     | > postnet_diff_spec_loss: 0.18556  (0.18292)\n",
            "     | > decoder_ssim_loss: 0.18896  (0.22029)\n",
            "     | > postnet_ssim_loss: 0.17992  (0.21038)\n",
            "     | > loss: 0.74611  (0.61333)\n",
            "     | > align_error: 0.25853  (0.21685)\n",
            "     | > grad_norm: 0.49300  (0.82557)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 5.63470  (2.94693)\n",
            "     | > loader_time: 0.00560  (0.00537)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.41093  (0.41093)\n",
            "     | > postnet_loss: 0.37092  (0.37092)\n",
            "     | > stopnet_loss: 0.22632  (0.22632)\n",
            "     | > decoder_coarse_loss: 0.62760  (0.62760)\n",
            "     | > decoder_ddc_loss: 0.00838  (0.00838)\n",
            "     | > ga_loss: 0.00062  (0.00062)\n",
            "     | > decoder_diff_spec_loss: 0.18644  (0.18644)\n",
            "     | > postnet_diff_spec_loss: 0.15463  (0.15463)\n",
            "     | > decoder_ssim_loss: 0.21528  (0.21528)\n",
            "     | > postnet_ssim_loss: 0.20086  (0.20086)\n",
            "     | > loss: 0.77318  (0.77318)\n",
            "     | > align_error: 0.36493  (0.36493)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.35131  (0.35131)\n",
            "     | > postnet_loss: 0.31266  (0.31266)\n",
            "     | > stopnet_loss: 0.10633  (0.10633)\n",
            "     | > decoder_coarse_loss: 0.57505  (0.57505)\n",
            "     | > decoder_ddc_loss: 0.00849  (0.00849)\n",
            "     | > ga_loss: 0.00027  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.19274  (0.19274)\n",
            "     | > postnet_diff_spec_loss: 0.15824  (0.15824)\n",
            "     | > decoder_ssim_loss: 0.23543  (0.23543)\n",
            "     | > postnet_ssim_loss: 0.21808  (0.21808)\n",
            "     | > loss: 0.62067  (0.62067)\n",
            "     | > align_error: 0.26729  (0.26729)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.37709  (0.36420)\n",
            "     | > postnet_loss: 0.33802  (0.32534)\n",
            "     | > stopnet_loss: 0.14715  (0.12674)\n",
            "     | > decoder_coarse_loss: 0.59102  (0.58304)\n",
            "     | > decoder_ddc_loss: 0.00781  (0.00815)\n",
            "     | > ga_loss: 0.00020  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.18997  (0.19135)\n",
            "     | > postnet_diff_spec_loss: 0.15614  (0.15719)\n",
            "     | > decoder_ssim_loss: 0.21609  (0.22576)\n",
            "     | > postnet_ssim_loss: 0.20060  (0.20934)\n",
            "     | > loss: 0.66736  (0.64401)\n",
            "     | > align_error: 0.27962  (0.27346)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.39563  (0.37468)\n",
            "     | > postnet_loss: 0.34948  (0.33339)\n",
            "     | > stopnet_loss: 0.13187  (0.12845)\n",
            "     | > decoder_coarse_loss: 0.65436  (0.60681)\n",
            "     | > decoder_ddc_loss: 0.00630  (0.00754)\n",
            "     | > ga_loss: 0.00012  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.20439  (0.19570)\n",
            "     | > postnet_diff_spec_loss: 0.16508  (0.15982)\n",
            "     | > decoder_ssim_loss: 0.22187  (0.22446)\n",
            "     | > postnet_ssim_loss: 0.20500  (0.20789)\n",
            "     | > loss: 0.68300  (0.65701)\n",
            "     | > align_error: 0.27331  (0.27341)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.36374  (0.37195)\n",
            "     | > postnet_loss: 0.31953  (0.32992)\n",
            "     | > stopnet_loss: 0.07586  (0.11530)\n",
            "     | > decoder_coarse_loss: 0.61801  (0.60961)\n",
            "     | > decoder_ddc_loss: 0.00601  (0.00715)\n",
            "     | > ga_loss: 0.00010  (0.00017)\n",
            "     | > decoder_diff_spec_loss: 0.20077  (0.19697)\n",
            "     | > postnet_diff_spec_loss: 0.16263  (0.16052)\n",
            "     | > decoder_ssim_loss: 0.23405  (0.22686)\n",
            "     | > postnet_ssim_loss: 0.21585  (0.20988)\n",
            "     | > loss: 0.60649  (0.64438)\n",
            "     | > align_error: 0.25399  (0.26855)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.36294  (0.37015)\n",
            "     | > postnet_loss: 0.32198  (0.32833)\n",
            "     | > stopnet_loss: 0.14598  (0.12144)\n",
            "     | > decoder_coarse_loss: 0.60354  (0.60839)\n",
            "     | > decoder_ddc_loss: 0.00559  (0.00684)\n",
            "     | > ga_loss: 0.00007  (0.00015)\n",
            "     | > decoder_diff_spec_loss: 0.19168  (0.19591)\n",
            "     | > postnet_diff_spec_loss: 0.15489  (0.15940)\n",
            "     | > decoder_ssim_loss: 0.20965  (0.22342)\n",
            "     | > postnet_ssim_loss: 0.19306  (0.20652)\n",
            "     | > loss: 0.65714  (0.64693)\n",
            "     | > align_error: 0.27566  (0.26997)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.34983  (0.36676)\n",
            "     | > postnet_loss: 0.30712  (0.32480)\n",
            "     | > stopnet_loss: 0.16968  (0.12948)\n",
            "     | > decoder_coarse_loss: 0.60208  (0.60734)\n",
            "     | > decoder_ddc_loss: 0.00391  (0.00635)\n",
            "     | > ga_loss: 0.00004  (0.00013)\n",
            "     | > decoder_diff_spec_loss: 0.19122  (0.19513)\n",
            "     | > postnet_diff_spec_loss: 0.15486  (0.15864)\n",
            "     | > decoder_ssim_loss: 0.20030  (0.21957)\n",
            "     | > postnet_ssim_loss: 0.18383  (0.20274)\n",
            "     | > loss: 0.66817  (0.65047)\n",
            "     | > align_error: 0.30419  (0.27568)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.40540  (0.37228)\n",
            "     | > postnet_loss: 0.35202  (0.32869)\n",
            "     | > stopnet_loss: 0.05435  (0.11875)\n",
            "     | > decoder_coarse_loss: 0.70934  (0.62191)\n",
            "     | > decoder_ddc_loss: 0.00395  (0.00601)\n",
            "     | > ga_loss: 0.00003  (0.00012)\n",
            "     | > decoder_diff_spec_loss: 0.20509  (0.19655)\n",
            "     | > postnet_diff_spec_loss: 0.16508  (0.15956)\n",
            "     | > decoder_ssim_loss: 0.24385  (0.22303)\n",
            "     | > postnet_ssim_loss: 0.22487  (0.20590)\n",
            "     | > loss: 0.63190  (0.64782)\n",
            "     | > align_error: 0.26535  (0.27420)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00521 \u001b[0m(-0.00038)\n",
            "     | > avg_decoder_loss:\u001b[91m 0.37228 \u001b[0m(+0.00025)\n",
            "     | > avg_postnet_loss:\u001b[91m 0.32869 \u001b[0m(+0.00024)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.11875 \u001b[0m(-0.00008)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.62191 \u001b[0m(-0.00167)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00601 \u001b[0m(-0.00001)\n",
            "     | > avg_ga_loss:\u001b[91m 0.00012 \u001b[0m(+0.00000)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.19655 \u001b[0m(-0.00013)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.15956 \u001b[0m(-0.00030)\n",
            "     | > avg_decoder_ssim_loss:\u001b[91m 0.22303 \u001b[0m(+0.00027)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.20590 \u001b[0m(-0.00002)\n",
            "     | > avg_loss:\u001b[92m 0.64782 \u001b[0m(-0.00042)\n",
            "     | > avg_align_error:\u001b[91m 0.27420 \u001b[0m(+0.00216)\n",
            "\n",
            "\n",
            " > Number of output frames: 1\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 16/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2022-01-01 03:19:12) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 7/722 -- GLOBAL_STEP: 271575\u001b[0m\n",
            "     | > decoder_loss: 0.28066  (0.27242)\n",
            "     | > postnet_loss: 0.26124  (0.25343)\n",
            "     | > stopnet_loss: 0.17921  (0.21378)\n",
            "     | > decoder_coarse_loss: 0.50275  (0.47164)\n",
            "     | > decoder_ddc_loss: 0.01630  (0.01562)\n",
            "     | > ga_loss: 0.00121  (0.00135)\n",
            "     | > decoder_diff_spec_loss: 0.19976  (0.19771)\n",
            "     | > postnet_diff_spec_loss: 0.18193  (0.17993)\n",
            "     | > decoder_ssim_loss: 0.22314  (0.22454)\n",
            "     | > postnet_ssim_loss: 0.21433  (0.21597)\n",
            "     | > loss: 0.65531  (0.67835)\n",
            "     | > align_error: 0.17676  (0.18784)\n",
            "     | > grad_norm: 1.20882  (1.27699)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.38080  (1.32780)\n",
            "     | > loader_time: 0.00510  (0.00430)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 32/722 -- GLOBAL_STEP: 271600\u001b[0m\n",
            "     | > decoder_loss: 0.28213  (0.27663)\n",
            "     | > postnet_loss: 0.26231  (0.25786)\n",
            "     | > stopnet_loss: 0.18367  (0.20277)\n",
            "     | > decoder_coarse_loss: 0.43699  (0.46609)\n",
            "     | > decoder_ddc_loss: 0.01138  (0.01361)\n",
            "     | > ga_loss: 0.00057  (0.00092)\n",
            "     | > decoder_diff_spec_loss: 0.20032  (0.19693)\n",
            "     | > postnet_diff_spec_loss: 0.18115  (0.17929)\n",
            "     | > decoder_ssim_loss: 0.22999  (0.22773)\n",
            "     | > postnet_ssim_loss: 0.22045  (0.21884)\n",
            "     | > loss: 0.64268  (0.66660)\n",
            "     | > align_error: 0.18887  (0.19128)\n",
            "     | > grad_norm: 0.77356  (1.50020)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.68180  (1.52920)\n",
            "     | > loader_time: 0.00500  (0.00481)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 57/722 -- GLOBAL_STEP: 271625\u001b[0m\n",
            "     | > decoder_loss: 0.29390  (0.27765)\n",
            "     | > postnet_loss: 0.27388  (0.25876)\n",
            "     | > stopnet_loss: 0.15420  (0.18964)\n",
            "     | > decoder_coarse_loss: 0.51637  (0.46924)\n",
            "     | > decoder_ddc_loss: 0.01188  (0.01250)\n",
            "     | > ga_loss: 0.00057  (0.00074)\n",
            "     | > decoder_diff_spec_loss: 0.20086  (0.19711)\n",
            "     | > postnet_diff_spec_loss: 0.18240  (0.17935)\n",
            "     | > decoder_ssim_loss: 0.23387  (0.22624)\n",
            "     | > postnet_ssim_loss: 0.22469  (0.21725)\n",
            "     | > loss: 0.64153  (0.65287)\n",
            "     | > align_error: 0.19290  (0.19199)\n",
            "     | > grad_norm: 0.93552  (1.27994)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.75180  (1.62461)\n",
            "     | > loader_time: 0.00430  (0.00485)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 82/722 -- GLOBAL_STEP: 271650\u001b[0m\n",
            "     | > decoder_loss: 0.27892  (0.27937)\n",
            "     | > postnet_loss: 0.25974  (0.26016)\n",
            "     | > stopnet_loss: 0.15044  (0.18593)\n",
            "     | > decoder_coarse_loss: 0.44747  (0.47151)\n",
            "     | > decoder_ddc_loss: 0.01007  (0.01189)\n",
            "     | > ga_loss: 0.00041  (0.00065)\n",
            "     | > decoder_diff_spec_loss: 0.19432  (0.19793)\n",
            "     | > postnet_diff_spec_loss: 0.17750  (0.17993)\n",
            "     | > decoder_ssim_loss: 0.23261  (0.22486)\n",
            "     | > postnet_ssim_loss: 0.22284  (0.21576)\n",
            "     | > loss: 0.60837  (0.64953)\n",
            "     | > align_error: 0.19799  (0.19380)\n",
            "     | > grad_norm: 0.79381  (1.25234)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.96460  (1.71730)\n",
            "     | > loader_time: 0.00400  (0.00480)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 107/722 -- GLOBAL_STEP: 271675\u001b[0m\n",
            "     | > decoder_loss: 0.29033  (0.28076)\n",
            "     | > postnet_loss: 0.26901  (0.26128)\n",
            "     | > stopnet_loss: 0.13570  (0.17872)\n",
            "     | > decoder_coarse_loss: 0.50431  (0.47425)\n",
            "     | > decoder_ddc_loss: 0.00957  (0.01144)\n",
            "     | > ga_loss: 0.00028  (0.00058)\n",
            "     | > decoder_diff_spec_loss: 0.20959  (0.19884)\n",
            "     | > postnet_diff_spec_loss: 0.18816  (0.18052)\n",
            "     | > decoder_ssim_loss: 0.21949  (0.22437)\n",
            "     | > postnet_ssim_loss: 0.20996  (0.21518)\n",
            "     | > loss: 0.61220  (0.64327)\n",
            "     | > align_error: 0.19632  (0.19531)\n",
            "     | > grad_norm: 0.87896  (1.17004)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.97780  (1.78437)\n",
            "     | > loader_time: 0.00400  (0.00470)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 132/722 -- GLOBAL_STEP: 271700\u001b[0m\n",
            "     | > decoder_loss: 0.28515  (0.28169)\n",
            "     | > postnet_loss: 0.26445  (0.26204)\n",
            "     | > stopnet_loss: 0.11794  (0.17546)\n",
            "     | > decoder_coarse_loss: 0.46819  (0.47572)\n",
            "     | > decoder_ddc_loss: 0.00908  (0.01104)\n",
            "     | > ga_loss: 0.00028  (0.00053)\n",
            "     | > decoder_diff_spec_loss: 0.19742  (0.19910)\n",
            "     | > postnet_diff_spec_loss: 0.17884  (0.18072)\n",
            "     | > decoder_ssim_loss: 0.24454  (0.22364)\n",
            "     | > postnet_ssim_loss: 0.23374  (0.21440)\n",
            "     | > loss: 0.58970  (0.64019)\n",
            "     | > align_error: 0.20583  (0.19635)\n",
            "     | > grad_norm: 0.81701  (1.11057)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.09980  (1.85258)\n",
            "     | > loader_time: 0.00480  (0.00468)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 157/722 -- GLOBAL_STEP: 271725\u001b[0m\n",
            "     | > decoder_loss: 0.27993  (0.28271)\n",
            "     | > postnet_loss: 0.26091  (0.26291)\n",
            "     | > stopnet_loss: 0.08162  (0.17216)\n",
            "     | > decoder_coarse_loss: 0.45974  (0.47695)\n",
            "     | > decoder_ddc_loss: 0.00934  (0.01071)\n",
            "     | > ga_loss: 0.00026  (0.00049)\n",
            "     | > decoder_diff_spec_loss: 0.19845  (0.19935)\n",
            "     | > postnet_diff_spec_loss: 0.18033  (0.18087)\n",
            "     | > decoder_ssim_loss: 0.25087  (0.22348)\n",
            "     | > postnet_ssim_loss: 0.24029  (0.21418)\n",
            "     | > loss: 0.55287  (0.63739)\n",
            "     | > align_error: 0.21112  (0.19765)\n",
            "     | > grad_norm: 0.53761  (1.06905)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.97290  (1.90416)\n",
            "     | > loader_time: 0.00450  (0.00467)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 182/722 -- GLOBAL_STEP: 271750\u001b[0m\n",
            "     | > decoder_loss: 0.29337  (0.28339)\n",
            "     | > postnet_loss: 0.27281  (0.26349)\n",
            "     | > stopnet_loss: 0.12008  (0.17022)\n",
            "     | > decoder_coarse_loss: 0.48100  (0.47665)\n",
            "     | > decoder_ddc_loss: 0.00892  (0.01042)\n",
            "     | > ga_loss: 0.00021  (0.00046)\n",
            "     | > decoder_diff_spec_loss: 0.19738  (0.19948)\n",
            "     | > postnet_diff_spec_loss: 0.18048  (0.18093)\n",
            "     | > decoder_ssim_loss: 0.22698  (0.22339)\n",
            "     | > postnet_ssim_loss: 0.21690  (0.21404)\n",
            "     | > loss: 0.59059  (0.63545)\n",
            "     | > align_error: 0.20943  (0.19902)\n",
            "     | > grad_norm: 1.46178  (1.07565)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.36280  (1.96200)\n",
            "     | > loader_time: 0.00580  (0.00478)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 207/722 -- GLOBAL_STEP: 271775\u001b[0m\n",
            "     | > decoder_loss: 0.28729  (0.28370)\n",
            "     | > postnet_loss: 0.26534  (0.26371)\n",
            "     | > stopnet_loss: 0.11941  (0.16722)\n",
            "     | > decoder_coarse_loss: 0.48610  (0.47728)\n",
            "     | > decoder_ddc_loss: 0.00828  (0.01018)\n",
            "     | > ga_loss: 0.00025  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.20254  (0.19965)\n",
            "     | > postnet_diff_spec_loss: 0.18201  (0.18105)\n",
            "     | > decoder_ssim_loss: 0.21714  (0.22282)\n",
            "     | > postnet_ssim_loss: 0.20738  (0.21344)\n",
            "     | > loss: 0.58466  (0.63233)\n",
            "     | > align_error: 0.21038  (0.20010)\n",
            "     | > grad_norm: 0.94659  (1.03890)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.22080  (2.00565)\n",
            "     | > loader_time: 0.00540  (0.00481)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 232/722 -- GLOBAL_STEP: 271800\u001b[0m\n",
            "     | > decoder_loss: 0.28757  (0.28412)\n",
            "     | > postnet_loss: 0.26799  (0.26405)\n",
            "     | > stopnet_loss: 0.09799  (0.16486)\n",
            "     | > decoder_coarse_loss: 0.48422  (0.47787)\n",
            "     | > decoder_ddc_loss: 0.00827  (0.00997)\n",
            "     | > ga_loss: 0.00023  (0.00041)\n",
            "     | > decoder_diff_spec_loss: 0.20030  (0.19978)\n",
            "     | > postnet_diff_spec_loss: 0.18209  (0.18111)\n",
            "     | > decoder_ssim_loss: 0.22675  (0.22260)\n",
            "     | > postnet_ssim_loss: 0.21718  (0.21319)\n",
            "     | > loss: 0.56773  (0.63006)\n",
            "     | > align_error: 0.22201  (0.20133)\n",
            "     | > grad_norm: 1.19458  (1.01735)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.27030  (2.05238)\n",
            "     | > loader_time: 0.00520  (0.00481)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 257/722 -- GLOBAL_STEP: 271825\u001b[0m\n",
            "     | > decoder_loss: 0.28429  (0.28471)\n",
            "     | > postnet_loss: 0.26368  (0.26452)\n",
            "     | > stopnet_loss: 0.17455  (0.16236)\n",
            "     | > decoder_coarse_loss: 0.47730  (0.47891)\n",
            "     | > decoder_ddc_loss: 0.00722  (0.00977)\n",
            "     | > ga_loss: 0.00022  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.20379  (0.20012)\n",
            "     | > postnet_diff_spec_loss: 0.18426  (0.18134)\n",
            "     | > decoder_ssim_loss: 0.20669  (0.22235)\n",
            "     | > postnet_ssim_loss: 0.19744  (0.21289)\n",
            "     | > loss: 0.63182  (0.62794)\n",
            "     | > align_error: 0.20199  (0.20251)\n",
            "     | > grad_norm: 0.59619  (0.99272)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.60790  (2.09492)\n",
            "     | > loader_time: 0.00540  (0.00484)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 282/722 -- GLOBAL_STEP: 271850\u001b[0m\n",
            "     | > decoder_loss: 0.28879  (0.28515)\n",
            "     | > postnet_loss: 0.26820  (0.26485)\n",
            "     | > stopnet_loss: 0.12398  (0.16022)\n",
            "     | > decoder_coarse_loss: 0.48855  (0.47967)\n",
            "     | > decoder_ddc_loss: 0.00722  (0.00959)\n",
            "     | > ga_loss: 0.00016  (0.00037)\n",
            "     | > decoder_diff_spec_loss: 0.20304  (0.20040)\n",
            "     | > postnet_diff_spec_loss: 0.18333  (0.18151)\n",
            "     | > decoder_ssim_loss: 0.22425  (0.22225)\n",
            "     | > postnet_ssim_loss: 0.21447  (0.21276)\n",
            "     | > loss: 0.59423  (0.62609)\n",
            "     | > align_error: 0.20585  (0.20338)\n",
            "     | > grad_norm: 0.78280  (0.97146)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.60570  (2.13704)\n",
            "     | > loader_time: 0.00460  (0.00487)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 307/722 -- GLOBAL_STEP: 271875\u001b[0m\n",
            "     | > decoder_loss: 0.29509  (0.28533)\n",
            "     | > postnet_loss: 0.27201  (0.26498)\n",
            "     | > stopnet_loss: 0.13451  (0.15881)\n",
            "     | > decoder_coarse_loss: 0.49800  (0.48019)\n",
            "     | > decoder_ddc_loss: 0.00706  (0.00940)\n",
            "     | > ga_loss: 0.00015  (0.00035)\n",
            "     | > decoder_diff_spec_loss: 0.20703  (0.20040)\n",
            "     | > postnet_diff_spec_loss: 0.18583  (0.18147)\n",
            "     | > decoder_ssim_loss: 0.22558  (0.22189)\n",
            "     | > postnet_ssim_loss: 0.21520  (0.21239)\n",
            "     | > loss: 0.61171  (0.62457)\n",
            "     | > align_error: 0.22397  (0.20440)\n",
            "     | > grad_norm: 0.77140  (0.95163)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.73100  (2.18062)\n",
            "     | > loader_time: 0.00520  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 332/722 -- GLOBAL_STEP: 271900\u001b[0m\n",
            "     | > decoder_loss: 0.29668  (0.28560)\n",
            "     | > postnet_loss: 0.27384  (0.26518)\n",
            "     | > stopnet_loss: 0.12036  (0.15678)\n",
            "     | > decoder_coarse_loss: 0.48830  (0.48091)\n",
            "     | > decoder_ddc_loss: 0.00741  (0.00924)\n",
            "     | > ga_loss: 0.00014  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.20961  (0.20052)\n",
            "     | > postnet_diff_spec_loss: 0.18857  (0.18154)\n",
            "     | > decoder_ssim_loss: 0.22584  (0.22186)\n",
            "     | > postnet_ssim_loss: 0.21499  (0.21233)\n",
            "     | > loss: 0.59740  (0.62275)\n",
            "     | > align_error: 0.21816  (0.20546)\n",
            "     | > grad_norm: 0.72370  (0.93187)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.65350  (2.22186)\n",
            "     | > loader_time: 0.00520  (0.00492)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 357/722 -- GLOBAL_STEP: 271925\u001b[0m\n",
            "     | > decoder_loss: 0.28539  (0.28585)\n",
            "     | > postnet_loss: 0.26434  (0.26535)\n",
            "     | > stopnet_loss: 0.11157  (0.15562)\n",
            "     | > decoder_coarse_loss: 0.47271  (0.48169)\n",
            "     | > decoder_ddc_loss: 0.00672  (0.00909)\n",
            "     | > ga_loss: 0.00014  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.20445  (0.20070)\n",
            "     | > postnet_diff_spec_loss: 0.18377  (0.18165)\n",
            "     | > decoder_ssim_loss: 0.22531  (0.22163)\n",
            "     | > postnet_ssim_loss: 0.21482  (0.21207)\n",
            "     | > loss: 0.57666  (0.62172)\n",
            "     | > align_error: 0.21902  (0.20652)\n",
            "     | > grad_norm: 0.57121  (0.92712)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.70760  (2.26163)\n",
            "     | > loader_time: 0.00530  (0.00493)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 382/722 -- GLOBAL_STEP: 271950\u001b[0m\n",
            "     | > decoder_loss: 0.29705  (0.28608)\n",
            "     | > postnet_loss: 0.27434  (0.26551)\n",
            "     | > stopnet_loss: 0.09494  (0.15398)\n",
            "     | > decoder_coarse_loss: 0.51327  (0.48228)\n",
            "     | > decoder_ddc_loss: 0.00784  (0.00894)\n",
            "     | > ga_loss: 0.00013  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.20624  (0.20078)\n",
            "     | > postnet_diff_spec_loss: 0.18488  (0.18166)\n",
            "     | > decoder_ssim_loss: 0.22853  (0.22167)\n",
            "     | > postnet_ssim_loss: 0.21825  (0.21207)\n",
            "     | > loss: 0.57817  (0.62027)\n",
            "     | > align_error: 0.22132  (0.20748)\n",
            "     | > grad_norm: 0.52084  (0.92416)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.86580  (2.30052)\n",
            "     | > loader_time: 0.00500  (0.00494)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 407/722 -- GLOBAL_STEP: 271975\u001b[0m\n",
            "     | > decoder_loss: 0.28946  (0.28654)\n",
            "     | > postnet_loss: 0.26971  (0.26589)\n",
            "     | > stopnet_loss: 0.17097  (0.15230)\n",
            "     | > decoder_coarse_loss: 0.48504  (0.48330)\n",
            "     | > decoder_ddc_loss: 0.00672  (0.00881)\n",
            "     | > ga_loss: 0.00012  (0.00030)\n",
            "     | > decoder_diff_spec_loss: 0.19589  (0.20096)\n",
            "     | > postnet_diff_spec_loss: 0.17846  (0.18179)\n",
            "     | > decoder_ssim_loss: 0.21463  (0.22171)\n",
            "     | > postnet_ssim_loss: 0.20572  (0.21209)\n",
            "     | > loss: 0.63296  (0.61905)\n",
            "     | > align_error: 0.21923  (0.20826)\n",
            "     | > grad_norm: 0.74292  (0.91028)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.01390  (2.33760)\n",
            "     | > loader_time: 0.00460  (0.00495)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 432/722 -- GLOBAL_STEP: 272000\u001b[0m\n",
            "     | > decoder_loss: 0.29109  (0.28682)\n",
            "     | > postnet_loss: 0.26918  (0.26609)\n",
            "     | > stopnet_loss: 0.15897  (0.15130)\n",
            "     | > decoder_coarse_loss: 0.48855  (0.48415)\n",
            "     | > decoder_ddc_loss: 0.00649  (0.00867)\n",
            "     | > ga_loss: 0.00011  (0.00029)\n",
            "     | > decoder_diff_spec_loss: 0.20593  (0.20110)\n",
            "     | > postnet_diff_spec_loss: 0.18609  (0.18185)\n",
            "     | > decoder_ssim_loss: 0.20664  (0.22151)\n",
            "     | > postnet_ssim_loss: 0.19629  (0.21187)\n",
            "     | > loss: 0.62206  (0.61824)\n",
            "     | > align_error: 0.21216  (0.20899)\n",
            "     | > grad_norm: 0.75492  (0.89676)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.17760  (2.37934)\n",
            "     | > loader_time: 0.00490  (0.00496)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/checkpoint_272000.pth.tar\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "\u001b[1m   --> STEP: 457/722 -- GLOBAL_STEP: 272025\u001b[0m\n",
            "     | > decoder_loss: 0.28560  (0.28726)\n",
            "     | > postnet_loss: 0.26569  (0.26643)\n",
            "     | > stopnet_loss: 0.07965  (0.15078)\n",
            "     | > decoder_coarse_loss: 0.48119  (0.48547)\n",
            "     | > decoder_ddc_loss: 0.00637  (0.00854)\n",
            "     | > ga_loss: 0.00008  (0.00028)\n",
            "     | > decoder_diff_spec_loss: 0.19637  (0.20134)\n",
            "     | > postnet_diff_spec_loss: 0.17812  (0.18201)\n",
            "     | > decoder_ssim_loss: 0.23431  (0.22132)\n",
            "     | > postnet_ssim_loss: 0.22422  (0.21165)\n",
            "     | > loss: 0.54802  (0.61816)\n",
            "     | > align_error: 0.22249  (0.20964)\n",
            "     | > grad_norm: 0.75204  (0.89495)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.93550  (2.42561)\n",
            "     | > loader_time: 0.00530  (0.00512)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 482/722 -- GLOBAL_STEP: 272050\u001b[0m\n",
            "     | > decoder_loss: 0.29647  (0.28770)\n",
            "     | > postnet_loss: 0.27359  (0.26677)\n",
            "     | > stopnet_loss: 0.16344  (0.15001)\n",
            "     | > decoder_coarse_loss: 0.49804  (0.48651)\n",
            "     | > decoder_ddc_loss: 0.00629  (0.00840)\n",
            "     | > ga_loss: 0.00009  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.20244  (0.20156)\n",
            "     | > postnet_diff_spec_loss: 0.18291  (0.18215)\n",
            "     | > decoder_ssim_loss: 0.20541  (0.22112)\n",
            "     | > postnet_ssim_loss: 0.19579  (0.21142)\n",
            "     | > loss: 0.62912  (0.61775)\n",
            "     | > align_error: 0.22131  (0.21041)\n",
            "     | > grad_norm: 0.74458  (0.88669)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.36560  (2.46739)\n",
            "     | > loader_time: 0.00550  (0.00514)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 507/722 -- GLOBAL_STEP: 272075\u001b[0m\n",
            "     | > decoder_loss: 0.29570  (0.28809)\n",
            "     | > postnet_loss: 0.27253  (0.26706)\n",
            "     | > stopnet_loss: 0.10109  (0.14920)\n",
            "     | > decoder_coarse_loss: 0.53123  (0.48768)\n",
            "     | > decoder_ddc_loss: 0.00671  (0.00828)\n",
            "     | > ga_loss: 0.00011  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.20687  (0.20183)\n",
            "     | > postnet_diff_spec_loss: 0.18518  (0.18232)\n",
            "     | > decoder_ssim_loss: 0.22654  (0.22088)\n",
            "     | > postnet_ssim_loss: 0.21594  (0.21116)\n",
            "     | > loss: 0.58679  (0.61731)\n",
            "     | > align_error: 0.23207  (0.21118)\n",
            "     | > grad_norm: 0.50624  (0.87436)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.15490  (2.51025)\n",
            "     | > loader_time: 0.00470  (0.00515)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 532/722 -- GLOBAL_STEP: 272100\u001b[0m\n",
            "     | > decoder_loss: 0.30545  (0.28845)\n",
            "     | > postnet_loss: 0.28104  (0.26733)\n",
            "     | > stopnet_loss: 0.10388  (0.14845)\n",
            "     | > decoder_coarse_loss: 0.52000  (0.48879)\n",
            "     | > decoder_ddc_loss: 0.00587  (0.00816)\n",
            "     | > ga_loss: 0.00011  (0.00025)\n",
            "     | > decoder_diff_spec_loss: 0.21094  (0.20204)\n",
            "     | > postnet_diff_spec_loss: 0.18752  (0.18245)\n",
            "     | > decoder_ssim_loss: 0.22697  (0.22078)\n",
            "     | > postnet_ssim_loss: 0.21586  (0.21104)\n",
            "     | > loss: 0.59282  (0.61696)\n",
            "     | > align_error: 0.22232  (0.21175)\n",
            "     | > grad_norm: 0.70067  (0.86317)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.39290  (2.55422)\n",
            "     | > loader_time: 0.00480  (0.00515)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 557/722 -- GLOBAL_STEP: 272125\u001b[0m\n",
            "     | > decoder_loss: 0.28763  (0.28877)\n",
            "     | > postnet_loss: 0.26539  (0.26756)\n",
            "     | > stopnet_loss: 0.12221  (0.14721)\n",
            "     | > decoder_coarse_loss: 0.49025  (0.48979)\n",
            "     | > decoder_ddc_loss: 0.00557  (0.00805)\n",
            "     | > ga_loss: 0.00006  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.20158  (0.20226)\n",
            "     | > postnet_diff_spec_loss: 0.18104  (0.18258)\n",
            "     | > decoder_ssim_loss: 0.21890  (0.22072)\n",
            "     | > postnet_ssim_loss: 0.20888  (0.21095)\n",
            "     | > loss: 0.58732  (0.61609)\n",
            "     | > align_error: 0.22194  (0.21249)\n",
            "     | > grad_norm: 0.56931  (0.85265)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.57500  (2.59493)\n",
            "     | > loader_time: 0.00590  (0.00515)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 582/722 -- GLOBAL_STEP: 272150\u001b[0m\n",
            "     | > decoder_loss: 0.30002  (0.28916)\n",
            "     | > postnet_loss: 0.27499  (0.26785)\n",
            "     | > stopnet_loss: 0.19320  (0.14629)\n",
            "     | > decoder_coarse_loss: 0.53956  (0.49107)\n",
            "     | > decoder_ddc_loss: 0.00471  (0.00794)\n",
            "     | > ga_loss: 0.00006  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.20854  (0.20245)\n",
            "     | > postnet_diff_spec_loss: 0.18616  (0.18269)\n",
            "     | > decoder_ssim_loss: 0.19868  (0.22071)\n",
            "     | > postnet_ssim_loss: 0.18894  (0.21091)\n",
            "     | > loss: 0.66888  (0.61566)\n",
            "     | > align_error: 0.22305  (0.21310)\n",
            "     | > grad_norm: 0.55768  (0.84397)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.14260  (2.64327)\n",
            "     | > loader_time: 0.00640  (0.00518)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 607/722 -- GLOBAL_STEP: 272175\u001b[0m\n",
            "     | > decoder_loss: 0.29846  (0.28948)\n",
            "     | > postnet_loss: 0.27434  (0.26808)\n",
            "     | > stopnet_loss: 0.11037  (0.14521)\n",
            "     | > decoder_coarse_loss: 0.52508  (0.49204)\n",
            "     | > decoder_ddc_loss: 0.00576  (0.00783)\n",
            "     | > ga_loss: 0.00005  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.20750  (0.20263)\n",
            "     | > postnet_diff_spec_loss: 0.18539  (0.18279)\n",
            "     | > decoder_ssim_loss: 0.22883  (0.22072)\n",
            "     | > postnet_ssim_loss: 0.21802  (0.21088)\n",
            "     | > loss: 0.59648  (0.61496)\n",
            "     | > align_error: 0.22213  (0.21373)\n",
            "     | > grad_norm: 0.50034  (0.83576)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.77190  (2.69035)\n",
            "     | > loader_time: 0.00580  (0.00519)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 632/722 -- GLOBAL_STEP: 272200\u001b[0m\n",
            "     | > decoder_loss: 0.30601  (0.28987)\n",
            "     | > postnet_loss: 0.28119  (0.26837)\n",
            "     | > stopnet_loss: 0.16115  (0.14428)\n",
            "     | > decoder_coarse_loss: 0.54203  (0.49325)\n",
            "     | > decoder_ddc_loss: 0.00495  (0.00772)\n",
            "     | > ga_loss: 0.00005  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.21061  (0.20284)\n",
            "     | > postnet_diff_spec_loss: 0.18874  (0.18292)\n",
            "     | > decoder_ssim_loss: 0.21403  (0.22069)\n",
            "     | > postnet_ssim_loss: 0.20332  (0.21083)\n",
            "     | > loss: 0.64912  (0.61451)\n",
            "     | > align_error: 0.22998  (0.21434)\n",
            "     | > grad_norm: 0.70779  (0.82951)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.22810  (2.74088)\n",
            "     | > loader_time: 0.00530  (0.00521)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 657/722 -- GLOBAL_STEP: 272225\u001b[0m\n",
            "     | > decoder_loss: 0.31237  (0.29034)\n",
            "     | > postnet_loss: 0.28763  (0.26874)\n",
            "     | > stopnet_loss: 0.14638  (0.14321)\n",
            "     | > decoder_coarse_loss: 0.54039  (0.49462)\n",
            "     | > decoder_ddc_loss: 0.00447  (0.00761)\n",
            "     | > ga_loss: 0.00004  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.21391  (0.20310)\n",
            "     | > postnet_diff_spec_loss: 0.18985  (0.18309)\n",
            "     | > decoder_ssim_loss: 0.21975  (0.22063)\n",
            "     | > postnet_ssim_loss: 0.20914  (0.21074)\n",
            "     | > loss: 0.64096  (0.61399)\n",
            "     | > align_error: 0.23270  (0.21504)\n",
            "     | > grad_norm: 1.07638  (0.82733)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.74040  (2.79083)\n",
            "     | > loader_time: 0.00510  (0.00523)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 682/722 -- GLOBAL_STEP: 272250\u001b[0m\n",
            "     | > decoder_loss: 0.29836  (0.29081)\n",
            "     | > postnet_loss: 0.27410  (0.26910)\n",
            "     | > stopnet_loss: 0.09109  (0.14273)\n",
            "     | > decoder_coarse_loss: 0.54567  (0.49625)\n",
            "     | > decoder_ddc_loss: 0.00463  (0.00750)\n",
            "     | > ga_loss: 0.00004  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.20880  (0.20332)\n",
            "     | > postnet_diff_spec_loss: 0.18618  (0.18322)\n",
            "     | > decoder_ssim_loss: 0.22589  (0.22055)\n",
            "     | > postnet_ssim_loss: 0.21483  (0.21063)\n",
            "     | > loss: 0.58090  (0.61411)\n",
            "     | > align_error: 0.23489  (0.21579)\n",
            "     | > grad_norm: 0.76538  (0.82199)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.37870  (2.84983)\n",
            "     | > loader_time: 0.00640  (0.00525)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 707/722 -- GLOBAL_STEP: 272275\u001b[0m\n",
            "     | > decoder_loss: 0.31742  (0.29130)\n",
            "     | > postnet_loss: 0.29008  (0.26947)\n",
            "     | > stopnet_loss: 0.19844  (0.14261)\n",
            "     | > decoder_coarse_loss: 0.56101  (0.49790)\n",
            "     | > decoder_ddc_loss: 0.00368  (0.00738)\n",
            "     | > ga_loss: 0.00004  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.21968  (0.20360)\n",
            "     | > postnet_diff_spec_loss: 0.19457  (0.18340)\n",
            "     | > decoder_ssim_loss: 0.20475  (0.22039)\n",
            "     | > postnet_ssim_loss: 0.19399  (0.21045)\n",
            "     | > loss: 0.69495  (0.61459)\n",
            "     | > align_error: 0.24167  (0.21662)\n",
            "     | > grad_norm: 0.56017  (0.81352)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 5.44800  (2.91997)\n",
            "     | > loader_time: 0.00620  (0.00528)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.41480  (0.41480)\n",
            "     | > postnet_loss: 0.37349  (0.37349)\n",
            "     | > stopnet_loss: 0.22587  (0.22587)\n",
            "     | > decoder_coarse_loss: 0.62469  (0.62469)\n",
            "     | > decoder_ddc_loss: 0.00838  (0.00838)\n",
            "     | > ga_loss: 0.00062  (0.00062)\n",
            "     | > decoder_diff_spec_loss: 0.18797  (0.18797)\n",
            "     | > postnet_diff_spec_loss: 0.15546  (0.15546)\n",
            "     | > decoder_ssim_loss: 0.21564  (0.21564)\n",
            "     | > postnet_ssim_loss: 0.20115  (0.20115)\n",
            "     | > loss: 0.77437  (0.77437)\n",
            "     | > align_error: 0.36526  (0.36526)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.35250  (0.35250)\n",
            "     | > postnet_loss: 0.31368  (0.31368)\n",
            "     | > stopnet_loss: 0.10625  (0.10625)\n",
            "     | > decoder_coarse_loss: 0.57668  (0.57668)\n",
            "     | > decoder_ddc_loss: 0.00848  (0.00848)\n",
            "     | > ga_loss: 0.00027  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.19328  (0.19328)\n",
            "     | > postnet_diff_spec_loss: 0.15836  (0.15836)\n",
            "     | > decoder_ssim_loss: 0.23536  (0.23536)\n",
            "     | > postnet_ssim_loss: 0.21803  (0.21803)\n",
            "     | > loss: 0.62168  (0.62168)\n",
            "     | > align_error: 0.26777  (0.26777)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.37865  (0.36557)\n",
            "     | > postnet_loss: 0.33921  (0.32644)\n",
            "     | > stopnet_loss: 0.14706  (0.12665)\n",
            "     | > decoder_coarse_loss: 0.59304  (0.58486)\n",
            "     | > decoder_ddc_loss: 0.00783  (0.00815)\n",
            "     | > ga_loss: 0.00020  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.18920  (0.19124)\n",
            "     | > postnet_diff_spec_loss: 0.15586  (0.15711)\n",
            "     | > decoder_ssim_loss: 0.21617  (0.22576)\n",
            "     | > postnet_ssim_loss: 0.20075  (0.20939)\n",
            "     | > loss: 0.66825  (0.64497)\n",
            "     | > align_error: 0.28055  (0.27416)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.39631  (0.37582)\n",
            "     | > postnet_loss: 0.35020  (0.33436)\n",
            "     | > stopnet_loss: 0.13187  (0.12839)\n",
            "     | > decoder_coarse_loss: 0.65809  (0.60927)\n",
            "     | > decoder_ddc_loss: 0.00629  (0.00753)\n",
            "     | > ga_loss: 0.00012  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.20366  (0.19538)\n",
            "     | > postnet_diff_spec_loss: 0.16459  (0.15960)\n",
            "     | > decoder_ssim_loss: 0.22176  (0.22443)\n",
            "     | > postnet_ssim_loss: 0.20497  (0.20792)\n",
            "     | > loss: 0.68394  (0.65796)\n",
            "     | > align_error: 0.27462  (0.27431)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.36528  (0.37318)\n",
            "     | > postnet_loss: 0.32068  (0.33094)\n",
            "     | > stopnet_loss: 0.07584  (0.11526)\n",
            "     | > decoder_coarse_loss: 0.61941  (0.61181)\n",
            "     | > decoder_ddc_loss: 0.00601  (0.00715)\n",
            "     | > ga_loss: 0.00010  (0.00017)\n",
            "     | > decoder_diff_spec_loss: 0.20208  (0.19706)\n",
            "     | > postnet_diff_spec_loss: 0.16320  (0.16050)\n",
            "     | > decoder_ssim_loss: 0.23428  (0.22689)\n",
            "     | > postnet_ssim_loss: 0.21605  (0.20995)\n",
            "     | > loss: 0.60807  (0.64549)\n",
            "     | > align_error: 0.25480  (0.26943)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.36224  (0.37100)\n",
            "     | > postnet_loss: 0.32165  (0.32908)\n",
            "     | > stopnet_loss: 0.14601  (0.12141)\n",
            "     | > decoder_coarse_loss: 0.60476  (0.61040)\n",
            "     | > decoder_ddc_loss: 0.00560  (0.00684)\n",
            "     | > ga_loss: 0.00007  (0.00015)\n",
            "     | > decoder_diff_spec_loss: 0.19029  (0.19570)\n",
            "     | > postnet_diff_spec_loss: 0.15415  (0.15923)\n",
            "     | > decoder_ssim_loss: 0.20951  (0.22342)\n",
            "     | > postnet_ssim_loss: 0.19304  (0.20657)\n",
            "     | > loss: 0.65666  (0.64772)\n",
            "     | > align_error: 0.27707  (0.27096)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.34807  (0.36717)\n",
            "     | > postnet_loss: 0.30585  (0.32521)\n",
            "     | > stopnet_loss: 0.16956  (0.12943)\n",
            "     | > decoder_coarse_loss: 0.60192  (0.60898)\n",
            "     | > decoder_ddc_loss: 0.00390  (0.00635)\n",
            "     | > ga_loss: 0.00004  (0.00013)\n",
            "     | > decoder_diff_spec_loss: 0.19017  (0.19478)\n",
            "     | > postnet_diff_spec_loss: 0.15406  (0.15837)\n",
            "     | > decoder_ssim_loss: 0.20000  (0.21951)\n",
            "     | > postnet_ssim_loss: 0.18361  (0.20274)\n",
            "     | > loss: 0.66665  (0.65087)\n",
            "     | > align_error: 0.30540  (0.27670)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.40412  (0.37245)\n",
            "     | > postnet_loss: 0.35180  (0.32901)\n",
            "     | > stopnet_loss: 0.05413  (0.11867)\n",
            "     | > decoder_coarse_loss: 0.70084  (0.62211)\n",
            "     | > decoder_ddc_loss: 0.00396  (0.00601)\n",
            "     | > ga_loss: 0.00003  (0.00012)\n",
            "     | > decoder_diff_spec_loss: 0.20325  (0.19599)\n",
            "     | > postnet_diff_spec_loss: 0.16383  (0.15915)\n",
            "     | > decoder_ssim_loss: 0.24376  (0.22298)\n",
            "     | > postnet_ssim_loss: 0.22491  (0.20591)\n",
            "     | > loss: 0.62838  (0.64766)\n",
            "     | > align_error: 0.26663  (0.27526)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00582 \u001b[0m(+0.00061)\n",
            "     | > avg_decoder_loss:\u001b[91m 0.37245 \u001b[0m(+0.00017)\n",
            "     | > avg_postnet_loss:\u001b[91m 0.32901 \u001b[0m(+0.00032)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.11867 \u001b[0m(-0.00007)\n",
            "     | > avg_decoder_coarse_loss:\u001b[91m 0.62211 \u001b[0m(+0.00019)\n",
            "     | > avg_decoder_ddc_loss:\u001b[92m 0.00601 \u001b[0m(-0.00000)\n",
            "     | > avg_ga_loss:\u001b[91m 0.00012 \u001b[0m(+0.00000)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.19599 \u001b[0m(-0.00056)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.15915 \u001b[0m(-0.00041)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.22298 \u001b[0m(-0.00006)\n",
            "     | > avg_postnet_ssim_loss:\u001b[91m 0.20591 \u001b[0m(+0.00001)\n",
            "     | > avg_loss:\u001b[92m 0.64766 \u001b[0m(-0.00016)\n",
            "     | > avg_align_error:\u001b[91m 0.27526 \u001b[0m(+0.00106)\n",
            "\n",
            "\n",
            " > Number of output frames: 1\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 17/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2022-01-01 03:55:55) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 9/722 -- GLOBAL_STEP: 272300\u001b[0m\n",
            "     | > decoder_loss: 0.27037  (0.27485)\n",
            "     | > postnet_loss: 0.25323  (0.25568)\n",
            "     | > stopnet_loss: 0.14699  (0.20420)\n",
            "     | > decoder_coarse_loss: 0.43211  (0.46914)\n",
            "     | > decoder_ddc_loss: 0.01421  (0.01535)\n",
            "     | > ga_loss: 0.00103  (0.00127)\n",
            "     | > decoder_diff_spec_loss: 0.19390  (0.19847)\n",
            "     | > postnet_diff_spec_loss: 0.17729  (0.18047)\n",
            "     | > decoder_ssim_loss: 0.24165  (0.22705)\n",
            "     | > postnet_ssim_loss: 0.23278  (0.21840)\n",
            "     | > loss: 0.60605  (0.67039)\n",
            "     | > align_error: 0.18243  (0.18520)\n",
            "     | > grad_norm: 1.26889  (1.44498)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.43720  (1.38243)\n",
            "     | > loader_time: 0.00430  (0.00344)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 34/722 -- GLOBAL_STEP: 272325\u001b[0m\n",
            "     | > decoder_loss: 0.27025  (0.27722)\n",
            "     | > postnet_loss: 0.25216  (0.25834)\n",
            "     | > stopnet_loss: 0.22756  (0.20283)\n",
            "     | > decoder_coarse_loss: 0.42925  (0.46401)\n",
            "     | > decoder_ddc_loss: 0.01032  (0.01344)\n",
            "     | > ga_loss: 0.00061  (0.00090)\n",
            "     | > decoder_diff_spec_loss: 0.19227  (0.19769)\n",
            "     | > postnet_diff_spec_loss: 0.17503  (0.17988)\n",
            "     | > decoder_ssim_loss: 0.20967  (0.22764)\n",
            "     | > postnet_ssim_loss: 0.20154  (0.21876)\n",
            "     | > loss: 0.66574  (0.66657)\n",
            "     | > align_error: 0.18533  (0.19129)\n",
            "     | > grad_norm: 0.60538  (1.41865)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.88120  (1.61257)\n",
            "     | > loader_time: 0.00480  (0.00437)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 59/722 -- GLOBAL_STEP: 272350\u001b[0m\n",
            "     | > decoder_loss: 0.27821  (0.27883)\n",
            "     | > postnet_loss: 0.25971  (0.25978)\n",
            "     | > stopnet_loss: 0.11170  (0.18767)\n",
            "     | > decoder_coarse_loss: 0.47166  (0.46885)\n",
            "     | > decoder_ddc_loss: 0.01106  (0.01245)\n",
            "     | > ga_loss: 0.00045  (0.00073)\n",
            "     | > decoder_diff_spec_loss: 0.19951  (0.19800)\n",
            "     | > postnet_diff_spec_loss: 0.18275  (0.18012)\n",
            "     | > decoder_ssim_loss: 0.23420  (0.22672)\n",
            "     | > postnet_ssim_loss: 0.22507  (0.21774)\n",
            "     | > loss: 0.57950  (0.65195)\n",
            "     | > align_error: 0.19867  (0.19238)\n",
            "     | > grad_norm: 1.09534  (1.23643)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.76550  (1.69849)\n",
            "     | > loader_time: 0.00490  (0.00454)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 84/722 -- GLOBAL_STEP: 272375\u001b[0m\n",
            "     | > decoder_loss: 0.29825  (0.28100)\n",
            "     | > postnet_loss: 0.27732  (0.26155)\n",
            "     | > stopnet_loss: 0.16918  (0.18538)\n",
            "     | > decoder_coarse_loss: 0.52550  (0.47397)\n",
            "     | > decoder_ddc_loss: 0.00951  (0.01184)\n",
            "     | > ga_loss: 0.00033  (0.00064)\n",
            "     | > decoder_diff_spec_loss: 0.21198  (0.19908)\n",
            "     | > postnet_diff_spec_loss: 0.19153  (0.18078)\n",
            "     | > decoder_ssim_loss: 0.21715  (0.22488)\n",
            "     | > postnet_ssim_loss: 0.20760  (0.21581)\n",
            "     | > loss: 0.65552  (0.65083)\n",
            "     | > align_error: 0.19416  (0.19392)\n",
            "     | > grad_norm: 0.73068  (1.21837)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.10170  (1.79627)\n",
            "     | > loader_time: 0.00450  (0.00471)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 109/722 -- GLOBAL_STEP: 272400\u001b[0m\n",
            "     | > decoder_loss: 0.27762  (0.28222)\n",
            "     | > postnet_loss: 0.25830  (0.26248)\n",
            "     | > stopnet_loss: 0.15318  (0.17858)\n",
            "     | > decoder_coarse_loss: 0.46102  (0.47755)\n",
            "     | > decoder_ddc_loss: 0.01081  (0.01140)\n",
            "     | > ga_loss: 0.00034  (0.00058)\n",
            "     | > decoder_diff_spec_loss: 0.19469  (0.19994)\n",
            "     | > postnet_diff_spec_loss: 0.17620  (0.18127)\n",
            "     | > decoder_ssim_loss: 0.22341  (0.22440)\n",
            "     | > postnet_ssim_loss: 0.21420  (0.21525)\n",
            "     | > loss: 0.60896  (0.64508)\n",
            "     | > align_error: 0.21129  (0.19548)\n",
            "     | > grad_norm: 0.70703  (1.13396)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.07950  (1.87550)\n",
            "     | > loader_time: 0.00580  (0.00478)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 134/722 -- GLOBAL_STEP: 272425\u001b[0m\n",
            "     | > decoder_loss: 0.28565  (0.28306)\n",
            "     | > postnet_loss: 0.26494  (0.26314)\n",
            "     | > stopnet_loss: 0.19935  (0.17521)\n",
            "     | > decoder_coarse_loss: 0.48367  (0.47842)\n",
            "     | > decoder_ddc_loss: 0.00853  (0.01100)\n",
            "     | > ga_loss: 0.00037  (0.00053)\n",
            "     | > decoder_diff_spec_loss: 0.20609  (0.20036)\n",
            "     | > postnet_diff_spec_loss: 0.18638  (0.18155)\n",
            "     | > decoder_ssim_loss: 0.20796  (0.22387)\n",
            "     | > postnet_ssim_loss: 0.19882  (0.21463)\n",
            "     | > loss: 0.66170  (0.64185)\n",
            "     | > align_error: 0.20540  (0.19690)\n",
            "     | > grad_norm: 0.76586  (1.06768)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.32400  (1.94652)\n",
            "     | > loader_time: 0.00530  (0.00484)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 159/722 -- GLOBAL_STEP: 272450\u001b[0m\n",
            "     | > decoder_loss: 0.28581  (0.28370)\n",
            "     | > postnet_loss: 0.26540  (0.26366)\n",
            "     | > stopnet_loss: 0.18051  (0.17164)\n",
            "     | > decoder_coarse_loss: 0.47193  (0.47936)\n",
            "     | > decoder_ddc_loss: 0.00850  (0.01068)\n",
            "     | > ga_loss: 0.00025  (0.00049)\n",
            "     | > decoder_diff_spec_loss: 0.19936  (0.20042)\n",
            "     | > postnet_diff_spec_loss: 0.18060  (0.18156)\n",
            "     | > decoder_ssim_loss: 0.22176  (0.22383)\n",
            "     | > postnet_ssim_loss: 0.21222  (0.21452)\n",
            "     | > loss: 0.64316  (0.63850)\n",
            "     | > align_error: 0.20756  (0.19852)\n",
            "     | > grad_norm: 0.71866  (1.03054)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.39080  (2.00819)\n",
            "     | > loader_time: 0.00560  (0.00492)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 184/722 -- GLOBAL_STEP: 272475\u001b[0m\n",
            "     | > decoder_loss: 0.28386  (0.28436)\n",
            "     | > postnet_loss: 0.26411  (0.26424)\n",
            "     | > stopnet_loss: 0.15399  (0.16989)\n",
            "     | > decoder_coarse_loss: 0.49947  (0.47970)\n",
            "     | > decoder_ddc_loss: 0.00861  (0.01040)\n",
            "     | > ga_loss: 0.00023  (0.00045)\n",
            "     | > decoder_diff_spec_loss: 0.19765  (0.20058)\n",
            "     | > postnet_diff_spec_loss: 0.17894  (0.18165)\n",
            "     | > decoder_ssim_loss: 0.21892  (0.22364)\n",
            "     | > postnet_ssim_loss: 0.20958  (0.21429)\n",
            "     | > loss: 0.62043  (0.63688)\n",
            "     | > align_error: 0.20926  (0.19981)\n",
            "     | > grad_norm: 0.94549  (1.02870)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.26510  (2.06116)\n",
            "     | > loader_time: 0.00530  (0.00496)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 209/722 -- GLOBAL_STEP: 272500\u001b[0m\n",
            "     | > decoder_loss: 0.29002  (0.28483)\n",
            "     | > postnet_loss: 0.26961  (0.26463)\n",
            "     | > stopnet_loss: 0.15924  (0.16712)\n",
            "     | > decoder_coarse_loss: 0.47665  (0.48012)\n",
            "     | > decoder_ddc_loss: 0.00798  (0.01016)\n",
            "     | > ga_loss: 0.00024  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.20362  (0.20071)\n",
            "     | > postnet_diff_spec_loss: 0.18448  (0.18174)\n",
            "     | > decoder_ssim_loss: 0.23935  (0.22317)\n",
            "     | > postnet_ssim_loss: 0.22973  (0.21378)\n",
            "     | > loss: 0.63578  (0.63404)\n",
            "     | > align_error: 0.20696  (0.20091)\n",
            "     | > grad_norm: 2.21357  (1.01234)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.45730  (2.10791)\n",
            "     | > loader_time: 0.00480  (0.00498)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 234/722 -- GLOBAL_STEP: 272525\u001b[0m\n",
            "     | > decoder_loss: 0.28726  (0.28529)\n",
            "     | > postnet_loss: 0.26630  (0.26499)\n",
            "     | > stopnet_loss: 0.16324  (0.16469)\n",
            "     | > decoder_coarse_loss: 0.46843  (0.48050)\n",
            "     | > decoder_ddc_loss: 0.00785  (0.00995)\n",
            "     | > ga_loss: 0.00019  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.20388  (0.20089)\n",
            "     | > postnet_diff_spec_loss: 0.18386  (0.18186)\n",
            "     | > decoder_ssim_loss: 0.21385  (0.22284)\n",
            "     | > postnet_ssim_loss: 0.20441  (0.21341)\n",
            "     | > loss: 0.62314  (0.63164)\n",
            "     | > align_error: 0.20708  (0.20192)\n",
            "     | > grad_norm: 0.48033  (0.98938)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.62110  (2.15433)\n",
            "     | > loader_time: 0.00480  (0.00501)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 259/722 -- GLOBAL_STEP: 272550\u001b[0m\n",
            "     | > decoder_loss: 0.29053  (0.28593)\n",
            "     | > postnet_loss: 0.27026  (0.26550)\n",
            "     | > stopnet_loss: 0.11978  (0.16192)\n",
            "     | > decoder_coarse_loss: 0.48429  (0.48175)\n",
            "     | > decoder_ddc_loss: 0.00770  (0.00976)\n",
            "     | > ga_loss: 0.00014  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.19972  (0.20131)\n",
            "     | > postnet_diff_spec_loss: 0.18032  (0.18213)\n",
            "     | > decoder_ssim_loss: 0.23231  (0.22272)\n",
            "     | > postnet_ssim_loss: 0.22218  (0.21324)\n",
            "     | > loss: 0.59231  (0.62942)\n",
            "     | > align_error: 0.20979  (0.20301)\n",
            "     | > grad_norm: 1.17664  (0.96834)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.76270  (2.19752)\n",
            "     | > loader_time: 0.00440  (0.00502)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 284/722 -- GLOBAL_STEP: 272575\u001b[0m\n",
            "     | > decoder_loss: 0.27893  (0.28628)\n",
            "     | > postnet_loss: 0.25839  (0.26576)\n",
            "     | > stopnet_loss: 0.17693  (0.16010)\n",
            "     | > decoder_coarse_loss: 0.46088  (0.48239)\n",
            "     | > decoder_ddc_loss: 0.00719  (0.00958)\n",
            "     | > ga_loss: 0.00017  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.19573  (0.20156)\n",
            "     | > postnet_diff_spec_loss: 0.17694  (0.18229)\n",
            "     | > decoder_ssim_loss: 0.20427  (0.22255)\n",
            "     | > postnet_ssim_loss: 0.19516  (0.21304)\n",
            "     | > loss: 0.62215  (0.62779)\n",
            "     | > align_error: 0.22070  (0.20388)\n",
            "     | > grad_norm: 0.98270  (0.95260)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.66860  (2.23929)\n",
            "     | > loader_time: 0.00510  (0.00504)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 309/722 -- GLOBAL_STEP: 272600\u001b[0m\n",
            "     | > decoder_loss: 0.29169  (0.28657)\n",
            "     | > postnet_loss: 0.27023  (0.26600)\n",
            "     | > stopnet_loss: 0.09504  (0.15842)\n",
            "     | > decoder_coarse_loss: 0.48032  (0.48315)\n",
            "     | > decoder_ddc_loss: 0.00793  (0.00940)\n",
            "     | > ga_loss: 0.00019  (0.00035)\n",
            "     | > decoder_diff_spec_loss: 0.20338  (0.20160)\n",
            "     | > postnet_diff_spec_loss: 0.18276  (0.18230)\n",
            "     | > decoder_ssim_loss: 0.24339  (0.22232)\n",
            "     | > postnet_ssim_loss: 0.23286  (0.21279)\n",
            "     | > loss: 0.57411  (0.62619)\n",
            "     | > align_error: 0.21875  (0.20490)\n",
            "     | > grad_norm: 0.61566  (0.94128)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.66090  (2.28272)\n",
            "     | > loader_time: 0.00510  (0.00505)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 334/722 -- GLOBAL_STEP: 272625\u001b[0m\n",
            "     | > decoder_loss: 0.28055  (0.28687)\n",
            "     | > postnet_loss: 0.25973  (0.26622)\n",
            "     | > stopnet_loss: 0.11277  (0.15663)\n",
            "     | > decoder_coarse_loss: 0.49772  (0.48362)\n",
            "     | > decoder_ddc_loss: 0.00715  (0.00924)\n",
            "     | > ga_loss: 0.00012  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.19884  (0.20165)\n",
            "     | > postnet_diff_spec_loss: 0.17899  (0.18232)\n",
            "     | > decoder_ssim_loss: 0.22543  (0.22221)\n",
            "     | > postnet_ssim_loss: 0.21564  (0.21264)\n",
            "     | > loss: 0.57939  (0.62448)\n",
            "     | > align_error: 0.22092  (0.20593)\n",
            "     | > grad_norm: 0.81474  (0.92477)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.81950  (2.32435)\n",
            "     | > loader_time: 0.00470  (0.00508)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 359/722 -- GLOBAL_STEP: 272650\u001b[0m\n",
            "     | > decoder_loss: 0.28754  (0.28725)\n",
            "     | > postnet_loss: 0.26553  (0.26651)\n",
            "     | > stopnet_loss: 0.15938  (0.15642)\n",
            "     | > decoder_coarse_loss: 0.50218  (0.48456)\n",
            "     | > decoder_ddc_loss: 0.00660  (0.00907)\n",
            "     | > ga_loss: 0.00011  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.20508  (0.20191)\n",
            "     | > postnet_diff_spec_loss: 0.18390  (0.18249)\n",
            "     | > decoder_ssim_loss: 0.21527  (0.22183)\n",
            "     | > postnet_ssim_loss: 0.20500  (0.21224)\n",
            "     | > loss: 0.62770  (0.62449)\n",
            "     | > align_error: 0.22125  (0.20695)\n",
            "     | > grad_norm: 0.92934  (0.93710)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.97650  (2.36886)\n",
            "     | > loader_time: 0.00500  (0.00510)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 384/722 -- GLOBAL_STEP: 272675\u001b[0m\n",
            "     | > decoder_loss: 0.28847  (0.28748)\n",
            "     | > postnet_loss: 0.26739  (0.26668)\n",
            "     | > stopnet_loss: 0.15204  (0.15395)\n",
            "     | > decoder_coarse_loss: 0.48753  (0.48511)\n",
            "     | > decoder_ddc_loss: 0.00640  (0.00893)\n",
            "     | > ga_loss: 0.00015  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.19718  (0.20195)\n",
            "     | > postnet_diff_spec_loss: 0.17862  (0.18248)\n",
            "     | > decoder_ssim_loss: 0.22659  (0.22199)\n",
            "     | > postnet_ssim_loss: 0.21704  (0.21237)\n",
            "     | > loss: 0.62012  (0.62223)\n",
            "     | > align_error: 0.21941  (0.20790)\n",
            "     | > grad_norm: 0.74181  (0.92351)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.13820  (2.40595)\n",
            "     | > loader_time: 0.00540  (0.00512)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 409/722 -- GLOBAL_STEP: 272700\u001b[0m\n",
            "     | > decoder_loss: 0.29643  (0.28790)\n",
            "     | > postnet_loss: 0.27388  (0.26702)\n",
            "     | > stopnet_loss: 0.10192  (0.15222)\n",
            "     | > decoder_coarse_loss: 0.49092  (0.48604)\n",
            "     | > decoder_ddc_loss: 0.00699  (0.00880)\n",
            "     | > ga_loss: 0.00011  (0.00030)\n",
            "     | > decoder_diff_spec_loss: 0.20927  (0.20213)\n",
            "     | > postnet_diff_spec_loss: 0.18829  (0.18259)\n",
            "     | > decoder_ssim_loss: 0.23119  (0.22203)\n",
            "     | > postnet_ssim_loss: 0.22047  (0.21237)\n",
            "     | > loss: 0.58186  (0.62092)\n",
            "     | > align_error: 0.22008  (0.20872)\n",
            "     | > grad_norm: 0.81872  (0.90981)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.92790  (2.44384)\n",
            "     | > loader_time: 0.00500  (0.00514)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 434/722 -- GLOBAL_STEP: 272725\u001b[0m\n",
            "     | > decoder_loss: 0.29341  (0.28818)\n",
            "     | > postnet_loss: 0.27113  (0.26721)\n",
            "     | > stopnet_loss: 0.23694  (0.15155)\n",
            "     | > decoder_coarse_loss: 0.50978  (0.48690)\n",
            "     | > decoder_ddc_loss: 0.00560  (0.00866)\n",
            "     | > ga_loss: 0.00016  (0.00029)\n",
            "     | > decoder_diff_spec_loss: 0.20227  (0.20224)\n",
            "     | > postnet_diff_spec_loss: 0.18241  (0.18264)\n",
            "     | > decoder_ssim_loss: 0.19469  (0.22176)\n",
            "     | > postnet_ssim_loss: 0.18582  (0.21208)\n",
            "     | > loss: 0.69901  (0.62039)\n",
            "     | > align_error: 0.22722  (0.20956)\n",
            "     | > grad_norm: 0.62621  (0.89475)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.67450  (2.48712)\n",
            "     | > loader_time: 0.00480  (0.00517)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 459/722 -- GLOBAL_STEP: 272750\u001b[0m\n",
            "     | > decoder_loss: 0.29441  (0.28858)\n",
            "     | > postnet_loss: 0.27190  (0.26752)\n",
            "     | > stopnet_loss: 0.13487  (0.15084)\n",
            "     | > decoder_coarse_loss: 0.50487  (0.48796)\n",
            "     | > decoder_ddc_loss: 0.00597  (0.00853)\n",
            "     | > ga_loss: 0.00012  (0.00028)\n",
            "     | > decoder_diff_spec_loss: 0.20494  (0.20243)\n",
            "     | > postnet_diff_spec_loss: 0.18482  (0.18276)\n",
            "     | > decoder_ssim_loss: 0.21491  (0.22157)\n",
            "     | > postnet_ssim_loss: 0.20474  (0.21187)\n",
            "     | > loss: 0.60713  (0.62002)\n",
            "     | > align_error: 0.22457  (0.21020)\n",
            "     | > grad_norm: 0.57222  (0.89739)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.28250  (2.52784)\n",
            "     | > loader_time: 0.00610  (0.00519)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 484/722 -- GLOBAL_STEP: 272775\u001b[0m\n",
            "     | > decoder_loss: 0.29675  (0.28896)\n",
            "     | > postnet_loss: 0.27456  (0.26782)\n",
            "     | > stopnet_loss: 0.10994  (0.15010)\n",
            "     | > decoder_coarse_loss: 0.50565  (0.48894)\n",
            "     | > decoder_ddc_loss: 0.00600  (0.00839)\n",
            "     | > ga_loss: 0.00009  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.20717  (0.20258)\n",
            "     | > postnet_diff_spec_loss: 0.18558  (0.18286)\n",
            "     | > decoder_ssim_loss: 0.22565  (0.22138)\n",
            "     | > postnet_ssim_loss: 0.21530  (0.21165)\n",
            "     | > loss: 0.58954  (0.61957)\n",
            "     | > align_error: 0.22383  (0.21097)\n",
            "     | > grad_norm: 0.47795  (0.89028)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.19970  (2.57033)\n",
            "     | > loader_time: 0.00550  (0.00521)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 509/722 -- GLOBAL_STEP: 272800\u001b[0m\n",
            "     | > decoder_loss: 0.29736  (0.28930)\n",
            "     | > postnet_loss: 0.27430  (0.26807)\n",
            "     | > stopnet_loss: 0.11332  (0.14920)\n",
            "     | > decoder_coarse_loss: 0.52143  (0.49004)\n",
            "     | > decoder_ddc_loss: 0.00610  (0.00827)\n",
            "     | > ga_loss: 0.00008  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.20939  (0.20280)\n",
            "     | > postnet_diff_spec_loss: 0.18774  (0.18300)\n",
            "     | > decoder_ssim_loss: 0.22010  (0.22111)\n",
            "     | > postnet_ssim_loss: 0.20956  (0.21136)\n",
            "     | > loss: 0.59521  (0.61898)\n",
            "     | > align_error: 0.21906  (0.21164)\n",
            "     | > grad_norm: 0.88983  (0.87908)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.36670  (2.61322)\n",
            "     | > loader_time: 0.00600  (0.00523)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 534/722 -- GLOBAL_STEP: 272825\u001b[0m\n",
            "     | > decoder_loss: 0.29962  (0.28958)\n",
            "     | > postnet_loss: 0.27520  (0.26827)\n",
            "     | > stopnet_loss: 0.12153  (0.14848)\n",
            "     | > decoder_coarse_loss: 0.51646  (0.49103)\n",
            "     | > decoder_ddc_loss: 0.00640  (0.00816)\n",
            "     | > ga_loss: 0.00007  (0.00025)\n",
            "     | > decoder_diff_spec_loss: 0.21070  (0.20293)\n",
            "     | > postnet_diff_spec_loss: 0.18809  (0.18306)\n",
            "     | > decoder_ssim_loss: 0.21649  (0.22099)\n",
            "     | > postnet_ssim_loss: 0.20588  (0.21122)\n",
            "     | > loss: 0.60161  (0.61853)\n",
            "     | > align_error: 0.22703  (0.21220)\n",
            "     | > grad_norm: 0.66429  (0.86937)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.40140  (2.65831)\n",
            "     | > loader_time: 0.00600  (0.00526)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 559/722 -- GLOBAL_STEP: 272850\u001b[0m\n",
            "     | > decoder_loss: 0.29811  (0.28986)\n",
            "     | > postnet_loss: 0.27538  (0.26846)\n",
            "     | > stopnet_loss: 0.10006  (0.14724)\n",
            "     | > decoder_coarse_loss: 0.50387  (0.49198)\n",
            "     | > decoder_ddc_loss: 0.00539  (0.00804)\n",
            "     | > ga_loss: 0.00007  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.20282  (0.20309)\n",
            "     | > postnet_diff_spec_loss: 0.18144  (0.18315)\n",
            "     | > decoder_ssim_loss: 0.23791  (0.22096)\n",
            "     | > postnet_ssim_loss: 0.22769  (0.21116)\n",
            "     | > loss: 0.58358  (0.61763)\n",
            "     | > align_error: 0.23017  (0.21298)\n",
            "     | > grad_norm: 1.16070  (0.86079)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.57650  (2.70088)\n",
            "     | > loader_time: 0.00520  (0.00527)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 584/722 -- GLOBAL_STEP: 272875\u001b[0m\n",
            "     | > decoder_loss: 0.29414  (0.29020)\n",
            "     | > postnet_loss: 0.27082  (0.26872)\n",
            "     | > stopnet_loss: 0.09842  (0.14623)\n",
            "     | > decoder_coarse_loss: 0.50800  (0.49315)\n",
            "     | > decoder_ddc_loss: 0.00565  (0.00794)\n",
            "     | > ga_loss: 0.00007  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.20870  (0.20324)\n",
            "     | > postnet_diff_spec_loss: 0.18656  (0.18323)\n",
            "     | > decoder_ssim_loss: 0.21606  (0.22090)\n",
            "     | > postnet_ssim_loss: 0.20578  (0.21107)\n",
            "     | > loss: 0.57268  (0.61701)\n",
            "     | > align_error: 0.22251  (0.21364)\n",
            "     | > grad_norm: 0.55074  (0.85337)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.65540  (2.74751)\n",
            "     | > loader_time: 0.00510  (0.00529)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 609/722 -- GLOBAL_STEP: 272900\u001b[0m\n",
            "     | > decoder_loss: 0.31468  (0.29052)\n",
            "     | > postnet_loss: 0.28852  (0.26895)\n",
            "     | > stopnet_loss: 0.13056  (0.14527)\n",
            "     | > decoder_coarse_loss: 0.54538  (0.49404)\n",
            "     | > decoder_ddc_loss: 0.00486  (0.00783)\n",
            "     | > ga_loss: 0.00005  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.22138  (0.20342)\n",
            "     | > postnet_diff_spec_loss: 0.19641  (0.18333)\n",
            "     | > decoder_ssim_loss: 0.22976  (0.22090)\n",
            "     | > postnet_ssim_loss: 0.21824  (0.21104)\n",
            "     | > loss: 0.63561  (0.61641)\n",
            "     | > align_error: 0.23691  (0.21441)\n",
            "     | > grad_norm: 1.00616  (0.84583)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.02230  (2.79248)\n",
            "     | > loader_time: 0.00740  (0.00530)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 634/722 -- GLOBAL_STEP: 272925\u001b[0m\n",
            "     | > decoder_loss: 0.30650  (0.29092)\n",
            "     | > postnet_loss: 0.28187  (0.26925)\n",
            "     | > stopnet_loss: 0.10748  (0.14429)\n",
            "     | > decoder_coarse_loss: 0.52916  (0.49508)\n",
            "     | > decoder_ddc_loss: 0.00494  (0.00771)\n",
            "     | > ga_loss: 0.00005  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.20809  (0.20361)\n",
            "     | > postnet_diff_spec_loss: 0.18616  (0.18345)\n",
            "     | > decoder_ssim_loss: 0.22082  (0.22086)\n",
            "     | > postnet_ssim_loss: 0.20978  (0.21098)\n",
            "     | > loss: 0.59456  (0.61586)\n",
            "     | > align_error: 0.23491  (0.21515)\n",
            "     | > grad_norm: 0.62541  (0.84076)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.89770  (2.83947)\n",
            "     | > loader_time: 0.00530  (0.00532)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 659/722 -- GLOBAL_STEP: 272950\u001b[0m\n",
            "     | > decoder_loss: 0.29359  (0.29131)\n",
            "     | > postnet_loss: 0.26951  (0.26954)\n",
            "     | > stopnet_loss: 0.16254  (0.14330)\n",
            "     | > decoder_coarse_loss: 0.53641  (0.49637)\n",
            "     | > decoder_ddc_loss: 0.00432  (0.00761)\n",
            "     | > ga_loss: 0.00005  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.20551  (0.20382)\n",
            "     | > postnet_diff_spec_loss: 0.18381  (0.18358)\n",
            "     | > decoder_ssim_loss: 0.20303  (0.22078)\n",
            "     | > postnet_ssim_loss: 0.19304  (0.21086)\n",
            "     | > loss: 0.63511  (0.61533)\n",
            "     | > align_error: 0.24186  (0.21586)\n",
            "     | > grad_norm: 0.42280  (0.83397)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.57750  (2.88823)\n",
            "     | > loader_time: 0.00640  (0.00533)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 684/722 -- GLOBAL_STEP: 272975\u001b[0m\n",
            "     | > decoder_loss: 0.30161  (0.29174)\n",
            "     | > postnet_loss: 0.27672  (0.26986)\n",
            "     | > stopnet_loss: 0.13688  (0.14275)\n",
            "     | > decoder_coarse_loss: 0.53458  (0.49788)\n",
            "     | > decoder_ddc_loss: 0.00420  (0.00749)\n",
            "     | > ga_loss: 0.00004  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.21049  (0.20401)\n",
            "     | > postnet_diff_spec_loss: 0.18749  (0.18369)\n",
            "     | > decoder_ssim_loss: 0.20950  (0.22070)\n",
            "     | > postnet_ssim_loss: 0.19898  (0.21075)\n",
            "     | > loss: 0.61799  (0.61531)\n",
            "     | > align_error: 0.24242  (0.21664)\n",
            "     | > grad_norm: 0.67068  (0.82760)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.44020  (2.94346)\n",
            "     | > loader_time: 0.00550  (0.00535)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 709/722 -- GLOBAL_STEP: 273000\u001b[0m\n",
            "     | > decoder_loss: 0.31596  (0.29222)\n",
            "     | > postnet_loss: 0.28988  (0.27023)\n",
            "     | > stopnet_loss: 0.13430  (0.14259)\n",
            "     | > decoder_coarse_loss: 0.55234  (0.49944)\n",
            "     | > decoder_ddc_loss: 0.00379  (0.00738)\n",
            "     | > ga_loss: 0.00003  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.21703  (0.20428)\n",
            "     | > postnet_diff_spec_loss: 0.19283  (0.18385)\n",
            "     | > decoder_ssim_loss: 0.20981  (0.22055)\n",
            "     | > postnet_ssim_loss: 0.19907  (0.21058)\n",
            "     | > loss: 0.62964  (0.61572)\n",
            "     | > align_error: 0.23542  (0.21752)\n",
            "     | > grad_norm: 0.60783  (0.82014)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 5.06110  (3.01240)\n",
            "     | > loader_time: 0.00670  (0.00539)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/checkpoint_273000.pth.tar\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.41458  (0.41458)\n",
            "     | > postnet_loss: 0.37342  (0.37342)\n",
            "     | > stopnet_loss: 0.22569  (0.22569)\n",
            "     | > decoder_coarse_loss: 0.62252  (0.62252)\n",
            "     | > decoder_ddc_loss: 0.00841  (0.00841)\n",
            "     | > ga_loss: 0.00062  (0.00062)\n",
            "     | > decoder_diff_spec_loss: 0.18652  (0.18652)\n",
            "     | > postnet_diff_spec_loss: 0.15439  (0.15439)\n",
            "     | > decoder_ssim_loss: 0.21504  (0.21504)\n",
            "     | > postnet_ssim_loss: 0.20053  (0.20053)\n",
            "     | > loss: 0.77264  (0.77264)\n",
            "     | > align_error: 0.36422  (0.36422)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.35413  (0.35413)\n",
            "     | > postnet_loss: 0.31505  (0.31505)\n",
            "     | > stopnet_loss: 0.10652  (0.10652)\n",
            "     | > decoder_coarse_loss: 0.57203  (0.57203)\n",
            "     | > decoder_ddc_loss: 0.00850  (0.00850)\n",
            "     | > ga_loss: 0.00027  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.19209  (0.19209)\n",
            "     | > postnet_diff_spec_loss: 0.15760  (0.15760)\n",
            "     | > decoder_ssim_loss: 0.23496  (0.23496)\n",
            "     | > postnet_ssim_loss: 0.21748  (0.21748)\n",
            "     | > loss: 0.62082  (0.62082)\n",
            "     | > align_error: 0.26673  (0.26673)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.37577  (0.36495)\n",
            "     | > postnet_loss: 0.33650  (0.32578)\n",
            "     | > stopnet_loss: 0.14692  (0.12672)\n",
            "     | > decoder_coarse_loss: 0.59212  (0.58208)\n",
            "     | > decoder_ddc_loss: 0.00781  (0.00816)\n",
            "     | > ga_loss: 0.00020  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.18868  (0.19038)\n",
            "     | > postnet_diff_spec_loss: 0.15510  (0.15635)\n",
            "     | > decoder_ssim_loss: 0.21551  (0.22524)\n",
            "     | > postnet_ssim_loss: 0.20003  (0.20875)\n",
            "     | > loss: 0.66582  (0.64332)\n",
            "     | > align_error: 0.27967  (0.27320)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.39276  (0.37422)\n",
            "     | > postnet_loss: 0.34608  (0.33254)\n",
            "     | > stopnet_loss: 0.13211  (0.12852)\n",
            "     | > decoder_coarse_loss: 0.65961  (0.60792)\n",
            "     | > decoder_ddc_loss: 0.00630  (0.00754)\n",
            "     | > ga_loss: 0.00012  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.20377  (0.19485)\n",
            "     | > postnet_diff_spec_loss: 0.16429  (0.15900)\n",
            "     | > decoder_ssim_loss: 0.22153  (0.22400)\n",
            "     | > postnet_ssim_loss: 0.20456  (0.20736)\n",
            "     | > loss: 0.68244  (0.65636)\n",
            "     | > align_error: 0.27373  (0.27338)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.36463  (0.37182)\n",
            "     | > postnet_loss: 0.31995  (0.32940)\n",
            "     | > stopnet_loss: 0.07616  (0.11543)\n",
            "     | > decoder_coarse_loss: 0.61731  (0.61027)\n",
            "     | > decoder_ddc_loss: 0.00602  (0.00716)\n",
            "     | > ga_loss: 0.00010  (0.00017)\n",
            "     | > decoder_diff_spec_loss: 0.19996  (0.19613)\n",
            "     | > postnet_diff_spec_loss: 0.16180  (0.15970)\n",
            "     | > decoder_ssim_loss: 0.23340  (0.22635)\n",
            "     | > postnet_ssim_loss: 0.21511  (0.20930)\n",
            "     | > loss: 0.60619  (0.64382)\n",
            "     | > align_error: 0.25392  (0.26851)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.36430  (0.37032)\n",
            "     | > postnet_loss: 0.32284  (0.32809)\n",
            "     | > stopnet_loss: 0.14634  (0.12161)\n",
            "     | > decoder_coarse_loss: 0.60379  (0.60897)\n",
            "     | > decoder_ddc_loss: 0.00559  (0.00684)\n",
            "     | > ga_loss: 0.00007  (0.00015)\n",
            "     | > decoder_diff_spec_loss: 0.19130  (0.19516)\n",
            "     | > postnet_diff_spec_loss: 0.15444  (0.15865)\n",
            "     | > decoder_ssim_loss: 0.20929  (0.22294)\n",
            "     | > postnet_ssim_loss: 0.19262  (0.20596)\n",
            "     | > loss: 0.65771  (0.64660)\n",
            "     | > align_error: 0.27638  (0.27009)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.34781  (0.36657)\n",
            "     | > postnet_loss: 0.30530  (0.32429)\n",
            "     | > stopnet_loss: 0.16985  (0.12965)\n",
            "     | > decoder_coarse_loss: 0.60183  (0.60778)\n",
            "     | > decoder_ddc_loss: 0.00389  (0.00635)\n",
            "     | > ga_loss: 0.00004  (0.00013)\n",
            "     | > decoder_diff_spec_loss: 0.19001  (0.19430)\n",
            "     | > postnet_diff_spec_loss: 0.15379  (0.15784)\n",
            "     | > decoder_ssim_loss: 0.19968  (0.21906)\n",
            "     | > postnet_ssim_loss: 0.18316  (0.20216)\n",
            "     | > loss: 0.66641  (0.64990)\n",
            "     | > align_error: 0.30461  (0.27584)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.41055  (0.37285)\n",
            "     | > postnet_loss: 0.35591  (0.32880)\n",
            "     | > stopnet_loss: 0.05481  (0.11896)\n",
            "     | > decoder_coarse_loss: 0.70772  (0.62206)\n",
            "     | > decoder_ddc_loss: 0.00395  (0.00601)\n",
            "     | > ga_loss: 0.00003  (0.00012)\n",
            "     | > decoder_diff_spec_loss: 0.20600  (0.19597)\n",
            "     | > postnet_diff_spec_loss: 0.16531  (0.15890)\n",
            "     | > decoder_ssim_loss: 0.24388  (0.22261)\n",
            "     | > postnet_ssim_loss: 0.22475  (0.20539)\n",
            "     | > loss: 0.63447  (0.64769)\n",
            "     | > align_error: 0.26606  (0.27444)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00571 \u001b[0m(-0.00010)\n",
            "     | > avg_decoder_loss:\u001b[91m 0.37285 \u001b[0m(+0.00040)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.32880 \u001b[0m(-0.00020)\n",
            "     | > avg_stopnet_loss:\u001b[91m 0.11896 \u001b[0m(+0.00028)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.62206 \u001b[0m(-0.00005)\n",
            "     | > avg_decoder_ddc_loss:\u001b[91m 0.00601 \u001b[0m(+0.00000)\n",
            "     | > avg_ga_loss:\u001b[91m 0.00012 \u001b[0m(+0.00000)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.19597 \u001b[0m(-0.00002)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.15890 \u001b[0m(-0.00024)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.22261 \u001b[0m(-0.00037)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.20539 \u001b[0m(-0.00052)\n",
            "     | > avg_loss:\u001b[91m 0.64769 \u001b[0m(+0.00003)\n",
            "     | > avg_align_error:\u001b[92m 0.27444 \u001b[0m(-0.00082)\n",
            "\n",
            "\n",
            " > Number of output frames: 1\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 18/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2022-01-01 04:33:45) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 11/722 -- GLOBAL_STEP: 273025\u001b[0m\n",
            "     | > decoder_loss: 0.27142  (0.27330)\n",
            "     | > postnet_loss: 0.25317  (0.25421)\n",
            "     | > stopnet_loss: 0.19588  (0.20057)\n",
            "     | > decoder_coarse_loss: 0.45880  (0.46540)\n",
            "     | > decoder_ddc_loss: 0.01307  (0.01498)\n",
            "     | > ga_loss: 0.00077  (0.00120)\n",
            "     | > decoder_diff_spec_loss: 0.19737  (0.19851)\n",
            "     | > postnet_diff_spec_loss: 0.17911  (0.18040)\n",
            "     | > decoder_ssim_loss: 0.23948  (0.22789)\n",
            "     | > postnet_ssim_loss: 0.23043  (0.21917)\n",
            "     | > loss: 0.66046  (0.66503)\n",
            "     | > align_error: 0.16743  (0.18620)\n",
            "     | > grad_norm: 0.75240  (1.15922)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.55700  (1.42253)\n",
            "     | > loader_time: 0.00420  (0.00456)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 36/722 -- GLOBAL_STEP: 273050\u001b[0m\n",
            "     | > decoder_loss: 0.26967  (0.27764)\n",
            "     | > postnet_loss: 0.25106  (0.25871)\n",
            "     | > stopnet_loss: 0.18679  (0.20143)\n",
            "     | > decoder_coarse_loss: 0.44590  (0.46335)\n",
            "     | > decoder_ddc_loss: 0.01188  (0.01336)\n",
            "     | > ga_loss: 0.00056  (0.00088)\n",
            "     | > decoder_diff_spec_loss: 0.19603  (0.19743)\n",
            "     | > postnet_diff_spec_loss: 0.17783  (0.17979)\n",
            "     | > decoder_ssim_loss: 0.21478  (0.22760)\n",
            "     | > postnet_ssim_loss: 0.20589  (0.21872)\n",
            "     | > loss: 0.63286  (0.66498)\n",
            "     | > align_error: 0.21055  (0.19156)\n",
            "     | > grad_norm: 1.30347  (1.46731)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.80380  (1.62100)\n",
            "     | > loader_time: 0.00450  (0.00470)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 61/722 -- GLOBAL_STEP: 273075\u001b[0m\n",
            "     | > decoder_loss: 0.28888  (0.27915)\n",
            "     | > postnet_loss: 0.26724  (0.26012)\n",
            "     | > stopnet_loss: 0.14981  (0.18645)\n",
            "     | > decoder_coarse_loss: 0.50124  (0.46956)\n",
            "     | > decoder_ddc_loss: 0.01053  (0.01240)\n",
            "     | > ga_loss: 0.00042  (0.00072)\n",
            "     | > decoder_diff_spec_loss: 0.20732  (0.19767)\n",
            "     | > postnet_diff_spec_loss: 0.18682  (0.17999)\n",
            "     | > decoder_ssim_loss: 0.22566  (0.22674)\n",
            "     | > postnet_ssim_loss: 0.21524  (0.21773)\n",
            "     | > loss: 0.62766  (0.65089)\n",
            "     | > align_error: 0.19703  (0.19312)\n",
            "     | > grad_norm: 1.09533  (1.25333)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.83310  (1.71218)\n",
            "     | > loader_time: 0.00480  (0.00473)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 86/722 -- GLOBAL_STEP: 273100\u001b[0m\n",
            "     | > decoder_loss: 0.27369  (0.28069)\n",
            "     | > postnet_loss: 0.25423  (0.26131)\n",
            "     | > stopnet_loss: 0.13580  (0.18488)\n",
            "     | > decoder_coarse_loss: 0.46742  (0.47342)\n",
            "     | > decoder_ddc_loss: 0.00968  (0.01181)\n",
            "     | > ga_loss: 0.00035  (0.00064)\n",
            "     | > decoder_diff_spec_loss: 0.20174  (0.19867)\n",
            "     | > postnet_diff_spec_loss: 0.18264  (0.18056)\n",
            "     | > decoder_ssim_loss: 0.22088  (0.22457)\n",
            "     | > postnet_ssim_loss: 0.21202  (0.21551)\n",
            "     | > loss: 0.59312  (0.64969)\n",
            "     | > align_error: 0.20005  (0.19534)\n",
            "     | > grad_norm: 0.57304  (1.22731)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.00770  (1.80857)\n",
            "     | > loader_time: 0.00490  (0.00480)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 111/722 -- GLOBAL_STEP: 273125\u001b[0m\n",
            "     | > decoder_loss: 0.28196  (0.28160)\n",
            "     | > postnet_loss: 0.26266  (0.26201)\n",
            "     | > stopnet_loss: 0.23778  (0.17873)\n",
            "     | > decoder_coarse_loss: 0.47490  (0.47553)\n",
            "     | > decoder_ddc_loss: 0.00845  (0.01137)\n",
            "     | > ga_loss: 0.00035  (0.00057)\n",
            "     | > decoder_diff_spec_loss: 0.19435  (0.19935)\n",
            "     | > postnet_diff_spec_loss: 0.17770  (0.18093)\n",
            "     | > decoder_ssim_loss: 0.20523  (0.22417)\n",
            "     | > postnet_ssim_loss: 0.19630  (0.21504)\n",
            "     | > loss: 0.68990  (0.64409)\n",
            "     | > align_error: 0.21099  (0.19715)\n",
            "     | > grad_norm: 0.64081  (1.14590)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.28460  (1.88321)\n",
            "     | > loader_time: 0.00540  (0.00483)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 136/722 -- GLOBAL_STEP: 273150\u001b[0m\n",
            "     | > decoder_loss: 0.28960  (0.28277)\n",
            "     | > postnet_loss: 0.26862  (0.26296)\n",
            "     | > stopnet_loss: 0.09643  (0.17445)\n",
            "     | > decoder_coarse_loss: 0.48232  (0.47720)\n",
            "     | > decoder_ddc_loss: 0.00953  (0.01099)\n",
            "     | > ga_loss: 0.00023  (0.00052)\n",
            "     | > decoder_diff_spec_loss: 0.19779  (0.20000)\n",
            "     | > postnet_diff_spec_loss: 0.17896  (0.18134)\n",
            "     | > decoder_ssim_loss: 0.24441  (0.22407)\n",
            "     | > postnet_ssim_loss: 0.23334  (0.21482)\n",
            "     | > loss: 0.57372  (0.64060)\n",
            "     | > align_error: 0.19611  (0.19831)\n",
            "     | > grad_norm: 0.64549  (1.08777)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.08320  (1.94318)\n",
            "     | > loader_time: 0.00450  (0.00487)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 161/722 -- GLOBAL_STEP: 273175\u001b[0m\n",
            "     | > decoder_loss: 0.27972  (0.28344)\n",
            "     | > postnet_loss: 0.26117  (0.26351)\n",
            "     | > stopnet_loss: 0.09992  (0.17113)\n",
            "     | > decoder_coarse_loss: 0.46685  (0.47786)\n",
            "     | > decoder_ddc_loss: 0.00926  (0.01068)\n",
            "     | > ga_loss: 0.00023  (0.00048)\n",
            "     | > decoder_diff_spec_loss: 0.19468  (0.20013)\n",
            "     | > postnet_diff_spec_loss: 0.17733  (0.18140)\n",
            "     | > decoder_ssim_loss: 0.23917  (0.22395)\n",
            "     | > postnet_ssim_loss: 0.22977  (0.21466)\n",
            "     | > loss: 0.56555  (0.63746)\n",
            "     | > align_error: 0.20794  (0.19981)\n",
            "     | > grad_norm: 0.72915  (1.05994)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.19210  (1.99315)\n",
            "     | > loader_time: 0.00470  (0.00486)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 186/722 -- GLOBAL_STEP: 273200\u001b[0m\n",
            "     | > decoder_loss: 0.28648  (0.28409)\n",
            "     | > postnet_loss: 0.26591  (0.26408)\n",
            "     | > stopnet_loss: 0.13000  (0.16966)\n",
            "     | > decoder_coarse_loss: 0.45601  (0.47828)\n",
            "     | > decoder_ddc_loss: 0.00853  (0.01039)\n",
            "     | > ga_loss: 0.00023  (0.00045)\n",
            "     | > decoder_diff_spec_loss: 0.19814  (0.20025)\n",
            "     | > postnet_diff_spec_loss: 0.17968  (0.18146)\n",
            "     | > decoder_ssim_loss: 0.22473  (0.22368)\n",
            "     | > postnet_ssim_loss: 0.21516  (0.21433)\n",
            "     | > loss: 0.58980  (0.63606)\n",
            "     | > align_error: 0.21114  (0.20134)\n",
            "     | > grad_norm: 0.57673  (1.05942)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.32880  (2.04359)\n",
            "     | > loader_time: 0.00450  (0.00489)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 211/722 -- GLOBAL_STEP: 273225\u001b[0m\n",
            "     | > decoder_loss: 0.29098  (0.28451)\n",
            "     | > postnet_loss: 0.26969  (0.26440)\n",
            "     | > stopnet_loss: 0.18971  (0.16735)\n",
            "     | > decoder_coarse_loss: 0.48696  (0.47896)\n",
            "     | > decoder_ddc_loss: 0.00850  (0.01015)\n",
            "     | > ga_loss: 0.00020  (0.00043)\n",
            "     | > decoder_diff_spec_loss: 0.20307  (0.20045)\n",
            "     | > postnet_diff_spec_loss: 0.18354  (0.18158)\n",
            "     | > decoder_ssim_loss: 0.21009  (0.22310)\n",
            "     | > postnet_ssim_loss: 0.20040  (0.21372)\n",
            "     | > loss: 0.65403  (0.63369)\n",
            "     | > align_error: 0.21596  (0.20269)\n",
            "     | > grad_norm: 0.60238  (1.03605)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.60470  (2.08905)\n",
            "     | > loader_time: 0.00500  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 236/722 -- GLOBAL_STEP: 273250\u001b[0m\n",
            "     | > decoder_loss: 0.30310  (0.28520)\n",
            "     | > postnet_loss: 0.28027  (0.26498)\n",
            "     | > stopnet_loss: 0.10387  (0.16460)\n",
            "     | > decoder_coarse_loss: 0.52088  (0.48001)\n",
            "     | > decoder_ddc_loss: 0.00792  (0.00994)\n",
            "     | > ga_loss: 0.00020  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.20758  (0.20072)\n",
            "     | > postnet_diff_spec_loss: 0.18578  (0.18176)\n",
            "     | > decoder_ssim_loss: 0.23018  (0.22288)\n",
            "     | > postnet_ssim_loss: 0.21988  (0.21346)\n",
            "     | > loss: 0.59379  (0.63135)\n",
            "     | > align_error: 0.20875  (0.20375)\n",
            "     | > grad_norm: 0.80851  (1.01117)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.49430  (2.13170)\n",
            "     | > loader_time: 0.00540  (0.00491)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 261/722 -- GLOBAL_STEP: 273275\u001b[0m\n",
            "     | > decoder_loss: 0.28749  (0.28587)\n",
            "     | > postnet_loss: 0.26621  (0.26550)\n",
            "     | > stopnet_loss: 0.16161  (0.16212)\n",
            "     | > decoder_coarse_loss: 0.48970  (0.48131)\n",
            "     | > decoder_ddc_loss: 0.00714  (0.00976)\n",
            "     | > ga_loss: 0.00016  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.19928  (0.20111)\n",
            "     | > postnet_diff_spec_loss: 0.17930  (0.18201)\n",
            "     | > decoder_ssim_loss: 0.21772  (0.22266)\n",
            "     | > postnet_ssim_loss: 0.20864  (0.21319)\n",
            "     | > loss: 0.62628  (0.62938)\n",
            "     | > align_error: 0.20996  (0.20470)\n",
            "     | > grad_norm: 0.80691  (0.99579)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.82480  (2.17359)\n",
            "     | > loader_time: 0.00510  (0.00495)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 286/722 -- GLOBAL_STEP: 273300\u001b[0m\n",
            "     | > decoder_loss: 0.28405  (0.28632)\n",
            "     | > postnet_loss: 0.26377  (0.26585)\n",
            "     | > stopnet_loss: 0.16633  (0.16049)\n",
            "     | > decoder_coarse_loss: 0.49044  (0.48171)\n",
            "     | > decoder_ddc_loss: 0.00709  (0.00957)\n",
            "     | > ga_loss: 0.00017  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.19526  (0.20134)\n",
            "     | > postnet_diff_spec_loss: 0.17781  (0.18214)\n",
            "     | > decoder_ssim_loss: 0.21529  (0.22245)\n",
            "     | > postnet_ssim_loss: 0.20642  (0.21295)\n",
            "     | > loss: 0.62719  (0.62789)\n",
            "     | > align_error: 0.21383  (0.20558)\n",
            "     | > grad_norm: 0.71152  (0.98011)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.72440  (2.21631)\n",
            "     | > loader_time: 0.00540  (0.00499)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 311/722 -- GLOBAL_STEP: 273325\u001b[0m\n",
            "     | > decoder_loss: 0.28315  (0.28674)\n",
            "     | > postnet_loss: 0.26247  (0.26617)\n",
            "     | > stopnet_loss: 0.07771  (0.15841)\n",
            "     | > decoder_coarse_loss: 0.49405  (0.48246)\n",
            "     | > decoder_ddc_loss: 0.00783  (0.00940)\n",
            "     | > ga_loss: 0.00012  (0.00035)\n",
            "     | > decoder_diff_spec_loss: 0.20089  (0.20149)\n",
            "     | > postnet_diff_spec_loss: 0.18035  (0.18222)\n",
            "     | > decoder_ssim_loss: 0.22847  (0.22229)\n",
            "     | > postnet_ssim_loss: 0.21885  (0.21276)\n",
            "     | > loss: 0.54734  (0.62602)\n",
            "     | > align_error: 0.21854  (0.20658)\n",
            "     | > grad_norm: 0.59540  (0.97790)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.63720  (2.25776)\n",
            "     | > loader_time: 0.00460  (0.00501)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 336/722 -- GLOBAL_STEP: 273350\u001b[0m\n",
            "     | > decoder_loss: 0.29369  (0.28701)\n",
            "     | > postnet_loss: 0.27148  (0.26636)\n",
            "     | > stopnet_loss: 0.08113  (0.15655)\n",
            "     | > decoder_coarse_loss: 0.49523  (0.48308)\n",
            "     | > decoder_ddc_loss: 0.00727  (0.00923)\n",
            "     | > ga_loss: 0.00018  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.20961  (0.20159)\n",
            "     | > postnet_diff_spec_loss: 0.18771  (0.18227)\n",
            "     | > decoder_ssim_loss: 0.23284  (0.22223)\n",
            "     | > postnet_ssim_loss: 0.22200  (0.21267)\n",
            "     | > loss: 0.56199  (0.62432)\n",
            "     | > align_error: 0.22364  (0.20749)\n",
            "     | > grad_norm: 0.57648  (0.96123)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.60010  (2.29903)\n",
            "     | > loader_time: 0.00470  (0.00503)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 361/722 -- GLOBAL_STEP: 273375\u001b[0m\n",
            "     | > decoder_loss: 0.30089  (0.28742)\n",
            "     | > postnet_loss: 0.27828  (0.26668)\n",
            "     | > stopnet_loss: 0.15629  (0.15637)\n",
            "     | > decoder_coarse_loss: 0.51004  (0.48412)\n",
            "     | > decoder_ddc_loss: 0.00676  (0.00907)\n",
            "     | > ga_loss: 0.00014  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.20466  (0.20181)\n",
            "     | > postnet_diff_spec_loss: 0.18387  (0.18241)\n",
            "     | > decoder_ssim_loss: 0.22164  (0.22185)\n",
            "     | > postnet_ssim_loss: 0.21166  (0.21225)\n",
            "     | > loss: 0.63642  (0.62437)\n",
            "     | > align_error: 0.22819  (0.20856)\n",
            "     | > grad_norm: 0.74995  (0.96657)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.17010  (2.34319)\n",
            "     | > loader_time: 0.00510  (0.00505)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 386/722 -- GLOBAL_STEP: 273400\u001b[0m\n",
            "     | > decoder_loss: 0.30732  (0.28767)\n",
            "     | > postnet_loss: 0.28292  (0.26687)\n",
            "     | > stopnet_loss: 0.12338  (0.15382)\n",
            "     | > decoder_coarse_loss: 0.52383  (0.48472)\n",
            "     | > decoder_ddc_loss: 0.00656  (0.00893)\n",
            "     | > ga_loss: 0.00012  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.21179  (0.20188)\n",
            "     | > postnet_diff_spec_loss: 0.18913  (0.18244)\n",
            "     | > decoder_ssim_loss: 0.21977  (0.22203)\n",
            "     | > postnet_ssim_loss: 0.20920  (0.21240)\n",
            "     | > loss: 0.61162  (0.62209)\n",
            "     | > align_error: 0.21785  (0.20945)\n",
            "     | > grad_norm: 0.75522  (0.95377)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.85340  (2.37870)\n",
            "     | > loader_time: 0.00500  (0.00506)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 411/722 -- GLOBAL_STEP: 273425\u001b[0m\n",
            "     | > decoder_loss: 0.29790  (0.28804)\n",
            "     | > postnet_loss: 0.27491  (0.26715)\n",
            "     | > stopnet_loss: 0.18537  (0.15223)\n",
            "     | > decoder_coarse_loss: 0.52279  (0.48577)\n",
            "     | > decoder_ddc_loss: 0.00623  (0.00880)\n",
            "     | > ga_loss: 0.00013  (0.00030)\n",
            "     | > decoder_diff_spec_loss: 0.20670  (0.20205)\n",
            "     | > postnet_diff_spec_loss: 0.18631  (0.18254)\n",
            "     | > decoder_ssim_loss: 0.21290  (0.22202)\n",
            "     | > postnet_ssim_loss: 0.20246  (0.21237)\n",
            "     | > loss: 0.66358  (0.62088)\n",
            "     | > align_error: 0.22453  (0.21024)\n",
            "     | > grad_norm: 0.64191  (0.93936)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.18330  (2.41611)\n",
            "     | > loader_time: 0.00510  (0.00508)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 436/722 -- GLOBAL_STEP: 273450\u001b[0m\n",
            "     | > decoder_loss: 0.29578  (0.28830)\n",
            "     | > postnet_loss: 0.27372  (0.26732)\n",
            "     | > stopnet_loss: 0.09274  (0.15132)\n",
            "     | > decoder_coarse_loss: 0.49926  (0.48661)\n",
            "     | > decoder_ddc_loss: 0.00651  (0.00865)\n",
            "     | > ga_loss: 0.00010  (0.00028)\n",
            "     | > decoder_diff_spec_loss: 0.19908  (0.20216)\n",
            "     | > postnet_diff_spec_loss: 0.18019  (0.18257)\n",
            "     | > decoder_ssim_loss: 0.23656  (0.22181)\n",
            "     | > postnet_ssim_loss: 0.22592  (0.21213)\n",
            "     | > loss: 0.57251  (0.62013)\n",
            "     | > align_error: 0.22453  (0.21102)\n",
            "     | > grad_norm: 0.84004  (0.92559)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.91530  (2.45840)\n",
            "     | > loader_time: 0.00540  (0.00510)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 461/722 -- GLOBAL_STEP: 273475\u001b[0m\n",
            "     | > decoder_loss: 0.29583  (0.28870)\n",
            "     | > postnet_loss: 0.27372  (0.26762)\n",
            "     | > stopnet_loss: 0.09760  (0.15064)\n",
            "     | > decoder_coarse_loss: 0.51381  (0.48776)\n",
            "     | > decoder_ddc_loss: 0.00613  (0.00853)\n",
            "     | > ga_loss: 0.00011  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.20372  (0.20241)\n",
            "     | > postnet_diff_spec_loss: 0.18390  (0.18273)\n",
            "     | > decoder_ssim_loss: 0.23981  (0.22162)\n",
            "     | > postnet_ssim_loss: 0.22898  (0.21191)\n",
            "     | > loss: 0.58462  (0.61983)\n",
            "     | > align_error: 0.23110  (0.21176)\n",
            "     | > grad_norm: 0.85348  (0.92774)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.98480  (2.49991)\n",
            "     | > loader_time: 0.00480  (0.00511)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 486/722 -- GLOBAL_STEP: 273500\u001b[0m\n",
            "     | > decoder_loss: 0.29524  (0.28902)\n",
            "     | > postnet_loss: 0.27106  (0.26786)\n",
            "     | > stopnet_loss: 0.16997  (0.15015)\n",
            "     | > decoder_coarse_loss: 0.53839  (0.48875)\n",
            "     | > decoder_ddc_loss: 0.00589  (0.00839)\n",
            "     | > ga_loss: 0.00008  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.20705  (0.20256)\n",
            "     | > postnet_diff_spec_loss: 0.18549  (0.18283)\n",
            "     | > decoder_ssim_loss: 0.19911  (0.22133)\n",
            "     | > postnet_ssim_loss: 0.18952  (0.21160)\n",
            "     | > loss: 0.64332  (0.61956)\n",
            "     | > align_error: 0.23216  (0.21258)\n",
            "     | > grad_norm: 0.88936  (0.91658)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.50490  (2.54409)\n",
            "     | > loader_time: 0.00600  (0.00514)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 511/722 -- GLOBAL_STEP: 273525\u001b[0m\n",
            "     | > decoder_loss: 0.30456  (0.28934)\n",
            "     | > postnet_loss: 0.28120  (0.26810)\n",
            "     | > stopnet_loss: 0.11330  (0.14914)\n",
            "     | > decoder_coarse_loss: 0.52292  (0.48974)\n",
            "     | > decoder_ddc_loss: 0.00593  (0.00827)\n",
            "     | > ga_loss: 0.00007  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.21041  (0.20277)\n",
            "     | > postnet_diff_spec_loss: 0.18812  (0.18296)\n",
            "     | > decoder_ssim_loss: 0.23045  (0.22113)\n",
            "     | > postnet_ssim_loss: 0.22007  (0.21137)\n",
            "     | > loss: 0.60454  (0.61884)\n",
            "     | > align_error: 0.22487  (0.21332)\n",
            "     | > grad_norm: 0.72813  (0.90392)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.44870  (2.58785)\n",
            "     | > loader_time: 0.00540  (0.00515)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 536/722 -- GLOBAL_STEP: 273550\u001b[0m\n",
            "     | > decoder_loss: 0.30766  (0.28962)\n",
            "     | > postnet_loss: 0.28325  (0.26830)\n",
            "     | > stopnet_loss: 0.17042  (0.14838)\n",
            "     | > decoder_coarse_loss: 0.53407  (0.49078)\n",
            "     | > decoder_ddc_loss: 0.00509  (0.00815)\n",
            "     | > ga_loss: 0.00007  (0.00025)\n",
            "     | > decoder_diff_spec_loss: 0.20953  (0.20289)\n",
            "     | > postnet_diff_spec_loss: 0.18799  (0.18303)\n",
            "     | > decoder_ssim_loss: 0.21277  (0.22100)\n",
            "     | > postnet_ssim_loss: 0.20240  (0.21123)\n",
            "     | > loss: 0.65646  (0.61837)\n",
            "     | > align_error: 0.23429  (0.21401)\n",
            "     | > grad_norm: 0.56110  (0.89025)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.82530  (2.63307)\n",
            "     | > loader_time: 0.00600  (0.00518)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 561/722 -- GLOBAL_STEP: 273575\u001b[0m\n",
            "     | > decoder_loss: 0.30501  (0.28985)\n",
            "     | > postnet_loss: 0.28075  (0.26845)\n",
            "     | > stopnet_loss: 0.12270  (0.14706)\n",
            "     | > decoder_coarse_loss: 0.51569  (0.49154)\n",
            "     | > decoder_ddc_loss: 0.00601  (0.00804)\n",
            "     | > ga_loss: 0.00007  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.21039  (0.20304)\n",
            "     | > postnet_diff_spec_loss: 0.18786  (0.18312)\n",
            "     | > decoder_ssim_loss: 0.22957  (0.22101)\n",
            "     | > postnet_ssim_loss: 0.21816  (0.21120)\n",
            "     | > loss: 0.61139  (0.61732)\n",
            "     | > align_error: 0.22575  (0.21481)\n",
            "     | > grad_norm: 0.47626  (0.87602)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.65180  (2.67630)\n",
            "     | > loader_time: 0.00600  (0.00520)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 586/722 -- GLOBAL_STEP: 273600\u001b[0m\n",
            "     | > decoder_loss: 0.29446  (0.29017)\n",
            "     | > postnet_loss: 0.27127  (0.26868)\n",
            "     | > stopnet_loss: 0.08481  (0.14603)\n",
            "     | > decoder_coarse_loss: 0.49635  (0.49261)\n",
            "     | > decoder_ddc_loss: 0.00542  (0.00793)\n",
            "     | > ga_loss: 0.00006  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.20549  (0.20319)\n",
            "     | > postnet_diff_spec_loss: 0.18337  (0.18319)\n",
            "     | > decoder_ssim_loss: 0.23117  (0.22095)\n",
            "     | > postnet_ssim_loss: 0.22034  (0.21111)\n",
            "     | > loss: 0.56208  (0.61665)\n",
            "     | > align_error: 0.23111  (0.21549)\n",
            "     | > grad_norm: 1.09592  (0.86831)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.57910  (2.72268)\n",
            "     | > loader_time: 0.00570  (0.00522)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 611/722 -- GLOBAL_STEP: 273625\u001b[0m\n",
            "     | > decoder_loss: 0.30369  (0.29048)\n",
            "     | > postnet_loss: 0.27997  (0.26891)\n",
            "     | > stopnet_loss: 0.08268  (0.14510)\n",
            "     | > decoder_coarse_loss: 0.53587  (0.49359)\n",
            "     | > decoder_ddc_loss: 0.00529  (0.00782)\n",
            "     | > ga_loss: 0.00006  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.20851  (0.20335)\n",
            "     | > postnet_diff_spec_loss: 0.18634  (0.18328)\n",
            "     | > decoder_ssim_loss: 0.22218  (0.22091)\n",
            "     | > postnet_ssim_loss: 0.21148  (0.21105)\n",
            "     | > loss: 0.57133  (0.61608)\n",
            "     | > align_error: 0.23138  (0.21616)\n",
            "     | > grad_norm: 0.74577  (0.86047)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.59640  (2.76701)\n",
            "     | > loader_time: 0.00600  (0.00523)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 636/722 -- GLOBAL_STEP: 273650\u001b[0m\n",
            "     | > decoder_loss: 0.30363  (0.29084)\n",
            "     | > postnet_loss: 0.27898  (0.26918)\n",
            "     | > stopnet_loss: 0.09106  (0.14422)\n",
            "     | > decoder_coarse_loss: 0.51662  (0.49461)\n",
            "     | > decoder_ddc_loss: 0.00504  (0.00771)\n",
            "     | > ga_loss: 0.00006  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.21042  (0.20353)\n",
            "     | > postnet_diff_spec_loss: 0.18758  (0.18339)\n",
            "     | > decoder_ssim_loss: 0.22849  (0.22086)\n",
            "     | > postnet_ssim_loss: 0.21713  (0.21097)\n",
            "     | > loss: 0.57832  (0.61559)\n",
            "     | > align_error: 0.23288  (0.21684)\n",
            "     | > grad_norm: 0.82361  (0.85420)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.75940  (2.81512)\n",
            "     | > loader_time: 0.00600  (0.00526)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 661/722 -- GLOBAL_STEP: 273675\u001b[0m\n",
            "     | > decoder_loss: 0.30458  (0.29130)\n",
            "     | > postnet_loss: 0.27981  (0.26955)\n",
            "     | > stopnet_loss: 0.14004  (0.14324)\n",
            "     | > decoder_coarse_loss: 0.53165  (0.49596)\n",
            "     | > decoder_ddc_loss: 0.00449  (0.00760)\n",
            "     | > ga_loss: 0.00006  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.20897  (0.20374)\n",
            "     | > postnet_diff_spec_loss: 0.18664  (0.18352)\n",
            "     | > decoder_ssim_loss: 0.22297  (0.22079)\n",
            "     | > postnet_ssim_loss: 0.21210  (0.21088)\n",
            "     | > loss: 0.62812  (0.61514)\n",
            "     | > align_error: 0.24176  (0.21757)\n",
            "     | > grad_norm: 0.49708  (0.85118)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.40520  (2.86350)\n",
            "     | > loader_time: 0.00630  (0.00528)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 686/722 -- GLOBAL_STEP: 273700\u001b[0m\n",
            "     | > decoder_loss: 0.30602  (0.29170)\n",
            "     | > postnet_loss: 0.28150  (0.26984)\n",
            "     | > stopnet_loss: 0.07646  (0.14283)\n",
            "     | > decoder_coarse_loss: 0.53292  (0.49743)\n",
            "     | > decoder_ddc_loss: 0.00447  (0.00748)\n",
            "     | > ga_loss: 0.00003  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.21439  (0.20395)\n",
            "     | > postnet_diff_spec_loss: 0.19140  (0.18364)\n",
            "     | > decoder_ssim_loss: 0.22761  (0.22064)\n",
            "     | > postnet_ssim_loss: 0.21654  (0.21070)\n",
            "     | > loss: 0.57034  (0.61521)\n",
            "     | > align_error: 0.23996  (0.21832)\n",
            "     | > grad_norm: 0.51518  (0.84325)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.18200  (2.92142)\n",
            "     | > loader_time: 0.00660  (0.00531)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 711/722 -- GLOBAL_STEP: 273725\u001b[0m\n",
            "     | > decoder_loss: 0.31126  (0.29218)\n",
            "     | > postnet_loss: 0.28529  (0.27020)\n",
            "     | > stopnet_loss: 0.09555  (0.14247)\n",
            "     | > decoder_coarse_loss: 0.57186  (0.49914)\n",
            "     | > decoder_ddc_loss: 0.00397  (0.00737)\n",
            "     | > ga_loss: 0.00003  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.21574  (0.20419)\n",
            "     | > postnet_diff_spec_loss: 0.19172  (0.18379)\n",
            "     | > decoder_ssim_loss: 0.22095  (0.22053)\n",
            "     | > postnet_ssim_loss: 0.20969  (0.21056)\n",
            "     | > loss: 0.59833  (0.61547)\n",
            "     | > align_error: 0.24202  (0.21921)\n",
            "     | > grad_norm: 0.58318  (0.83521)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 5.25650  (2.99253)\n",
            "     | > loader_time: 0.00550  (0.00536)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.41458  (0.41458)\n",
            "     | > postnet_loss: 0.37054  (0.37054)\n",
            "     | > stopnet_loss: 0.22552  (0.22552)\n",
            "     | > decoder_coarse_loss: 0.63234  (0.63234)\n",
            "     | > decoder_ddc_loss: 0.00836  (0.00836)\n",
            "     | > ga_loss: 0.00062  (0.00062)\n",
            "     | > decoder_diff_spec_loss: 0.18670  (0.18670)\n",
            "     | > postnet_diff_spec_loss: 0.15441  (0.15441)\n",
            "     | > decoder_ssim_loss: 0.21507  (0.21507)\n",
            "     | > postnet_ssim_loss: 0.20037  (0.20037)\n",
            "     | > loss: 0.77421  (0.77421)\n",
            "     | > align_error: 0.36633  (0.36633)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.35085  (0.35085)\n",
            "     | > postnet_loss: 0.31061  (0.31061)\n",
            "     | > stopnet_loss: 0.10717  (0.10717)\n",
            "     | > decoder_coarse_loss: 0.57291  (0.57291)\n",
            "     | > decoder_ddc_loss: 0.00848  (0.00848)\n",
            "     | > ga_loss: 0.00027  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.19206  (0.19206)\n",
            "     | > postnet_diff_spec_loss: 0.15746  (0.15746)\n",
            "     | > decoder_ssim_loss: 0.23469  (0.23469)\n",
            "     | > postnet_ssim_loss: 0.21716  (0.21716)\n",
            "     | > loss: 0.61956  (0.61956)\n",
            "     | > align_error: 0.26874  (0.26874)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.37322  (0.36204)\n",
            "     | > postnet_loss: 0.33179  (0.32120)\n",
            "     | > stopnet_loss: 0.14714  (0.12715)\n",
            "     | > decoder_coarse_loss: 0.58668  (0.57980)\n",
            "     | > decoder_ddc_loss: 0.00784  (0.00816)\n",
            "     | > ga_loss: 0.00020  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.18883  (0.19044)\n",
            "     | > postnet_diff_spec_loss: 0.15466  (0.15606)\n",
            "     | > decoder_ssim_loss: 0.21538  (0.22503)\n",
            "     | > postnet_ssim_loss: 0.19969  (0.20843)\n",
            "     | > loss: 0.66268  (0.64112)\n",
            "     | > align_error: 0.28177  (0.27525)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.39146  (0.37184)\n",
            "     | > postnet_loss: 0.34397  (0.32879)\n",
            "     | > stopnet_loss: 0.13203  (0.12878)\n",
            "     | > decoder_coarse_loss: 0.65547  (0.60502)\n",
            "     | > decoder_ddc_loss: 0.00629  (0.00754)\n",
            "     | > ga_loss: 0.00012  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.20470  (0.19520)\n",
            "     | > postnet_diff_spec_loss: 0.16463  (0.15892)\n",
            "     | > decoder_ssim_loss: 0.22149  (0.22385)\n",
            "     | > postnet_ssim_loss: 0.20444  (0.20710)\n",
            "     | > loss: 0.68075  (0.65433)\n",
            "     | > align_error: 0.27574  (0.27542)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.36263  (0.36954)\n",
            "     | > postnet_loss: 0.31663  (0.32575)\n",
            "     | > stopnet_loss: 0.07620  (0.11563)\n",
            "     | > decoder_coarse_loss: 0.61656  (0.60791)\n",
            "     | > decoder_ddc_loss: 0.00603  (0.00716)\n",
            "     | > ga_loss: 0.00010  (0.00017)\n",
            "     | > decoder_diff_spec_loss: 0.20082  (0.19660)\n",
            "     | > postnet_diff_spec_loss: 0.16212  (0.15972)\n",
            "     | > decoder_ssim_loss: 0.23335  (0.22623)\n",
            "     | > postnet_ssim_loss: 0.21495  (0.20906)\n",
            "     | > loss: 0.60495  (0.64199)\n",
            "     | > align_error: 0.25610  (0.27059)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.36037  (0.36771)\n",
            "     | > postnet_loss: 0.31755  (0.32411)\n",
            "     | > stopnet_loss: 0.14620  (0.12175)\n",
            "     | > decoder_coarse_loss: 0.60097  (0.60652)\n",
            "     | > decoder_ddc_loss: 0.00561  (0.00685)\n",
            "     | > ga_loss: 0.00007  (0.00015)\n",
            "     | > decoder_diff_spec_loss: 0.19128  (0.19554)\n",
            "     | > postnet_diff_spec_loss: 0.15410  (0.15859)\n",
            "     | > decoder_ssim_loss: 0.20907  (0.22279)\n",
            "     | > postnet_ssim_loss: 0.19228  (0.20570)\n",
            "     | > loss: 0.65434  (0.64446)\n",
            "     | > align_error: 0.27833  (0.27214)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.34877  (0.36455)\n",
            "     | > postnet_loss: 0.30497  (0.32092)\n",
            "     | > stopnet_loss: 0.16983  (0.12976)\n",
            "     | > decoder_coarse_loss: 0.59888  (0.60525)\n",
            "     | > decoder_ddc_loss: 0.00391  (0.00636)\n",
            "     | > ga_loss: 0.00004  (0.00013)\n",
            "     | > decoder_diff_spec_loss: 0.19098  (0.19478)\n",
            "     | > postnet_diff_spec_loss: 0.15406  (0.15784)\n",
            "     | > decoder_ssim_loss: 0.19983  (0.21897)\n",
            "     | > postnet_ssim_loss: 0.18315  (0.20194)\n",
            "     | > loss: 0.66616  (0.64807)\n",
            "     | > align_error: 0.30669  (0.27789)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.41139  (0.37124)\n",
            "     | > postnet_loss: 0.35710  (0.32609)\n",
            "     | > stopnet_loss: 0.05427  (0.11898)\n",
            "     | > decoder_coarse_loss: 0.70850  (0.62000)\n",
            "     | > decoder_ddc_loss: 0.00395  (0.00601)\n",
            "     | > ga_loss: 0.00003  (0.00012)\n",
            "     | > decoder_diff_spec_loss: 0.20614  (0.19640)\n",
            "     | > postnet_diff_spec_loss: 0.16523  (0.15889)\n",
            "     | > decoder_ssim_loss: 0.24362  (0.22249)\n",
            "     | > postnet_ssim_loss: 0.22450  (0.20517)\n",
            "     | > loss: 0.63452  (0.64614)\n",
            "     | > align_error: 0.26818  (0.27651)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00384 \u001b[0m(-0.00188)\n",
            "     | > avg_decoder_loss:\u001b[92m 0.37124 \u001b[0m(-0.00161)\n",
            "     | > avg_postnet_loss:\u001b[92m 0.32609 \u001b[0m(-0.00272)\n",
            "     | > avg_stopnet_loss:\u001b[91m 0.11898 \u001b[0m(+0.00002)\n",
            "     | > avg_decoder_coarse_loss:\u001b[92m 0.62000 \u001b[0m(-0.00206)\n",
            "     | > avg_decoder_ddc_loss:\u001b[91m 0.00601 \u001b[0m(+0.00001)\n",
            "     | > avg_ga_loss:\u001b[91m 0.00012 \u001b[0m(+0.00000)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[91m 0.19640 \u001b[0m(+0.00043)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.15889 \u001b[0m(-0.00001)\n",
            "     | > avg_decoder_ssim_loss:\u001b[92m 0.22249 \u001b[0m(-0.00012)\n",
            "     | > avg_postnet_ssim_loss:\u001b[92m 0.20517 \u001b[0m(-0.00022)\n",
            "     | > avg_loss:\u001b[92m 0.64614 \u001b[0m(-0.00156)\n",
            "     | > avg_align_error:\u001b[91m 0.27651 \u001b[0m(+0.00207)\n",
            "\n",
            "\n",
            " > Number of output frames: 1\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 19/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2022-01-01 05:11:05) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 13/722 -- GLOBAL_STEP: 273750\u001b[0m\n",
            "     | > decoder_loss: 0.29495  (0.27569)\n",
            "     | > postnet_loss: 0.27441  (0.25638)\n",
            "     | > stopnet_loss: 0.18661  (0.19505)\n",
            "     | > decoder_coarse_loss: 0.50123  (0.46697)\n",
            "     | > decoder_ddc_loss: 0.01469  (0.01520)\n",
            "     | > ga_loss: 0.00091  (0.00115)\n",
            "     | > decoder_diff_spec_loss: 0.20232  (0.19877)\n",
            "     | > postnet_diff_spec_loss: 0.18369  (0.18073)\n",
            "     | > decoder_ssim_loss: 0.23983  (0.23005)\n",
            "     | > postnet_ssim_loss: 0.23020  (0.22103)\n",
            "     | > loss: 0.67651  (0.66199)\n",
            "     | > align_error: 0.20855  (0.18799)\n",
            "     | > grad_norm: 0.96925  (1.12784)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.46400  (1.43573)\n",
            "     | > loader_time: 0.00520  (0.00466)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 38/722 -- GLOBAL_STEP: 273775\u001b[0m\n",
            "     | > decoder_loss: 0.27682  (0.27738)\n",
            "     | > postnet_loss: 0.25792  (0.25844)\n",
            "     | > stopnet_loss: 0.17598  (0.20192)\n",
            "     | > decoder_coarse_loss: 0.51364  (0.46591)\n",
            "     | > decoder_ddc_loss: 0.01081  (0.01329)\n",
            "     | > ga_loss: 0.00058  (0.00086)\n",
            "     | > decoder_diff_spec_loss: 0.19193  (0.19744)\n",
            "     | > postnet_diff_spec_loss: 0.17535  (0.17972)\n",
            "     | > decoder_ssim_loss: 0.22624  (0.22741)\n",
            "     | > postnet_ssim_loss: 0.21761  (0.21842)\n",
            "     | > loss: 0.64645  (0.66573)\n",
            "     | > align_error: 0.18917  (0.19288)\n",
            "     | > grad_norm: 0.58009  (1.44425)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.82970  (1.61349)\n",
            "     | > loader_time: 0.00470  (0.00484)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 63/722 -- GLOBAL_STEP: 273800\u001b[0m\n",
            "     | > decoder_loss: 0.29726  (0.27960)\n",
            "     | > postnet_loss: 0.27481  (0.26037)\n",
            "     | > stopnet_loss: 0.13033  (0.18533)\n",
            "     | > decoder_coarse_loss: 0.50153  (0.47053)\n",
            "     | > decoder_ddc_loss: 0.01150  (0.01239)\n",
            "     | > ga_loss: 0.00049  (0.00071)\n",
            "     | > decoder_diff_spec_loss: 0.20225  (0.19828)\n",
            "     | > postnet_diff_spec_loss: 0.18213  (0.18032)\n",
            "     | > decoder_ssim_loss: 0.24169  (0.22702)\n",
            "     | > postnet_ssim_loss: 0.23136  (0.21790)\n",
            "     | > loss: 0.61839  (0.65050)\n",
            "     | > align_error: 0.20708  (0.19427)\n",
            "     | > grad_norm: 0.68400  (1.22184)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.85210  (1.70730)\n",
            "     | > loader_time: 0.00440  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/722 -- GLOBAL_STEP: 273825\u001b[0m\n",
            "     | > decoder_loss: 0.29785  (0.28141)\n",
            "     | > postnet_loss: 0.27526  (0.26187)\n",
            "     | > stopnet_loss: 0.14252  (0.18468)\n",
            "     | > decoder_coarse_loss: 0.56368  (0.47376)\n",
            "     | > decoder_ddc_loss: 0.01137  (0.01180)\n",
            "     | > ga_loss: 0.00039  (0.00063)\n",
            "     | > decoder_diff_spec_loss: 0.21775  (0.19928)\n",
            "     | > postnet_diff_spec_loss: 0.19584  (0.18098)\n",
            "     | > decoder_ssim_loss: 0.21647  (0.22452)\n",
            "     | > postnet_ssim_loss: 0.20652  (0.21538)\n",
            "     | > loss: 0.64065  (0.65007)\n",
            "     | > align_error: 0.20855  (0.19626)\n",
            "     | > grad_norm: 0.71605  (1.23737)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.00270  (1.80896)\n",
            "     | > loader_time: 0.00570  (0.00488)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 113/722 -- GLOBAL_STEP: 273850\u001b[0m\n",
            "     | > decoder_loss: 0.28941  (0.28242)\n",
            "     | > postnet_loss: 0.26872  (0.26267)\n",
            "     | > stopnet_loss: 0.12865  (0.17805)\n",
            "     | > decoder_coarse_loss: 0.47326  (0.47549)\n",
            "     | > decoder_ddc_loss: 0.00957  (0.01136)\n",
            "     | > ga_loss: 0.00032  (0.00057)\n",
            "     | > decoder_diff_spec_loss: 0.20490  (0.19979)\n",
            "     | > postnet_diff_spec_loss: 0.18477  (0.18124)\n",
            "     | > decoder_ssim_loss: 0.23296  (0.22441)\n",
            "     | > postnet_ssim_loss: 0.22252  (0.21518)\n",
            "     | > loss: 0.60178  (0.64402)\n",
            "     | > align_error: 0.20683  (0.19818)\n",
            "     | > grad_norm: 1.65986  (1.17483)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.10170  (1.87612)\n",
            "     | > loader_time: 0.00500  (0.00489)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 138/722 -- GLOBAL_STEP: 273875\u001b[0m\n",
            "     | > decoder_loss: 0.28684  (0.28364)\n",
            "     | > postnet_loss: 0.26720  (0.26369)\n",
            "     | > stopnet_loss: 0.22906  (0.17517)\n",
            "     | > decoder_coarse_loss: 0.48715  (0.47736)\n",
            "     | > decoder_ddc_loss: 0.00856  (0.01097)\n",
            "     | > ga_loss: 0.00028  (0.00052)\n",
            "     | > decoder_diff_spec_loss: 0.19613  (0.20034)\n",
            "     | > postnet_diff_spec_loss: 0.17869  (0.18161)\n",
            "     | > decoder_ssim_loss: 0.20737  (0.22398)\n",
            "     | > postnet_ssim_loss: 0.19920  (0.21467)\n",
            "     | > loss: 0.68824  (0.64182)\n",
            "     | > align_error: 0.20499  (0.19955)\n",
            "     | > grad_norm: 0.86751  (1.11245)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.30990  (1.94477)\n",
            "     | > loader_time: 0.00550  (0.00494)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 163/722 -- GLOBAL_STEP: 273900\u001b[0m\n",
            "     | > decoder_loss: 0.28623  (0.28426)\n",
            "     | > postnet_loss: 0.26619  (0.26417)\n",
            "     | > stopnet_loss: 0.11973  (0.17097)\n",
            "     | > decoder_coarse_loss: 0.47279  (0.47764)\n",
            "     | > decoder_ddc_loss: 0.00917  (0.01066)\n",
            "     | > ga_loss: 0.00026  (0.00048)\n",
            "     | > decoder_diff_spec_loss: 0.19982  (0.20040)\n",
            "     | > postnet_diff_spec_loss: 0.18013  (0.18160)\n",
            "     | > decoder_ssim_loss: 0.22638  (0.22410)\n",
            "     | > postnet_ssim_loss: 0.21602  (0.21472)\n",
            "     | > loss: 0.58521  (0.63776)\n",
            "     | > align_error: 0.21032  (0.20125)\n",
            "     | > grad_norm: 0.69281  (1.08783)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.20530  (1.99650)\n",
            "     | > loader_time: 0.00560  (0.00497)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 188/722 -- GLOBAL_STEP: 273925\u001b[0m\n",
            "     | > decoder_loss: 0.28697  (0.28482)\n",
            "     | > postnet_loss: 0.26640  (0.26462)\n",
            "     | > stopnet_loss: 0.13437  (0.16963)\n",
            "     | > decoder_coarse_loss: 0.48023  (0.47838)\n",
            "     | > decoder_ddc_loss: 0.00858  (0.01037)\n",
            "     | > ga_loss: 0.00030  (0.00045)\n",
            "     | > decoder_diff_spec_loss: 0.20034  (0.20058)\n",
            "     | > postnet_diff_spec_loss: 0.18141  (0.18169)\n",
            "     | > decoder_ssim_loss: 0.23172  (0.22382)\n",
            "     | > postnet_ssim_loss: 0.22205  (0.21440)\n",
            "     | > loss: 0.60532  (0.63655)\n",
            "     | > align_error: 0.21183  (0.20257)\n",
            "     | > grad_norm: 0.66690  (1.09558)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.56800  (2.05292)\n",
            "     | > loader_time: 0.00470  (0.00500)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 213/722 -- GLOBAL_STEP: 273950\u001b[0m\n",
            "     | > decoder_loss: 0.29446  (0.28517)\n",
            "     | > postnet_loss: 0.27381  (0.26490)\n",
            "     | > stopnet_loss: 0.15701  (0.16734)\n",
            "     | > decoder_coarse_loss: 0.48194  (0.47922)\n",
            "     | > decoder_ddc_loss: 0.00777  (0.01013)\n",
            "     | > ga_loss: 0.00019  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.19986  (0.20076)\n",
            "     | > postnet_diff_spec_loss: 0.18173  (0.18181)\n",
            "     | > decoder_ssim_loss: 0.22217  (0.22314)\n",
            "     | > postnet_ssim_loss: 0.21215  (0.21370)\n",
            "     | > loss: 0.62644  (0.63417)\n",
            "     | > align_error: 0.21694  (0.20378)\n",
            "     | > grad_norm: 0.65682  (1.06926)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.55610  (2.09885)\n",
            "     | > loader_time: 0.00530  (0.00499)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 238/722 -- GLOBAL_STEP: 273975\u001b[0m\n",
            "     | > decoder_loss: 0.28679  (0.28569)\n",
            "     | > postnet_loss: 0.26601  (0.26530)\n",
            "     | > stopnet_loss: 0.11621  (0.16430)\n",
            "     | > decoder_coarse_loss: 0.47319  (0.48015)\n",
            "     | > decoder_ddc_loss: 0.00848  (0.00993)\n",
            "     | > ga_loss: 0.00018  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.19786  (0.20100)\n",
            "     | > postnet_diff_spec_loss: 0.17843  (0.18196)\n",
            "     | > decoder_ssim_loss: 0.23576  (0.22301)\n",
            "     | > postnet_ssim_loss: 0.22575  (0.21352)\n",
            "     | > loss: 0.58517  (0.63145)\n",
            "     | > align_error: 0.22083  (0.20471)\n",
            "     | > grad_norm: 0.92319  (1.03841)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.38380  (2.14302)\n",
            "     | > loader_time: 0.00450  (0.00499)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 263/722 -- GLOBAL_STEP: 274000\u001b[0m\n",
            "     | > decoder_loss: 0.28646  (0.28625)\n",
            "     | > postnet_loss: 0.26570  (0.26575)\n",
            "     | > stopnet_loss: 0.11463  (0.16212)\n",
            "     | > decoder_coarse_loss: 0.49373  (0.48147)\n",
            "     | > decoder_ddc_loss: 0.00919  (0.00974)\n",
            "     | > ga_loss: 0.00017  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.20430  (0.20137)\n",
            "     | > postnet_diff_spec_loss: 0.18434  (0.18220)\n",
            "     | > decoder_ssim_loss: 0.22229  (0.22264)\n",
            "     | > postnet_ssim_loss: 0.21167  (0.21313)\n",
            "     | > loss: 0.58488  (0.62965)\n",
            "     | > align_error: 0.22143  (0.20560)\n",
            "     | > grad_norm: 0.69404  (1.01872)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.38920  (2.18555)\n",
            "     | > loader_time: 0.00510  (0.00501)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/checkpoint_274000.pth.tar\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "\u001b[1m   --> STEP: 288/722 -- GLOBAL_STEP: 274025\u001b[0m\n",
            "     | > decoder_loss: 0.27987  (0.28649)\n",
            "     | > postnet_loss: 0.26037  (0.26590)\n",
            "     | > stopnet_loss: 0.07434  (0.16045)\n",
            "     | > decoder_coarse_loss: 0.47224  (0.48171)\n",
            "     | > decoder_ddc_loss: 0.00775  (0.00955)\n",
            "     | > ga_loss: 0.00015  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.19386  (0.20145)\n",
            "     | > postnet_diff_spec_loss: 0.17571  (0.18223)\n",
            "     | > decoder_ssim_loss: 0.23798  (0.22245)\n",
            "     | > postnet_ssim_loss: 0.22878  (0.21292)\n",
            "     | > loss: 0.53923  (0.62794)\n",
            "     | > align_error: 0.22660  (0.20639)\n",
            "     | > grad_norm: 0.78441  (0.99344)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.74370  (2.23323)\n",
            "     | > loader_time: 0.00470  (0.00520)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 313/722 -- GLOBAL_STEP: 274050\u001b[0m\n",
            "     | > decoder_loss: 0.28975  (0.28684)\n",
            "     | > postnet_loss: 0.26946  (0.26620)\n",
            "     | > stopnet_loss: 0.14692  (0.15868)\n",
            "     | > decoder_coarse_loss: 0.47524  (0.48248)\n",
            "     | > decoder_ddc_loss: 0.00708  (0.00937)\n",
            "     | > ga_loss: 0.00013  (0.00035)\n",
            "     | > decoder_diff_spec_loss: 0.19966  (0.20154)\n",
            "     | > postnet_diff_spec_loss: 0.18085  (0.18229)\n",
            "     | > decoder_ssim_loss: 0.22217  (0.22223)\n",
            "     | > postnet_ssim_loss: 0.21266  (0.21268)\n",
            "     | > loss: 0.61181  (0.62631)\n",
            "     | > align_error: 0.21594  (0.20718)\n",
            "     | > grad_norm: 0.89681  (0.97945)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.08090  (2.27686)\n",
            "     | > loader_time: 0.00510  (0.00522)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 338/722 -- GLOBAL_STEP: 274075\u001b[0m\n",
            "     | > decoder_loss: 0.29771  (0.28709)\n",
            "     | > postnet_loss: 0.27545  (0.26638)\n",
            "     | > stopnet_loss: 0.13201  (0.15654)\n",
            "     | > decoder_coarse_loss: 0.53430  (0.48313)\n",
            "     | > decoder_ddc_loss: 0.00744  (0.00921)\n",
            "     | > ga_loss: 0.00013  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.20909  (0.20168)\n",
            "     | > postnet_diff_spec_loss: 0.18825  (0.18236)\n",
            "     | > decoder_ssim_loss: 0.21264  (0.22219)\n",
            "     | > postnet_ssim_loss: 0.20300  (0.21259)\n",
            "     | > loss: 0.61461  (0.62435)\n",
            "     | > align_error: 0.22332  (0.20807)\n",
            "     | > grad_norm: 0.90473  (0.96150)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.89040  (2.31694)\n",
            "     | > loader_time: 0.00560  (0.00523)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 363/722 -- GLOBAL_STEP: 274100\u001b[0m\n",
            "     | > decoder_loss: 0.29168  (0.28743)\n",
            "     | > postnet_loss: 0.27022  (0.26664)\n",
            "     | > stopnet_loss: 0.11783  (0.15626)\n",
            "     | > decoder_coarse_loss: 0.49584  (0.48402)\n",
            "     | > decoder_ddc_loss: 0.00681  (0.00905)\n",
            "     | > ga_loss: 0.00012  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.20466  (0.20185)\n",
            "     | > postnet_diff_spec_loss: 0.18364  (0.18247)\n",
            "     | > decoder_ssim_loss: 0.21680  (0.22180)\n",
            "     | > postnet_ssim_loss: 0.20705  (0.21219)\n",
            "     | > loss: 0.58762  (0.62421)\n",
            "     | > align_error: 0.22416  (0.20896)\n",
            "     | > grad_norm: 0.80637  (0.96667)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.80830  (2.35952)\n",
            "     | > loader_time: 0.00530  (0.00524)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 388/722 -- GLOBAL_STEP: 274125\u001b[0m\n",
            "     | > decoder_loss: 0.28760  (0.28762)\n",
            "     | > postnet_loss: 0.26686  (0.26678)\n",
            "     | > stopnet_loss: 0.09187  (0.15376)\n",
            "     | > decoder_coarse_loss: 0.48870  (0.48451)\n",
            "     | > decoder_ddc_loss: 0.00698  (0.00891)\n",
            "     | > ga_loss: 0.00011  (0.00031)\n",
            "     | > decoder_diff_spec_loss: 0.19703  (0.20194)\n",
            "     | > postnet_diff_spec_loss: 0.17845  (0.18249)\n",
            "     | > decoder_ssim_loss: 0.23134  (0.22201)\n",
            "     | > postnet_ssim_loss: 0.22110  (0.21236)\n",
            "     | > loss: 0.56196  (0.62194)\n",
            "     | > align_error: 0.21855  (0.20980)\n",
            "     | > grad_norm: 0.61978  (0.95228)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.80940  (2.39617)\n",
            "     | > loader_time: 0.00550  (0.00524)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 413/722 -- GLOBAL_STEP: 274150\u001b[0m\n",
            "     | > decoder_loss: 0.28483  (0.28804)\n",
            "     | > postnet_loss: 0.26364  (0.26711)\n",
            "     | > stopnet_loss: 0.10691  (0.15241)\n",
            "     | > decoder_coarse_loss: 0.48397  (0.48552)\n",
            "     | > decoder_ddc_loss: 0.00641  (0.00877)\n",
            "     | > ga_loss: 0.00010  (0.00029)\n",
            "     | > decoder_diff_spec_loss: 0.20235  (0.20212)\n",
            "     | > postnet_diff_spec_loss: 0.18223  (0.18261)\n",
            "     | > decoder_ssim_loss: 0.22213  (0.22195)\n",
            "     | > postnet_ssim_loss: 0.21210  (0.21227)\n",
            "     | > loss: 0.57182  (0.62098)\n",
            "     | > align_error: 0.21975  (0.21064)\n",
            "     | > grad_norm: 0.59258  (0.94319)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.93300  (2.43465)\n",
            "     | > loader_time: 0.00500  (0.00524)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 438/722 -- GLOBAL_STEP: 274175\u001b[0m\n",
            "     | > decoder_loss: 0.28881  (0.28829)\n",
            "     | > postnet_loss: 0.26707  (0.26729)\n",
            "     | > stopnet_loss: 0.37080  (0.15183)\n",
            "     | > decoder_coarse_loss: 0.47263  (0.48631)\n",
            "     | > decoder_ddc_loss: 0.00509  (0.00863)\n",
            "     | > ga_loss: 0.00022  (0.00028)\n",
            "     | > decoder_diff_spec_loss: 0.20127  (0.20217)\n",
            "     | > postnet_diff_spec_loss: 0.18240  (0.18261)\n",
            "     | > decoder_ssim_loss: 0.18763  (0.22173)\n",
            "     | > postnet_ssim_loss: 0.17934  (0.21203)\n",
            "     | > loss: 0.81799  (0.62052)\n",
            "     | > align_error: 0.22138  (0.21137)\n",
            "     | > grad_norm: 3.61224  (0.93623)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.04620  (2.47740)\n",
            "     | > loader_time: 0.00540  (0.00525)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 463/722 -- GLOBAL_STEP: 274200\u001b[0m\n",
            "     | > decoder_loss: 0.29117  (0.28865)\n",
            "     | > postnet_loss: 0.26867  (0.26757)\n",
            "     | > stopnet_loss: 0.13208  (0.15079)\n",
            "     | > decoder_coarse_loss: 0.49396  (0.48743)\n",
            "     | > decoder_ddc_loss: 0.00605  (0.00850)\n",
            "     | > ga_loss: 0.00009  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.20236  (0.20237)\n",
            "     | > postnet_diff_spec_loss: 0.18268  (0.18273)\n",
            "     | > decoder_ssim_loss: 0.21543  (0.22153)\n",
            "     | > postnet_ssim_loss: 0.20531  (0.21181)\n",
            "     | > loss: 0.59893  (0.61980)\n",
            "     | > align_error: 0.22005  (0.21195)\n",
            "     | > grad_norm: 0.63372  (0.92772)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.15200  (2.51715)\n",
            "     | > loader_time: 0.00560  (0.00525)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 488/722 -- GLOBAL_STEP: 274225\u001b[0m\n",
            "     | > decoder_loss: 0.30519  (0.28899)\n",
            "     | > postnet_loss: 0.28164  (0.26782)\n",
            "     | > stopnet_loss: 0.17655  (0.15023)\n",
            "     | > decoder_coarse_loss: 0.50944  (0.48846)\n",
            "     | > decoder_ddc_loss: 0.00530  (0.00837)\n",
            "     | > ga_loss: 0.00008  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.20743  (0.20256)\n",
            "     | > postnet_diff_spec_loss: 0.18617  (0.18286)\n",
            "     | > decoder_ssim_loss: 0.21178  (0.22125)\n",
            "     | > postnet_ssim_loss: 0.20191  (0.21151)\n",
            "     | > loss: 0.65415  (0.61950)\n",
            "     | > align_error: 0.22583  (0.21270)\n",
            "     | > grad_norm: 0.65348  (0.91720)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.61230  (2.56019)\n",
            "     | > loader_time: 0.00570  (0.00527)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 513/722 -- GLOBAL_STEP: 274250\u001b[0m\n",
            "     | > decoder_loss: 0.28380  (0.28928)\n",
            "     | > postnet_loss: 0.26238  (0.26803)\n",
            "     | > stopnet_loss: 0.15228  (0.14920)\n",
            "     | > decoder_coarse_loss: 0.47844  (0.48937)\n",
            "     | > decoder_ddc_loss: 0.00577  (0.00825)\n",
            "     | > ga_loss: 0.00009  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.19604  (0.20275)\n",
            "     | > postnet_diff_spec_loss: 0.17640  (0.18297)\n",
            "     | > decoder_ssim_loss: 0.21688  (0.22108)\n",
            "     | > postnet_ssim_loss: 0.20703  (0.21133)\n",
            "     | > loss: 0.60942  (0.61874)\n",
            "     | > align_error: 0.22899  (0.21337)\n",
            "     | > grad_norm: 0.68237  (0.90417)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.50840  (2.60314)\n",
            "     | > loader_time: 0.00470  (0.00528)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 538/722 -- GLOBAL_STEP: 274275\u001b[0m\n",
            "     | > decoder_loss: 0.28287  (0.28953)\n",
            "     | > postnet_loss: 0.26191  (0.26821)\n",
            "     | > stopnet_loss: 0.09304  (0.14826)\n",
            "     | > decoder_coarse_loss: 0.49405  (0.49049)\n",
            "     | > decoder_ddc_loss: 0.00565  (0.00813)\n",
            "     | > ga_loss: 0.00006  (0.00025)\n",
            "     | > decoder_diff_spec_loss: 0.19539  (0.20286)\n",
            "     | > postnet_diff_spec_loss: 0.17709  (0.18303)\n",
            "     | > decoder_ssim_loss: 0.23126  (0.22100)\n",
            "     | > postnet_ssim_loss: 0.22121  (0.21122)\n",
            "     | > loss: 0.56069  (0.61811)\n",
            "     | > align_error: 0.23782  (0.21400)\n",
            "     | > grad_norm: 0.51054  (0.89014)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.43660  (2.64706)\n",
            "     | > loader_time: 0.00550  (0.00529)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 563/722 -- GLOBAL_STEP: 274300\u001b[0m\n",
            "     | > decoder_loss: 0.29924  (0.28980)\n",
            "     | > postnet_loss: 0.27578  (0.26840)\n",
            "     | > stopnet_loss: 0.09645  (0.14719)\n",
            "     | > decoder_coarse_loss: 0.51416  (0.49147)\n",
            "     | > decoder_ddc_loss: 0.00590  (0.00802)\n",
            "     | > ga_loss: 0.00007  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.20565  (0.20304)\n",
            "     | > postnet_diff_spec_loss: 0.18395  (0.18314)\n",
            "     | > decoder_ssim_loss: 0.22419  (0.22093)\n",
            "     | > postnet_ssim_loss: 0.21335  (0.21112)\n",
            "     | > loss: 0.57736  (0.61737)\n",
            "     | > align_error: 0.24465  (0.21477)\n",
            "     | > grad_norm: 0.91973  (0.88181)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.48860  (2.68837)\n",
            "     | > loader_time: 0.00560  (0.00531)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 588/722 -- GLOBAL_STEP: 274325\u001b[0m\n",
            "     | > decoder_loss: 0.29691  (0.29013)\n",
            "     | > postnet_loss: 0.27446  (0.26864)\n",
            "     | > stopnet_loss: 0.09547  (0.14593)\n",
            "     | > decoder_coarse_loss: 0.51702  (0.49266)\n",
            "     | > decoder_ddc_loss: 0.00621  (0.00791)\n",
            "     | > ga_loss: 0.00006  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.20765  (0.20320)\n",
            "     | > postnet_diff_spec_loss: 0.18616  (0.18322)\n",
            "     | > decoder_ssim_loss: 0.22259  (0.22090)\n",
            "     | > postnet_ssim_loss: 0.21197  (0.21106)\n",
            "     | > loss: 0.57654  (0.61652)\n",
            "     | > align_error: 0.23233  (0.21538)\n",
            "     | > grad_norm: 0.58151  (0.87127)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.71640  (2.73396)\n",
            "     | > loader_time: 0.00480  (0.00531)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 613/722 -- GLOBAL_STEP: 274350\u001b[0m\n",
            "     | > decoder_loss: 0.30424  (0.29043)\n",
            "     | > postnet_loss: 0.27998  (0.26886)\n",
            "     | > stopnet_loss: 0.10038  (0.14505)\n",
            "     | > decoder_coarse_loss: 0.52670  (0.49360)\n",
            "     | > decoder_ddc_loss: 0.00500  (0.00780)\n",
            "     | > ga_loss: 0.00006  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.20783  (0.20335)\n",
            "     | > postnet_diff_spec_loss: 0.18593  (0.18331)\n",
            "     | > decoder_ssim_loss: 0.23212  (0.22090)\n",
            "     | > postnet_ssim_loss: 0.22064  (0.21103)\n",
            "     | > loss: 0.59129  (0.61600)\n",
            "     | > align_error: 0.23189  (0.21610)\n",
            "     | > grad_norm: 0.77899  (0.86200)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.11760  (2.77931)\n",
            "     | > loader_time: 0.00530  (0.00533)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 638/722 -- GLOBAL_STEP: 274375\u001b[0m\n",
            "     | > decoder_loss: 0.29767  (0.29076)\n",
            "     | > postnet_loss: 0.27462  (0.26911)\n",
            "     | > stopnet_loss: 0.07195  (0.14418)\n",
            "     | > decoder_coarse_loss: 0.52848  (0.49478)\n",
            "     | > decoder_ddc_loss: 0.00575  (0.00769)\n",
            "     | > ga_loss: 0.00005  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.20887  (0.20354)\n",
            "     | > postnet_diff_spec_loss: 0.18693  (0.18342)\n",
            "     | > decoder_ssim_loss: 0.23011  (0.22079)\n",
            "     | > postnet_ssim_loss: 0.21916  (0.21090)\n",
            "     | > loss: 0.56010  (0.61552)\n",
            "     | > align_error: 0.23257  (0.21681)\n",
            "     | > grad_norm: 0.62136  (0.85448)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.79010  (2.82703)\n",
            "     | > loader_time: 0.00610  (0.00534)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 663/722 -- GLOBAL_STEP: 274400\u001b[0m\n",
            "     | > decoder_loss: 0.30221  (0.29120)\n",
            "     | > postnet_loss: 0.27678  (0.26945)\n",
            "     | > stopnet_loss: 0.11366  (0.14317)\n",
            "     | > decoder_coarse_loss: 0.55562  (0.49610)\n",
            "     | > decoder_ddc_loss: 0.00445  (0.00758)\n",
            "     | > ga_loss: 0.00005  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.21135  (0.20375)\n",
            "     | > postnet_diff_spec_loss: 0.18703  (0.18355)\n",
            "     | > decoder_ssim_loss: 0.21773  (0.22077)\n",
            "     | > postnet_ssim_loss: 0.20677  (0.21084)\n",
            "     | > loss: 0.60438  (0.61504)\n",
            "     | > align_error: 0.23919  (0.21762)\n",
            "     | > grad_norm: 0.68052  (0.84945)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.17410  (2.87537)\n",
            "     | > loader_time: 0.00600  (0.00536)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 688/722 -- GLOBAL_STEP: 274425\u001b[0m\n",
            "     | > decoder_loss: 0.30054  (0.29162)\n",
            "     | > postnet_loss: 0.27595  (0.26976)\n",
            "     | > stopnet_loss: 0.09045  (0.14278)\n",
            "     | > decoder_coarse_loss: 0.53479  (0.49754)\n",
            "     | > decoder_ddc_loss: 0.00524  (0.00747)\n",
            "     | > ga_loss: 0.00004  (0.00021)\n",
            "     | > decoder_diff_spec_loss: 0.20799  (0.20396)\n",
            "     | > postnet_diff_spec_loss: 0.18492  (0.18367)\n",
            "     | > decoder_ssim_loss: 0.22038  (0.22062)\n",
            "     | > postnet_ssim_loss: 0.20932  (0.21066)\n",
            "     | > loss: 0.57541  (0.61513)\n",
            "     | > align_error: 0.23963  (0.21842)\n",
            "     | > grad_norm: 0.75347  (0.84347)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 4.20380  (2.93216)\n",
            "     | > loader_time: 0.00520  (0.00537)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 713/722 -- GLOBAL_STEP: 274450\u001b[0m\n",
            "     | > decoder_loss: 0.30779  (0.29213)\n",
            "     | > postnet_loss: 0.28189  (0.27015)\n",
            "     | > stopnet_loss: 0.13565  (0.14250)\n",
            "     | > decoder_coarse_loss: 0.55607  (0.49928)\n",
            "     | > decoder_ddc_loss: 0.00374  (0.00735)\n",
            "     | > ga_loss: 0.00003  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.21130  (0.20422)\n",
            "     | > postnet_diff_spec_loss: 0.18814  (0.18383)\n",
            "     | > decoder_ssim_loss: 0.22169  (0.22051)\n",
            "     | > postnet_ssim_loss: 0.21089  (0.21053)\n",
            "     | > loss: 0.63117  (0.61550)\n",
            "     | > align_error: 0.24996  (0.21933)\n",
            "     | > grad_norm: 1.03278  (0.83581)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 5.33830  (3.00277)\n",
            "     | > loader_time: 0.00670  (0.00540)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 116\n",
            " | > Max length sequence: 113\n",
            " | > Min length sequence: 20\n",
            " | > Avg length sequence: 59.12068965517241\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 0.41500  (0.41500)\n",
            "     | > postnet_loss: 0.37038  (0.37038)\n",
            "     | > stopnet_loss: 0.22594  (0.22594)\n",
            "     | > decoder_coarse_loss: 0.62842  (0.62842)\n",
            "     | > decoder_ddc_loss: 0.00831  (0.00831)\n",
            "     | > ga_loss: 0.00062  (0.00062)\n",
            "     | > decoder_diff_spec_loss: 0.18729  (0.18729)\n",
            "     | > postnet_diff_spec_loss: 0.15487  (0.15487)\n",
            "     | > decoder_ssim_loss: 0.21524  (0.21524)\n",
            "     | > postnet_ssim_loss: 0.20087  (0.20087)\n",
            "     | > loss: 0.77413  (0.77413)\n",
            "     | > align_error: 0.36481  (0.36481)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > decoder_loss: 0.35377  (0.35377)\n",
            "     | > postnet_loss: 0.31316  (0.31316)\n",
            "     | > stopnet_loss: 0.10679  (0.10679)\n",
            "     | > decoder_coarse_loss: 0.57272  (0.57272)\n",
            "     | > decoder_ddc_loss: 0.00846  (0.00846)\n",
            "     | > ga_loss: 0.00027  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.19258  (0.19258)\n",
            "     | > postnet_diff_spec_loss: 0.15786  (0.15786)\n",
            "     | > decoder_ssim_loss: 0.23504  (0.23504)\n",
            "     | > postnet_ssim_loss: 0.21777  (0.21777)\n",
            "     | > loss: 0.62096  (0.62096)\n",
            "     | > align_error: 0.26715  (0.26715)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > decoder_loss: 0.37585  (0.36481)\n",
            "     | > postnet_loss: 0.33367  (0.32342)\n",
            "     | > stopnet_loss: 0.14714  (0.12696)\n",
            "     | > decoder_coarse_loss: 0.59571  (0.58422)\n",
            "     | > decoder_ddc_loss: 0.00790  (0.00818)\n",
            "     | > ga_loss: 0.00020  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.18932  (0.19095)\n",
            "     | > postnet_diff_spec_loss: 0.15529  (0.15657)\n",
            "     | > decoder_ssim_loss: 0.21561  (0.22533)\n",
            "     | > postnet_ssim_loss: 0.20028  (0.20903)\n",
            "     | > loss: 0.66657  (0.64377)\n",
            "     | > align_error: 0.27991  (0.27353)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > decoder_loss: 0.39289  (0.37417)\n",
            "     | > postnet_loss: 0.34608  (0.33097)\n",
            "     | > stopnet_loss: 0.13193  (0.12862)\n",
            "     | > decoder_coarse_loss: 0.65641  (0.60828)\n",
            "     | > decoder_ddc_loss: 0.00626  (0.00754)\n",
            "     | > ga_loss: 0.00012  (0.00020)\n",
            "     | > decoder_diff_spec_loss: 0.20331  (0.19507)\n",
            "     | > postnet_diff_spec_loss: 0.16400  (0.15905)\n",
            "     | > decoder_ssim_loss: 0.22144  (0.22403)\n",
            "     | > postnet_ssim_loss: 0.20474  (0.20760)\n",
            "     | > loss: 0.68131  (0.65628)\n",
            "     | > align_error: 0.27378  (0.27361)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > decoder_loss: 0.36229  (0.37120)\n",
            "     | > postnet_loss: 0.31659  (0.32737)\n",
            "     | > stopnet_loss: 0.07604  (0.11547)\n",
            "     | > decoder_coarse_loss: 0.61756  (0.61060)\n",
            "     | > decoder_ddc_loss: 0.00602  (0.00716)\n",
            "     | > ga_loss: 0.00010  (0.00017)\n",
            "     | > decoder_diff_spec_loss: 0.20071  (0.19648)\n",
            "     | > postnet_diff_spec_loss: 0.16211  (0.15981)\n",
            "     | > decoder_ssim_loss: 0.23368  (0.22644)\n",
            "     | > postnet_ssim_loss: 0.21555  (0.20959)\n",
            "     | > loss: 0.60515  (0.64350)\n",
            "     | > align_error: 0.25415  (0.26875)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > decoder_loss: 0.36265  (0.36949)\n",
            "     | > postnet_loss: 0.31989  (0.32588)\n",
            "     | > stopnet_loss: 0.14602  (0.12158)\n",
            "     | > decoder_coarse_loss: 0.60507  (0.60950)\n",
            "     | > decoder_ddc_loss: 0.00565  (0.00686)\n",
            "     | > ga_loss: 0.00007  (0.00015)\n",
            "     | > decoder_diff_spec_loss: 0.19075  (0.19533)\n",
            "     | > postnet_diff_spec_loss: 0.15398  (0.15865)\n",
            "     | > decoder_ssim_loss: 0.20938  (0.22303)\n",
            "     | > postnet_ssim_loss: 0.19292  (0.20625)\n",
            "     | > loss: 0.65643  (0.64609)\n",
            "     | > align_error: 0.27640  (0.27028)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > decoder_loss: 0.34927  (0.36612)\n",
            "     | > postnet_loss: 0.30603  (0.32257)\n",
            "     | > stopnet_loss: 0.16960  (0.12959)\n",
            "     | > decoder_coarse_loss: 0.60184  (0.60822)\n",
            "     | > decoder_ddc_loss: 0.00394  (0.00637)\n",
            "     | > ga_loss: 0.00004  (0.00013)\n",
            "     | > decoder_diff_spec_loss: 0.19010  (0.19446)\n",
            "     | > postnet_diff_spec_loss: 0.15382  (0.15784)\n",
            "     | > decoder_ssim_loss: 0.19996  (0.21919)\n",
            "     | > postnet_ssim_loss: 0.18363  (0.20248)\n",
            "     | > loss: 0.66695  (0.64956)\n",
            "     | > align_error: 0.30503  (0.27607)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > decoder_loss: 0.40604  (0.37182)\n",
            "     | > postnet_loss: 0.35263  (0.32687)\n",
            "     | > stopnet_loss: 0.05424  (0.11882)\n",
            "     | > decoder_coarse_loss: 0.70993  (0.62275)\n",
            "     | > decoder_ddc_loss: 0.00397  (0.00603)\n",
            "     | > ga_loss: 0.00003  (0.00012)\n",
            "     | > decoder_diff_spec_loss: 0.20576  (0.19608)\n",
            "     | > postnet_diff_spec_loss: 0.16493  (0.15886)\n",
            "     | > decoder_ssim_loss: 0.24394  (0.22272)\n",
            "     | > postnet_ssim_loss: 0.22502  (0.20570)\n",
            "     | > loss: 0.63243  (0.64712)\n",
            "     | > align_error: 0.26642  (0.27469)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00532 \u001b[0m(+0.00148)\n",
            "     | > avg_decoder_loss:\u001b[91m 0.37182 \u001b[0m(+0.00058)\n",
            "     | > avg_postnet_loss:\u001b[91m 0.32687 \u001b[0m(+0.00078)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.11882 \u001b[0m(-0.00015)\n",
            "     | > avg_decoder_coarse_loss:\u001b[91m 0.62275 \u001b[0m(+0.00275)\n",
            "     | > avg_decoder_ddc_loss:\u001b[91m 0.00603 \u001b[0m(+0.00002)\n",
            "     | > avg_ga_loss:\u001b[92m 0.00012 \u001b[0m(-0.00000)\n",
            "     | > avg_decoder_diff_spec_loss:\u001b[92m 0.19608 \u001b[0m(-0.00033)\n",
            "     | > avg_postnet_diff_spec_loss:\u001b[92m 0.15886 \u001b[0m(-0.00004)\n",
            "     | > avg_decoder_ssim_loss:\u001b[91m 0.22272 \u001b[0m(+0.00023)\n",
            "     | > avg_postnet_ssim_loss:\u001b[91m 0.20570 \u001b[0m(+0.00054)\n",
            "     | > avg_loss:\u001b[91m 0.64712 \u001b[0m(+0.00098)\n",
            "     | > avg_align_error:\u001b[92m 0.27469 \u001b[0m(-0.00181)\n",
            "\n",
            "\n",
            " > Number of output frames: 1\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 20/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 11559\n",
            " | > Max length sequence: 147\n",
            " | > Min length sequence: 8\n",
            " | > Avg length sequence: 58.31533869711913\n",
            " | > Num. instances discarded by max-min (max=inf, min=1) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2022-01-01 05:48:34) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/722 -- GLOBAL_STEP: 274475\u001b[0m\n",
            "     | > decoder_loss: 0.27642  (0.27620)\n",
            "     | > postnet_loss: 0.25848  (0.25731)\n",
            "     | > stopnet_loss: 0.12715  (0.18549)\n",
            "     | > decoder_coarse_loss: 0.44374  (0.46454)\n",
            "     | > decoder_ddc_loss: 0.01435  (0.01506)\n",
            "     | > ga_loss: 0.00082  (0.00111)\n",
            "     | > decoder_diff_spec_loss: 0.19594  (0.19809)\n",
            "     | > postnet_diff_spec_loss: 0.17799  (0.18007)\n",
            "     | > decoder_ssim_loss: 0.25347  (0.23231)\n",
            "     | > postnet_ssim_loss: 0.24371  (0.22334)\n",
            "     | > loss: 0.59726  (0.65275)\n",
            "     | > align_error: 0.18113  (0.18834)\n",
            "     | > grad_norm: 1.47830  (1.17160)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.41070  (1.40823)\n",
            "     | > loader_time: 0.00520  (0.00471)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 40/722 -- GLOBAL_STEP: 274500\u001b[0m\n",
            "     | > decoder_loss: 0.28287  (0.27770)\n",
            "     | > postnet_loss: 0.26302  (0.25885)\n",
            "     | > stopnet_loss: 0.18759  (0.19861)\n",
            "     | > decoder_coarse_loss: 0.50250  (0.46612)\n",
            "     | > decoder_ddc_loss: 0.01138  (0.01316)\n",
            "     | > ga_loss: 0.00050  (0.00085)\n",
            "     | > decoder_diff_spec_loss: 0.20311  (0.19742)\n",
            "     | > postnet_diff_spec_loss: 0.18382  (0.17965)\n",
            "     | > decoder_ssim_loss: 0.21232  (0.22692)\n",
            "     | > postnet_ssim_loss: 0.20328  (0.21796)\n",
            "     | > loss: 0.65565  (0.66229)\n",
            "     | > align_error: 0.19633  (0.19229)\n",
            "     | > grad_norm: 1.86614  (1.37021)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.75450  (1.62969)\n",
            "     | > loader_time: 0.00440  (0.00478)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 65/722 -- GLOBAL_STEP: 274525\u001b[0m\n",
            "     | > decoder_loss: 0.29579  (0.28012)\n",
            "     | > postnet_loss: 0.27439  (0.26086)\n",
            "     | > stopnet_loss: 0.20980  (0.18491)\n",
            "     | > decoder_coarse_loss: 0.50260  (0.46979)\n",
            "     | > decoder_ddc_loss: 0.01035  (0.01231)\n",
            "     | > ga_loss: 0.00045  (0.00071)\n",
            "     | > decoder_diff_spec_loss: 0.20576  (0.19873)\n",
            "     | > postnet_diff_spec_loss: 0.18574  (0.18056)\n",
            "     | > decoder_ssim_loss: 0.21153  (0.22671)\n",
            "     | > postnet_ssim_loss: 0.20245  (0.21756)\n",
            "     | > loss: 0.68421  (0.65010)\n",
            "     | > align_error: 0.19112  (0.19395)\n",
            "     | > grad_norm: 0.89704  (1.18058)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.99390  (1.71734)\n",
            "     | > loader_time: 0.00560  (0.00495)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/722 -- GLOBAL_STEP: 274550\u001b[0m\n",
            "     | > decoder_loss: 0.27471  (0.28135)\n",
            "     | > postnet_loss: 0.25563  (0.26182)\n",
            "     | > stopnet_loss: 0.12741  (0.18314)\n",
            "     | > decoder_coarse_loss: 0.46413  (0.47361)\n",
            "     | > decoder_ddc_loss: 0.01002  (0.01173)\n",
            "     | > ga_loss: 0.00036  (0.00062)\n",
            "     | > decoder_diff_spec_loss: 0.19319  (0.19954)\n",
            "     | > postnet_diff_spec_loss: 0.17540  (0.18103)\n",
            "     | > decoder_ssim_loss: 0.22373  (0.22456)\n",
            "     | > postnet_ssim_loss: 0.21469  (0.21540)\n",
            "     | > loss: 0.58209  (0.64852)\n",
            "     | > align_error: 0.20331  (0.19585)\n",
            "     | > grad_norm: 1.55653  (1.18839)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 1.98750  (1.81238)\n",
            "     | > loader_time: 0.00600  (0.00495)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 115/722 -- GLOBAL_STEP: 274575\u001b[0m\n",
            "     | > decoder_loss: 0.29472  (0.28233)\n",
            "     | > postnet_loss: 0.27331  (0.26257)\n",
            "     | > stopnet_loss: 0.16290  (0.17796)\n",
            "     | > decoder_coarse_loss: 0.49755  (0.47568)\n",
            "     | > decoder_ddc_loss: 0.00948  (0.01129)\n",
            "     | > ga_loss: 0.00038  (0.00056)\n",
            "     | > decoder_diff_spec_loss: 0.21139  (0.20004)\n",
            "     | > postnet_diff_spec_loss: 0.19079  (0.18132)\n",
            "     | > decoder_ssim_loss: 0.21850  (0.22418)\n",
            "     | > postnet_ssim_loss: 0.20926  (0.21494)\n",
            "     | > loss: 0.64105  (0.64386)\n",
            "     | > align_error: 0.21314  (0.19804)\n",
            "     | > grad_norm: 0.85829  (1.11600)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.01150  (1.86693)\n",
            "     | > loader_time: 0.00440  (0.00486)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 140/722 -- GLOBAL_STEP: 274600\u001b[0m\n",
            "     | > decoder_loss: 0.29043  (0.28311)\n",
            "     | > postnet_loss: 0.26976  (0.26319)\n",
            "     | > stopnet_loss: 0.15495  (0.17496)\n",
            "     | > decoder_coarse_loss: 0.48022  (0.47726)\n",
            "     | > decoder_ddc_loss: 0.00890  (0.01090)\n",
            "     | > ga_loss: 0.00026  (0.00052)\n",
            "     | > decoder_diff_spec_loss: 0.20553  (0.20035)\n",
            "     | > postnet_diff_spec_loss: 0.18706  (0.18153)\n",
            "     | > decoder_ssim_loss: 0.22126  (0.22383)\n",
            "     | > postnet_ssim_loss: 0.21160  (0.21451)\n",
            "     | > loss: 0.62496  (0.64120)\n",
            "     | > align_error: 0.20784  (0.19931)\n",
            "     | > grad_norm: 0.90487  (1.05478)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.13960  (1.92083)\n",
            "     | > loader_time: 0.00400  (0.00481)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 165/722 -- GLOBAL_STEP: 274625\u001b[0m\n",
            "     | > decoder_loss: 0.29415  (0.28369)\n",
            "     | > postnet_loss: 0.27253  (0.26366)\n",
            "     | > stopnet_loss: 0.13312  (0.17111)\n",
            "     | > decoder_coarse_loss: 0.46860  (0.47800)\n",
            "     | > decoder_ddc_loss: 0.00859  (0.01060)\n",
            "     | > ga_loss: 0.00026  (0.00048)\n",
            "     | > decoder_diff_spec_loss: 0.20856  (0.20052)\n",
            "     | > postnet_diff_spec_loss: 0.18746  (0.18161)\n",
            "     | > decoder_ssim_loss: 0.22253  (0.22380)\n",
            "     | > postnet_ssim_loss: 0.21260  (0.21442)\n",
            "     | > loss: 0.60317  (0.63757)\n",
            "     | > align_error: 0.20900  (0.20090)\n",
            "     | > grad_norm: 1.30100  (1.01409)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.22320  (1.95980)\n",
            "     | > loader_time: 0.00740  (0.00481)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 190/722 -- GLOBAL_STEP: 274650\u001b[0m\n",
            "     | > decoder_loss: 0.29756  (0.28437)\n",
            "     | > postnet_loss: 0.27607  (0.26421)\n",
            "     | > stopnet_loss: 0.22915  (0.16973)\n",
            "     | > decoder_coarse_loss: 0.48560  (0.47853)\n",
            "     | > decoder_ddc_loss: 0.00741  (0.01033)\n",
            "     | > ga_loss: 0.00025  (0.00045)\n",
            "     | > decoder_diff_spec_loss: 0.20386  (0.20072)\n",
            "     | > postnet_diff_spec_loss: 0.18386  (0.18169)\n",
            "     | > decoder_ssim_loss: 0.20548  (0.22357)\n",
            "     | > postnet_ssim_loss: 0.19578  (0.21414)\n",
            "     | > loss: 0.69429  (0.63636)\n",
            "     | > align_error: 0.21462  (0.20226)\n",
            "     | > grad_norm: 1.12593  (1.02515)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.48390  (2.01028)\n",
            "     | > loader_time: 0.00540  (0.00481)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 215/722 -- GLOBAL_STEP: 274675\u001b[0m\n",
            "     | > decoder_loss: 0.28724  (0.28476)\n",
            "     | > postnet_loss: 0.26673  (0.26453)\n",
            "     | > stopnet_loss: 0.11616  (0.16695)\n",
            "     | > decoder_coarse_loss: 0.48033  (0.47935)\n",
            "     | > decoder_ddc_loss: 0.00829  (0.01009)\n",
            "     | > ga_loss: 0.00020  (0.00042)\n",
            "     | > decoder_diff_spec_loss: 0.20168  (0.20086)\n",
            "     | > postnet_diff_spec_loss: 0.18286  (0.18180)\n",
            "     | > decoder_ssim_loss: 0.23423  (0.22301)\n",
            "     | > postnet_ssim_loss: 0.22445  (0.21356)\n",
            "     | > loss: 0.58860  (0.63355)\n",
            "     | > align_error: 0.21827  (0.20389)\n",
            "     | > grad_norm: 0.71714  (0.99847)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.26040  (2.05238)\n",
            "     | > loader_time: 0.00460  (0.00481)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 240/722 -- GLOBAL_STEP: 274700\u001b[0m\n",
            "     | > decoder_loss: 0.30098  (0.28529)\n",
            "     | > postnet_loss: 0.27670  (0.26496)\n",
            "     | > stopnet_loss: 0.17838  (0.16413)\n",
            "     | > decoder_coarse_loss: 0.53801  (0.48049)\n",
            "     | > decoder_ddc_loss: 0.00751  (0.00989)\n",
            "     | > ga_loss: 0.00017  (0.00040)\n",
            "     | > decoder_diff_spec_loss: 0.21274  (0.20106)\n",
            "     | > postnet_diff_spec_loss: 0.19060  (0.18194)\n",
            "     | > decoder_ssim_loss: 0.20975  (0.22282)\n",
            "     | > postnet_ssim_loss: 0.20010  (0.21333)\n",
            "     | > loss: 0.66331  (0.63107)\n",
            "     | > align_error: 0.20203  (0.20499)\n",
            "     | > grad_norm: 0.74218  (0.98078)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.65060  (2.09283)\n",
            "     | > loader_time: 0.00560  (0.00482)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 265/722 -- GLOBAL_STEP: 274725\u001b[0m\n",
            "     | > decoder_loss: 0.29806  (0.28586)\n",
            "     | > postnet_loss: 0.27643  (0.26541)\n",
            "     | > stopnet_loss: 0.14677  (0.16185)\n",
            "     | > decoder_coarse_loss: 0.47338  (0.48169)\n",
            "     | > decoder_ddc_loss: 0.00746  (0.00971)\n",
            "     | > ga_loss: 0.00017  (0.00038)\n",
            "     | > decoder_diff_spec_loss: 0.20693  (0.20143)\n",
            "     | > postnet_diff_spec_loss: 0.18616  (0.18219)\n",
            "     | > decoder_ssim_loss: 0.21754  (0.22252)\n",
            "     | > postnet_ssim_loss: 0.20773  (0.21299)\n",
            "     | > loss: 0.61605  (0.62920)\n",
            "     | > align_error: 0.21432  (0.20599)\n",
            "     | > grad_norm: 1.63029  (0.96884)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.52260  (2.13182)\n",
            "     | > loader_time: 0.00430  (0.00483)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 290/722 -- GLOBAL_STEP: 274750\u001b[0m\n",
            "     | > decoder_loss: 0.28845  (0.28601)\n",
            "     | > postnet_loss: 0.26679  (0.26549)\n",
            "     | > stopnet_loss: 0.13346  (0.15997)\n",
            "     | > decoder_coarse_loss: 0.47924  (0.48193)\n",
            "     | > decoder_ddc_loss: 0.00732  (0.00952)\n",
            "     | > ga_loss: 0.00017  (0.00036)\n",
            "     | > decoder_diff_spec_loss: 0.20187  (0.20146)\n",
            "     | > postnet_diff_spec_loss: 0.18109  (0.18216)\n",
            "     | > decoder_ssim_loss: 0.21557  (0.22234)\n",
            "     | > postnet_ssim_loss: 0.20555  (0.21279)\n",
            "     | > loss: 0.59577  (0.62720)\n",
            "     | > align_error: 0.21033  (0.20685)\n",
            "     | > grad_norm: 1.36669  (0.94940)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.56530  (2.17012)\n",
            "     | > loader_time: 0.00930  (0.00483)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 315/722 -- GLOBAL_STEP: 274775\u001b[0m\n",
            "     | > decoder_loss: 0.28280  (0.28632)\n",
            "     | > postnet_loss: 0.26111  (0.26576)\n",
            "     | > stopnet_loss: 0.09975  (0.15827)\n",
            "     | > decoder_coarse_loss: 0.47625  (0.48276)\n",
            "     | > decoder_ddc_loss: 0.00721  (0.00934)\n",
            "     | > ga_loss: 0.00016  (0.00034)\n",
            "     | > decoder_diff_spec_loss: 0.20209  (0.20152)\n",
            "     | > postnet_diff_spec_loss: 0.17975  (0.18219)\n",
            "     | > decoder_ssim_loss: 0.22337  (0.22216)\n",
            "     | > postnet_ssim_loss: 0.21313  (0.21259)\n",
            "     | > loss: 0.56197  (0.62565)\n",
            "     | > align_error: 0.22345  (0.20778)\n",
            "     | > grad_norm: 0.80389  (0.94095)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.60870  (2.21116)\n",
            "     | > loader_time: 0.00460  (0.00485)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 340/722 -- GLOBAL_STEP: 274800\u001b[0m\n",
            "     | > decoder_loss: 0.29459  (0.28667)\n",
            "     | > postnet_loss: 0.27316  (0.26603)\n",
            "     | > stopnet_loss: 0.11145  (0.15626)\n",
            "     | > decoder_coarse_loss: 0.48353  (0.48345)\n",
            "     | > decoder_ddc_loss: 0.00688  (0.00919)\n",
            "     | > ga_loss: 0.00015  (0.00033)\n",
            "     | > decoder_diff_spec_loss: 0.20426  (0.20165)\n",
            "     | > postnet_diff_spec_loss: 0.18514  (0.18227)\n",
            "     | > decoder_ssim_loss: 0.23250  (0.22215)\n",
            "     | > postnet_ssim_loss: 0.22203  (0.21255)\n",
            "     | > loss: 0.58770  (0.62389)\n",
            "     | > align_error: 0.21791  (0.20865)\n",
            "     | > grad_norm: 0.48993  (0.92452)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.71540  (2.24686)\n",
            "     | > loader_time: 0.00540  (0.00487)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 365/722 -- GLOBAL_STEP: 274825\u001b[0m\n",
            "     | > decoder_loss: 0.28328  (0.28706)\n",
            "     | > postnet_loss: 0.26313  (0.26632)\n",
            "     | > stopnet_loss: 0.13303  (0.15624)\n",
            "     | > decoder_coarse_loss: 0.48053  (0.48420)\n",
            "     | > decoder_ddc_loss: 0.00680  (0.00902)\n",
            "     | > ga_loss: 0.00018  (0.00032)\n",
            "     | > decoder_diff_spec_loss: 0.19699  (0.20185)\n",
            "     | > postnet_diff_spec_loss: 0.17785  (0.18240)\n",
            "     | > decoder_ssim_loss: 0.22154  (0.22171)\n",
            "     | > postnet_ssim_loss: 0.21193  (0.21209)\n",
            "     | > loss: 0.59442  (0.62398)\n",
            "     | > align_error: 0.23064  (0.20959)\n",
            "     | > grad_norm: 0.70137  (0.93540)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.90400  (2.28827)\n",
            "     | > loader_time: 0.00480  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 390/722 -- GLOBAL_STEP: 274850\u001b[0m\n",
            "     | > decoder_loss: 0.27962  (0.28726)\n",
            "     | > postnet_loss: 0.25981  (0.26646)\n",
            "     | > stopnet_loss: 0.10517  (0.15364)\n",
            "     | > decoder_coarse_loss: 0.47482  (0.48488)\n",
            "     | > decoder_ddc_loss: 0.00672  (0.00889)\n",
            "     | > ga_loss: 0.00013  (0.00030)\n",
            "     | > decoder_diff_spec_loss: 0.19385  (0.20189)\n",
            "     | > postnet_diff_spec_loss: 0.17636  (0.18240)\n",
            "     | > decoder_ssim_loss: 0.22583  (0.22192)\n",
            "     | > postnet_ssim_loss: 0.21613  (0.21227)\n",
            "     | > loss: 0.56410  (0.62165)\n",
            "     | > align_error: 0.22462  (0.21047)\n",
            "     | > grad_norm: 0.45112  (0.92245)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.83230  (2.32131)\n",
            "     | > loader_time: 0.00520  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 415/722 -- GLOBAL_STEP: 274875\u001b[0m\n",
            "     | > decoder_loss: 0.28902  (0.28766)\n",
            "     | > postnet_loss: 0.26705  (0.26677)\n",
            "     | > stopnet_loss: 0.13907  (0.15243)\n",
            "     | > decoder_coarse_loss: 0.50494  (0.48581)\n",
            "     | > decoder_ddc_loss: 0.00676  (0.00875)\n",
            "     | > ga_loss: 0.00012  (0.00029)\n",
            "     | > decoder_diff_spec_loss: 0.20576  (0.20207)\n",
            "     | > postnet_diff_spec_loss: 0.18428  (0.18250)\n",
            "     | > decoder_ssim_loss: 0.21421  (0.22184)\n",
            "     | > postnet_ssim_loss: 0.20451  (0.21216)\n",
            "     | > loss: 0.60877  (0.62078)\n",
            "     | > align_error: 0.21682  (0.21133)\n",
            "     | > grad_norm: 0.55856  (0.91173)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.93800  (2.35720)\n",
            "     | > loader_time: 0.00590  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 440/722 -- GLOBAL_STEP: 274900\u001b[0m\n",
            "     | > decoder_loss: 0.30314  (0.28799)\n",
            "     | > postnet_loss: 0.27936  (0.26701)\n",
            "     | > stopnet_loss: 0.10868  (0.15164)\n",
            "     | > decoder_coarse_loss: 0.53860  (0.48673)\n",
            "     | > decoder_ddc_loss: 0.00640  (0.00861)\n",
            "     | > ga_loss: 0.00011  (0.00028)\n",
            "     | > decoder_diff_spec_loss: 0.20846  (0.20220)\n",
            "     | > postnet_diff_spec_loss: 0.18584  (0.18256)\n",
            "     | > decoder_ssim_loss: 0.22188  (0.22169)\n",
            "     | > postnet_ssim_loss: 0.21159  (0.21198)\n",
            "     | > loss: 0.59804  (0.62025)\n",
            "     | > align_error: 0.22679  (0.21203)\n",
            "     | > grad_norm: 0.61941  (0.91030)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 2.93080  (2.39735)\n",
            "     | > loader_time: 0.00420  (0.00491)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 465/722 -- GLOBAL_STEP: 274925\u001b[0m\n",
            "     | > decoder_loss: 0.29437  (0.28837)\n",
            "     | > postnet_loss: 0.27228  (0.26731)\n",
            "     | > stopnet_loss: 0.13000  (0.15073)\n",
            "     | > decoder_coarse_loss: 0.49581  (0.48791)\n",
            "     | > decoder_ddc_loss: 0.00594  (0.00848)\n",
            "     | > ga_loss: 0.00010  (0.00027)\n",
            "     | > decoder_diff_spec_loss: 0.20599  (0.20238)\n",
            "     | > postnet_diff_spec_loss: 0.18496  (0.18268)\n",
            "     | > decoder_ssim_loss: 0.21697  (0.22146)\n",
            "     | > postnet_ssim_loss: 0.20679  (0.21173)\n",
            "     | > loss: 0.60126  (0.61968)\n",
            "     | > align_error: 0.22759  (0.21271)\n",
            "     | > grad_norm: 0.68646  (0.90539)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.01070  (2.43727)\n",
            "     | > loader_time: 0.00450  (0.00492)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 490/722 -- GLOBAL_STEP: 274950\u001b[0m\n",
            "     | > decoder_loss: 0.28811  (0.28879)\n",
            "     | > postnet_loss: 0.26570  (0.26764)\n",
            "     | > stopnet_loss: 0.13699  (0.15047)\n",
            "     | > decoder_coarse_loss: 0.49468  (0.48893)\n",
            "     | > decoder_ddc_loss: 0.00571  (0.00835)\n",
            "     | > ga_loss: 0.00009  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.20773  (0.20261)\n",
            "     | > postnet_diff_spec_loss: 0.18636  (0.18282)\n",
            "     | > decoder_ssim_loss: 0.21212  (0.22112)\n",
            "     | > postnet_ssim_loss: 0.20197  (0.21138)\n",
            "     | > loss: 0.60304  (0.61969)\n",
            "     | > align_error: 0.22539  (0.21350)\n",
            "     | > grad_norm: 0.74636  (0.89869)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.28250  (2.47947)\n",
            "     | > loader_time: 0.00470  (0.00493)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 515/722 -- GLOBAL_STEP: 274975\u001b[0m\n",
            "     | > decoder_loss: 0.29890  (0.28907)\n",
            "     | > postnet_loss: 0.27573  (0.26786)\n",
            "     | > stopnet_loss: 0.14849  (0.14929)\n",
            "     | > decoder_coarse_loss: 0.52900  (0.48996)\n",
            "     | > decoder_ddc_loss: 0.00619  (0.00823)\n",
            "     | > ga_loss: 0.00008  (0.00026)\n",
            "     | > decoder_diff_spec_loss: 0.20520  (0.20276)\n",
            "     | > postnet_diff_spec_loss: 0.18482  (0.18292)\n",
            "     | > decoder_ssim_loss: 0.21486  (0.22101)\n",
            "     | > postnet_ssim_loss: 0.20483  (0.21124)\n",
            "     | > loss: 0.62877  (0.61883)\n",
            "     | > align_error: 0.23048  (0.21409)\n",
            "     | > grad_norm: 0.47902  (0.88535)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.33230  (2.51985)\n",
            "     | > loader_time: 0.00530  (0.00495)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 540/722 -- GLOBAL_STEP: 275000\u001b[0m\n",
            "     | > decoder_loss: 0.29626  (0.28937)\n",
            "     | > postnet_loss: 0.27337  (0.26807)\n",
            "     | > stopnet_loss: 0.22413  (0.14847)\n",
            "     | > decoder_coarse_loss: 0.51998  (0.49100)\n",
            "     | > decoder_ddc_loss: 0.00500  (0.00811)\n",
            "     | > ga_loss: 0.00006  (0.00025)\n",
            "     | > decoder_diff_spec_loss: 0.20498  (0.20290)\n",
            "     | > postnet_diff_spec_loss: 0.18517  (0.18299)\n",
            "     | > decoder_ssim_loss: 0.19394  (0.22089)\n",
            "     | > postnet_ssim_loss: 0.18492  (0.21110)\n",
            "     | > loss: 0.69036  (0.61831)\n",
            "     | > align_error: 0.23610  (0.21478)\n",
            "     | > grad_norm: 0.48005  (0.87301)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.82840  (2.56276)\n",
            "     | > loader_time: 0.00590  (0.00496)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/checkpoint_275000.pth.tar\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            "\n",
            "\u001b[1m   --> STEP: 565/722 -- GLOBAL_STEP: 275025\u001b[0m\n",
            "     | > decoder_loss: 0.29327  (0.28964)\n",
            "     | > postnet_loss: 0.27111  (0.26826)\n",
            "     | > stopnet_loss: 0.11031  (0.14713)\n",
            "     | > decoder_coarse_loss: 0.51181  (0.49182)\n",
            "     | > decoder_ddc_loss: 0.00544  (0.00800)\n",
            "     | > ga_loss: 0.00006  (0.00024)\n",
            "     | > decoder_diff_spec_loss: 0.20605  (0.20308)\n",
            "     | > postnet_diff_spec_loss: 0.18507  (0.18309)\n",
            "     | > decoder_ssim_loss: 0.22528  (0.22090)\n",
            "     | > postnet_ssim_loss: 0.21509  (0.21108)\n",
            "     | > loss: 0.58891  (0.61729)\n",
            "     | > align_error: 0.22389  (0.21557)\n",
            "     | > grad_norm: 0.61957  (0.86347)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.64760  (2.60859)\n",
            "     | > loader_time: 0.00560  (0.00512)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 590/722 -- GLOBAL_STEP: 275050\u001b[0m\n",
            "     | > decoder_loss: 0.29588  (0.28994)\n",
            "     | > postnet_loss: 0.27206  (0.26847)\n",
            "     | > stopnet_loss: 0.10047  (0.14591)\n",
            "     | > decoder_coarse_loss: 0.50596  (0.49282)\n",
            "     | > decoder_ddc_loss: 0.00507  (0.00789)\n",
            "     | > ga_loss: 0.00006  (0.00023)\n",
            "     | > decoder_diff_spec_loss: 0.20882  (0.20322)\n",
            "     | > postnet_diff_spec_loss: 0.18593  (0.18316)\n",
            "     | > decoder_ssim_loss: 0.22687  (0.22086)\n",
            "     | > postnet_ssim_loss: 0.21568  (0.21101)\n",
            "     | > loss: 0.57982  (0.61642)\n",
            "     | > align_error: 0.22641  (0.21622)\n",
            "     | > grad_norm: 0.52328  (0.85560)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.58860  (2.65152)\n",
            "     | > loader_time: 0.00540  (0.00514)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 615/722 -- GLOBAL_STEP: 275075\u001b[0m\n",
            "     | > decoder_loss: 0.29540  (0.29023)\n",
            "     | > postnet_loss: 0.27292  (0.26869)\n",
            "     | > stopnet_loss: 0.11414  (0.14512)\n",
            "     | > decoder_coarse_loss: 0.51234  (0.49376)\n",
            "     | > decoder_ddc_loss: 0.00494  (0.00779)\n",
            "     | > ga_loss: 0.00006  (0.00022)\n",
            "     | > decoder_diff_spec_loss: 0.20585  (0.20337)\n",
            "     | > postnet_diff_spec_loss: 0.18514  (0.18325)\n",
            "     | > decoder_ssim_loss: 0.22125  (0.22083)\n",
            "     | > postnet_ssim_loss: 0.21125  (0.21096)\n",
            "     | > loss: 0.59170  (0.61597)\n",
            "     | > align_error: 0.22963  (0.21695)\n",
            "     | > grad_norm: 0.73855  (0.84680)\n",
            "     | > current_lr: 0.00001 \n",
            "     | > step_time: 3.61880  (2.69512)\n",
            "     | > loader_time: 0.00480  (0.00514)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Emergent/train/24dBtrain_tacotron_ddc.py \\\n",
        "    --continue_path /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Log Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPBeMaQOC_cX",
        "outputId": "b2c9195a-2d06-4050-95fd-fd7ddd2db10b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
            "    sys.exit(run_main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/main.py\", line 46, in run_main\n",
            "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/program.py\", line 276, in main\n",
            "    return runner(self.flags) or 0\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 657, in run\n",
            "    return _run(flags, self._experiment_url_callback)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 125, in _run\n",
            "    intent.execute(server_info, channel)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 471, in execute\n",
            "    if not self.dry_run and uploader.has_data():\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!tensorboard dev upload --logdir \"/content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-08-2021_09+51PM-33aa27e2\" --name 'Tacotron2 train , 11 day data with a preemphasis 0.98'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icRwpmSGEXtr",
        "outputId": "a2299034-61e4-43c8-b542-ae1192b8470d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Using model: vits\n",
            " > Text: Abazira ennyo abaatusooka.\n",
            " > Text splitted to sentences.\n",
            "['Abazira ennyo abaatusooka.']\n",
            " > Processing time: 2.5921273231506348\n",
            " > Real-time factor: 0.7086442109138997\n",
            " > Saving output to ../output1.wav\n"
          ]
        }
      ],
      "source": [
        "!tts --text \"Abazira ennyo abaatusooka.\" \\\n",
        "      --model_path /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2/best_model_41132.pth.tar \\\n",
        "      --config_path /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2/config.json \\\n",
        "      --out_path ../output1.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUjiNwUZ3DH7"
      },
      "source": [
        "24dB sound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzdwqeJR3CDp",
        "outputId": "6c9c773c-ad52-4f9d-9689-d19ff420f191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Using model: tacotron2\n",
            " > Model's reduction rate `r` is set to: 2\n",
            " > Using Griffin-Lim as no vocoder model defined\n",
            " > Text: Tolya, wadde okubaaga ekinyonyi ekifudde oba ekirwadde.\n",
            " > Text splitted to sentences.\n",
            "['Tolya, wadde okubaaga ekinyonyi ekifudde oba ekirwadde.']\n",
            " > Processing time: 2.984372615814209\n",
            " > Real-time factor: 0.6523138003440059\n",
            " > Saving output to ../output1.wav\n"
          ]
        }
      ],
      "source": [
        "!tts --text \"Tolya, wadde okubaaga ekinyonyi ekifudde oba ekirwadde.\" \\\n",
        "      --model_path /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_103422.pth.tar \\\n",
        "      --config_path /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/config.json \\\n",
        "      --out_path ../output1.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "QtL0sY6iEeZO",
        "outputId": "1eab415f-e81c-4e65-aa1b-e40cfaa486f7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "                <audio controls=\"controls\" >\n",
              "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@markdown ### The audio should play below after clicking the play icon. \n",
        "from IPython.display import Audio\n",
        "!cp /content/output1.wav .\n",
        "a = Audio('output1.wav')\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD9PlZssH7yB"
      },
      "source": [
        "**Vocoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSi5xyCuH-eL",
        "outputId": "4b2fbd94-3559-4ba3-81c5-31fb7988e17f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 517/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:21:11) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 46/261 -- GLOBAL_STEP: 135500\u001b[0m\n",
            "     | > loss: 0.00844  (0.01362)\n",
            "     | > grad_norm: 0.79848  (1.13963)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37620  (0.38243)\n",
            "     | > loader_time: 0.00330  (0.00702)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 96/261 -- GLOBAL_STEP: 135550\u001b[0m\n",
            "     | > loss: 0.01145  (0.01301)\n",
            "     | > grad_norm: 1.21771  (1.29429)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39240  (0.38008)\n",
            "     | > loader_time: 0.00310  (0.00544)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 146/261 -- GLOBAL_STEP: 135600\u001b[0m\n",
            "     | > loss: 0.02334  (0.01307)\n",
            "     | > grad_norm: 6.42139  (1.27054)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37968)\n",
            "     | > loader_time: 0.00230  (0.00494)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 196/261 -- GLOBAL_STEP: 135650\u001b[0m\n",
            "     | > loss: 0.01343  (0.01274)\n",
            "     | > grad_norm: 1.55160  (1.27385)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37510  (0.37940)\n",
            "     | > loader_time: 0.00360  (0.00489)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 246/261 -- GLOBAL_STEP: 135700\u001b[0m\n",
            "     | > loss: 0.00837  (0.01283)\n",
            "     | > grad_norm: 2.05839  (1.27501)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38600  (0.37916)\n",
            "     | > loader_time: 0.00350  (0.00486)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01002  (0.01002)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.04354  (0.04354)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00057 \u001b[0m(-0.00012)\n",
            "     | > avg_loss:\u001b[91m 0.04354 \u001b[0m(+0.03173)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 518/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:22:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 34/261 -- GLOBAL_STEP: 135750\u001b[0m\n",
            "     | > loss: 0.00914  (0.01195)\n",
            "     | > grad_norm: 1.27790  (1.06819)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37490  (0.37906)\n",
            "     | > loader_time: 0.00350  (0.00615)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 84/261 -- GLOBAL_STEP: 135800\u001b[0m\n",
            "     | > loss: 0.01415  (0.01258)\n",
            "     | > grad_norm: 0.94712  (0.98184)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39080  (0.37908)\n",
            "     | > loader_time: 0.00340  (0.00413)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 134/261 -- GLOBAL_STEP: 135850\u001b[0m\n",
            "     | > loss: 0.01261  (0.01318)\n",
            "     | > grad_norm: 1.16168  (1.03218)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37880)\n",
            "     | > loader_time: 0.00230  (0.00374)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 184/261 -- GLOBAL_STEP: 135900\u001b[0m\n",
            "     | > loss: 0.00896  (0.01375)\n",
            "     | > grad_norm: 1.88503  (1.12496)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38600  (0.37842)\n",
            "     | > loader_time: 0.00920  (0.00376)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 234/261 -- GLOBAL_STEP: 135950\u001b[0m\n",
            "     | > loss: 0.01053  (0.01361)\n",
            "     | > grad_norm: 0.85290  (1.12331)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37640  (0.37828)\n",
            "     | > loader_time: 0.00260  (0.00381)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01268  (0.01268)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00785  (0.00785)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00066 \u001b[0m(+0.00009)\n",
            "     | > avg_loss:\u001b[92m 0.00785 \u001b[0m(-0.03569)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 519/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:24:47) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/261 -- GLOBAL_STEP: 136000\u001b[0m\n",
            "     | > loss: 0.00812  (0.01347)\n",
            "     | > grad_norm: 1.42276  (1.62054)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.38382)\n",
            "     | > loader_time: 0.00400  (0.00635)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 72/261 -- GLOBAL_STEP: 136050\u001b[0m\n",
            "     | > loss: 0.01713  (0.01287)\n",
            "     | > grad_norm: 0.91791  (1.33374)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37908)\n",
            "     | > loader_time: 0.00220  (0.00494)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 122/261 -- GLOBAL_STEP: 136100\u001b[0m\n",
            "     | > loss: 0.00867  (0.01253)\n",
            "     | > grad_norm: 0.63809  (1.23327)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37854)\n",
            "     | > loader_time: 0.00220  (0.00454)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 172/261 -- GLOBAL_STEP: 136150\u001b[0m\n",
            "     | > loss: 0.01225  (0.01282)\n",
            "     | > grad_norm: 2.43398  (1.23981)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37863)\n",
            "     | > loader_time: 0.00390  (0.00441)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 222/261 -- GLOBAL_STEP: 136200\u001b[0m\n",
            "     | > loss: 0.01291  (0.01285)\n",
            "     | > grad_norm: 0.86676  (1.31428)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38670  (0.37853)\n",
            "     | > loader_time: 0.00300  (0.00431)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01529  (0.01529)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01835  (0.01835)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00063 \u001b[0m(-0.00004)\n",
            "     | > avg_loss:\u001b[91m 0.01835 \u001b[0m(+0.01050)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 520/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:26:35) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/261 -- GLOBAL_STEP: 136250\u001b[0m\n",
            "     | > loss: 0.01673  (0.01415)\n",
            "     | > grad_norm: 1.35042  (1.21247)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.38541)\n",
            "     | > loader_time: 0.00100  (0.00938)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/261 -- GLOBAL_STEP: 136300\u001b[0m\n",
            "     | > loss: 0.00928  (0.01288)\n",
            "     | > grad_norm: 0.65226  (1.07038)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37908)\n",
            "     | > loader_time: 0.00220  (0.00581)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 110/261 -- GLOBAL_STEP: 136350\u001b[0m\n",
            "     | > loss: 0.01429  (0.01245)\n",
            "     | > grad_norm: 2.10243  (1.02570)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.37927)\n",
            "     | > loader_time: 0.00330  (0.00480)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 160/261 -- GLOBAL_STEP: 136400\u001b[0m\n",
            "     | > loss: 0.01296  (0.01239)\n",
            "     | > grad_norm: 0.97015  (1.11118)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37470  (0.37882)\n",
            "     | > loader_time: 0.00340  (0.00451)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 210/261 -- GLOBAL_STEP: 136450\u001b[0m\n",
            "     | > loss: 0.00813  (0.01204)\n",
            "     | > grad_norm: 0.59198  (1.10521)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37837)\n",
            "     | > loader_time: 0.00250  (0.00423)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 260/261 -- GLOBAL_STEP: 136500\u001b[0m\n",
            "     | > loss: 0.01124  (0.01211)\n",
            "     | > grad_norm: 0.63157  (1.09922)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37480  (0.37825)\n",
            "     | > loader_time: 0.00180  (0.00414)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01969  (0.01969)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01053  (0.01053)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00067 \u001b[0m(+0.00004)\n",
            "     | > avg_loss:\u001b[92m 0.01053 \u001b[0m(-0.00783)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 521/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:28:23) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 48/261 -- GLOBAL_STEP: 136550\u001b[0m\n",
            "     | > loss: 0.00937  (0.01194)\n",
            "     | > grad_norm: 0.83642  (1.07482)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38450  (0.37990)\n",
            "     | > loader_time: 0.00390  (0.00459)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 98/261 -- GLOBAL_STEP: 136600\u001b[0m\n",
            "     | > loss: 0.01200  (0.01187)\n",
            "     | > grad_norm: 1.32486  (1.09289)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37894)\n",
            "     | > loader_time: 0.00210  (0.00424)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 148/261 -- GLOBAL_STEP: 136650\u001b[0m\n",
            "     | > loss: 0.00634  (0.01207)\n",
            "     | > grad_norm: 0.52156  (1.21342)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38220  (0.37879)\n",
            "     | > loader_time: 0.00300  (0.00411)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 198/261 -- GLOBAL_STEP: 136700\u001b[0m\n",
            "     | > loss: 0.01302  (0.01220)\n",
            "     | > grad_norm: 2.07678  (1.15180)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39080  (0.37866)\n",
            "     | > loader_time: 0.00380  (0.00390)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 248/261 -- GLOBAL_STEP: 136750\u001b[0m\n",
            "     | > loss: 0.01769  (0.01240)\n",
            "     | > grad_norm: 1.26164  (1.17492)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.40970  (0.37867)\n",
            "     | > loader_time: 0.00650  (0.00393)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00949  (0.00949)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.02936  (0.02936)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00058 \u001b[0m(-0.00009)\n",
            "     | > avg_loss:\u001b[91m 0.02936 \u001b[0m(+0.01884)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 522/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:30:11) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 36/261 -- GLOBAL_STEP: 136800\u001b[0m\n",
            "     | > loss: 0.01018  (0.01266)\n",
            "     | > grad_norm: 0.55897  (1.14201)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37820  (0.38382)\n",
            "     | > loader_time: 0.00240  (0.00708)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 86/261 -- GLOBAL_STEP: 136850\u001b[0m\n",
            "     | > loss: 0.00801  (0.01280)\n",
            "     | > grad_norm: 1.02957  (1.29432)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.38088)\n",
            "     | > loader_time: 0.00220  (0.00611)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 136/261 -- GLOBAL_STEP: 136900\u001b[0m\n",
            "     | > loss: 0.01704  (0.01316)\n",
            "     | > grad_norm: 0.45164  (1.18381)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37870  (0.37972)\n",
            "     | > loader_time: 0.00260  (0.00513)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 186/261 -- GLOBAL_STEP: 136950\u001b[0m\n",
            "     | > loss: 0.01418  (0.01285)\n",
            "     | > grad_norm: 1.22452  (1.13024)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37971)\n",
            "     | > loader_time: 0.00230  (0.00521)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 236/261 -- GLOBAL_STEP: 137000\u001b[0m\n",
            "     | > loss: 0.00808  (0.01301)\n",
            "     | > grad_norm: 1.01935  (1.17855)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39260  (0.37954)\n",
            "     | > loader_time: 0.00400  (0.00511)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01055  (0.01055)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.02021  (0.02021)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00067 \u001b[0m(+0.00009)\n",
            "     | > avg_loss:\u001b[92m 0.02021 \u001b[0m(-0.00915)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 523/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:31:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 24/261 -- GLOBAL_STEP: 137050\u001b[0m\n",
            "     | > loss: 0.01337  (0.01346)\n",
            "     | > grad_norm: 0.47974  (0.99915)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.38205)\n",
            "     | > loader_time: 0.00230  (0.00761)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 74/261 -- GLOBAL_STEP: 137100\u001b[0m\n",
            "     | > loss: 0.00762  (0.01350)\n",
            "     | > grad_norm: 1.45984  (1.11540)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38550  (0.37947)\n",
            "     | > loader_time: 0.01710  (0.00511)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 124/261 -- GLOBAL_STEP: 137150\u001b[0m\n",
            "     | > loss: 0.00747  (0.01307)\n",
            "     | > grad_norm: 0.95411  (1.23417)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37899)\n",
            "     | > loader_time: 0.00230  (0.00469)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 174/261 -- GLOBAL_STEP: 137200\u001b[0m\n",
            "     | > loss: 0.00873  (0.01301)\n",
            "     | > grad_norm: 0.73257  (1.18372)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38190  (0.37892)\n",
            "     | > loader_time: 0.00340  (0.00478)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 224/261 -- GLOBAL_STEP: 137250\u001b[0m\n",
            "     | > loss: 0.00780  (0.01280)\n",
            "     | > grad_norm: 0.51642  (1.10522)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38560  (0.37875)\n",
            "     | > loader_time: 0.00310  (0.00457)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01107  (0.01107)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00915  (0.00915)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00054 \u001b[0m(-0.00013)\n",
            "     | > avg_loss:\u001b[92m 0.00915 \u001b[0m(-0.01106)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 524/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:33:48) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 12/261 -- GLOBAL_STEP: 137300\u001b[0m\n",
            "     | > loss: 0.01016  (0.01135)\n",
            "     | > grad_norm: 1.17434  (0.96839)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.38491)\n",
            "     | > loader_time: 0.01750  (0.01088)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 62/261 -- GLOBAL_STEP: 137350\u001b[0m\n",
            "     | > loss: 0.00933  (0.01153)\n",
            "     | > grad_norm: 0.55751  (1.09890)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37884)\n",
            "     | > loader_time: 0.00300  (0.00577)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 112/261 -- GLOBAL_STEP: 137400\u001b[0m\n",
            "     | > loss: 0.01742  (0.01136)\n",
            "     | > grad_norm: 1.28752  (1.15202)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37897)\n",
            "     | > loader_time: 0.00380  (0.00526)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 162/261 -- GLOBAL_STEP: 137450\u001b[0m\n",
            "     | > loss: 0.00926  (0.01178)\n",
            "     | > grad_norm: 1.55305  (1.05794)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38680  (0.37936)\n",
            "     | > loader_time: 0.00310  (0.00517)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 212/261 -- GLOBAL_STEP: 137500\u001b[0m\n",
            "     | > loss: 0.01306  (0.01204)\n",
            "     | > grad_norm: 1.58442  (1.11567)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38300  (0.37927)\n",
            "     | > loader_time: 0.02320  (0.00491)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.03686  (0.03686)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00729  (0.00729)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00067 \u001b[0m(+0.00013)\n",
            "     | > avg_loss:\u001b[92m 0.00729 \u001b[0m(-0.00187)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 525/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:35:36) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/261 -- GLOBAL_STEP: 137550\u001b[0m\n",
            "     | > loss: 0.00843  (0.00843)\n",
            "     | > grad_norm: 0.73703  (0.73703)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.40820  (0.40818)\n",
            "     | > loader_time: 1.40780  (1.40781)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/261 -- GLOBAL_STEP: 137600\u001b[0m\n",
            "     | > loss: 0.00761  (0.01195)\n",
            "     | > grad_norm: 0.78733  (0.91704)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37946)\n",
            "     | > loader_time: 0.01000  (0.00695)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/261 -- GLOBAL_STEP: 137650\u001b[0m\n",
            "     | > loss: 0.01417  (0.01217)\n",
            "     | > grad_norm: 0.72422  (0.97400)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37917)\n",
            "     | > loader_time: 0.00220  (0.00579)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 150/261 -- GLOBAL_STEP: 137700\u001b[0m\n",
            "     | > loss: 0.01083  (0.01215)\n",
            "     | > grad_norm: 0.53669  (1.02785)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38990  (0.37897)\n",
            "     | > loader_time: 0.01150  (0.00534)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 200/261 -- GLOBAL_STEP: 137750\u001b[0m\n",
            "     | > loss: 0.01097  (0.01212)\n",
            "     | > grad_norm: 2.26981  (1.05566)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37670  (0.37924)\n",
            "     | > loader_time: 0.00260  (0.00513)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 250/261 -- GLOBAL_STEP: 137800\u001b[0m\n",
            "     | > loss: 0.01056  (0.01209)\n",
            "     | > grad_norm: 0.58059  (1.09788)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37760  (0.37905)\n",
            "     | > loader_time: 0.00230  (0.00496)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01071  (0.01071)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00829  (0.00829)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00062 \u001b[0m(-0.00005)\n",
            "     | > avg_loss:\u001b[91m 0.00829 \u001b[0m(+0.00100)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 526/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:37:24) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 38/261 -- GLOBAL_STEP: 137850\u001b[0m\n",
            "     | > loss: 0.01412  (0.01174)\n",
            "     | > grad_norm: 0.89652  (1.15363)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37650  (0.38107)\n",
            "     | > loader_time: 0.00280  (0.00572)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/261 -- GLOBAL_STEP: 137900\u001b[0m\n",
            "     | > loss: 0.01009  (0.01198)\n",
            "     | > grad_norm: 1.47172  (1.11405)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37850  (0.37914)\n",
            "     | > loader_time: 0.00400  (0.00500)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 138/261 -- GLOBAL_STEP: 137950\u001b[0m\n",
            "     | > loss: 0.01246  (0.01189)\n",
            "     | > grad_norm: 1.02121  (1.12470)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37650  (0.37854)\n",
            "     | > loader_time: 0.00230  (0.00458)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 188/261 -- GLOBAL_STEP: 138000\u001b[0m\n",
            "     | > loss: 0.01076  (0.01199)\n",
            "     | > grad_norm: 1.68557  (1.15352)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37868)\n",
            "     | > loader_time: 0.00220  (0.00412)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 238/261 -- GLOBAL_STEP: 138050\u001b[0m\n",
            "     | > loss: 0.01452  (0.01227)\n",
            "     | > grad_norm: 1.73186  (1.22284)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37846)\n",
            "     | > loader_time: 0.00220  (0.00405)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.02332  (0.02332)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00722  (0.00722)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00056 \u001b[0m(-0.00006)\n",
            "     | > avg_loss:\u001b[92m 0.00722 \u001b[0m(-0.00107)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 527/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:39:12) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 26/261 -- GLOBAL_STEP: 138100\u001b[0m\n",
            "     | > loss: 0.02180  (0.01226)\n",
            "     | > grad_norm: 1.16402  (1.11527)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37931)\n",
            "     | > loader_time: 0.00240  (0.00738)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 76/261 -- GLOBAL_STEP: 138150\u001b[0m\n",
            "     | > loss: 0.01543  (0.01243)\n",
            "     | > grad_norm: 1.51399  (1.08187)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37781)\n",
            "     | > loader_time: 0.00220  (0.00480)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 126/261 -- GLOBAL_STEP: 138200\u001b[0m\n",
            "     | > loss: 0.01061  (0.01229)\n",
            "     | > grad_norm: 1.30270  (1.12137)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38450  (0.37783)\n",
            "     | > loader_time: 0.00270  (0.00437)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 176/261 -- GLOBAL_STEP: 138250\u001b[0m\n",
            "     | > loss: 0.01497  (0.01229)\n",
            "     | > grad_norm: 2.34435  (1.08135)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37510  (0.37771)\n",
            "     | > loader_time: 0.00240  (0.00403)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 226/261 -- GLOBAL_STEP: 138300\u001b[0m\n",
            "     | > loss: 0.00801  (0.01231)\n",
            "     | > grad_norm: 1.37647  (1.11259)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37650  (0.37767)\n",
            "     | > loader_time: 0.00220  (0.00404)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01851  (0.01851)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01662  (0.01662)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00069 \u001b[0m(+0.00012)\n",
            "     | > avg_loss:\u001b[91m 0.01662 \u001b[0m(+0.00940)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 528/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:41:00) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 14/261 -- GLOBAL_STEP: 138350\u001b[0m\n",
            "     | > loss: 0.00904  (0.01469)\n",
            "     | > grad_norm: 2.03184  (1.15936)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.38879)\n",
            "     | > loader_time: 0.00410  (0.00924)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 64/261 -- GLOBAL_STEP: 138400\u001b[0m\n",
            "     | > loss: 0.02010  (0.01331)\n",
            "     | > grad_norm: 0.58133  (1.20681)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.38071)\n",
            "     | > loader_time: 0.01220  (0.00492)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 114/261 -- GLOBAL_STEP: 138450\u001b[0m\n",
            "     | > loss: 0.01046  (0.01275)\n",
            "     | > grad_norm: 0.65383  (1.14647)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37970)\n",
            "     | > loader_time: 0.00220  (0.00422)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 164/261 -- GLOBAL_STEP: 138500\u001b[0m\n",
            "     | > loss: 0.01292  (0.01253)\n",
            "     | > grad_norm: 1.80043  (1.13088)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39260  (0.38020)\n",
            "     | > loader_time: 0.01400  (0.00421)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 214/261 -- GLOBAL_STEP: 138550\u001b[0m\n",
            "     | > loss: 0.00805  (0.01244)\n",
            "     | > grad_norm: 1.07118  (1.13770)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38360  (0.37983)\n",
            "     | > loader_time: 0.00320  (0.00408)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00810  (0.00810)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00954  (0.00954)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00064 \u001b[0m(-0.00005)\n",
            "     | > avg_loss:\u001b[92m 0.00954 \u001b[0m(-0.00708)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 529/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:42:48) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/261 -- GLOBAL_STEP: 138600\u001b[0m\n",
            "     | > loss: 0.01524  (0.01273)\n",
            "     | > grad_norm: 1.49716  (1.19388)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38340  (0.38596)\n",
            "     | > loader_time: 0.01460  (0.00918)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 52/261 -- GLOBAL_STEP: 138650\u001b[0m\n",
            "     | > loss: 0.00891  (0.01270)\n",
            "     | > grad_norm: 0.80229  (1.10998)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37936)\n",
            "     | > loader_time: 0.00350  (0.00619)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 102/261 -- GLOBAL_STEP: 138700\u001b[0m\n",
            "     | > loss: 0.02327  (0.01291)\n",
            "     | > grad_norm: 1.28074  (1.28480)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39010  (0.37910)\n",
            "     | > loader_time: 0.00340  (0.00496)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 152/261 -- GLOBAL_STEP: 138750\u001b[0m\n",
            "     | > loss: 0.00937  (0.01245)\n",
            "     | > grad_norm: 1.00514  (1.19370)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39220  (0.37929)\n",
            "     | > loader_time: 0.00260  (0.00508)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 202/261 -- GLOBAL_STEP: 138800\u001b[0m\n",
            "     | > loss: 0.01935  (0.01262)\n",
            "     | > grad_norm: 3.17762  (1.25260)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37760  (0.37904)\n",
            "     | > loader_time: 0.02410  (0.00499)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 252/261 -- GLOBAL_STEP: 138850\u001b[0m\n",
            "     | > loss: 0.00704  (0.01277)\n",
            "     | > grad_norm: 0.89994  (1.26091)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37884)\n",
            "     | > loader_time: 0.00230  (0.00496)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01502  (0.01502)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.02475  (0.02475)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00071 \u001b[0m(+0.00007)\n",
            "     | > avg_loss:\u001b[91m 0.02475 \u001b[0m(+0.01521)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 530/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:44:37) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 40/261 -- GLOBAL_STEP: 138900\u001b[0m\n",
            "     | > loss: 0.01082  (0.01389)\n",
            "     | > grad_norm: 1.14342  (1.00630)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37870  (0.38290)\n",
            "     | > loader_time: 0.00290  (0.00616)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/261 -- GLOBAL_STEP: 138950\u001b[0m\n",
            "     | > loss: 0.01437  (0.01245)\n",
            "     | > grad_norm: 0.81012  (0.97452)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.38039)\n",
            "     | > loader_time: 0.00290  (0.00544)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 140/261 -- GLOBAL_STEP: 139000\u001b[0m\n",
            "     | > loss: 0.01033  (0.01260)\n",
            "     | > grad_norm: 0.78914  (1.06118)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37500  (0.37972)\n",
            "     | > loader_time: 0.00230  (0.00479)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 190/261 -- GLOBAL_STEP: 139050\u001b[0m\n",
            "     | > loss: 0.00898  (0.01289)\n",
            "     | > grad_norm: 1.08667  (1.17180)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37943)\n",
            "     | > loader_time: 0.00220  (0.00471)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 240/261 -- GLOBAL_STEP: 139100\u001b[0m\n",
            "     | > loss: 0.00992  (0.01292)\n",
            "     | > grad_norm: 0.64879  (1.21572)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37953)\n",
            "     | > loader_time: 0.00220  (0.00472)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01182  (0.01182)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01207  (0.01207)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00059 \u001b[0m(-0.00012)\n",
            "     | > avg_loss:\u001b[92m 0.01207 \u001b[0m(-0.01268)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 531/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:46:26) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 28/261 -- GLOBAL_STEP: 139150\u001b[0m\n",
            "     | > loss: 0.01682  (0.01243)\n",
            "     | > grad_norm: 0.98444  (1.05334)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37630  (0.38220)\n",
            "     | > loader_time: 0.00250  (0.00657)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 78/261 -- GLOBAL_STEP: 139200\u001b[0m\n",
            "     | > loss: 0.01107  (0.01291)\n",
            "     | > grad_norm: 0.77649  (1.15631)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.38045)\n",
            "     | > loader_time: 0.00380  (0.00465)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 128/261 -- GLOBAL_STEP: 139250\u001b[0m\n",
            "     | > loss: 0.00862  (0.01257)\n",
            "     | > grad_norm: 0.47383  (1.16275)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37956)\n",
            "     | > loader_time: 0.00220  (0.00421)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 178/261 -- GLOBAL_STEP: 139300\u001b[0m\n",
            "     | > loss: 0.00962  (0.01265)\n",
            "     | > grad_norm: 1.17980  (1.19301)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39080  (0.37948)\n",
            "     | > loader_time: 0.00300  (0.00402)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 228/261 -- GLOBAL_STEP: 139350\u001b[0m\n",
            "     | > loss: 0.01617  (0.01263)\n",
            "     | > grad_norm: 2.00921  (1.29727)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37500  (0.37906)\n",
            "     | > loader_time: 0.00330  (0.00393)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00720  (0.00720)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01087  (0.01087)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00082 \u001b[0m(+0.00023)\n",
            "     | > avg_loss:\u001b[92m 0.01087 \u001b[0m(-0.00120)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 532/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:48:14) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/261 -- GLOBAL_STEP: 139400\u001b[0m\n",
            "     | > loss: 0.01359  (0.01215)\n",
            "     | > grad_norm: 0.68167  (0.60695)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.38304)\n",
            "     | > loader_time: 0.00230  (0.01091)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 66/261 -- GLOBAL_STEP: 139450\u001b[0m\n",
            "     | > loss: 0.00707  (0.01337)\n",
            "     | > grad_norm: 1.11709  (1.01013)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37960  (0.37964)\n",
            "     | > loader_time: 0.02030  (0.00598)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 116/261 -- GLOBAL_STEP: 139500\u001b[0m\n",
            "     | > loss: 0.00773  (0.01295)\n",
            "     | > grad_norm: 0.52041  (1.05340)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37933)\n",
            "     | > loader_time: 0.00220  (0.00508)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 166/261 -- GLOBAL_STEP: 139550\u001b[0m\n",
            "     | > loss: 0.01078  (0.01290)\n",
            "     | > grad_norm: 1.35287  (1.13315)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.37896)\n",
            "     | > loader_time: 0.00220  (0.00451)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 216/261 -- GLOBAL_STEP: 139600\u001b[0m\n",
            "     | > loss: 0.01378  (0.01284)\n",
            "     | > grad_norm: 1.14815  (1.19340)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37909)\n",
            "     | > loader_time: 0.00220  (0.00460)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01185  (0.01185)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01313  (0.01313)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00053 \u001b[0m(-0.00030)\n",
            "     | > avg_loss:\u001b[91m 0.01313 \u001b[0m(+0.00226)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 533/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:50:02) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 4/261 -- GLOBAL_STEP: 139650\u001b[0m\n",
            "     | > loss: 0.01175  (0.01229)\n",
            "     | > grad_norm: 1.96747  (1.52432)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.40100  (0.40018)\n",
            "     | > loader_time: 0.01020  (0.01048)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 54/261 -- GLOBAL_STEP: 139700\u001b[0m\n",
            "     | > loss: 0.00888  (0.01293)\n",
            "     | > grad_norm: 0.91151  (1.11933)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39080  (0.38082)\n",
            "     | > loader_time: 0.00860  (0.00503)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 104/261 -- GLOBAL_STEP: 139750\u001b[0m\n",
            "     | > loss: 0.01493  (0.01295)\n",
            "     | > grad_norm: 1.10089  (1.01290)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37946)\n",
            "     | > loader_time: 0.00230  (0.00429)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 154/261 -- GLOBAL_STEP: 139800\u001b[0m\n",
            "     | > loss: 0.00928  (0.01266)\n",
            "     | > grad_norm: 0.96520  (1.01922)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37640  (0.37911)\n",
            "     | > loader_time: 0.00240  (0.00407)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 204/261 -- GLOBAL_STEP: 139850\u001b[0m\n",
            "     | > loss: 0.01958  (0.01314)\n",
            "     | > grad_norm: 3.82917  (1.09594)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37620  (0.37884)\n",
            "     | > loader_time: 0.00380  (0.00393)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 254/261 -- GLOBAL_STEP: 139900\u001b[0m\n",
            "     | > loss: 0.00686  (0.01285)\n",
            "     | > grad_norm: 0.63152  (1.05970)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37510  (0.37862)\n",
            "     | > loader_time: 0.00180  (0.00408)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.02326  (0.02326)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01927  (0.01927)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00062 \u001b[0m(+0.00009)\n",
            "     | > avg_loss:\u001b[91m 0.01927 \u001b[0m(+0.00614)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 534/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:51:50) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 42/261 -- GLOBAL_STEP: 139950\u001b[0m\n",
            "     | > loss: 0.01040  (0.01266)\n",
            "     | > grad_norm: 1.32158  (1.29405)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37690  (0.37893)\n",
            "     | > loader_time: 0.00260  (0.00514)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 92/261 -- GLOBAL_STEP: 140000\u001b[0m\n",
            "     | > loss: 0.01114  (0.01201)\n",
            "     | > grad_norm: 0.68412  (1.14175)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37912)\n",
            "     | > loader_time: 0.00240  (0.00426)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2/checkpoint_140000.pth.tar\n",
            "\n",
            "\u001b[1m   --> STEP: 142/261 -- GLOBAL_STEP: 140050\u001b[0m\n",
            "     | > loss: 0.01168  (0.01183)\n",
            "     | > grad_norm: 1.62891  (1.13329)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37886)\n",
            "     | > loader_time: 0.00380  (0.00434)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 192/261 -- GLOBAL_STEP: 140100\u001b[0m\n",
            "     | > loss: 0.01208  (0.01169)\n",
            "     | > grad_norm: 0.71886  (1.09737)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37610  (0.37877)\n",
            "     | > loader_time: 0.00220  (0.00438)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 242/261 -- GLOBAL_STEP: 140150\u001b[0m\n",
            "     | > loss: 0.00972  (0.01171)\n",
            "     | > grad_norm: 0.73365  (1.07400)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37820  (0.37901)\n",
            "     | > loader_time: 0.02210  (0.00452)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01118  (0.01118)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00894  (0.00894)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00068 \u001b[0m(+0.00007)\n",
            "     | > avg_loss:\u001b[92m 0.00894 \u001b[0m(-0.01032)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 535/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:53:39) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 30/261 -- GLOBAL_STEP: 140200\u001b[0m\n",
            "     | > loss: 0.01207  (0.01146)\n",
            "     | > grad_norm: 1.23840  (1.06306)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.38364)\n",
            "     | > loader_time: 0.00260  (0.00516)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/261 -- GLOBAL_STEP: 140250\u001b[0m\n",
            "     | > loss: 0.01508  (0.01184)\n",
            "     | > grad_norm: 0.61638  (1.12975)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.38144)\n",
            "     | > loader_time: 0.00440  (0.00427)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/261 -- GLOBAL_STEP: 140300\u001b[0m\n",
            "     | > loss: 0.00988  (0.01227)\n",
            "     | > grad_norm: 1.07034  (1.18367)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38700  (0.37986)\n",
            "     | > loader_time: 0.00280  (0.00410)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 180/261 -- GLOBAL_STEP: 140350\u001b[0m\n",
            "     | > loss: 0.01374  (0.01230)\n",
            "     | > grad_norm: 2.63970  (1.18215)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37981)\n",
            "     | > loader_time: 0.00230  (0.00399)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/261 -- GLOBAL_STEP: 140400\u001b[0m\n",
            "     | > loss: 0.01038  (0.01242)\n",
            "     | > grad_norm: 1.01317  (1.18069)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37939)\n",
            "     | > loader_time: 0.00230  (0.00406)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01453  (0.01453)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01234  (0.01234)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00073 \u001b[0m(+0.00004)\n",
            "     | > avg_loss:\u001b[91m 0.01234 \u001b[0m(+0.00339)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 536/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:55:28) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 18/261 -- GLOBAL_STEP: 140450\u001b[0m\n",
            "     | > loss: 0.00926  (0.01358)\n",
            "     | > grad_norm: 1.60208  (1.46052)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.38128)\n",
            "     | > loader_time: 0.00230  (0.00687)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 68/261 -- GLOBAL_STEP: 140500\u001b[0m\n",
            "     | > loss: 0.01561  (0.01236)\n",
            "     | > grad_norm: 0.97583  (1.13179)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37610  (0.37936)\n",
            "     | > loader_time: 0.00330  (0.00528)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 118/261 -- GLOBAL_STEP: 140550\u001b[0m\n",
            "     | > loss: 0.01467  (0.01226)\n",
            "     | > grad_norm: 0.85935  (1.05603)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37874)\n",
            "     | > loader_time: 0.00220  (0.00533)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 168/261 -- GLOBAL_STEP: 140600\u001b[0m\n",
            "     | > loss: 0.01254  (0.01246)\n",
            "     | > grad_norm: 1.00120  (1.13088)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39500  (0.37906)\n",
            "     | > loader_time: 0.00220  (0.00522)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 218/261 -- GLOBAL_STEP: 140650\u001b[0m\n",
            "     | > loss: 0.01239  (0.01229)\n",
            "     | > grad_norm: 1.35448  (1.12304)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37909)\n",
            "     | > loader_time: 0.00220  (0.00484)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01189  (0.01189)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01301  (0.01301)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00280 \u001b[0m(+0.00208)\n",
            "     | > avg_loss:\u001b[91m 0.01301 \u001b[0m(+0.00067)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 537/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:57:16) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 6/261 -- GLOBAL_STEP: 140700\u001b[0m\n",
            "     | > loss: 0.00767  (0.01247)\n",
            "     | > grad_norm: 1.12853  (1.25994)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39520  (0.39346)\n",
            "     | > loader_time: 0.00130  (0.01354)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 56/261 -- GLOBAL_STEP: 140750\u001b[0m\n",
            "     | > loss: 0.02071  (0.01215)\n",
            "     | > grad_norm: 0.57551  (0.95632)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.38124)\n",
            "     | > loader_time: 0.00220  (0.00670)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 106/261 -- GLOBAL_STEP: 140800\u001b[0m\n",
            "     | > loss: 0.02891  (0.01292)\n",
            "     | > grad_norm: 1.20000  (1.06196)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37640  (0.38003)\n",
            "     | > loader_time: 0.02590  (0.00612)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 156/261 -- GLOBAL_STEP: 140850\u001b[0m\n",
            "     | > loss: 0.00811  (0.01284)\n",
            "     | > grad_norm: 0.61325  (1.05515)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39350  (0.37933)\n",
            "     | > loader_time: 0.00280  (0.00531)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 206/261 -- GLOBAL_STEP: 140900\u001b[0m\n",
            "     | > loss: 0.03551  (0.01281)\n",
            "     | > grad_norm: 1.43786  (1.11349)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37924)\n",
            "     | > loader_time: 0.00230  (0.00480)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 256/261 -- GLOBAL_STEP: 140950\u001b[0m\n",
            "     | > loss: 0.00723  (0.01293)\n",
            "     | > grad_norm: 1.17532  (1.13986)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37490  (0.37904)\n",
            "     | > loader_time: 0.00180  (0.00465)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01742  (0.01742)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01006  (0.01006)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00054 \u001b[0m(-0.00227)\n",
            "     | > avg_loss:\u001b[92m 0.01006 \u001b[0m(-0.00295)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 538/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 00:59:04) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 44/261 -- GLOBAL_STEP: 141000\u001b[0m\n",
            "     | > loss: 0.00821  (0.01266)\n",
            "     | > grad_norm: 0.90242  (1.19165)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.38133)\n",
            "     | > loader_time: 0.00220  (0.00765)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 94/261 -- GLOBAL_STEP: 141050\u001b[0m\n",
            "     | > loss: 0.01266  (0.01306)\n",
            "     | > grad_norm: 2.87486  (1.25032)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39330  (0.38009)\n",
            "     | > loader_time: 0.00300  (0.00525)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 144/261 -- GLOBAL_STEP: 141100\u001b[0m\n",
            "     | > loss: 0.01509  (0.01287)\n",
            "     | > grad_norm: 1.22963  (1.14320)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37640  (0.37987)\n",
            "     | > loader_time: 0.01620  (0.00522)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 194/261 -- GLOBAL_STEP: 141150\u001b[0m\n",
            "     | > loss: 0.01277  (0.01272)\n",
            "     | > grad_norm: 1.44849  (1.16537)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37954)\n",
            "     | > loader_time: 0.00330  (0.00503)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 244/261 -- GLOBAL_STEP: 141200\u001b[0m\n",
            "     | > loss: 0.01177  (0.01259)\n",
            "     | > grad_norm: 0.48174  (1.18653)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37922)\n",
            "     | > loader_time: 0.00230  (0.00469)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.02226  (0.02226)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01281  (0.01281)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00057 \u001b[0m(+0.00004)\n",
            "     | > avg_loss:\u001b[91m 0.01281 \u001b[0m(+0.00275)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 539/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:00:53) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 32/261 -- GLOBAL_STEP: 141250\u001b[0m\n",
            "     | > loss: 0.00908  (0.01184)\n",
            "     | > grad_norm: 0.91672  (0.86285)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.38424)\n",
            "     | > loader_time: 0.00410  (0.00602)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 82/261 -- GLOBAL_STEP: 141300\u001b[0m\n",
            "     | > loss: 0.00908  (0.01260)\n",
            "     | > grad_norm: 0.89580  (1.05752)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.38130)\n",
            "     | > loader_time: 0.00320  (0.00454)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 132/261 -- GLOBAL_STEP: 141350\u001b[0m\n",
            "     | > loss: 0.01073  (0.01218)\n",
            "     | > grad_norm: 0.83868  (1.09751)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39200  (0.38043)\n",
            "     | > loader_time: 0.00280  (0.00457)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 182/261 -- GLOBAL_STEP: 141400\u001b[0m\n",
            "     | > loss: 0.02427  (0.01245)\n",
            "     | > grad_norm: 1.44696  (1.13349)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37995)\n",
            "     | > loader_time: 0.00230  (0.00439)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 232/261 -- GLOBAL_STEP: 141450\u001b[0m\n",
            "     | > loss: 0.01604  (0.01275)\n",
            "     | > grad_norm: 1.26423  (1.18408)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37967)\n",
            "     | > loader_time: 0.00240  (0.00430)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.03073  (0.03073)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.02235  (0.02235)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00069 \u001b[0m(+0.00011)\n",
            "     | > avg_loss:\u001b[91m 0.02235 \u001b[0m(+0.00954)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 540/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:02:41) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/261 -- GLOBAL_STEP: 141500\u001b[0m\n",
            "     | > loss: 0.01438  (0.01257)\n",
            "     | > grad_norm: 0.77335  (0.94876)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38530  (0.38842)\n",
            "     | > loader_time: 0.00810  (0.00877)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/261 -- GLOBAL_STEP: 141550\u001b[0m\n",
            "     | > loss: 0.00790  (0.01297)\n",
            "     | > grad_norm: 1.05687  (1.27599)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37690  (0.38101)\n",
            "     | > loader_time: 0.00310  (0.00507)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/261 -- GLOBAL_STEP: 141600\u001b[0m\n",
            "     | > loss: 0.01441  (0.01257)\n",
            "     | > grad_norm: 1.41565  (1.23385)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.38000)\n",
            "     | > loader_time: 0.00240  (0.00459)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 170/261 -- GLOBAL_STEP: 141650\u001b[0m\n",
            "     | > loss: 0.00653  (0.01276)\n",
            "     | > grad_norm: 1.48696  (1.25904)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.38009)\n",
            "     | > loader_time: 0.00340  (0.00505)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 220/261 -- GLOBAL_STEP: 141700\u001b[0m\n",
            "     | > loss: 0.01634  (0.01259)\n",
            "     | > grad_norm: 0.57510  (1.20373)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37991)\n",
            "     | > loader_time: 0.00220  (0.00477)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01018  (0.01018)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.04194  (0.04194)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00077 \u001b[0m(+0.00009)\n",
            "     | > avg_loss:\u001b[91m 0.04194 \u001b[0m(+0.01959)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 541/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:04:30) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 8/261 -- GLOBAL_STEP: 141750\u001b[0m\n",
            "     | > loss: 0.00831  (0.01018)\n",
            "     | > grad_norm: 0.65314  (0.78722)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.39239)\n",
            "     | > loader_time: 0.00390  (0.00890)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 58/261 -- GLOBAL_STEP: 141800\u001b[0m\n",
            "     | > loss: 0.01748  (0.01338)\n",
            "     | > grad_norm: 1.81610  (1.72224)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37630  (0.37956)\n",
            "     | > loader_time: 0.00360  (0.00479)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 108/261 -- GLOBAL_STEP: 141850\u001b[0m\n",
            "     | > loss: 0.00731  (0.01279)\n",
            "     | > grad_norm: 1.17633  (1.46985)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39140  (0.37920)\n",
            "     | > loader_time: 0.00310  (0.00424)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 158/261 -- GLOBAL_STEP: 141900\u001b[0m\n",
            "     | > loss: 0.01993  (0.01282)\n",
            "     | > grad_norm: 0.46411  (1.36937)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39170  (0.37864)\n",
            "     | > loader_time: 0.00330  (0.00415)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 208/261 -- GLOBAL_STEP: 141950\u001b[0m\n",
            "     | > loss: 0.01014  (0.01299)\n",
            "     | > grad_norm: 0.50425  (1.34015)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37933)\n",
            "     | > loader_time: 0.00280  (0.00430)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 258/261 -- GLOBAL_STEP: 142000\u001b[0m\n",
            "     | > loss: 0.02402  (0.01309)\n",
            "     | > grad_norm: 1.63830  (1.31200)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37500  (0.37944)\n",
            "     | > loader_time: 0.00180  (0.00420)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01375  (0.01375)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01126  (0.01126)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00052 \u001b[0m(-0.00025)\n",
            "     | > avg_loss:\u001b[92m 0.01126 \u001b[0m(-0.03068)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 542/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:06:18) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 46/261 -- GLOBAL_STEP: 142050\u001b[0m\n",
            "     | > loss: 0.01127  (0.01326)\n",
            "     | > grad_norm: 1.18993  (1.69118)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.38027)\n",
            "     | > loader_time: 0.00250  (0.00721)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 96/261 -- GLOBAL_STEP: 142100\u001b[0m\n",
            "     | > loss: 0.00948  (0.01292)\n",
            "     | > grad_norm: 0.72648  (1.36055)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38030  (0.37987)\n",
            "     | > loader_time: 0.00350  (0.00527)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 146/261 -- GLOBAL_STEP: 142150\u001b[0m\n",
            "     | > loss: 0.00833  (0.01270)\n",
            "     | > grad_norm: 1.08720  (1.23572)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39210  (0.37929)\n",
            "     | > loader_time: 0.00300  (0.00497)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 196/261 -- GLOBAL_STEP: 142200\u001b[0m\n",
            "     | > loss: 0.00874  (0.01254)\n",
            "     | > grad_norm: 1.11338  (1.21399)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37500  (0.37884)\n",
            "     | > loader_time: 0.00220  (0.00509)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 246/261 -- GLOBAL_STEP: 142250\u001b[0m\n",
            "     | > loss: 0.01387  (0.01245)\n",
            "     | > grad_norm: 1.60156  (1.20411)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37855)\n",
            "     | > loader_time: 0.00230  (0.00474)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00962  (0.00962)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01757  (0.01757)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00066 \u001b[0m(+0.00013)\n",
            "     | > avg_loss:\u001b[91m 0.01757 \u001b[0m(+0.00631)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 543/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:08:06) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 34/261 -- GLOBAL_STEP: 142300\u001b[0m\n",
            "     | > loss: 0.01046  (0.01247)\n",
            "     | > grad_norm: 1.03526  (1.06292)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.38443)\n",
            "     | > loader_time: 0.00240  (0.00675)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 84/261 -- GLOBAL_STEP: 142350\u001b[0m\n",
            "     | > loss: 0.00914  (0.01257)\n",
            "     | > grad_norm: 1.93288  (1.18260)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38170  (0.38058)\n",
            "     | > loader_time: 0.00280  (0.00448)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 134/261 -- GLOBAL_STEP: 142400\u001b[0m\n",
            "     | > loss: 0.01225  (0.01267)\n",
            "     | > grad_norm: 0.84816  (1.16744)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37620  (0.37969)\n",
            "     | > loader_time: 0.00420  (0.00442)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 184/261 -- GLOBAL_STEP: 142450\u001b[0m\n",
            "     | > loss: 0.01041  (0.01269)\n",
            "     | > grad_norm: 1.73528  (1.21044)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37934)\n",
            "     | > loader_time: 0.00240  (0.00423)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 234/261 -- GLOBAL_STEP: 142500\u001b[0m\n",
            "     | > loss: 0.01754  (0.01259)\n",
            "     | > grad_norm: 1.53973  (1.16893)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37895)\n",
            "     | > loader_time: 0.00220  (0.00420)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01573  (0.01573)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00899  (0.00899)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00071 \u001b[0m(+0.00005)\n",
            "     | > avg_loss:\u001b[92m 0.00899 \u001b[0m(-0.00858)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 544/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:09:54) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/261 -- GLOBAL_STEP: 142550\u001b[0m\n",
            "     | > loss: 0.00959  (0.01109)\n",
            "     | > grad_norm: 1.47140  (1.42953)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38600  (0.38178)\n",
            "     | > loader_time: 0.00280  (0.00509)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 72/261 -- GLOBAL_STEP: 142600\u001b[0m\n",
            "     | > loss: 0.01257  (0.01175)\n",
            "     | > grad_norm: 3.27807  (1.24930)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37943)\n",
            "     | > loader_time: 0.00230  (0.00431)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 122/261 -- GLOBAL_STEP: 142650\u001b[0m\n",
            "     | > loss: 0.00918  (0.01195)\n",
            "     | > grad_norm: 0.47929  (1.27861)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.37850)\n",
            "     | > loader_time: 0.00420  (0.00418)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 172/261 -- GLOBAL_STEP: 142700\u001b[0m\n",
            "     | > loss: 0.01202  (0.01233)\n",
            "     | > grad_norm: 1.45312  (1.27260)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38810  (0.37852)\n",
            "     | > loader_time: 0.00270  (0.00407)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 222/261 -- GLOBAL_STEP: 142750\u001b[0m\n",
            "     | > loss: 0.01646  (0.01257)\n",
            "     | > grad_norm: 1.18473  (1.27775)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37844)\n",
            "     | > loader_time: 0.00250  (0.00409)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01495  (0.01495)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01044  (0.01044)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00059 \u001b[0m(-0.00012)\n",
            "     | > avg_loss:\u001b[91m 0.01044 \u001b[0m(+0.00145)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 545/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:11:42) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/261 -- GLOBAL_STEP: 142800\u001b[0m\n",
            "     | > loss: 0.00775  (0.01825)\n",
            "     | > grad_norm: 1.20726  (2.54947)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.38290)\n",
            "     | > loader_time: 0.00110  (0.01381)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/261 -- GLOBAL_STEP: 142850\u001b[0m\n",
            "     | > loss: 0.01918  (0.01410)\n",
            "     | > grad_norm: 2.29381  (1.60562)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37500  (0.37981)\n",
            "     | > loader_time: 0.00220  (0.00599)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 110/261 -- GLOBAL_STEP: 142900\u001b[0m\n",
            "     | > loss: 0.01181  (0.01314)\n",
            "     | > grad_norm: 1.33369  (1.37340)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37908)\n",
            "     | > loader_time: 0.00220  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 160/261 -- GLOBAL_STEP: 142950\u001b[0m\n",
            "     | > loss: 0.01562  (0.01300)\n",
            "     | > grad_norm: 0.90784  (1.27654)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37932)\n",
            "     | > loader_time: 0.00220  (0.00470)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 210/261 -- GLOBAL_STEP: 143000\u001b[0m\n",
            "     | > loss: 0.01167  (0.01289)\n",
            "     | > grad_norm: 1.14262  (1.22397)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37510  (0.37909)\n",
            "     | > loader_time: 0.00320  (0.00438)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 260/261 -- GLOBAL_STEP: 143050\u001b[0m\n",
            "     | > loss: 0.01383  (0.01286)\n",
            "     | > grad_norm: 0.99615  (1.25923)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37480  (0.37901)\n",
            "     | > loader_time: 0.00180  (0.00454)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01189  (0.01189)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.02997  (0.02997)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00070 \u001b[0m(+0.00010)\n",
            "     | > avg_loss:\u001b[91m 0.02997 \u001b[0m(+0.01953)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 546/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:13:31) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 48/261 -- GLOBAL_STEP: 143100\u001b[0m\n",
            "     | > loss: 0.00786  (0.01208)\n",
            "     | > grad_norm: 1.02417  (1.37881)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37640  (0.38104)\n",
            "     | > loader_time: 0.00240  (0.00524)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 98/261 -- GLOBAL_STEP: 143150\u001b[0m\n",
            "     | > loss: 0.00806  (0.01228)\n",
            "     | > grad_norm: 0.93382  (1.23618)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37620  (0.38034)\n",
            "     | > loader_time: 0.00270  (0.00461)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 148/261 -- GLOBAL_STEP: 143200\u001b[0m\n",
            "     | > loss: 0.01374  (0.01212)\n",
            "     | > grad_norm: 0.62108  (1.21720)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.38006)\n",
            "     | > loader_time: 0.00220  (0.00461)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 198/261 -- GLOBAL_STEP: 143250\u001b[0m\n",
            "     | > loss: 0.01336  (0.01236)\n",
            "     | > grad_norm: 0.53304  (1.21646)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38390  (0.37983)\n",
            "     | > loader_time: 0.00220  (0.00471)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 248/261 -- GLOBAL_STEP: 143300\u001b[0m\n",
            "     | > loss: 0.01086  (0.01259)\n",
            "     | > grad_norm: 1.12345  (1.23453)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39280  (0.37974)\n",
            "     | > loader_time: 0.00300  (0.00466)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01368  (0.01368)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01060  (0.01060)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00071 \u001b[0m(+0.00001)\n",
            "     | > avg_loss:\u001b[92m 0.01060 \u001b[0m(-0.01937)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 547/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:15:20) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 36/261 -- GLOBAL_STEP: 143350\u001b[0m\n",
            "     | > loss: 0.02395  (0.01220)\n",
            "     | > grad_norm: 1.37889  (0.91271)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.38136)\n",
            "     | > loader_time: 0.00360  (0.00719)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 86/261 -- GLOBAL_STEP: 143400\u001b[0m\n",
            "     | > loss: 0.00884  (0.01315)\n",
            "     | > grad_norm: 0.77805  (1.08339)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37973)\n",
            "     | > loader_time: 0.00220  (0.00507)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 136/261 -- GLOBAL_STEP: 143450\u001b[0m\n",
            "     | > loss: 0.00613  (0.01348)\n",
            "     | > grad_norm: 1.26170  (1.26152)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38100  (0.37941)\n",
            "     | > loader_time: 0.00930  (0.00489)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 186/261 -- GLOBAL_STEP: 143500\u001b[0m\n",
            "     | > loss: 0.02341  (0.01298)\n",
            "     | > grad_norm: 1.43588  (1.21898)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37933)\n",
            "     | > loader_time: 0.00220  (0.00454)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 236/261 -- GLOBAL_STEP: 143550\u001b[0m\n",
            "     | > loss: 0.01104  (0.01284)\n",
            "     | > grad_norm: 0.69369  (1.24506)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39230  (0.37927)\n",
            "     | > loader_time: 0.00260  (0.00448)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.02030  (0.02030)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00670  (0.00670)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00057 \u001b[0m(-0.00014)\n",
            "     | > avg_loss:\u001b[92m 0.00670 \u001b[0m(-0.00390)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 548/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:17:08) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 24/261 -- GLOBAL_STEP: 143600\u001b[0m\n",
            "     | > loss: 0.01110  (0.01169)\n",
            "     | > grad_norm: 0.95574  (1.05548)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38570  (0.38253)\n",
            "     | > loader_time: 0.00290  (0.00635)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 74/261 -- GLOBAL_STEP: 143650\u001b[0m\n",
            "     | > loss: 0.01236  (0.01149)\n",
            "     | > grad_norm: 1.36646  (1.00465)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38500  (0.38034)\n",
            "     | > loader_time: 0.00310  (0.00498)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 124/261 -- GLOBAL_STEP: 143700\u001b[0m\n",
            "     | > loss: 0.00836  (0.01210)\n",
            "     | > grad_norm: 0.76630  (1.18718)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37939)\n",
            "     | > loader_time: 0.00200  (0.00451)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 174/261 -- GLOBAL_STEP: 143750\u001b[0m\n",
            "     | > loss: 0.01085  (0.01200)\n",
            "     | > grad_norm: 1.20804  (1.18400)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37888)\n",
            "     | > loader_time: 0.00220  (0.00418)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 224/261 -- GLOBAL_STEP: 143800\u001b[0m\n",
            "     | > loss: 0.00809  (0.01213)\n",
            "     | > grad_norm: 0.55752  (1.16677)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37630  (0.37891)\n",
            "     | > loader_time: 0.00310  (0.00405)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01534  (0.01534)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01208  (0.01208)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00074 \u001b[0m(+0.00017)\n",
            "     | > avg_loss:\u001b[91m 0.01208 \u001b[0m(+0.00539)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 549/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:18:56) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 12/261 -- GLOBAL_STEP: 143850\u001b[0m\n",
            "     | > loss: 0.00679  (0.01431)\n",
            "     | > grad_norm: 0.65636  (1.21606)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.39063)\n",
            "     | > loader_time: 0.00270  (0.00888)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 62/261 -- GLOBAL_STEP: 143900\u001b[0m\n",
            "     | > loss: 0.01202  (0.01273)\n",
            "     | > grad_norm: 1.63960  (1.21843)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37994)\n",
            "     | > loader_time: 0.00230  (0.00451)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 112/261 -- GLOBAL_STEP: 143950\u001b[0m\n",
            "     | > loss: 0.01873  (0.01309)\n",
            "     | > grad_norm: 4.58694  (1.17797)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.37940)\n",
            "     | > loader_time: 0.00340  (0.00404)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 162/261 -- GLOBAL_STEP: 144000\u001b[0m\n",
            "     | > loss: 0.00987  (0.01261)\n",
            "     | > grad_norm: 0.84099  (1.16100)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37620  (0.37963)\n",
            "     | > loader_time: 0.00310  (0.00376)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 212/261 -- GLOBAL_STEP: 144050\u001b[0m\n",
            "     | > loss: 0.01010  (0.01271)\n",
            "     | > grad_norm: 0.88370  (1.20226)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.40220  (0.37943)\n",
            "     | > loader_time: 0.00280  (0.00363)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.02177  (0.02177)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01394  (0.01394)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00072 \u001b[0m(-0.00003)\n",
            "     | > avg_loss:\u001b[91m 0.01394 \u001b[0m(+0.00186)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 550/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:20:44) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/261 -- GLOBAL_STEP: 144100\u001b[0m\n",
            "     | > loss: 0.01148  (0.01148)\n",
            "     | > grad_norm: 0.80273  (0.80273)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39510  (0.39506)\n",
            "     | > loader_time: 1.17290  (1.17289)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/261 -- GLOBAL_STEP: 144150\u001b[0m\n",
            "     | > loss: 0.00839  (0.01243)\n",
            "     | > grad_norm: 0.90113  (1.39326)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37640  (0.37850)\n",
            "     | > loader_time: 0.00370  (0.00482)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/261 -- GLOBAL_STEP: 144200\u001b[0m\n",
            "     | > loss: 0.00948  (0.01247)\n",
            "     | > grad_norm: 1.77878  (1.25695)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37846)\n",
            "     | > loader_time: 0.00230  (0.00413)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 150/261 -- GLOBAL_STEP: 144250\u001b[0m\n",
            "     | > loss: 0.01245  (0.01267)\n",
            "     | > grad_norm: 0.71073  (1.22306)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38230  (0.37808)\n",
            "     | > loader_time: 0.01580  (0.00414)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 200/261 -- GLOBAL_STEP: 144300\u001b[0m\n",
            "     | > loss: 0.00771  (0.01253)\n",
            "     | > grad_norm: 0.78592  (1.16895)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37800)\n",
            "     | > loader_time: 0.01400  (0.00414)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 250/261 -- GLOBAL_STEP: 144350\u001b[0m\n",
            "     | > loss: 0.00911  (0.01264)\n",
            "     | > grad_norm: 1.59181  (1.15934)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37804)\n",
            "     | > loader_time: 0.00410  (0.00407)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00956  (0.00956)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01242  (0.01242)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00075 \u001b[0m(+0.00003)\n",
            "     | > avg_loss:\u001b[92m 0.01242 \u001b[0m(-0.00152)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 551/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:22:32) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 38/261 -- GLOBAL_STEP: 144400\u001b[0m\n",
            "     | > loss: 0.01057  (0.01174)\n",
            "     | > grad_norm: 1.09820  (1.16227)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37640  (0.38096)\n",
            "     | > loader_time: 0.00360  (0.00630)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/261 -- GLOBAL_STEP: 144450\u001b[0m\n",
            "     | > loss: 0.01023  (0.01267)\n",
            "     | > grad_norm: 0.86703  (1.25845)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.38009)\n",
            "     | > loader_time: 0.00230  (0.00505)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 138/261 -- GLOBAL_STEP: 144500\u001b[0m\n",
            "     | > loss: 0.00462  (0.01252)\n",
            "     | > grad_norm: 0.66385  (1.18455)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37929)\n",
            "     | > loader_time: 0.00240  (0.00451)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 188/261 -- GLOBAL_STEP: 144550\u001b[0m\n",
            "     | > loss: 0.01654  (0.01275)\n",
            "     | > grad_norm: 2.17751  (1.18449)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38370  (0.37946)\n",
            "     | > loader_time: 0.00280  (0.00465)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 238/261 -- GLOBAL_STEP: 144600\u001b[0m\n",
            "     | > loss: 0.02135  (0.01260)\n",
            "     | > grad_norm: 1.77958  (1.19018)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37907)\n",
            "     | > loader_time: 0.00190  (0.00448)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01995  (0.01995)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01000  (0.01000)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00060 \u001b[0m(-0.00015)\n",
            "     | > avg_loss:\u001b[92m 0.01000 \u001b[0m(-0.00242)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 552/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:24:20) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 26/261 -- GLOBAL_STEP: 144650\u001b[0m\n",
            "     | > loss: 0.01087  (0.01197)\n",
            "     | > grad_norm: 0.58397  (1.06750)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37900  (0.38602)\n",
            "     | > loader_time: 0.02100  (0.00855)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 76/261 -- GLOBAL_STEP: 144700\u001b[0m\n",
            "     | > loss: 0.02221  (0.01204)\n",
            "     | > grad_norm: 2.33540  (1.07074)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.38113)\n",
            "     | > loader_time: 0.00240  (0.00555)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 126/261 -- GLOBAL_STEP: 144750\u001b[0m\n",
            "     | > loss: 0.01561  (0.01216)\n",
            "     | > grad_norm: 1.15837  (1.12386)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39130  (0.38011)\n",
            "     | > loader_time: 0.00310  (0.00522)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 176/261 -- GLOBAL_STEP: 144800\u001b[0m\n",
            "     | > loss: 0.01513  (0.01233)\n",
            "     | > grad_norm: 1.14552  (1.20757)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37973)\n",
            "     | > loader_time: 0.00220  (0.00475)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 226/261 -- GLOBAL_STEP: 144850\u001b[0m\n",
            "     | > loss: 0.01039  (0.01212)\n",
            "     | > grad_norm: 1.90895  (1.20890)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38380  (0.37921)\n",
            "     | > loader_time: 0.00280  (0.00462)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00597  (0.00597)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01192  (0.01192)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00068 \u001b[0m(+0.00009)\n",
            "     | > avg_loss:\u001b[91m 0.01192 \u001b[0m(+0.00193)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 553/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:26:09) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 14/261 -- GLOBAL_STEP: 144900\u001b[0m\n",
            "     | > loss: 0.01156  (0.01221)\n",
            "     | > grad_norm: 0.97743  (1.43446)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.38527)\n",
            "     | > loader_time: 0.00230  (0.01080)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 64/261 -- GLOBAL_STEP: 144950\u001b[0m\n",
            "     | > loss: 0.01015  (0.01310)\n",
            "     | > grad_norm: 1.02934  (1.40458)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37964)\n",
            "     | > loader_time: 0.00240  (0.00594)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 114/261 -- GLOBAL_STEP: 145000\u001b[0m\n",
            "     | > loss: 0.00792  (0.01323)\n",
            "     | > grad_norm: 0.96226  (1.48621)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37894)\n",
            "     | > loader_time: 0.00230  (0.00505)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 164/261 -- GLOBAL_STEP: 145050\u001b[0m\n",
            "     | > loss: 0.01440  (0.01333)\n",
            "     | > grad_norm: 0.90071  (1.30911)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37740  (0.37858)\n",
            "     | > loader_time: 0.00340  (0.00476)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 214/261 -- GLOBAL_STEP: 145100\u001b[0m\n",
            "     | > loss: 0.00590  (0.01316)\n",
            "     | > grad_norm: 0.91849  (1.23627)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37510  (0.37838)\n",
            "     | > loader_time: 0.00220  (0.00463)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01075  (0.01075)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01701  (0.01701)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00055 \u001b[0m(-0.00014)\n",
            "     | > avg_loss:\u001b[91m 0.01701 \u001b[0m(+0.00509)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 554/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:27:57) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/261 -- GLOBAL_STEP: 145150\u001b[0m\n",
            "     | > loss: 0.01481  (0.01186)\n",
            "     | > grad_norm: 0.52798  (0.72401)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39770  (0.40160)\n",
            "     | > loader_time: 0.00680  (0.01495)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 52/261 -- GLOBAL_STEP: 145200\u001b[0m\n",
            "     | > loss: 0.01208  (0.01222)\n",
            "     | > grad_norm: 1.79675  (1.10588)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.37854)\n",
            "     | > loader_time: 0.00240  (0.00568)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 102/261 -- GLOBAL_STEP: 145250\u001b[0m\n",
            "     | > loss: 0.01054  (0.01245)\n",
            "     | > grad_norm: 0.82984  (1.12703)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37891)\n",
            "     | > loader_time: 0.00230  (0.00510)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 152/261 -- GLOBAL_STEP: 145300\u001b[0m\n",
            "     | > loss: 0.00877  (0.01207)\n",
            "     | > grad_norm: 1.46978  (1.14559)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37863)\n",
            "     | > loader_time: 0.00310  (0.00480)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 202/261 -- GLOBAL_STEP: 145350\u001b[0m\n",
            "     | > loss: 0.01889  (0.01217)\n",
            "     | > grad_norm: 0.79130  (1.17943)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37878)\n",
            "     | > loader_time: 0.00260  (0.00464)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 252/261 -- GLOBAL_STEP: 145400\u001b[0m\n",
            "     | > loss: 0.00736  (0.01222)\n",
            "     | > grad_norm: 0.93486  (1.15891)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39320  (0.37879)\n",
            "     | > loader_time: 0.00280  (0.00444)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01861  (0.01861)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01782  (0.01782)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00067 \u001b[0m(+0.00012)\n",
            "     | > avg_loss:\u001b[91m 0.01782 \u001b[0m(+0.00081)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 555/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:29:45) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 40/261 -- GLOBAL_STEP: 145450\u001b[0m\n",
            "     | > loss: 0.00989  (0.01226)\n",
            "     | > grad_norm: 1.44403  (1.25234)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37800  (0.38021)\n",
            "     | > loader_time: 0.00720  (0.00637)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/261 -- GLOBAL_STEP: 145500\u001b[0m\n",
            "     | > loss: 0.01438  (0.01276)\n",
            "     | > grad_norm: 2.65226  (1.09810)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37806)\n",
            "     | > loader_time: 0.00220  (0.00538)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 140/261 -- GLOBAL_STEP: 145550\u001b[0m\n",
            "     | > loss: 0.00991  (0.01286)\n",
            "     | > grad_norm: 1.14275  (1.14951)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37847)\n",
            "     | > loader_time: 0.00250  (0.00496)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 190/261 -- GLOBAL_STEP: 145600\u001b[0m\n",
            "     | > loss: 0.01396  (0.01271)\n",
            "     | > grad_norm: 1.07601  (1.17151)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37828)\n",
            "     | > loader_time: 0.00360  (0.00487)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 240/261 -- GLOBAL_STEP: 145650\u001b[0m\n",
            "     | > loss: 0.01029  (0.01261)\n",
            "     | > grad_norm: 1.26610  (1.15218)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37832)\n",
            "     | > loader_time: 0.00280  (0.00485)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00917  (0.00917)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01665  (0.01665)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00056 \u001b[0m(-0.00011)\n",
            "     | > avg_loss:\u001b[92m 0.01665 \u001b[0m(-0.00116)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 556/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:31:33) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 28/261 -- GLOBAL_STEP: 145700\u001b[0m\n",
            "     | > loss: 0.01477  (0.01317)\n",
            "     | > grad_norm: 0.84353  (1.12860)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39230  (0.38084)\n",
            "     | > loader_time: 0.00310  (0.00794)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 78/261 -- GLOBAL_STEP: 145750\u001b[0m\n",
            "     | > loss: 0.02477  (0.01344)\n",
            "     | > grad_norm: 1.07876  (1.10159)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37964)\n",
            "     | > loader_time: 0.00330  (0.00482)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 128/261 -- GLOBAL_STEP: 145800\u001b[0m\n",
            "     | > loss: 0.01096  (0.01275)\n",
            "     | > grad_norm: 0.65617  (1.14729)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37932)\n",
            "     | > loader_time: 0.00220  (0.00530)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 178/261 -- GLOBAL_STEP: 145850\u001b[0m\n",
            "     | > loss: 0.00736  (0.01267)\n",
            "     | > grad_norm: 0.85187  (1.09862)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37670  (0.37939)\n",
            "     | > loader_time: 0.00230  (0.00521)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 228/261 -- GLOBAL_STEP: 145900\u001b[0m\n",
            "     | > loss: 0.02545  (0.01251)\n",
            "     | > grad_norm: 2.69963  (1.09420)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37879)\n",
            "     | > loader_time: 0.00440  (0.00484)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00963  (0.00963)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01039  (0.01039)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00068 \u001b[0m(+0.00012)\n",
            "     | > avg_loss:\u001b[92m 0.01039 \u001b[0m(-0.00626)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 557/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:33:22) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/261 -- GLOBAL_STEP: 145950\u001b[0m\n",
            "     | > loss: 0.01024  (0.01210)\n",
            "     | > grad_norm: 1.02276  (1.18673)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.38634)\n",
            "     | > loader_time: 0.00250  (0.01414)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 66/261 -- GLOBAL_STEP: 146000\u001b[0m\n",
            "     | > loss: 0.00663  (0.01227)\n",
            "     | > grad_norm: 1.18370  (1.23181)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38900  (0.38155)\n",
            "     | > loader_time: 0.01010  (0.00723)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 116/261 -- GLOBAL_STEP: 146050\u001b[0m\n",
            "     | > loss: 0.01150  (0.01204)\n",
            "     | > grad_norm: 0.60658  (1.32010)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37650  (0.38042)\n",
            "     | > loader_time: 0.00220  (0.00611)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 166/261 -- GLOBAL_STEP: 146100\u001b[0m\n",
            "     | > loss: 0.01881  (0.01251)\n",
            "     | > grad_norm: 2.18682  (1.29439)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37983)\n",
            "     | > loader_time: 0.00350  (0.00551)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 216/261 -- GLOBAL_STEP: 146150\u001b[0m\n",
            "     | > loss: 0.02830  (0.01263)\n",
            "     | > grad_norm: 1.48572  (1.32618)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37500  (0.37938)\n",
            "     | > loader_time: 0.00230  (0.00524)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01334  (0.01334)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01819  (0.01819)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00057 \u001b[0m(-0.00011)\n",
            "     | > avg_loss:\u001b[91m 0.01819 \u001b[0m(+0.00780)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 558/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:35:10) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 4/261 -- GLOBAL_STEP: 146200\u001b[0m\n",
            "     | > loss: 0.01168  (0.00950)\n",
            "     | > grad_norm: 1.01022  (0.90924)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39860  (0.39139)\n",
            "     | > loader_time: 0.01090  (0.00722)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 54/261 -- GLOBAL_STEP: 146250\u001b[0m\n",
            "     | > loss: 0.01637  (0.01249)\n",
            "     | > grad_norm: 1.07923  (1.32196)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.38043)\n",
            "     | > loader_time: 0.00330  (0.00485)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 104/261 -- GLOBAL_STEP: 146300\u001b[0m\n",
            "     | > loss: 0.01043  (0.01247)\n",
            "     | > grad_norm: 1.34883  (1.10124)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37630  (0.37896)\n",
            "     | > loader_time: 0.00220  (0.00436)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 154/261 -- GLOBAL_STEP: 146350\u001b[0m\n",
            "     | > loss: 0.01413  (0.01261)\n",
            "     | > grad_norm: 0.94011  (1.07506)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37700  (0.37911)\n",
            "     | > loader_time: 0.02070  (0.00446)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 204/261 -- GLOBAL_STEP: 146400\u001b[0m\n",
            "     | > loss: 0.01116  (0.01241)\n",
            "     | > grad_norm: 0.67482  (1.06063)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37904)\n",
            "     | > loader_time: 0.00230  (0.00417)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 254/261 -- GLOBAL_STEP: 146450\u001b[0m\n",
            "     | > loss: 0.01719  (0.01231)\n",
            "     | > grad_norm: 1.21022  (1.06248)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37907)\n",
            "     | > loader_time: 0.00210  (0.00429)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.02946  (0.02946)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01433  (0.01433)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00055 \u001b[0m(-0.00002)\n",
            "     | > avg_loss:\u001b[92m 0.01433 \u001b[0m(-0.00386)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 559/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:36:58) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 42/261 -- GLOBAL_STEP: 146500\u001b[0m\n",
            "     | > loss: 0.00890  (0.01187)\n",
            "     | > grad_norm: 0.88502  (0.99366)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37974)\n",
            "     | > loader_time: 0.00220  (0.00500)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 92/261 -- GLOBAL_STEP: 146550\u001b[0m\n",
            "     | > loss: 0.01258  (0.01241)\n",
            "     | > grad_norm: 1.27008  (1.11320)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37891)\n",
            "     | > loader_time: 0.00230  (0.00478)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 142/261 -- GLOBAL_STEP: 146600\u001b[0m\n",
            "     | > loss: 0.00870  (0.01226)\n",
            "     | > grad_norm: 0.56275  (1.08283)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38350  (0.37898)\n",
            "     | > loader_time: 0.00610  (0.00441)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 192/261 -- GLOBAL_STEP: 146650\u001b[0m\n",
            "     | > loss: 0.00549  (0.01232)\n",
            "     | > grad_norm: 1.28288  (1.08150)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37892)\n",
            "     | > loader_time: 0.00220  (0.00414)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 242/261 -- GLOBAL_STEP: 146700\u001b[0m\n",
            "     | > loss: 0.01291  (0.01241)\n",
            "     | > grad_norm: 1.08794  (1.13815)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37868)\n",
            "     | > loader_time: 0.00260  (0.00407)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01120  (0.01120)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01179  (0.01179)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00062 \u001b[0m(+0.00007)\n",
            "     | > avg_loss:\u001b[92m 0.01179 \u001b[0m(-0.00254)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 560/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:38:46) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 30/261 -- GLOBAL_STEP: 146750\u001b[0m\n",
            "     | > loss: 0.01365  (0.01377)\n",
            "     | > grad_norm: 1.30155  (1.48091)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37610  (0.38033)\n",
            "     | > loader_time: 0.00250  (0.00518)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/261 -- GLOBAL_STEP: 146800\u001b[0m\n",
            "     | > loss: 0.01865  (0.01313)\n",
            "     | > grad_norm: 3.21097  (1.30764)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37908)\n",
            "     | > loader_time: 0.00220  (0.00503)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/261 -- GLOBAL_STEP: 146850\u001b[0m\n",
            "     | > loss: 0.00836  (0.01300)\n",
            "     | > grad_norm: 0.41804  (1.31289)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37710  (0.37865)\n",
            "     | > loader_time: 0.00380  (0.00458)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 180/261 -- GLOBAL_STEP: 146900\u001b[0m\n",
            "     | > loss: 0.01355  (0.01306)\n",
            "     | > grad_norm: 1.40763  (1.25804)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.37847)\n",
            "     | > loader_time: 0.00270  (0.00454)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/261 -- GLOBAL_STEP: 146950\u001b[0m\n",
            "     | > loss: 0.01277  (0.01305)\n",
            "     | > grad_norm: 0.66830  (1.24154)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37845)\n",
            "     | > loader_time: 0.00340  (0.00449)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00761  (0.00761)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01128  (0.01128)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00067 \u001b[0m(+0.00005)\n",
            "     | > avg_loss:\u001b[92m 0.01128 \u001b[0m(-0.00051)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 561/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:40:35) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 18/261 -- GLOBAL_STEP: 147000\u001b[0m\n",
            "     | > loss: 0.01982  (0.01265)\n",
            "     | > grad_norm: 0.92858  (1.09367)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.38330)\n",
            "     | > loader_time: 0.00190  (0.00722)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 68/261 -- GLOBAL_STEP: 147050\u001b[0m\n",
            "     | > loss: 0.01560  (0.01249)\n",
            "     | > grad_norm: 0.85133  (1.29412)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37470  (0.37946)\n",
            "     | > loader_time: 0.00230  (0.00541)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 118/261 -- GLOBAL_STEP: 147100\u001b[0m\n",
            "     | > loss: 0.00965  (0.01249)\n",
            "     | > grad_norm: 0.92629  (1.20625)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37620  (0.37878)\n",
            "     | > loader_time: 0.00240  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 168/261 -- GLOBAL_STEP: 147150\u001b[0m\n",
            "     | > loss: 0.01634  (0.01230)\n",
            "     | > grad_norm: 2.02299  (1.26762)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37490  (0.37837)\n",
            "     | > loader_time: 0.00330  (0.00441)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 218/261 -- GLOBAL_STEP: 147200\u001b[0m\n",
            "     | > loss: 0.01343  (0.01265)\n",
            "     | > grad_norm: 1.09452  (1.27403)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37830)\n",
            "     | > loader_time: 0.00370  (0.00427)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01131  (0.01131)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01145  (0.01145)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00063 \u001b[0m(-0.00004)\n",
            "     | > avg_loss:\u001b[91m 0.01145 \u001b[0m(+0.00017)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 562/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:42:23) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 6/261 -- GLOBAL_STEP: 147250\u001b[0m\n",
            "     | > loss: 0.01001  (0.01380)\n",
            "     | > grad_norm: 0.76862  (0.93903)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39070  (0.39297)\n",
            "     | > loader_time: 0.03670  (0.01720)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 56/261 -- GLOBAL_STEP: 147300\u001b[0m\n",
            "     | > loss: 0.01362  (0.01410)\n",
            "     | > grad_norm: 1.33913  (1.01010)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38520  (0.38019)\n",
            "     | > loader_time: 0.00330  (0.00469)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 106/261 -- GLOBAL_STEP: 147350\u001b[0m\n",
            "     | > loss: 0.00898  (0.01381)\n",
            "     | > grad_norm: 0.87276  (1.20162)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37915)\n",
            "     | > loader_time: 0.00280  (0.00461)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 156/261 -- GLOBAL_STEP: 147400\u001b[0m\n",
            "     | > loss: 0.01621  (0.01329)\n",
            "     | > grad_norm: 3.30865  (1.24324)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37913)\n",
            "     | > loader_time: 0.00220  (0.00419)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 206/261 -- GLOBAL_STEP: 147450\u001b[0m\n",
            "     | > loss: 0.01500  (0.01306)\n",
            "     | > grad_norm: 1.69621  (1.25408)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38530  (0.37897)\n",
            "     | > loader_time: 0.00300  (0.00412)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 256/261 -- GLOBAL_STEP: 147500\u001b[0m\n",
            "     | > loss: 0.01161  (0.01306)\n",
            "     | > grad_norm: 1.54335  (1.26136)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37450  (0.37908)\n",
            "     | > loader_time: 0.00180  (0.00405)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01916  (0.01916)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01372  (0.01372)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00071 \u001b[0m(+0.00008)\n",
            "     | > avg_loss:\u001b[91m 0.01372 \u001b[0m(+0.00227)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 563/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:44:11) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 44/261 -- GLOBAL_STEP: 147550\u001b[0m\n",
            "     | > loss: 0.01527  (0.01194)\n",
            "     | > grad_norm: 0.74658  (0.97506)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39380  (0.38169)\n",
            "     | > loader_time: 0.00230  (0.00534)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 94/261 -- GLOBAL_STEP: 147600\u001b[0m\n",
            "     | > loss: 0.00812  (0.01217)\n",
            "     | > grad_norm: 1.14321  (1.01481)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37650  (0.38052)\n",
            "     | > loader_time: 0.01030  (0.00454)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 144/261 -- GLOBAL_STEP: 147650\u001b[0m\n",
            "     | > loss: 0.00920  (0.01217)\n",
            "     | > grad_norm: 1.06730  (1.09626)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38300  (0.38030)\n",
            "     | > loader_time: 0.00240  (0.00449)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 194/261 -- GLOBAL_STEP: 147700\u001b[0m\n",
            "     | > loss: 0.01519  (0.01223)\n",
            "     | > grad_norm: 2.00764  (1.10782)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37510  (0.37978)\n",
            "     | > loader_time: 0.00220  (0.00439)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 244/261 -- GLOBAL_STEP: 147750\u001b[0m\n",
            "     | > loss: 0.01177  (0.01216)\n",
            "     | > grad_norm: 1.10537  (1.13818)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37915)\n",
            "     | > loader_time: 0.00220  (0.00413)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01018  (0.01018)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00858  (0.00858)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00052 \u001b[0m(-0.00019)\n",
            "     | > avg_loss:\u001b[92m 0.00858 \u001b[0m(-0.00514)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 564/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:45:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 32/261 -- GLOBAL_STEP: 147800\u001b[0m\n",
            "     | > loss: 0.01566  (0.01180)\n",
            "     | > grad_norm: 0.96785  (1.08779)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.38138)\n",
            "     | > loader_time: 0.00220  (0.00799)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 82/261 -- GLOBAL_STEP: 147850\u001b[0m\n",
            "     | > loss: 0.01109  (0.01172)\n",
            "     | > grad_norm: 1.01965  (1.09046)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.38041)\n",
            "     | > loader_time: 0.00210  (0.00619)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 132/261 -- GLOBAL_STEP: 147900\u001b[0m\n",
            "     | > loss: 0.01117  (0.01187)\n",
            "     | > grad_norm: 1.07900  (1.11338)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37941)\n",
            "     | > loader_time: 0.00220  (0.00509)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 182/261 -- GLOBAL_STEP: 147950\u001b[0m\n",
            "     | > loss: 0.01095  (0.01191)\n",
            "     | > grad_norm: 1.47179  (1.14928)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37936)\n",
            "     | > loader_time: 0.00210  (0.00531)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 232/261 -- GLOBAL_STEP: 148000\u001b[0m\n",
            "     | > loss: 0.00616  (0.01228)\n",
            "     | > grad_norm: 0.45355  (1.15173)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37860  (0.37921)\n",
            "     | > loader_time: 0.02220  (0.00495)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01198  (0.01198)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00680  (0.00680)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00069 \u001b[0m(+0.00017)\n",
            "     | > avg_loss:\u001b[92m 0.00680 \u001b[0m(-0.00178)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 565/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:47:47) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/261 -- GLOBAL_STEP: 148050\u001b[0m\n",
            "     | > loss: 0.01413  (0.01158)\n",
            "     | > grad_norm: 2.68165  (1.10843)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.38397)\n",
            "     | > loader_time: 0.00360  (0.00706)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/261 -- GLOBAL_STEP: 148100\u001b[0m\n",
            "     | > loss: 0.00964  (0.01255)\n",
            "     | > grad_norm: 1.80831  (1.34545)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37999)\n",
            "     | > loader_time: 0.02010  (0.00487)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/261 -- GLOBAL_STEP: 148150\u001b[0m\n",
            "     | > loss: 0.00713  (0.01250)\n",
            "     | > grad_norm: 0.73574  (1.33826)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37880  (0.37966)\n",
            "     | > loader_time: 0.00290  (0.00429)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 170/261 -- GLOBAL_STEP: 148200\u001b[0m\n",
            "     | > loss: 0.02385  (0.01261)\n",
            "     | > grad_norm: 1.12919  (1.37913)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37903)\n",
            "     | > loader_time: 0.00210  (0.00416)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 220/261 -- GLOBAL_STEP: 148250\u001b[0m\n",
            "     | > loss: 0.00833  (0.01288)\n",
            "     | > grad_norm: 1.09654  (1.35558)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38110  (0.37880)\n",
            "     | > loader_time: 0.01820  (0.00412)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01582  (0.01582)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01351  (0.01351)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00065 \u001b[0m(-0.00004)\n",
            "     | > avg_loss:\u001b[91m 0.01351 \u001b[0m(+0.00671)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 566/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:49:35) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 8/261 -- GLOBAL_STEP: 148300\u001b[0m\n",
            "     | > loss: 0.01552  (0.01072)\n",
            "     | > grad_norm: 1.04969  (1.14246)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37630  (0.38910)\n",
            "     | > loader_time: 0.00960  (0.01060)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 58/261 -- GLOBAL_STEP: 148350\u001b[0m\n",
            "     | > loss: 0.00889  (0.01146)\n",
            "     | > grad_norm: 0.48168  (1.09446)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38500  (0.37982)\n",
            "     | > loader_time: 0.00290  (0.00486)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 108/261 -- GLOBAL_STEP: 148400\u001b[0m\n",
            "     | > loss: 0.01358  (0.01190)\n",
            "     | > grad_norm: 1.00262  (1.19099)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37839)\n",
            "     | > loader_time: 0.00330  (0.00445)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 158/261 -- GLOBAL_STEP: 148450\u001b[0m\n",
            "     | > loss: 0.01228  (0.01219)\n",
            "     | > grad_norm: 1.35857  (1.23437)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39020  (0.37850)\n",
            "     | > loader_time: 0.00280  (0.00394)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 208/261 -- GLOBAL_STEP: 148500\u001b[0m\n",
            "     | > loss: 0.01483  (0.01242)\n",
            "     | > grad_norm: 2.07255  (1.22596)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37851)\n",
            "     | > loader_time: 0.00350  (0.00386)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 258/261 -- GLOBAL_STEP: 148550\u001b[0m\n",
            "     | > loss: 0.00886  (0.01220)\n",
            "     | > grad_norm: 2.67149  (1.19001)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37470  (0.37812)\n",
            "     | > loader_time: 0.00170  (0.00381)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01040  (0.01040)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00804  (0.00804)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00069 \u001b[0m(+0.00004)\n",
            "     | > avg_loss:\u001b[92m 0.00804 \u001b[0m(-0.00547)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 567/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:51:23) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 46/261 -- GLOBAL_STEP: 148600\u001b[0m\n",
            "     | > loss: 0.02357  (0.01291)\n",
            "     | > grad_norm: 1.05905  (1.50671)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.37955)\n",
            "     | > loader_time: 0.00280  (0.00386)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 96/261 -- GLOBAL_STEP: 148650\u001b[0m\n",
            "     | > loss: 0.01118  (0.01229)\n",
            "     | > grad_norm: 1.89373  (1.27289)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37902)\n",
            "     | > loader_time: 0.00350  (0.00396)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 146/261 -- GLOBAL_STEP: 148700\u001b[0m\n",
            "     | > loss: 0.01390  (0.01222)\n",
            "     | > grad_norm: 0.98948  (1.20104)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39500  (0.37857)\n",
            "     | > loader_time: 0.00320  (0.00437)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 196/261 -- GLOBAL_STEP: 148750\u001b[0m\n",
            "     | > loss: 0.00631  (0.01230)\n",
            "     | > grad_norm: 0.70556  (1.20316)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38450  (0.37879)\n",
            "     | > loader_time: 0.00710  (0.00434)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 246/261 -- GLOBAL_STEP: 148800\u001b[0m\n",
            "     | > loss: 0.00644  (0.01237)\n",
            "     | > grad_norm: 1.10789  (1.21691)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37867)\n",
            "     | > loader_time: 0.00230  (0.00434)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01558  (0.01558)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01883  (0.01883)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00055 \u001b[0m(-0.00014)\n",
            "     | > avg_loss:\u001b[91m 0.01883 \u001b[0m(+0.01079)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 568/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:53:11) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 34/261 -- GLOBAL_STEP: 148850\u001b[0m\n",
            "     | > loss: 0.01552  (0.01295)\n",
            "     | > grad_norm: 0.75056  (1.06147)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.38327)\n",
            "     | > loader_time: 0.00220  (0.00620)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 84/261 -- GLOBAL_STEP: 148900\u001b[0m\n",
            "     | > loss: 0.01209  (0.01273)\n",
            "     | > grad_norm: 0.91345  (1.10938)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37984)\n",
            "     | > loader_time: 0.00340  (0.00479)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 134/261 -- GLOBAL_STEP: 148950\u001b[0m\n",
            "     | > loss: 0.01046  (0.01285)\n",
            "     | > grad_norm: 1.15164  (1.21741)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37936)\n",
            "     | > loader_time: 0.00330  (0.00461)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 184/261 -- GLOBAL_STEP: 149000\u001b[0m\n",
            "     | > loss: 0.01998  (0.01250)\n",
            "     | > grad_norm: 0.53337  (1.21144)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38140  (0.37911)\n",
            "     | > loader_time: 0.00240  (0.00455)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 234/261 -- GLOBAL_STEP: 149050\u001b[0m\n",
            "     | > loss: 0.01388  (0.01251)\n",
            "     | > grad_norm: 1.86887  (1.18334)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37879)\n",
            "     | > loader_time: 0.00250  (0.00431)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01369  (0.01369)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00930  (0.00930)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00068 \u001b[0m(+0.00012)\n",
            "     | > avg_loss:\u001b[92m 0.00930 \u001b[0m(-0.00953)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 569/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:54:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/261 -- GLOBAL_STEP: 149100\u001b[0m\n",
            "     | > loss: 0.00860  (0.01352)\n",
            "     | > grad_norm: 0.76505  (1.54530)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.38147)\n",
            "     | > loader_time: 0.00240  (0.00796)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 72/261 -- GLOBAL_STEP: 149150\u001b[0m\n",
            "     | > loss: 0.01052  (0.01243)\n",
            "     | > grad_norm: 1.67094  (1.46951)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37836)\n",
            "     | > loader_time: 0.00230  (0.00472)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 122/261 -- GLOBAL_STEP: 149200\u001b[0m\n",
            "     | > loss: 0.01865  (0.01234)\n",
            "     | > grad_norm: 3.35929  (1.35156)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37807)\n",
            "     | > loader_time: 0.00230  (0.00467)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 172/261 -- GLOBAL_STEP: 149250\u001b[0m\n",
            "     | > loss: 0.01107  (0.01236)\n",
            "     | > grad_norm: 0.65132  (1.30213)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37805)\n",
            "     | > loader_time: 0.00220  (0.00435)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 222/261 -- GLOBAL_STEP: 149300\u001b[0m\n",
            "     | > loss: 0.00898  (0.01250)\n",
            "     | > grad_norm: 0.74289  (1.25084)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37620  (0.37799)\n",
            "     | > loader_time: 0.00270  (0.00420)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01663  (0.01663)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01114  (0.01114)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00064 \u001b[0m(-0.00004)\n",
            "     | > avg_loss:\u001b[91m 0.01114 \u001b[0m(+0.00185)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 570/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:56:48) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/261 -- GLOBAL_STEP: 149350\u001b[0m\n",
            "     | > loss: 0.02362  (0.01322)\n",
            "     | > grad_norm: 2.21517  (1.06485)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37610  (0.38818)\n",
            "     | > loader_time: 0.00080  (0.01411)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/261 -- GLOBAL_STEP: 149400\u001b[0m\n",
            "     | > loss: 0.01237  (0.01215)\n",
            "     | > grad_norm: 1.07807  (1.44131)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.38007)\n",
            "     | > loader_time: 0.00230  (0.00677)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 110/261 -- GLOBAL_STEP: 149450\u001b[0m\n",
            "     | > loss: 0.00881  (0.01201)\n",
            "     | > grad_norm: 1.38807  (1.23457)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37972)\n",
            "     | > loader_time: 0.00240  (0.00566)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 160/261 -- GLOBAL_STEP: 149500\u001b[0m\n",
            "     | > loss: 0.01494  (0.01175)\n",
            "     | > grad_norm: 1.14256  (1.14586)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37973)\n",
            "     | > loader_time: 0.00330  (0.00498)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 210/261 -- GLOBAL_STEP: 149550\u001b[0m\n",
            "     | > loss: 0.02616  (0.01185)\n",
            "     | > grad_norm: 0.43581  (1.14333)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39390  (0.37971)\n",
            "     | > loader_time: 0.00310  (0.00480)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 260/261 -- GLOBAL_STEP: 149600\u001b[0m\n",
            "     | > loss: 0.01506  (0.01203)\n",
            "     | > grad_norm: 0.77311  (1.11438)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37960)\n",
            "     | > loader_time: 0.00220  (0.00464)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01241  (0.01241)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01132  (0.01132)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00065 \u001b[0m(+0.00001)\n",
            "     | > avg_loss:\u001b[91m 0.01132 \u001b[0m(+0.00017)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 571/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 01:58:36) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 48/261 -- GLOBAL_STEP: 149650\u001b[0m\n",
            "     | > loss: 0.01121  (0.01281)\n",
            "     | > grad_norm: 1.62991  (1.35962)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.38079)\n",
            "     | > loader_time: 0.00230  (0.00578)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 98/261 -- GLOBAL_STEP: 149700\u001b[0m\n",
            "     | > loss: 0.00741  (0.01239)\n",
            "     | > grad_norm: 1.14341  (1.35455)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37960)\n",
            "     | > loader_time: 0.00330  (0.00478)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 148/261 -- GLOBAL_STEP: 149750\u001b[0m\n",
            "     | > loss: 0.01067  (0.01232)\n",
            "     | > grad_norm: 0.66470  (1.30597)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37896)\n",
            "     | > loader_time: 0.00320  (0.00452)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 198/261 -- GLOBAL_STEP: 149800\u001b[0m\n",
            "     | > loss: 0.01788  (0.01246)\n",
            "     | > grad_norm: 2.00792  (1.31515)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37610  (0.37924)\n",
            "     | > loader_time: 0.00220  (0.00429)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 248/261 -- GLOBAL_STEP: 149850\u001b[0m\n",
            "     | > loss: 0.01145  (0.01270)\n",
            "     | > grad_norm: 1.02235  (1.27869)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37740  (0.37903)\n",
            "     | > loader_time: 0.02060  (0.00429)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01131  (0.01131)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01766  (0.01766)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00057 \u001b[0m(-0.00008)\n",
            "     | > avg_loss:\u001b[91m 0.01766 \u001b[0m(+0.00635)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 572/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:00:24) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 36/261 -- GLOBAL_STEP: 149900\u001b[0m\n",
            "     | > loss: 0.01884  (0.01333)\n",
            "     | > grad_norm: 1.28945  (1.28320)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38290  (0.38061)\n",
            "     | > loader_time: 0.00330  (0.00791)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 86/261 -- GLOBAL_STEP: 149950\u001b[0m\n",
            "     | > loss: 0.00971  (0.01281)\n",
            "     | > grad_norm: 0.91527  (1.24221)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38370  (0.38009)\n",
            "     | > loader_time: 0.00300  (0.00513)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 136/261 -- GLOBAL_STEP: 150000\u001b[0m\n",
            "     | > loss: 0.01485  (0.01251)\n",
            "     | > grad_norm: 2.02735  (1.26198)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37943)\n",
            "     | > loader_time: 0.00340  (0.00472)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2/checkpoint_150000.pth.tar\n",
            "\n",
            "\u001b[1m   --> STEP: 186/261 -- GLOBAL_STEP: 150050\u001b[0m\n",
            "     | > loss: 0.00992  (0.01278)\n",
            "     | > grad_norm: 0.87122  (1.24842)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37610  (0.37962)\n",
            "     | > loader_time: 0.00230  (0.00456)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 236/261 -- GLOBAL_STEP: 150100\u001b[0m\n",
            "     | > loss: 0.00712  (0.01277)\n",
            "     | > grad_norm: 1.11081  (1.22712)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37933)\n",
            "     | > loader_time: 0.01320  (0.00476)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01234  (0.01234)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01760  (0.01760)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00063 \u001b[0m(+0.00006)\n",
            "     | > avg_loss:\u001b[92m 0.01760 \u001b[0m(-0.00007)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 573/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:02:13) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 24/261 -- GLOBAL_STEP: 150150\u001b[0m\n",
            "     | > loss: 0.01326  (0.01229)\n",
            "     | > grad_norm: 1.13446  (1.16416)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39050  (0.37881)\n",
            "     | > loader_time: 0.00390  (0.00659)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 74/261 -- GLOBAL_STEP: 150200\u001b[0m\n",
            "     | > loss: 0.01094  (0.01262)\n",
            "     | > grad_norm: 0.77688  (1.09014)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38780  (0.37853)\n",
            "     | > loader_time: 0.00300  (0.00494)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 124/261 -- GLOBAL_STEP: 150250\u001b[0m\n",
            "     | > loss: 0.00974  (0.01238)\n",
            "     | > grad_norm: 0.84984  (1.06661)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37878)\n",
            "     | > loader_time: 0.00340  (0.00476)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 174/261 -- GLOBAL_STEP: 150300\u001b[0m\n",
            "     | > loss: 0.00629  (0.01274)\n",
            "     | > grad_norm: 1.03530  (1.14928)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37842)\n",
            "     | > loader_time: 0.00210  (0.00439)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 224/261 -- GLOBAL_STEP: 150350\u001b[0m\n",
            "     | > loss: 0.00859  (0.01260)\n",
            "     | > grad_norm: 1.06188  (1.16738)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37847)\n",
            "     | > loader_time: 0.00240  (0.00448)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01248  (0.01248)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01373  (0.01373)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00056 \u001b[0m(-0.00007)\n",
            "     | > avg_loss:\u001b[92m 0.01373 \u001b[0m(-0.00387)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 574/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:04:01) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 12/261 -- GLOBAL_STEP: 150400\u001b[0m\n",
            "     | > loss: 0.01036  (0.01170)\n",
            "     | > grad_norm: 0.65422  (1.09909)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.39078)\n",
            "     | > loader_time: 0.00220  (0.01414)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 62/261 -- GLOBAL_STEP: 150450\u001b[0m\n",
            "     | > loss: 0.00955  (0.01267)\n",
            "     | > grad_norm: 1.17694  (1.31964)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.38169)\n",
            "     | > loader_time: 0.00220  (0.00667)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 112/261 -- GLOBAL_STEP: 150500\u001b[0m\n",
            "     | > loss: 0.00912  (0.01277)\n",
            "     | > grad_norm: 0.98324  (1.16192)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.38008)\n",
            "     | > loader_time: 0.00250  (0.00576)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 162/261 -- GLOBAL_STEP: 150550\u001b[0m\n",
            "     | > loss: 0.01868  (0.01262)\n",
            "     | > grad_norm: 0.90833  (1.17631)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37982)\n",
            "     | > loader_time: 0.00320  (0.00559)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 212/261 -- GLOBAL_STEP: 150600\u001b[0m\n",
            "     | > loss: 0.01159  (0.01311)\n",
            "     | > grad_norm: 0.87967  (1.26070)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39980  (0.37999)\n",
            "     | > loader_time: 0.00470  (0.00520)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01441  (0.01441)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00829  (0.00829)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01015 \u001b[0m(+0.00960)\n",
            "     | > avg_loss:\u001b[92m 0.00829 \u001b[0m(-0.00544)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 575/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:05:50) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/261 -- GLOBAL_STEP: 150650\u001b[0m\n",
            "     | > loss: 0.01613  (0.01613)\n",
            "     | > grad_norm: 1.14974  (1.14974)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.42320  (0.42319)\n",
            "     | > loader_time: 1.43980  (1.43978)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/261 -- GLOBAL_STEP: 150700\u001b[0m\n",
            "     | > loss: 0.01576  (0.01232)\n",
            "     | > grad_norm: 1.44558  (1.14086)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39670  (0.38084)\n",
            "     | > loader_time: 0.01160  (0.00415)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/261 -- GLOBAL_STEP: 150750\u001b[0m\n",
            "     | > loss: 0.01354  (0.01258)\n",
            "     | > grad_norm: 1.24756  (1.12258)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37640  (0.38029)\n",
            "     | > loader_time: 0.00390  (0.00476)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 150/261 -- GLOBAL_STEP: 150800\u001b[0m\n",
            "     | > loss: 0.00769  (0.01309)\n",
            "     | > grad_norm: 0.31339  (1.18495)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38530  (0.37970)\n",
            "     | > loader_time: 0.00320  (0.00494)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 200/261 -- GLOBAL_STEP: 150850\u001b[0m\n",
            "     | > loss: 0.01000  (0.01265)\n",
            "     | > grad_norm: 0.75453  (1.14681)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37935)\n",
            "     | > loader_time: 0.00230  (0.00482)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 250/261 -- GLOBAL_STEP: 150900\u001b[0m\n",
            "     | > loss: 0.01073  (0.01235)\n",
            "     | > grad_norm: 0.93179  (1.14883)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38250  (0.37927)\n",
            "     | > loader_time: 0.00260  (0.00460)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01998  (0.01998)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01438  (0.01438)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00068 \u001b[0m(-0.00947)\n",
            "     | > avg_loss:\u001b[91m 0.01438 \u001b[0m(+0.00609)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 576/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:07:38) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 38/261 -- GLOBAL_STEP: 150950\u001b[0m\n",
            "     | > loss: 0.01306  (0.01299)\n",
            "     | > grad_norm: 0.97996  (1.34878)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.38004)\n",
            "     | > loader_time: 0.00240  (0.00732)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/261 -- GLOBAL_STEP: 151000\u001b[0m\n",
            "     | > loss: 0.00915  (0.01231)\n",
            "     | > grad_norm: 1.09080  (1.24939)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37934)\n",
            "     | > loader_time: 0.01190  (0.00529)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 138/261 -- GLOBAL_STEP: 151050\u001b[0m\n",
            "     | > loss: 0.01174  (0.01244)\n",
            "     | > grad_norm: 0.94433  (1.34776)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37932)\n",
            "     | > loader_time: 0.01650  (0.00478)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 188/261 -- GLOBAL_STEP: 151100\u001b[0m\n",
            "     | > loss: 0.00691  (0.01277)\n",
            "     | > grad_norm: 0.57845  (1.26754)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37904)\n",
            "     | > loader_time: 0.00240  (0.00438)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 238/261 -- GLOBAL_STEP: 151150\u001b[0m\n",
            "     | > loss: 0.00937  (0.01269)\n",
            "     | > grad_norm: 0.71965  (1.25094)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37902)\n",
            "     | > loader_time: 0.00350  (0.00461)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00914  (0.00914)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.03384  (0.03384)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00071 \u001b[0m(+0.00003)\n",
            "     | > avg_loss:\u001b[91m 0.03384 \u001b[0m(+0.01946)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 577/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:09:27) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 26/261 -- GLOBAL_STEP: 151200\u001b[0m\n",
            "     | > loss: 0.00852  (0.01152)\n",
            "     | > grad_norm: 1.93450  (1.10179)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.38611)\n",
            "     | > loader_time: 0.00230  (0.00666)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 76/261 -- GLOBAL_STEP: 151250\u001b[0m\n",
            "     | > loss: 0.01291  (0.01197)\n",
            "     | > grad_norm: 1.13308  (1.20888)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.38086)\n",
            "     | > loader_time: 0.00210  (0.00442)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 126/261 -- GLOBAL_STEP: 151300\u001b[0m\n",
            "     | > loss: 0.02541  (0.01252)\n",
            "     | > grad_norm: 1.73987  (1.27316)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38730  (0.37949)\n",
            "     | > loader_time: 0.00920  (0.00448)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 176/261 -- GLOBAL_STEP: 151350\u001b[0m\n",
            "     | > loss: 0.02716  (0.01259)\n",
            "     | > grad_norm: 1.09920  (1.30602)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37942)\n",
            "     | > loader_time: 0.00240  (0.00416)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 226/261 -- GLOBAL_STEP: 151400\u001b[0m\n",
            "     | > loss: 0.02895  (0.01258)\n",
            "     | > grad_norm: 1.39556  (1.28436)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37913)\n",
            "     | > loader_time: 0.00220  (0.00435)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00918  (0.00918)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01190  (0.01190)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00068 \u001b[0m(-0.00004)\n",
            "     | > avg_loss:\u001b[92m 0.01190 \u001b[0m(-0.02194)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 578/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:11:15) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 14/261 -- GLOBAL_STEP: 151450\u001b[0m\n",
            "     | > loss: 0.01215  (0.01227)\n",
            "     | > grad_norm: 0.57932  (1.17525)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38210  (0.38198)\n",
            "     | > loader_time: 0.02140  (0.01223)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 64/261 -- GLOBAL_STEP: 151500\u001b[0m\n",
            "     | > loss: 0.01303  (0.01397)\n",
            "     | > grad_norm: 1.06266  (1.11316)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39320  (0.37934)\n",
            "     | > loader_time: 0.00310  (0.00567)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 114/261 -- GLOBAL_STEP: 151550\u001b[0m\n",
            "     | > loss: 0.01187  (0.01320)\n",
            "     | > grad_norm: 1.08380  (1.26390)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.37891)\n",
            "     | > loader_time: 0.00350  (0.00464)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 164/261 -- GLOBAL_STEP: 151600\u001b[0m\n",
            "     | > loss: 0.01541  (0.01305)\n",
            "     | > grad_norm: 0.34421  (1.22399)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37903)\n",
            "     | > loader_time: 0.00330  (0.00444)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 214/261 -- GLOBAL_STEP: 151650\u001b[0m\n",
            "     | > loss: 0.01011  (0.01285)\n",
            "     | > grad_norm: 1.13452  (1.19290)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39110  (0.37912)\n",
            "     | > loader_time: 0.00270  (0.00439)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01625  (0.01625)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01174  (0.01174)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00069 \u001b[0m(+0.00001)\n",
            "     | > avg_loss:\u001b[92m 0.01174 \u001b[0m(-0.00016)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 579/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:13:03) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/261 -- GLOBAL_STEP: 151700\u001b[0m\n",
            "     | > loss: 0.02034  (0.01495)\n",
            "     | > grad_norm: 1.09804  (1.26831)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37490  (0.37903)\n",
            "     | > loader_time: 0.00960  (0.00707)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 52/261 -- GLOBAL_STEP: 151750\u001b[0m\n",
            "     | > loss: 0.02023  (0.01215)\n",
            "     | > grad_norm: 1.12802  (0.93071)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37936)\n",
            "     | > loader_time: 0.00220  (0.00492)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 102/261 -- GLOBAL_STEP: 151800\u001b[0m\n",
            "     | > loss: 0.00841  (0.01190)\n",
            "     | > grad_norm: 0.80158  (1.09582)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37868)\n",
            "     | > loader_time: 0.00240  (0.00389)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 152/261 -- GLOBAL_STEP: 151850\u001b[0m\n",
            "     | > loss: 0.01058  (0.01194)\n",
            "     | > grad_norm: 1.19089  (1.08368)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37828)\n",
            "     | > loader_time: 0.00260  (0.00388)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 202/261 -- GLOBAL_STEP: 151900\u001b[0m\n",
            "     | > loss: 0.01121  (0.01189)\n",
            "     | > grad_norm: 1.49801  (1.16087)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.37817)\n",
            "     | > loader_time: 0.00220  (0.00373)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 252/261 -- GLOBAL_STEP: 151950\u001b[0m\n",
            "     | > loss: 0.02105  (0.01201)\n",
            "     | > grad_norm: 0.58086  (1.16136)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39200  (0.37820)\n",
            "     | > loader_time: 0.00290  (0.00379)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01344  (0.01344)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.02190  (0.02190)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00066 \u001b[0m(-0.00003)\n",
            "     | > avg_loss:\u001b[91m 0.02190 \u001b[0m(+0.01015)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 580/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:14:51) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 40/261 -- GLOBAL_STEP: 152000\u001b[0m\n",
            "     | > loss: 0.01139  (0.01362)\n",
            "     | > grad_norm: 1.33891  (1.24104)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.40010  (0.38321)\n",
            "     | > loader_time: 0.00580  (0.00512)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/261 -- GLOBAL_STEP: 152050\u001b[0m\n",
            "     | > loss: 0.01645  (0.01280)\n",
            "     | > grad_norm: 1.03849  (1.21747)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.38072)\n",
            "     | > loader_time: 0.00320  (0.00435)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 140/261 -- GLOBAL_STEP: 152100\u001b[0m\n",
            "     | > loss: 0.01585  (0.01263)\n",
            "     | > grad_norm: 3.26721  (1.27606)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37999)\n",
            "     | > loader_time: 0.00230  (0.00418)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 190/261 -- GLOBAL_STEP: 152150\u001b[0m\n",
            "     | > loss: 0.02076  (0.01226)\n",
            "     | > grad_norm: 1.50314  (1.24642)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37510  (0.37933)\n",
            "     | > loader_time: 0.00220  (0.00401)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 240/261 -- GLOBAL_STEP: 152200\u001b[0m\n",
            "     | > loss: 0.01101  (0.01226)\n",
            "     | > grad_norm: 0.82600  (1.25537)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38620  (0.37909)\n",
            "     | > loader_time: 0.00300  (0.00402)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01903  (0.01903)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00903  (0.00903)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00064 \u001b[0m(-0.00002)\n",
            "     | > avg_loss:\u001b[92m 0.00903 \u001b[0m(-0.01287)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 581/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:16:39) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 28/261 -- GLOBAL_STEP: 152250\u001b[0m\n",
            "     | > loss: 0.01007  (0.01271)\n",
            "     | > grad_norm: 0.68061  (1.06779)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.38379)\n",
            "     | > loader_time: 0.00220  (0.00573)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 78/261 -- GLOBAL_STEP: 152300\u001b[0m\n",
            "     | > loss: 0.00923  (0.01357)\n",
            "     | > grad_norm: 1.39170  (1.17140)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37510  (0.38036)\n",
            "     | > loader_time: 0.00330  (0.00513)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 128/261 -- GLOBAL_STEP: 152350\u001b[0m\n",
            "     | > loss: 0.00849  (0.01311)\n",
            "     | > grad_norm: 1.48013  (1.30885)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37979)\n",
            "     | > loader_time: 0.00230  (0.00465)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 178/261 -- GLOBAL_STEP: 152400\u001b[0m\n",
            "     | > loss: 0.01321  (0.01336)\n",
            "     | > grad_norm: 1.54025  (1.29388)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38800  (0.37953)\n",
            "     | > loader_time: 0.00280  (0.00431)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 228/261 -- GLOBAL_STEP: 152450\u001b[0m\n",
            "     | > loss: 0.01291  (0.01304)\n",
            "     | > grad_norm: 0.99335  (1.26777)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37927)\n",
            "     | > loader_time: 0.00210  (0.00424)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01538  (0.01538)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00740  (0.00740)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00065 \u001b[0m(+0.00002)\n",
            "     | > avg_loss:\u001b[92m 0.00740 \u001b[0m(-0.00163)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 582/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:18:27) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/261 -- GLOBAL_STEP: 152500\u001b[0m\n",
            "     | > loss: 0.00800  (0.01194)\n",
            "     | > grad_norm: 1.55607  (1.25636)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37490  (0.38288)\n",
            "     | > loader_time: 0.00220  (0.00892)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 66/261 -- GLOBAL_STEP: 152550\u001b[0m\n",
            "     | > loss: 0.01489  (0.01211)\n",
            "     | > grad_norm: 0.83360  (1.16211)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37960  (0.38028)\n",
            "     | > loader_time: 0.00550  (0.00543)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 116/261 -- GLOBAL_STEP: 152600\u001b[0m\n",
            "     | > loss: 0.01602  (0.01212)\n",
            "     | > grad_norm: 4.44650  (1.12619)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37954)\n",
            "     | > loader_time: 0.00270  (0.00475)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 166/261 -- GLOBAL_STEP: 152650\u001b[0m\n",
            "     | > loss: 0.01404  (0.01232)\n",
            "     | > grad_norm: 0.39579  (1.18954)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37610  (0.37902)\n",
            "     | > loader_time: 0.00590  (0.00448)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 216/261 -- GLOBAL_STEP: 152700\u001b[0m\n",
            "     | > loss: 0.01115  (0.01228)\n",
            "     | > grad_norm: 1.37749  (1.17251)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37902)\n",
            "     | > loader_time: 0.00250  (0.00432)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01139  (0.01139)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00941  (0.00941)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00069 \u001b[0m(+0.00004)\n",
            "     | > avg_loss:\u001b[91m 0.00941 \u001b[0m(+0.00202)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 583/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:20:16) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 4/261 -- GLOBAL_STEP: 152750\u001b[0m\n",
            "     | > loss: 0.00656  (0.01299)\n",
            "     | > grad_norm: 1.37510  (1.35930)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.40080  (0.40934)\n",
            "     | > loader_time: 0.01800  (0.01003)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 54/261 -- GLOBAL_STEP: 152800\u001b[0m\n",
            "     | > loss: 0.01244  (0.01246)\n",
            "     | > grad_norm: 1.39385  (1.02734)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.38181)\n",
            "     | > loader_time: 0.00230  (0.00477)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 104/261 -- GLOBAL_STEP: 152850\u001b[0m\n",
            "     | > loss: 0.00920  (0.01293)\n",
            "     | > grad_norm: 0.62032  (1.09704)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.38003)\n",
            "     | > loader_time: 0.00230  (0.00445)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 154/261 -- GLOBAL_STEP: 152900\u001b[0m\n",
            "     | > loss: 0.01333  (0.01300)\n",
            "     | > grad_norm: 2.46257  (1.14562)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37960)\n",
            "     | > loader_time: 0.01450  (0.00422)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 204/261 -- GLOBAL_STEP: 152950\u001b[0m\n",
            "     | > loss: 0.01935  (0.01297)\n",
            "     | > grad_norm: 0.61044  (1.12143)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37911)\n",
            "     | > loader_time: 0.00240  (0.00432)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 254/261 -- GLOBAL_STEP: 153000\u001b[0m\n",
            "     | > loss: 0.01908  (0.01288)\n",
            "     | > grad_norm: 2.44063  (1.13399)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37480  (0.37883)\n",
            "     | > loader_time: 0.00160  (0.00425)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01321  (0.01321)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01391  (0.01391)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00058 \u001b[0m(-0.00011)\n",
            "     | > avg_loss:\u001b[91m 0.01391 \u001b[0m(+0.00450)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 584/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:22:04) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 42/261 -- GLOBAL_STEP: 153050\u001b[0m\n",
            "     | > loss: 0.01287  (0.01329)\n",
            "     | > grad_norm: 1.84490  (1.47053)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39580  (0.37995)\n",
            "     | > loader_time: 0.00320  (0.00472)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 92/261 -- GLOBAL_STEP: 153100\u001b[0m\n",
            "     | > loss: 0.00712  (0.01259)\n",
            "     | > grad_norm: 1.13633  (1.46278)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37929)\n",
            "     | > loader_time: 0.00230  (0.00409)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 142/261 -- GLOBAL_STEP: 153150\u001b[0m\n",
            "     | > loss: 0.00599  (0.01244)\n",
            "     | > grad_norm: 0.85473  (1.35865)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37878)\n",
            "     | > loader_time: 0.00240  (0.00401)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 192/261 -- GLOBAL_STEP: 153200\u001b[0m\n",
            "     | > loss: 0.00965  (0.01244)\n",
            "     | > grad_norm: 0.65442  (1.26803)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38400  (0.37904)\n",
            "     | > loader_time: 0.01270  (0.00417)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 242/261 -- GLOBAL_STEP: 153250\u001b[0m\n",
            "     | > loss: 0.01020  (0.01255)\n",
            "     | > grad_norm: 0.64946  (1.24547)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37889)\n",
            "     | > loader_time: 0.00230  (0.00416)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01018  (0.01018)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01030  (0.01030)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00064 \u001b[0m(+0.00006)\n",
            "     | > avg_loss:\u001b[92m 0.01030 \u001b[0m(-0.00361)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 585/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:23:52) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 30/261 -- GLOBAL_STEP: 153300\u001b[0m\n",
            "     | > loss: 0.00735  (0.01341)\n",
            "     | > grad_norm: 0.83847  (1.19644)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37970  (0.38365)\n",
            "     | > loader_time: 0.02290  (0.00697)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/261 -- GLOBAL_STEP: 153350\u001b[0m\n",
            "     | > loss: 0.01554  (0.01274)\n",
            "     | > grad_norm: 1.12317  (1.26428)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.38010)\n",
            "     | > loader_time: 0.00260  (0.00538)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/261 -- GLOBAL_STEP: 153400\u001b[0m\n",
            "     | > loss: 0.01298  (0.01279)\n",
            "     | > grad_norm: 0.70312  (1.40548)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38480  (0.38033)\n",
            "     | > loader_time: 0.00290  (0.00509)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 180/261 -- GLOBAL_STEP: 153450\u001b[0m\n",
            "     | > loss: 0.01164  (0.01275)\n",
            "     | > grad_norm: 0.78577  (1.28595)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39230  (0.37975)\n",
            "     | > loader_time: 0.00300  (0.00458)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/261 -- GLOBAL_STEP: 153500\u001b[0m\n",
            "     | > loss: 0.01931  (0.01271)\n",
            "     | > grad_norm: 0.87947  (1.32411)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37780  (0.37946)\n",
            "     | > loader_time: 0.00240  (0.00435)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01745  (0.01745)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00735  (0.00735)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00070 \u001b[0m(+0.00006)\n",
            "     | > avg_loss:\u001b[92m 0.00735 \u001b[0m(-0.00295)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 586/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:25:40) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 18/261 -- GLOBAL_STEP: 153550\u001b[0m\n",
            "     | > loss: 0.00947  (0.01254)\n",
            "     | > grad_norm: 0.87641  (0.88611)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38220  (0.38111)\n",
            "     | > loader_time: 0.00220  (0.00845)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 68/261 -- GLOBAL_STEP: 153600\u001b[0m\n",
            "     | > loss: 0.01118  (0.01284)\n",
            "     | > grad_norm: 1.02740  (1.46964)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38600  (0.37903)\n",
            "     | > loader_time: 0.00270  (0.00493)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 118/261 -- GLOBAL_STEP: 153650\u001b[0m\n",
            "     | > loss: 0.01170  (0.01227)\n",
            "     | > grad_norm: 1.06196  (1.26203)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37899)\n",
            "     | > loader_time: 0.00390  (0.00417)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 168/261 -- GLOBAL_STEP: 153700\u001b[0m\n",
            "     | > loss: 0.01128  (0.01228)\n",
            "     | > grad_norm: 0.66986  (1.30568)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39460  (0.37868)\n",
            "     | > loader_time: 0.00350  (0.00428)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 218/261 -- GLOBAL_STEP: 153750\u001b[0m\n",
            "     | > loss: 0.00959  (0.01237)\n",
            "     | > grad_norm: 0.44307  (1.26777)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37853)\n",
            "     | > loader_time: 0.00230  (0.00443)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01097  (0.01097)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01010  (0.01010)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00069 \u001b[0m(-0.00001)\n",
            "     | > avg_loss:\u001b[91m 0.01010 \u001b[0m(+0.00274)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 587/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:27:28) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 6/261 -- GLOBAL_STEP: 153800\u001b[0m\n",
            "     | > loss: 0.02758  (0.01477)\n",
            "     | > grad_norm: 0.94849  (0.90381)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.40490  (0.39217)\n",
            "     | > loader_time: 0.00580  (0.01292)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 56/261 -- GLOBAL_STEP: 153850\u001b[0m\n",
            "     | > loss: 0.00962  (0.01358)\n",
            "     | > grad_norm: 0.70347  (1.06517)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.38026)\n",
            "     | > loader_time: 0.00230  (0.00549)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 106/261 -- GLOBAL_STEP: 153900\u001b[0m\n",
            "     | > loss: 0.00587  (0.01299)\n",
            "     | > grad_norm: 0.97641  (1.10238)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37630  (0.37952)\n",
            "     | > loader_time: 0.00310  (0.00430)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 156/261 -- GLOBAL_STEP: 153950\u001b[0m\n",
            "     | > loss: 0.01291  (0.01297)\n",
            "     | > grad_norm: 1.22948  (1.16616)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37927)\n",
            "     | > loader_time: 0.00230  (0.00439)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 206/261 -- GLOBAL_STEP: 154000\u001b[0m\n",
            "     | > loss: 0.01087  (0.01310)\n",
            "     | > grad_norm: 2.59654  (1.20634)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37891)\n",
            "     | > loader_time: 0.00340  (0.00426)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 256/261 -- GLOBAL_STEP: 154050\u001b[0m\n",
            "     | > loss: 0.00931  (0.01286)\n",
            "     | > grad_norm: 1.03479  (1.23572)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37460  (0.37882)\n",
            "     | > loader_time: 0.00180  (0.00404)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01476  (0.01476)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01852  (0.01852)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00064 \u001b[0m(-0.00005)\n",
            "     | > avg_loss:\u001b[91m 0.01852 \u001b[0m(+0.00842)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 588/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:29:16) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 44/261 -- GLOBAL_STEP: 154100\u001b[0m\n",
            "     | > loss: 0.00896  (0.01180)\n",
            "     | > grad_norm: 0.52040  (1.04443)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.38097)\n",
            "     | > loader_time: 0.00220  (0.00561)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 94/261 -- GLOBAL_STEP: 154150\u001b[0m\n",
            "     | > loss: 0.00818  (0.01236)\n",
            "     | > grad_norm: 0.48369  (1.17364)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37570  (0.37946)\n",
            "     | > loader_time: 0.01930  (0.00470)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 144/261 -- GLOBAL_STEP: 154200\u001b[0m\n",
            "     | > loss: 0.02202  (0.01241)\n",
            "     | > grad_norm: 0.74362  (1.14154)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37953)\n",
            "     | > loader_time: 0.00240  (0.00468)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 194/261 -- GLOBAL_STEP: 154250\u001b[0m\n",
            "     | > loss: 0.01264  (0.01275)\n",
            "     | > grad_norm: 1.49129  (1.17458)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38740  (0.37945)\n",
            "     | > loader_time: 0.00280  (0.00439)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 244/261 -- GLOBAL_STEP: 154300\u001b[0m\n",
            "     | > loss: 0.00809  (0.01276)\n",
            "     | > grad_norm: 0.83508  (1.17925)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37913)\n",
            "     | > loader_time: 0.00230  (0.00424)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.02366  (0.02366)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00941  (0.00941)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00062 \u001b[0m(-0.00001)\n",
            "     | > avg_loss:\u001b[92m 0.00941 \u001b[0m(-0.00911)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 589/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:31:04) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 32/261 -- GLOBAL_STEP: 154350\u001b[0m\n",
            "     | > loss: 0.00961  (0.01276)\n",
            "     | > grad_norm: 1.20394  (1.32114)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.38392)\n",
            "     | > loader_time: 0.00230  (0.00521)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 82/261 -- GLOBAL_STEP: 154400\u001b[0m\n",
            "     | > loss: 0.01006  (0.01266)\n",
            "     | > grad_norm: 1.00459  (1.34002)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37949)\n",
            "     | > loader_time: 0.00220  (0.00463)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 132/261 -- GLOBAL_STEP: 154450\u001b[0m\n",
            "     | > loss: 0.01668  (0.01242)\n",
            "     | > grad_norm: 1.00579  (1.25560)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37854)\n",
            "     | > loader_time: 0.00230  (0.00449)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 182/261 -- GLOBAL_STEP: 154500\u001b[0m\n",
            "     | > loss: 0.01247  (0.01248)\n",
            "     | > grad_norm: 1.29416  (1.25638)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37811)\n",
            "     | > loader_time: 0.00220  (0.00420)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 232/261 -- GLOBAL_STEP: 154550\u001b[0m\n",
            "     | > loss: 0.00833  (0.01229)\n",
            "     | > grad_norm: 1.44647  (1.27270)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37792)\n",
            "     | > loader_time: 0.00240  (0.00448)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.02073  (0.02073)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01255  (0.01255)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00057 \u001b[0m(-0.00005)\n",
            "     | > avg_loss:\u001b[91m 0.01255 \u001b[0m(+0.00314)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 590/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:32:52) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/261 -- GLOBAL_STEP: 154600\u001b[0m\n",
            "     | > loss: 0.01195  (0.01140)\n",
            "     | > grad_norm: 0.84112  (1.06460)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.38270)\n",
            "     | > loader_time: 0.00230  (0.00879)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/261 -- GLOBAL_STEP: 154650\u001b[0m\n",
            "     | > loss: 0.01517  (0.01247)\n",
            "     | > grad_norm: 1.45681  (1.13286)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38550  (0.37957)\n",
            "     | > loader_time: 0.00270  (0.00493)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/261 -- GLOBAL_STEP: 154700\u001b[0m\n",
            "     | > loss: 0.01459  (0.01280)\n",
            "     | > grad_norm: 1.76400  (1.13678)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37490  (0.37882)\n",
            "     | > loader_time: 0.00230  (0.00427)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 170/261 -- GLOBAL_STEP: 154750\u001b[0m\n",
            "     | > loss: 0.01104  (0.01302)\n",
            "     | > grad_norm: 2.27673  (1.16943)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37620  (0.37835)\n",
            "     | > loader_time: 0.00570  (0.00401)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 220/261 -- GLOBAL_STEP: 154800\u001b[0m\n",
            "     | > loss: 0.00875  (0.01293)\n",
            "     | > grad_norm: 0.57396  (1.17195)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37846)\n",
            "     | > loader_time: 0.00220  (0.00428)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01435  (0.01435)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01097  (0.01097)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00065 \u001b[0m(+0.00007)\n",
            "     | > avg_loss:\u001b[92m 0.01097 \u001b[0m(-0.00158)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 591/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:34:40) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 8/261 -- GLOBAL_STEP: 154850\u001b[0m\n",
            "     | > loss: 0.00960  (0.01295)\n",
            "     | > grad_norm: 1.07843  (0.96126)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.40460  (0.39482)\n",
            "     | > loader_time: 0.01280  (0.01898)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 58/261 -- GLOBAL_STEP: 154900\u001b[0m\n",
            "     | > loss: 0.00922  (0.01263)\n",
            "     | > grad_norm: 1.53663  (0.96816)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.38188)\n",
            "     | > loader_time: 0.00330  (0.00688)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 108/261 -- GLOBAL_STEP: 154950\u001b[0m\n",
            "     | > loss: 0.02010  (0.01303)\n",
            "     | > grad_norm: 1.39028  (1.08155)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37670  (0.38068)\n",
            "     | > loader_time: 0.01870  (0.00573)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 158/261 -- GLOBAL_STEP: 155000\u001b[0m\n",
            "     | > loss: 0.01341  (0.01288)\n",
            "     | > grad_norm: 0.94824  (1.14414)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.38049)\n",
            "     | > loader_time: 0.00370  (0.00522)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 208/261 -- GLOBAL_STEP: 155050\u001b[0m\n",
            "     | > loss: 0.01180  (0.01265)\n",
            "     | > grad_norm: 0.78586  (1.13735)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38380  (0.38034)\n",
            "     | > loader_time: 0.00310  (0.00510)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 258/261 -- GLOBAL_STEP: 155100\u001b[0m\n",
            "     | > loss: 0.01135  (0.01254)\n",
            "     | > grad_norm: 0.80150  (1.14539)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37450  (0.38016)\n",
            "     | > loader_time: 0.00170  (0.00497)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01244  (0.01244)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.02570  (0.02570)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00070 \u001b[0m(+0.00006)\n",
            "     | > avg_loss:\u001b[91m 0.02570 \u001b[0m(+0.01473)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 592/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:36:29) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 46/261 -- GLOBAL_STEP: 155150\u001b[0m\n",
            "     | > loss: 0.00786  (0.01307)\n",
            "     | > grad_norm: 1.56868  (1.41176)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.38168)\n",
            "     | > loader_time: 0.00240  (0.00602)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 96/261 -- GLOBAL_STEP: 155200\u001b[0m\n",
            "     | > loss: 0.00469  (0.01263)\n",
            "     | > grad_norm: 0.67872  (1.34578)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.38012)\n",
            "     | > loader_time: 0.00220  (0.00453)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 146/261 -- GLOBAL_STEP: 155250\u001b[0m\n",
            "     | > loss: 0.01204  (0.01255)\n",
            "     | > grad_norm: 0.74611  (1.28376)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37640  (0.37981)\n",
            "     | > loader_time: 0.00240  (0.00467)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 196/261 -- GLOBAL_STEP: 155300\u001b[0m\n",
            "     | > loss: 0.02570  (0.01261)\n",
            "     | > grad_norm: 3.19192  (1.27758)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37480  (0.37969)\n",
            "     | > loader_time: 0.00370  (0.00509)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 246/261 -- GLOBAL_STEP: 155350\u001b[0m\n",
            "     | > loss: 0.01650  (0.01290)\n",
            "     | > grad_norm: 2.80161  (1.32199)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37500  (0.37938)\n",
            "     | > loader_time: 0.00510  (0.00472)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01056  (0.01056)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.04563  (0.04563)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00050 \u001b[0m(-0.00020)\n",
            "     | > avg_loss:\u001b[91m 0.04563 \u001b[0m(+0.01993)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 593/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:38:17) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 34/261 -- GLOBAL_STEP: 155400\u001b[0m\n",
            "     | > loss: 0.02064  (0.01153)\n",
            "     | > grad_norm: 1.16925  (1.46497)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37730  (0.38279)\n",
            "     | > loader_time: 0.00270  (0.00613)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 84/261 -- GLOBAL_STEP: 155450\u001b[0m\n",
            "     | > loss: 0.00928  (0.01213)\n",
            "     | > grad_norm: 0.81440  (1.37143)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37981)\n",
            "     | > loader_time: 0.00350  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 134/261 -- GLOBAL_STEP: 155500\u001b[0m\n",
            "     | > loss: 0.01070  (0.01217)\n",
            "     | > grad_norm: 1.51508  (1.30593)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37630  (0.37940)\n",
            "     | > loader_time: 0.00300  (0.00466)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 184/261 -- GLOBAL_STEP: 155550\u001b[0m\n",
            "     | > loss: 0.01615  (0.01215)\n",
            "     | > grad_norm: 1.21337  (1.25866)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37911)\n",
            "     | > loader_time: 0.00630  (0.00441)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 234/261 -- GLOBAL_STEP: 155600\u001b[0m\n",
            "     | > loss: 0.02203  (0.01234)\n",
            "     | > grad_norm: 1.18929  (1.24950)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38650  (0.37891)\n",
            "     | > loader_time: 0.01180  (0.00463)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.00949  (0.00949)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01190  (0.01190)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00074 \u001b[0m(+0.00024)\n",
            "     | > avg_loss:\u001b[92m 0.01190 \u001b[0m(-0.03373)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 594/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:40:05) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/261 -- GLOBAL_STEP: 155650\u001b[0m\n",
            "     | > loss: 0.01709  (0.01315)\n",
            "     | > grad_norm: 1.43815  (1.19169)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38310  (0.38358)\n",
            "     | > loader_time: 0.01440  (0.00996)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 72/261 -- GLOBAL_STEP: 155700\u001b[0m\n",
            "     | > loss: 0.01128  (0.01230)\n",
            "     | > grad_norm: 0.35191  (1.06914)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.38064)\n",
            "     | > loader_time: 0.00250  (0.00654)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 122/261 -- GLOBAL_STEP: 155750\u001b[0m\n",
            "     | > loss: 0.00962  (0.01201)\n",
            "     | > grad_norm: 1.22910  (1.07451)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37600  (0.37978)\n",
            "     | > loader_time: 0.00230  (0.00525)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 172/261 -- GLOBAL_STEP: 155800\u001b[0m\n",
            "     | > loss: 0.00954  (0.01186)\n",
            "     | > grad_norm: 0.76544  (1.05992)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37925)\n",
            "     | > loader_time: 0.01770  (0.00475)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 222/261 -- GLOBAL_STEP: 155850\u001b[0m\n",
            "     | > loss: 0.01374  (0.01198)\n",
            "     | > grad_norm: 0.96917  (1.15462)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37913)\n",
            "     | > loader_time: 0.00340  (0.00490)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01061  (0.01061)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01316  (0.01316)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00068 \u001b[0m(-0.00006)\n",
            "     | > avg_loss:\u001b[91m 0.01316 \u001b[0m(+0.00125)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 595/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:41:54) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/261 -- GLOBAL_STEP: 155900\u001b[0m\n",
            "     | > loss: 0.01031  (0.01111)\n",
            "     | > grad_norm: 0.91472  (1.04335)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.38550)\n",
            "     | > loader_time: 0.00240  (0.01163)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/261 -- GLOBAL_STEP: 155950\u001b[0m\n",
            "     | > loss: 0.01064  (0.01394)\n",
            "     | > grad_norm: 1.34492  (1.18911)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38210  (0.37900)\n",
            "     | > loader_time: 0.00270  (0.00491)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 110/261 -- GLOBAL_STEP: 156000\u001b[0m\n",
            "     | > loss: 0.01187  (0.01289)\n",
            "     | > grad_norm: 1.46715  (1.16387)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37824)\n",
            "     | > loader_time: 0.00360  (0.00445)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 160/261 -- GLOBAL_STEP: 156050\u001b[0m\n",
            "     | > loss: 0.00549  (0.01262)\n",
            "     | > grad_norm: 0.75797  (1.17673)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37825)\n",
            "     | > loader_time: 0.00420  (0.00437)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 210/261 -- GLOBAL_STEP: 156100\u001b[0m\n",
            "     | > loss: 0.01182  (0.01259)\n",
            "     | > grad_norm: 1.76449  (1.21174)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37590  (0.37833)\n",
            "     | > loader_time: 0.00360  (0.00416)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 260/261 -- GLOBAL_STEP: 156150\u001b[0m\n",
            "     | > loss: 0.01368  (0.01272)\n",
            "     | > grad_norm: 0.69999  (1.22453)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37480  (0.37860)\n",
            "     | > loader_time: 0.00170  (0.00416)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01667  (0.01667)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01811  (0.01811)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00065 \u001b[0m(-0.00002)\n",
            "     | > avg_loss:\u001b[91m 0.01811 \u001b[0m(+0.00495)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 596/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:43:42) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 48/261 -- GLOBAL_STEP: 156200\u001b[0m\n",
            "     | > loss: 0.01334  (0.01275)\n",
            "     | > grad_norm: 0.72553  (1.14111)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37978)\n",
            "     | > loader_time: 0.00470  (0.00444)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 98/261 -- GLOBAL_STEP: 156250\u001b[0m\n",
            "     | > loss: 0.00727  (0.01236)\n",
            "     | > grad_norm: 1.14054  (1.02158)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37520  (0.37858)\n",
            "     | > loader_time: 0.00330  (0.00395)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 148/261 -- GLOBAL_STEP: 156300\u001b[0m\n",
            "     | > loss: 0.00982  (0.01233)\n",
            "     | > grad_norm: 1.11277  (1.13024)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38520  (0.37889)\n",
            "     | > loader_time: 0.00280  (0.00409)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 198/261 -- GLOBAL_STEP: 156350\u001b[0m\n",
            "     | > loss: 0.01099  (0.01232)\n",
            "     | > grad_norm: 2.02968  (1.12405)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37879)\n",
            "     | > loader_time: 0.00220  (0.00438)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 248/261 -- GLOBAL_STEP: 156400\u001b[0m\n",
            "     | > loss: 0.01014  (0.01223)\n",
            "     | > grad_norm: 1.31649  (1.14040)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37580  (0.37857)\n",
            "     | > loader_time: 0.00300  (0.00420)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01403  (0.01403)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.00879  (0.00879)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00082 \u001b[0m(+0.00017)\n",
            "     | > avg_loss:\u001b[92m 0.00879 \u001b[0m(-0.00931)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 597/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:45:30) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 36/261 -- GLOBAL_STEP: 156450\u001b[0m\n",
            "     | > loss: 0.02026  (0.01390)\n",
            "     | > grad_norm: 5.22562  (1.26743)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37790  (0.38158)\n",
            "     | > loader_time: 0.00250  (0.00544)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 86/261 -- GLOBAL_STEP: 156500\u001b[0m\n",
            "     | > loss: 0.02622  (0.01344)\n",
            "     | > grad_norm: 0.80684  (1.27273)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37530  (0.37904)\n",
            "     | > loader_time: 0.00220  (0.00441)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 136/261 -- GLOBAL_STEP: 156550\u001b[0m\n",
            "     | > loss: 0.00792  (0.01319)\n",
            "     | > grad_norm: 0.48671  (1.25707)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37560  (0.37879)\n",
            "     | > loader_time: 0.00220  (0.00401)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 186/261 -- GLOBAL_STEP: 156600\u001b[0m\n",
            "     | > loss: 0.01440  (0.01298)\n",
            "     | > grad_norm: 1.32533  (1.22974)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.39590  (0.37875)\n",
            "     | > loader_time: 0.00280  (0.00389)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 236/261 -- GLOBAL_STEP: 156650\u001b[0m\n",
            "     | > loss: 0.01608  (0.01271)\n",
            "     | > grad_norm: 3.56527  (1.21371)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37841)\n",
            "     | > loader_time: 0.00230  (0.00385)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01015  (0.01015)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01089  (0.01089)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00066 \u001b[0m(-0.00017)\n",
            "     | > avg_loss:\u001b[91m 0.01089 \u001b[0m(+0.00210)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 598/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:47:18) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 24/261 -- GLOBAL_STEP: 156700\u001b[0m\n",
            "     | > loss: 0.01318  (0.01228)\n",
            "     | > grad_norm: 0.89272  (1.20632)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.38360  (0.38685)\n",
            "     | > loader_time: 0.00300  (0.00797)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 74/261 -- GLOBAL_STEP: 156750\u001b[0m\n",
            "     | > loss: 0.01010  (0.01227)\n",
            "     | > grad_norm: 0.36562  (1.16886)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.38107)\n",
            "     | > loader_time: 0.00350  (0.00481)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 124/261 -- GLOBAL_STEP: 156800\u001b[0m\n",
            "     | > loss: 0.00726  (0.01198)\n",
            "     | > grad_norm: 0.31995  (1.13342)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37500  (0.37975)\n",
            "     | > loader_time: 0.00220  (0.00486)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 174/261 -- GLOBAL_STEP: 156850\u001b[0m\n",
            "     | > loss: 0.01219  (0.01220)\n",
            "     | > grad_norm: 1.26243  (1.14264)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37540  (0.37942)\n",
            "     | > loader_time: 0.00250  (0.00455)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 224/261 -- GLOBAL_STEP: 156900\u001b[0m\n",
            "     | > loss: 0.02336  (0.01248)\n",
            "     | > grad_norm: 0.76675  (1.14726)\n",
            "     | > current_lr: 0.00005 \n",
            "     | > step_time: 0.37550  (0.37914)\n",
            "     | > loader_time: 0.00750  (0.00467)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.01385  (0.01385)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.01171  (0.01171)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00069 \u001b[0m(+0.00003)\n",
            "     | > avg_loss:\u001b[91m 0.01171 \u001b[0m(+0.00082)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 599/1000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/coqui_tts-November-18-2021_08+19AM-33aa27e2\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-19 02:49:07) \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Emergent/train_wavegrad.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DbfCQ2dBdrY",
        "outputId": "9d520f84-cf0f-4db2-a336-e16ee9084b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "     | > loss: 4.44080  (4.38507)\n",
            "     | > grad_norm: 7876.50391  (3572.50293)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67010  (0.66376)\n",
            "     | > loader_time: 0.00290  (0.69693)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 116/131 -- GLOBAL_STEP: 278100\u001b[0m\n",
            "     | > loss: 3.99475  (4.33498)\n",
            "     | > grad_norm: 1692.90784  (3658.41431)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71030  (0.66789)\n",
            "     | > loader_time: 2.80000  (0.71516)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.56482  (2.56482)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.56234  (3.56234)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.96135  (3.76184)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.40429  (3.97599)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.25341  (3.79535)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.80799  (3.59788)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 6.06900  (4.00973)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.36923  (4.20394)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.41430  (3.98024)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.18335  (3.89170)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00254 \u001b[0m(+0.00090)\n",
            "     | > avg_loss:\u001b[91m 3.89170 \u001b[0m(+0.29384)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 213/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 08:45:28) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 9/131 -- GLOBAL_STEP: 278125\u001b[0m\n",
            "     | > loss: 5.23084  (5.07045)\n",
            "     | > grad_norm: 4034.74023  (4817.79883)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67470  (0.67176)\n",
            "     | > loader_time: 0.00390  (0.71530)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 34/131 -- GLOBAL_STEP: 278150\u001b[0m\n",
            "     | > loss: 4.20972  (4.51135)\n",
            "     | > grad_norm: 3162.54175  (3259.55566)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.73300  (0.68070)\n",
            "     | > loader_time: 0.01180  (0.72373)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 59/131 -- GLOBAL_STEP: 278175\u001b[0m\n",
            "     | > loss: 4.15948  (4.35574)\n",
            "     | > grad_norm: 3823.20605  (3476.93604)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68280  (0.67664)\n",
            "     | > loader_time: 0.00510  (0.72852)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 84/131 -- GLOBAL_STEP: 278200\u001b[0m\n",
            "     | > loss: 4.12137  (4.39911)\n",
            "     | > grad_norm: 3743.25195  (3613.69141)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66030  (0.67603)\n",
            "     | > loader_time: 2.58630  (0.77177)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 109/131 -- GLOBAL_STEP: 278225\u001b[0m\n",
            "     | > loss: 4.43970  (4.39782)\n",
            "     | > grad_norm: 6124.96924  (3632.63696)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65760  (0.67527)\n",
            "     | > loader_time: 0.00450  (0.76198)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.45899  (2.45899)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 2.75042  (2.75042)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.28035  (3.01539)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.98794  (3.67290)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.00701  (3.50643)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 4.17852  (3.64085)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.33744  (3.92361)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.91993  (4.20880)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.52367  (4.12316)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 4.30843  (4.14375)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00055 \u001b[0m(-0.00198)\n",
            "     | > avg_loss:\u001b[91m 4.14375 \u001b[0m(+0.25205)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 214/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 08:49:00) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/131 -- GLOBAL_STEP: 278250\u001b[0m\n",
            "     | > loss: 4.53702  (4.50927)\n",
            "     | > grad_norm: 3621.89526  (3879.50293)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65660  (0.67868)\n",
            "     | > loader_time: 0.00350  (0.01076)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 27/131 -- GLOBAL_STEP: 278275\u001b[0m\n",
            "     | > loss: 4.09352  (4.12936)\n",
            "     | > grad_norm: 2674.98193  (3111.40918)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64260  (0.67010)\n",
            "     | > loader_time: 0.00730  (0.68831)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 52/131 -- GLOBAL_STEP: 278300\u001b[0m\n",
            "     | > loss: 4.24208  (4.05375)\n",
            "     | > grad_norm: 1375.54175  (3202.82544)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66160  (0.67278)\n",
            "     | > loader_time: 3.09700  (0.79389)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 77/131 -- GLOBAL_STEP: 278325\u001b[0m\n",
            "     | > loss: 3.99945  (4.03665)\n",
            "     | > grad_norm: 3848.96802  (3264.65405)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.74290  (0.67867)\n",
            "     | > loader_time: 0.00590  (0.78984)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 102/131 -- GLOBAL_STEP: 278350\u001b[0m\n",
            "     | > loss: 4.03330  (4.02370)\n",
            "     | > grad_norm: 4289.12305  (3285.17114)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64540  (0.68096)\n",
            "     | > loader_time: 0.00690  (0.77076)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 127/131 -- GLOBAL_STEP: 278375\u001b[0m\n",
            "     | > loss: 4.14292  (4.01337)\n",
            "     | > grad_norm: 2631.80469  (3310.48975)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68180  (0.68260)\n",
            "     | > loader_time: 0.00070  (0.75580)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.29016  (3.29016)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.60872  (3.60872)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.37328  (3.99100)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 3.86222  (3.94807)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.04664  (3.72272)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.36749  (3.45167)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 3.64411  (3.48374)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 7.22268  (4.01788)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.48894  (3.95176)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.11709  (3.85902)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00040 \u001b[0m(-0.00015)\n",
            "     | > avg_loss:\u001b[92m 3.85902 \u001b[0m(-0.28473)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 215/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 08:52:32) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/131 -- GLOBAL_STEP: 278400\u001b[0m\n",
            "     | > loss: 4.12748  (4.03079)\n",
            "     | > grad_norm: 2494.97534  (3441.67505)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68690  (0.68542)\n",
            "     | > loader_time: 2.75690  (0.79912)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 45/131 -- GLOBAL_STEP: 278425\u001b[0m\n",
            "     | > loss: 4.06639  (4.02242)\n",
            "     | > grad_norm: 4292.76855  (3481.72583)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67980  (0.68686)\n",
            "     | > loader_time: 0.00710  (0.77662)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/131 -- GLOBAL_STEP: 278450\u001b[0m\n",
            "     | > loss: 4.02797  (4.01283)\n",
            "     | > grad_norm: 4891.94385  (3463.84863)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65500  (0.68743)\n",
            "     | > loader_time: 0.00600  (0.77870)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 95/131 -- GLOBAL_STEP: 278475\u001b[0m\n",
            "     | > loss: 3.84478  (4.00217)\n",
            "     | > grad_norm: 1345.99548  (3455.60449)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69930  (0.68540)\n",
            "     | > loader_time: 0.01670  (0.78085)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/131 -- GLOBAL_STEP: 278500\u001b[0m\n",
            "     | > loss: 3.77163  (3.99852)\n",
            "     | > grad_norm: 711.20691  (3443.38184)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64730  (0.68285)\n",
            "     | > loader_time: 0.97520  (0.79329)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.16786  (3.16786)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 2.61968  (2.61968)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 5.92506  (4.27237)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.58998  (4.37824)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.25056  (4.09632)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.67282  (3.81162)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 3.24614  (3.71737)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.74513  (3.86420)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.50125  (3.81883)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.98003  (3.72563)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00044 \u001b[0m(+0.00004)\n",
            "     | > avg_loss:\u001b[92m 3.72563 \u001b[0m(-0.13339)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 216/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 08:56:09) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 13/131 -- GLOBAL_STEP: 278525\u001b[0m\n",
            "     | > loss: 3.94150  (3.94923)\n",
            "     | > grad_norm: 4563.04395  (3770.22363)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66260  (0.66861)\n",
            "     | > loader_time: 0.01080  (0.69810)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 38/131 -- GLOBAL_STEP: 278550\u001b[0m\n",
            "     | > loss: 4.04428  (3.98526)\n",
            "     | > grad_norm: 3934.46729  (3627.75464)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68340  (0.67684)\n",
            "     | > loader_time: 0.00240  (0.71392)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 63/131 -- GLOBAL_STEP: 278575\u001b[0m\n",
            "     | > loss: 4.16440  (3.99469)\n",
            "     | > grad_norm: 4013.82617  (3477.34546)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67780  (0.67793)\n",
            "     | > loader_time: 0.00180  (0.74604)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/131 -- GLOBAL_STEP: 278600\u001b[0m\n",
            "     | > loss: 3.92669  (3.99549)\n",
            "     | > grad_norm: 2849.08936  (3455.67090)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66320  (0.67936)\n",
            "     | > loader_time: 3.60560  (0.79404)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 113/131 -- GLOBAL_STEP: 278625\u001b[0m\n",
            "     | > loss: 4.24933  (4.00217)\n",
            "     | > grad_norm: 4435.84326  (3456.16919)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68730  (0.68125)\n",
            "     | > loader_time: 0.00590  (0.76555)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.22146  (3.22146)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.26980  (3.26980)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 2.78514  (3.02747)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.15080  (3.40191)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.50283  (3.42714)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.38689  (3.41909)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.54447  (3.60665)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.40669  (3.86380)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 5.77168  (4.10229)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.58338  (4.04463)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00041 \u001b[0m(-0.00003)\n",
            "     | > avg_loss:\u001b[91m 4.04463 \u001b[0m(+0.31900)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 217/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 08:59:42) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 6/131 -- GLOBAL_STEP: 278650\u001b[0m\n",
            "     | > loss: 4.15203  (4.09399)\n",
            "     | > grad_norm: 4322.07373  (3580.12939)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68180  (0.69543)\n",
            "     | > loader_time: 0.00240  (0.44724)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 31/131 -- GLOBAL_STEP: 278675\u001b[0m\n",
            "     | > loss: 3.75887  (4.03176)\n",
            "     | > grad_norm: 3443.60254  (3487.95190)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67970  (0.67913)\n",
            "     | > loader_time: 0.00110  (0.69538)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 56/131 -- GLOBAL_STEP: 278700\u001b[0m\n",
            "     | > loss: 3.91530  (4.01438)\n",
            "     | > grad_norm: 1148.81323  (3418.44287)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67010  (0.67921)\n",
            "     | > loader_time: 3.05970  (0.77858)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 81/131 -- GLOBAL_STEP: 278725\u001b[0m\n",
            "     | > loss: 4.44346  (4.00999)\n",
            "     | > grad_norm: 2808.66846  (3482.91821)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64730  (0.68084)\n",
            "     | > loader_time: 0.01320  (0.78179)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 106/131 -- GLOBAL_STEP: 278750\u001b[0m\n",
            "     | > loss: 4.10512  (4.01084)\n",
            "     | > grad_norm: 3546.58008  (3460.59180)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68920  (0.68051)\n",
            "     | > loader_time: 0.00350  (0.76637)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 131/131 -- GLOBAL_STEP: 278775\u001b[0m\n",
            "     | > loss: 3.83077  (4.01958)\n",
            "     | > grad_norm: 1009.40582  (3519.11987)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.40790  (0.67216)\n",
            "     | > loader_time: 0.00040  (0.74196)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.25889  (2.25889)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 2.72284  (2.72284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.99451  (3.35867)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.59897  (3.77211)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.73887  (3.76380)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.41799  (3.69464)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.26501  (3.95637)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.35538  (4.01337)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.13335  (3.90336)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.57152  (3.75538)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00045 \u001b[0m(+0.00004)\n",
            "     | > avg_loss:\u001b[92m 3.75538 \u001b[0m(-0.28925)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 218/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:03:14) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 24/131 -- GLOBAL_STEP: 278800\u001b[0m\n",
            "     | > loss: 3.77813  (4.02467)\n",
            "     | > grad_norm: 1108.59778  (3534.75757)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66470  (0.66854)\n",
            "     | > loader_time: 3.24100  (0.85568)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 49/131 -- GLOBAL_STEP: 278825\u001b[0m\n",
            "     | > loss: 4.36490  (4.00747)\n",
            "     | > grad_norm: 3484.65820  (3510.14795)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69490  (0.67162)\n",
            "     | > loader_time: 0.00850  (0.82983)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 74/131 -- GLOBAL_STEP: 278850\u001b[0m\n",
            "     | > loss: 4.04959  (4.01858)\n",
            "     | > grad_norm: 4153.86182  (3533.55811)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65190  (0.67344)\n",
            "     | > loader_time: 0.00590  (0.82903)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 99/131 -- GLOBAL_STEP: 278875\u001b[0m\n",
            "     | > loss: 4.02826  (4.01075)\n",
            "     | > grad_norm: 1977.74377  (3480.03125)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63820  (0.67305)\n",
            "     | > loader_time: 0.00310  (0.81164)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 124/131 -- GLOBAL_STEP: 278900\u001b[0m\n",
            "     | > loss: 4.10385  (4.00727)\n",
            "     | > grad_norm: 2140.97363  (3472.15820)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70130  (0.67366)\n",
            "     | > loader_time: 2.59210  (0.82060)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.87138  (2.87138)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.58942  (4.58942)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.06303  (4.32623)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.86707  (4.83984)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.67216  (4.54792)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.22491  (4.28332)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 3.34657  (4.12719)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.54433  (4.18678)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.26682  (4.07179)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.92842  (4.05586)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00044 \u001b[0m(-0.00001)\n",
            "     | > avg_loss:\u001b[91m 4.05586 \u001b[0m(+0.30048)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 219/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:06:52) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 17/131 -- GLOBAL_STEP: 278925\u001b[0m\n",
            "     | > loss: 4.09129  (4.04118)\n",
            "     | > grad_norm: 4117.09082  (3563.71362)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68150  (0.66355)\n",
            "     | > loader_time: 0.00280  (0.74701)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 42/131 -- GLOBAL_STEP: 278950\u001b[0m\n",
            "     | > loss: 4.15624  (4.02779)\n",
            "     | > grad_norm: 3333.96802  (3471.12329)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66790  (0.67374)\n",
            "     | > loader_time: 0.00400  (0.74994)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 67/131 -- GLOBAL_STEP: 278975\u001b[0m\n",
            "     | > loss: 3.85883  (4.01557)\n",
            "     | > grad_norm: 5257.06738  (3424.42090)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63680  (0.67778)\n",
            "     | > loader_time: 0.00520  (0.75592)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 92/131 -- GLOBAL_STEP: 279000\u001b[0m\n",
            "     | > loss: 3.92185  (4.00726)\n",
            "     | > grad_norm: 2078.30420  (3382.21777)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65240  (0.67569)\n",
            "     | > loader_time: 3.47160  (0.80376)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 117/131 -- GLOBAL_STEP: 279025\u001b[0m\n",
            "     | > loss: 3.90168  (4.00840)\n",
            "     | > grad_norm: 5478.05566  (3439.13721)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71050  (0.67456)\n",
            "     | > loader_time: 0.00660  (0.78098)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.75180  (3.75180)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.24739  (3.24739)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.46564  (3.35652)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 3.93721  (3.55008)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.93985  (3.39752)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.89096  (3.29621)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.92828  (3.73489)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.62647  (3.86226)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.76527  (3.72514)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.72342  (3.61383)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00047 \u001b[0m(+0.00003)\n",
            "     | > avg_loss:\u001b[92m 3.61383 \u001b[0m(-0.44203)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 220/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:10:26) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/131 -- GLOBAL_STEP: 279050\u001b[0m\n",
            "     | > loss: 3.95432  (4.02707)\n",
            "     | > grad_norm: 4565.75098  (3715.94336)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72320  (0.67376)\n",
            "     | > loader_time: 0.00380  (0.59879)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 35/131 -- GLOBAL_STEP: 279075\u001b[0m\n",
            "     | > loss: 3.93131  (3.99970)\n",
            "     | > grad_norm: 5319.32178  (3495.29834)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.74340  (0.67498)\n",
            "     | > loader_time: 0.01650  (0.71207)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/131 -- GLOBAL_STEP: 279100\u001b[0m\n",
            "     | > loss: 3.89762  (4.01552)\n",
            "     | > grad_norm: 814.59814  (3439.71729)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66490  (0.67948)\n",
            "     | > loader_time: 2.96970  (0.74367)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 85/131 -- GLOBAL_STEP: 279125\u001b[0m\n",
            "     | > loss: 4.11324  (4.01251)\n",
            "     | > grad_norm: 6205.31982  (3504.17505)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68070  (0.68246)\n",
            "     | > loader_time: 0.00540  (0.74493)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 110/131 -- GLOBAL_STEP: 279150\u001b[0m\n",
            "     | > loss: 3.93083  (4.01436)\n",
            "     | > grad_norm: 3474.15674  (3460.61230)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65920  (0.67919)\n",
            "     | > loader_time: 1.49770  (0.73933)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.01116  (3.01116)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.19867  (3.19867)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 5.04120  (4.11993)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.10902  (4.44963)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.49824  (4.21178)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 4.04634  (4.17869)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.99270  (4.31436)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.68530  (4.36735)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.50887  (4.26004)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.93316  (4.11261)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00060 \u001b[0m(+0.00013)\n",
            "     | > avg_loss:\u001b[91m 4.11261 \u001b[0m(+0.49878)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 221/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:13:54) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 3/131 -- GLOBAL_STEP: 279175\u001b[0m\n",
            "     | > loss: 4.06945  (3.90589)\n",
            "     | > grad_norm: 4256.86035  (3522.56592)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66110  (0.66462)\n",
            "     | > loader_time: 0.01720  (0.00987)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 28/131 -- GLOBAL_STEP: 279200\u001b[0m\n",
            "     | > loss: 3.97944  (4.01961)\n",
            "     | > grad_norm: 656.03949  (3242.40088)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.62040  (0.67703)\n",
            "     | > loader_time: 3.17680  (0.76044)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 53/131 -- GLOBAL_STEP: 279225\u001b[0m\n",
            "     | > loss: 4.18183  (4.01476)\n",
            "     | > grad_norm: 5831.21826  (3435.94775)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68910  (0.67575)\n",
            "     | > loader_time: 0.00910  (0.74843)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 78/131 -- GLOBAL_STEP: 279250\u001b[0m\n",
            "     | > loss: 4.01172  (4.02138)\n",
            "     | > grad_norm: 4845.82520  (3505.52417)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72440  (0.68122)\n",
            "     | > loader_time: 0.01330  (0.75178)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 103/131 -- GLOBAL_STEP: 279275\u001b[0m\n",
            "     | > loss: 4.18890  (4.02805)\n",
            "     | > grad_norm: 4342.08350  (3485.94751)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72700  (0.68106)\n",
            "     | > loader_time: 0.01700  (0.74766)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 128/131 -- GLOBAL_STEP: 279300\u001b[0m\n",
            "     | > loss: 3.93094  (4.02294)\n",
            "     | > grad_norm: 1516.00610  (3486.45996)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.56100  (0.67973)\n",
            "     | > loader_time: 1.12020  (0.74468)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.17685  (3.17685)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.57844  (3.57844)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 6.76387  (5.17116)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.09966  (4.81399)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.09706  (4.38476)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 5.57822  (4.62345)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 6.37515  (4.91540)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 3.46552  (4.70827)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.81917  (4.47214)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.67154  (4.38318)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00041 \u001b[0m(-0.00020)\n",
            "     | > avg_loss:\u001b[91m 4.38318 \u001b[0m(+0.27057)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 222/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:17:24) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 21/131 -- GLOBAL_STEP: 279325\u001b[0m\n",
            "     | > loss: 4.18114  (4.04311)\n",
            "     | > grad_norm: 5368.31934  (3708.71875)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66880  (0.68030)\n",
            "     | > loader_time: 0.00800  (0.78858)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 46/131 -- GLOBAL_STEP: 279350\u001b[0m\n",
            "     | > loss: 4.15814  (4.01402)\n",
            "     | > grad_norm: 4105.40234  (3685.66968)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68020  (0.68627)\n",
            "     | > loader_time: 0.00540  (0.76203)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 71/131 -- GLOBAL_STEP: 279375\u001b[0m\n",
            "     | > loss: 3.92479  (4.01561)\n",
            "     | > grad_norm: 3113.08130  (3514.57202)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68280  (0.68550)\n",
            "     | > loader_time: 0.00100  (0.78003)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 96/131 -- GLOBAL_STEP: 279400\u001b[0m\n",
            "     | > loss: 3.71915  (4.00686)\n",
            "     | > grad_norm: 2403.60645  (3437.59082)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67190  (0.68221)\n",
            "     | > loader_time: 3.64230  (0.83971)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 121/131 -- GLOBAL_STEP: 279425\u001b[0m\n",
            "     | > loss: 4.28593  (4.00279)\n",
            "     | > grad_norm: 3734.09546  (3433.05176)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71840  (0.68205)\n",
            "     | > loader_time: 0.00680  (0.82795)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.25415  (3.25415)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.06863  (3.06863)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.45411  (3.76137)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.05627  (3.85967)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.92925  (3.62707)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.28937  (3.55953)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.09336  (3.81517)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.44123  (4.04746)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 5.81528  (4.26844)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.71681  (4.09603)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00170 \u001b[0m(+0.00129)\n",
            "     | > avg_loss:\u001b[92m 4.09603 \u001b[0m(-0.28715)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 223/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:21:02) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 14/131 -- GLOBAL_STEP: 279450\u001b[0m\n",
            "     | > loss: 3.70371  (3.97868)\n",
            "     | > grad_norm: 4294.22168  (3451.63696)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67360  (0.68350)\n",
            "     | > loader_time: 0.00270  (0.65367)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 39/131 -- GLOBAL_STEP: 279475\u001b[0m\n",
            "     | > loss: 3.99703  (3.98649)\n",
            "     | > grad_norm: 3787.95459  (3413.64746)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63810  (0.67282)\n",
            "     | > loader_time: 0.00330  (0.74516)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 64/131 -- GLOBAL_STEP: 279500\u001b[0m\n",
            "     | > loss: 3.96175  (4.01082)\n",
            "     | > grad_norm: 4700.27246  (3416.49194)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68910  (0.67432)\n",
            "     | > loader_time: 3.02740  (0.80727)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 89/131 -- GLOBAL_STEP: 279525\u001b[0m\n",
            "     | > loss: 4.12627  (3.99950)\n",
            "     | > grad_norm: 5112.05127  (3401.74902)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67540  (0.67786)\n",
            "     | > loader_time: 0.00490  (0.79970)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 114/131 -- GLOBAL_STEP: 279550\u001b[0m\n",
            "     | > loss: 3.98461  (3.99823)\n",
            "     | > grad_norm: 3797.67993  (3423.53198)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69060  (0.67655)\n",
            "     | > loader_time: 0.00800  (0.77714)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.56654  (2.56654)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.40871  (4.40871)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.67299  (4.54085)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.81098  (4.96423)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 4.91540  (4.95202)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.65133  (4.49188)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.36449  (4.47065)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 7.79463  (4.94550)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.52573  (4.64303)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.92167  (4.56288)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00063 \u001b[0m(-0.00106)\n",
            "     | > avg_loss:\u001b[91m 4.56288 \u001b[0m(+0.46685)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 224/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:24:36) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 7/131 -- GLOBAL_STEP: 279575\u001b[0m\n",
            "     | > loss: 4.08167  (4.01752)\n",
            "     | > grad_norm: 3511.29688  (3457.70435)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66630  (0.67266)\n",
            "     | > loader_time: 0.00330  (0.41087)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 32/131 -- GLOBAL_STEP: 279600\u001b[0m\n",
            "     | > loss: 3.92768  (4.04099)\n",
            "     | > grad_norm: 3270.52832  (3609.27319)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65470  (0.67533)\n",
            "     | > loader_time: 3.03770  (0.78090)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 57/131 -- GLOBAL_STEP: 279625\u001b[0m\n",
            "     | > loss: 4.06243  (4.00487)\n",
            "     | > grad_norm: 3663.33594  (3529.40186)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68550  (0.67558)\n",
            "     | > loader_time: 0.55250  (0.74953)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 82/131 -- GLOBAL_STEP: 279650\u001b[0m\n",
            "     | > loss: 3.91929  (4.00154)\n",
            "     | > grad_norm: 3049.47583  (3538.88550)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63190  (0.67419)\n",
            "     | > loader_time: 0.00530  (0.74760)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 107/131 -- GLOBAL_STEP: 279675\u001b[0m\n",
            "     | > loss: 3.92403  (3.99845)\n",
            "     | > grad_norm: 2595.26367  (3495.06250)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67510  (0.67373)\n",
            "     | > loader_time: 0.01360  (0.75133)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.56041  (3.56041)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.60292  (3.60292)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.77896  (3.69094)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.52112  (3.96766)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.22247  (3.78136)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.98507  (3.82210)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.55724  (4.11129)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.95055  (4.23119)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.35020  (4.12106)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.06529  (4.00376)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00053 \u001b[0m(-0.00011)\n",
            "     | > avg_loss:\u001b[92m 4.00376 \u001b[0m(-0.55912)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 225/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:28:06) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/131 -- GLOBAL_STEP: 279700\u001b[0m\n",
            "     | > loss: 4.36903  (4.36903)\n",
            "     | > grad_norm: 3263.92358  (3263.92358)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.73210  (0.73209)\n",
            "     | > loader_time: 6.49000  (6.49004)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 25/131 -- GLOBAL_STEP: 279725\u001b[0m\n",
            "     | > loss: 3.96484  (4.42936)\n",
            "     | > grad_norm: 757.56689  (3488.38525)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69440  (0.67173)\n",
            "     | > loader_time: 0.00850  (0.80272)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/131 -- GLOBAL_STEP: 279750\u001b[0m\n",
            "     | > loss: 5.34426  (4.58998)\n",
            "     | > grad_norm: 7855.85791  (4204.99561)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66220  (0.67240)\n",
            "     | > loader_time: 0.00430  (0.78454)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 75/131 -- GLOBAL_STEP: 279775\u001b[0m\n",
            "     | > loss: 5.34412  (4.47663)\n",
            "     | > grad_norm: 5445.33057  (3870.24146)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72250  (0.67532)\n",
            "     | > loader_time: 0.00650  (0.80144)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/131 -- GLOBAL_STEP: 279800\u001b[0m\n",
            "     | > loss: 3.97395  (4.44551)\n",
            "     | > grad_norm: 2316.94556  (3761.96997)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66540  (0.67513)\n",
            "     | > loader_time: 2.15310  (0.80240)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 125/131 -- GLOBAL_STEP: 279825\u001b[0m\n",
            "     | > loss: 4.26015  (4.39921)\n",
            "     | > grad_norm: 3470.20679  (3650.66870)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71580  (0.67655)\n",
            "     | > loader_time: 1.82200  (0.79880)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.65874  (3.65874)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.63278  (3.63278)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.82352  (3.72815)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.49112  (3.98247)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.71770  (3.91628)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.78688  (3.89040)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 3.92064  (3.89544)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.46987  (4.12036)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.81560  (4.08227)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.54299  (4.02235)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00042 \u001b[0m(-0.00011)\n",
            "     | > avg_loss:\u001b[91m 4.02235 \u001b[0m(+0.01859)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 226/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:31:41) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 18/131 -- GLOBAL_STEP: 279850\u001b[0m\n",
            "     | > loss: 4.18386  (4.22718)\n",
            "     | > grad_norm: 3612.83716  (3298.19434)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65600  (0.66676)\n",
            "     | > loader_time: 0.00790  (0.71364)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 43/131 -- GLOBAL_STEP: 279875\u001b[0m\n",
            "     | > loss: 4.07531  (4.23552)\n",
            "     | > grad_norm: 1171.88733  (3206.16309)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71730  (0.68110)\n",
            "     | > loader_time: 0.00110  (0.73026)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 68/131 -- GLOBAL_STEP: 279900\u001b[0m\n",
            "     | > loss: 4.39035  (4.24098)\n",
            "     | > grad_norm: 3529.04980  (3257.77051)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67710  (0.67836)\n",
            "     | > loader_time: 3.28780  (0.79718)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 93/131 -- GLOBAL_STEP: 279925\u001b[0m\n",
            "     | > loss: 4.38635  (4.22435)\n",
            "     | > grad_norm: 5421.74512  (3323.73218)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68590  (0.67837)\n",
            "     | > loader_time: 0.33000  (0.78758)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 118/131 -- GLOBAL_STEP: 279950\u001b[0m\n",
            "     | > loss: 3.97666  (4.23424)\n",
            "     | > grad_norm: 3943.86475  (3344.42163)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71190  (0.67752)\n",
            "     | > loader_time: 0.00870  (0.77971)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.10570  (3.10570)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 5.60970  (5.60970)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.66260  (4.63615)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.58687  (4.95306)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.49419  (4.58834)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 6.08069  (4.88681)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.90816  (5.05704)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 6.38309  (5.24647)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.61139  (5.04209)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.64879  (4.88728)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00248 \u001b[0m(+0.00206)\n",
            "     | > avg_loss:\u001b[91m 4.88728 \u001b[0m(+0.86493)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 227/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:35:15) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 11/131 -- GLOBAL_STEP: 279975\u001b[0m\n",
            "     | > loss: 4.78854  (4.23396)\n",
            "     | > grad_norm: 5982.37402  (3747.25220)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67160  (0.68624)\n",
            "     | > loader_time: 0.01350  (0.54575)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 36/131 -- GLOBAL_STEP: 280000\u001b[0m\n",
            "     | > loss: 4.35685  (4.27119)\n",
            "     | > grad_norm: 3193.88208  (3797.97095)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64660  (0.68457)\n",
            "     | > loader_time: 2.63280  (0.77721)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/checkpoint_280000.pth.tar\n",
            "\n",
            "\u001b[1m   --> STEP: 61/131 -- GLOBAL_STEP: 280025\u001b[0m\n",
            "     | > loss: 4.93251  (4.36170)\n",
            "     | > grad_norm: 5737.97363  (3793.12183)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.73070  (0.68601)\n",
            "     | > loader_time: 0.01810  (0.75634)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 86/131 -- GLOBAL_STEP: 280050\u001b[0m\n",
            "     | > loss: 4.50080  (4.39820)\n",
            "     | > grad_norm: 5982.24023  (3685.31836)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70840  (0.68690)\n",
            "     | > loader_time: 0.00960  (0.74909)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 111/131 -- GLOBAL_STEP: 280075\u001b[0m\n",
            "     | > loss: 4.35669  (4.37719)\n",
            "     | > grad_norm: 3607.16846  (3586.88306)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65330  (0.68587)\n",
            "     | > loader_time: 0.01010  (0.74285)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 4.56640  (4.56640)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 5.17502  (5.17502)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.02785  (4.60143)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.07367  (4.75885)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 4.25939  (4.63398)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 4.18422  (4.54403)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.31532  (4.67258)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.04918  (4.72638)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.20427  (4.66111)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 4.06510  (4.59489)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00058 \u001b[0m(-0.00191)\n",
            "     | > avg_loss:\u001b[92m 4.59489 \u001b[0m(-0.29239)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 228/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:38:47) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 4/131 -- GLOBAL_STEP: 280100\u001b[0m\n",
            "     | > loss: 4.01487  (4.14785)\n",
            "     | > grad_norm: 1268.02905  (2527.76221)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69370  (0.68302)\n",
            "     | > loader_time: 3.10240  (0.78136)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 29/131 -- GLOBAL_STEP: 280125\u001b[0m\n",
            "     | > loss: 4.71223  (4.29088)\n",
            "     | > grad_norm: 5075.26660  (3113.43750)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67680  (0.67603)\n",
            "     | > loader_time: 0.02110  (0.77675)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 54/131 -- GLOBAL_STEP: 280150\u001b[0m\n",
            "     | > loss: 5.35372  (4.35177)\n",
            "     | > grad_norm: 8052.43408  (3778.59058)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67500  (0.68388)\n",
            "     | > loader_time: 0.00370  (0.76669)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 79/131 -- GLOBAL_STEP: 280175\u001b[0m\n",
            "     | > loss: 4.20890  (4.37260)\n",
            "     | > grad_norm: 4129.83154  (3947.69507)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69970  (0.68502)\n",
            "     | > loader_time: 0.00280  (0.77513)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 104/131 -- GLOBAL_STEP: 280200\u001b[0m\n",
            "     | > loss: 4.56523  (4.43982)\n",
            "     | > grad_norm: 5236.14941  (4164.59424)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65010  (0.68587)\n",
            "     | > loader_time: 3.11610  (0.79474)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 129/131 -- GLOBAL_STEP: 280225\u001b[0m\n",
            "     | > loss: 4.30985  (4.44255)\n",
            "     | > grad_norm: 5754.69092  (4232.63916)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.54570  (0.68188)\n",
            "     | > loader_time: 0.00060  (0.76451)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 4.90218  (4.90218)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.89827  (4.89827)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.44137  (4.66982)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.48319  (4.94094)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 5.51838  (5.08530)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 5.59043  (5.18633)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.19744  (5.18818)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.48084  (5.08713)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 5.84187  (5.18147)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 6.38134  (5.31479)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00267 \u001b[0m(+0.00209)\n",
            "     | > avg_loss:\u001b[91m 5.31479 \u001b[0m(+0.71990)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 229/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:42:20) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/131 -- GLOBAL_STEP: 280250\u001b[0m\n",
            "     | > loss: 3.99805  (3.98401)\n",
            "     | > grad_norm: 3452.02734  (2666.20337)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.74720  (0.68731)\n",
            "     | > loader_time: 0.01410  (0.73653)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 47/131 -- GLOBAL_STEP: 280275\u001b[0m\n",
            "     | > loss: 3.68232  (3.88776)\n",
            "     | > grad_norm: 1164.18958  (2580.01074)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69760  (0.68694)\n",
            "     | > loader_time: 0.00670  (0.76232)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 72/131 -- GLOBAL_STEP: 280300\u001b[0m\n",
            "     | > loss: 3.99001  (3.89121)\n",
            "     | > grad_norm: 4787.32324  (2709.79053)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.62920  (0.68339)\n",
            "     | > loader_time: 3.04300  (0.81908)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 97/131 -- GLOBAL_STEP: 280325\u001b[0m\n",
            "     | > loss: 3.66047  (3.88327)\n",
            "     | > grad_norm: 1926.87634  (2844.57471)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64200  (0.68067)\n",
            "     | > loader_time: 1.11510  (0.81484)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 122/131 -- GLOBAL_STEP: 280350\u001b[0m\n",
            "     | > loss: 4.05504  (3.87448)\n",
            "     | > grad_norm: 5595.62744  (2972.46704)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70360  (0.68046)\n",
            "     | > loader_time: 0.56560  (0.81534)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.09150  (3.09150)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 5.33502  (5.33502)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.36788  (4.85145)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.61006  (5.10432)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.67669  (4.74741)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.98281  (4.39449)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.22106  (4.36559)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.40208  (4.37080)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.29000  (4.23570)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.31383  (4.13327)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00058 \u001b[0m(-0.00208)\n",
            "     | > avg_loss:\u001b[92m 4.13327 \u001b[0m(-1.18152)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 230/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:45:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/131 -- GLOBAL_STEP: 280375\u001b[0m\n",
            "     | > loss: 3.99044  (3.84466)\n",
            "     | > grad_norm: 1807.56787  (3424.10718)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67620  (0.68905)\n",
            "     | > loader_time: 0.00310  (0.66526)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 40/131 -- GLOBAL_STEP: 280400\u001b[0m\n",
            "     | > loss: 3.78463  (3.86836)\n",
            "     | > grad_norm: 3896.31274  (3424.68359)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64540  (0.67936)\n",
            "     | > loader_time: 3.42410  (0.80090)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 65/131 -- GLOBAL_STEP: 280425\u001b[0m\n",
            "     | > loss: 3.88963  (3.85979)\n",
            "     | > grad_norm: 3144.13403  (3338.37939)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64570  (0.68181)\n",
            "     | > loader_time: 0.00520  (0.80578)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/131 -- GLOBAL_STEP: 280450\u001b[0m\n",
            "     | > loss: 4.11060  (3.84356)\n",
            "     | > grad_norm: 3323.39722  (3275.25317)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.59380  (0.68181)\n",
            "     | > loader_time: 0.00210  (0.80397)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 115/131 -- GLOBAL_STEP: 280475\u001b[0m\n",
            "     | > loss: 4.75964  (3.84892)\n",
            "     | > grad_norm: 9076.98633  (3297.75684)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66090  (0.68427)\n",
            "     | > loader_time: 0.00560  (0.77824)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.89897  (2.89897)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.75469  (4.75469)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.91817  (4.33643)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.65162  (4.77483)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.84590  (4.54260)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.97870  (4.42982)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 3.37035  (4.25324)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.39776  (4.27388)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.35699  (4.15927)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.71401  (3.99869)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00048 \u001b[0m(-0.00010)\n",
            "     | > avg_loss:\u001b[92m 3.99869 \u001b[0m(-0.13458)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 231/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:49:34) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 8/131 -- GLOBAL_STEP: 280500\u001b[0m\n",
            "     | > loss: 4.22050  (4.00603)\n",
            "     | > grad_norm: 7965.61572  (4369.53271)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64510  (0.66902)\n",
            "     | > loader_time: 3.51350  (0.80167)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 33/131 -- GLOBAL_STEP: 280525\u001b[0m\n",
            "     | > loss: 4.11178  (4.07130)\n",
            "     | > grad_norm: 6259.90967  (4532.63086)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68490  (0.68344)\n",
            "     | > loader_time: 0.00510  (0.75802)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 58/131 -- GLOBAL_STEP: 280550\u001b[0m\n",
            "     | > loss: 4.23988  (4.04614)\n",
            "     | > grad_norm: 3365.37378  (4257.99951)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.74450  (0.68655)\n",
            "     | > loader_time: 0.01690  (0.75549)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 83/131 -- GLOBAL_STEP: 280575\u001b[0m\n",
            "     | > loss: 3.99112  (4.01529)\n",
            "     | > grad_norm: 2811.68359  (4000.32178)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65170  (0.68191)\n",
            "     | > loader_time: 0.00530  (0.76755)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 108/131 -- GLOBAL_STEP: 280600\u001b[0m\n",
            "     | > loss: 4.05157  (4.00108)\n",
            "     | > grad_norm: 2536.86157  (3917.39185)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67250  (0.68252)\n",
            "     | > loader_time: 3.27520  (0.78718)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 4.83295  (4.83295)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.30905  (3.30905)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.95584  (3.63244)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 3.39228  (3.55239)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.20671  (3.46597)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.97402  (3.36758)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.60935  (3.74121)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.07608  (3.78905)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.92039  (3.68047)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.01722  (3.60677)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00058 \u001b[0m(+0.00009)\n",
            "     | > avg_loss:\u001b[92m 3.60677 \u001b[0m(-0.39192)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 232/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:53:07) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 1/131 -- GLOBAL_STEP: 280625\u001b[0m\n",
            "     | > loss: 3.78005  (3.78005)\n",
            "     | > grad_norm: 3490.70728  (3490.70728)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70620  (0.70624)\n",
            "     | > loader_time: 0.00110  (0.00115)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 26/131 -- GLOBAL_STEP: 280650\u001b[0m\n",
            "     | > loss: 4.15230  (3.99114)\n",
            "     | > grad_norm: 3836.67871  (3602.01196)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64240  (0.68740)\n",
            "     | > loader_time: 0.00460  (0.74235)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 51/131 -- GLOBAL_STEP: 280675\u001b[0m\n",
            "     | > loss: 4.00680  (3.98458)\n",
            "     | > grad_norm: 3550.20020  (3560.07983)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.73970  (0.68241)\n",
            "     | > loader_time: 0.01690  (0.78382)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 76/131 -- GLOBAL_STEP: 280700\u001b[0m\n",
            "     | > loss: 3.98248  (3.99955)\n",
            "     | > grad_norm: 3115.83496  (3434.21436)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67050  (0.67653)\n",
            "     | > loader_time: 2.92700  (0.83946)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 101/131 -- GLOBAL_STEP: 280725\u001b[0m\n",
            "     | > loss: 4.23489  (3.99279)\n",
            "     | > grad_norm: 6585.82520  (3410.05444)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65280  (0.67397)\n",
            "     | > loader_time: 0.75010  (0.83056)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 126/131 -- GLOBAL_STEP: 280750\u001b[0m\n",
            "     | > loss: 3.77323  (3.99409)\n",
            "     | > grad_norm: 1664.32898  (3436.68481)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68610  (0.67444)\n",
            "     | > loader_time: 0.00070  (0.81397)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.40884  (3.40884)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.63569  (3.63569)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.26853  (3.95211)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 3.98196  (3.96206)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.99961  (3.72145)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.44689  (3.66654)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.85435  (4.03117)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.10311  (4.04145)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.44870  (3.96736)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.81194  (3.95009)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00233 \u001b[0m(+0.00175)\n",
            "     | > avg_loss:\u001b[91m 3.95009 \u001b[0m(+0.34332)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 233/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 09:56:45) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 19/131 -- GLOBAL_STEP: 280775\u001b[0m\n",
            "     | > loss: 4.35377  (4.13609)\n",
            "     | > grad_norm: 2411.88159  (3157.42261)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64400  (0.68978)\n",
            "     | > loader_time: 0.00920  (0.65910)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 44/131 -- GLOBAL_STEP: 280800\u001b[0m\n",
            "     | > loss: 3.67210  (4.02909)\n",
            "     | > grad_norm: 3766.71362  (3125.95312)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67310  (0.68369)\n",
            "     | > loader_time: 2.12320  (0.76887)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 69/131 -- GLOBAL_STEP: 280825\u001b[0m\n",
            "     | > loss: 4.75601  (4.06026)\n",
            "     | > grad_norm: 1937.80640  (3299.30542)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65840  (0.68281)\n",
            "     | > loader_time: 0.94010  (0.78460)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 94/131 -- GLOBAL_STEP: 280850\u001b[0m\n",
            "     | > loss: 4.55456  (4.17009)\n",
            "     | > grad_norm: 2992.75854  (3321.44043)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70690  (0.68483)\n",
            "     | > loader_time: 0.00530  (0.78153)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 119/131 -- GLOBAL_STEP: 280875\u001b[0m\n",
            "     | > loss: 4.11505  (4.19049)\n",
            "     | > grad_norm: 156.06799  (3221.33887)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69990  (0.68458)\n",
            "     | > loader_time: 0.01410  (0.77844)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 4.04766  (4.04766)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 5.60024  (5.60024)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.99924  (5.29974)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 6.43045  (5.67664)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.28881  (5.07968)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 4.54376  (4.97250)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 6.35709  (5.20326)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.17372  (5.05619)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.31819  (4.83894)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 4.02738  (4.74876)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00047 \u001b[0m(-0.00186)\n",
            "     | > avg_loss:\u001b[91m 4.74876 \u001b[0m(+0.79868)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 234/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:00:20) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 12/131 -- GLOBAL_STEP: 280900\u001b[0m\n",
            "     | > loss: 3.95828  (4.01441)\n",
            "     | > grad_norm: 1976.79199  (1568.64001)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70690  (0.68582)\n",
            "     | > loader_time: 3.29020  (0.80565)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 37/131 -- GLOBAL_STEP: 280925\u001b[0m\n",
            "     | > loss: 4.09815  (3.91807)\n",
            "     | > grad_norm: 3710.90723  (2335.86060)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69140  (0.68720)\n",
            "     | > loader_time: 0.00450  (0.76875)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 62/131 -- GLOBAL_STEP: 280950\u001b[0m\n",
            "     | > loss: 3.95321  (3.90597)\n",
            "     | > grad_norm: 3203.10132  (2617.69019)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69410  (0.68912)\n",
            "     | > loader_time: 0.00590  (0.75961)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 87/131 -- GLOBAL_STEP: 280975\u001b[0m\n",
            "     | > loss: 3.96774  (3.88108)\n",
            "     | > grad_norm: 2426.41455  (2731.56201)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71070  (0.68635)\n",
            "     | > loader_time: 0.00470  (0.76069)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 112/131 -- GLOBAL_STEP: 281000\u001b[0m\n",
            "     | > loss: 4.42192  (4.03405)\n",
            "     | > grad_norm: 4896.78662  (2797.39233)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66770  (0.68713)\n",
            "     | > loader_time: 2.32650  (0.77580)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.90758  (3.90758)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.53134  (3.53134)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.33587  (3.93360)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.26337  (4.37686)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 4.32833  (4.36473)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.90796  (4.27337)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.73674  (4.35060)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 6.10427  (4.60113)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.92227  (4.51627)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 4.04621  (4.46404)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00045 \u001b[0m(-0.00002)\n",
            "     | > avg_loss:\u001b[92m 4.46404 \u001b[0m(-0.28472)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 235/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:03:53) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/131 -- GLOBAL_STEP: 281025\u001b[0m\n",
            "     | > loss: 4.05377  (4.41353)\n",
            "     | > grad_norm: 2822.74829  (4818.79150)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68920  (0.68993)\n",
            "     | > loader_time: 0.01190  (0.50316)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/131 -- GLOBAL_STEP: 281050\u001b[0m\n",
            "     | > loss: 5.07659  (4.34507)\n",
            "     | > grad_norm: 6650.74268  (3649.74854)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71650  (0.67535)\n",
            "     | > loader_time: 0.00670  (0.70209)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/131 -- GLOBAL_STEP: 281075\u001b[0m\n",
            "     | > loss: 4.18817  (4.35010)\n",
            "     | > grad_norm: 2933.67676  (3890.82007)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66760  (0.67673)\n",
            "     | > loader_time: 0.09120  (0.71097)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/131 -- GLOBAL_STEP: 281100\u001b[0m\n",
            "     | > loss: 3.76468  (4.32352)\n",
            "     | > grad_norm: 1182.27686  (3929.43726)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66780  (0.67801)\n",
            "     | > loader_time: 3.11780  (0.76604)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/131 -- GLOBAL_STEP: 281125\u001b[0m\n",
            "     | > loss: 3.79217  (4.33510)\n",
            "     | > grad_norm: 2974.19507  (4069.26001)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68380  (0.67587)\n",
            "     | > loader_time: 1.21940  (0.76349)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/131 -- GLOBAL_STEP: 281150\u001b[0m\n",
            "     | > loss: 4.50486  (4.32353)\n",
            "     | > grad_norm: 5173.33740  (4041.57446)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.54460  (0.66986)\n",
            "     | > loader_time: 0.00070  (0.74614)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.55765  (2.55765)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.53672  (3.53672)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.97475  (4.25573)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.33034  (4.61394)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.56765  (4.10237)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.41767  (3.76543)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.69222  (3.91989)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 3.92569  (3.92072)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.78705  (3.77901)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.55552  (3.64307)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.5 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00057 \u001b[0m(+0.00012)\n",
            "     | > avg_loss:\u001b[92m 3.64307 \u001b[0m(-0.82097)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 236/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:07:23) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 23/131 -- GLOBAL_STEP: 281175\u001b[0m\n",
            "     | > loss: 4.13496  (4.42200)\n",
            "     | > grad_norm: 4480.15088  (4553.93262)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69520  (0.68476)\n",
            "     | > loader_time: 0.00910  (0.70886)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 48/131 -- GLOBAL_STEP: 281200\u001b[0m\n",
            "     | > loss: 5.39205  (4.37773)\n",
            "     | > grad_norm: 8086.90771  (4176.51367)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65510  (0.67424)\n",
            "     | > loader_time: 1.30930  (0.83785)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 73/131 -- GLOBAL_STEP: 281225\u001b[0m\n",
            "     | > loss: 4.44433  (4.44059)\n",
            "     | > grad_norm: 3741.61328  (4103.38672)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66210  (0.67525)\n",
            "     | > loader_time: 0.00520  (0.82962)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 98/131 -- GLOBAL_STEP: 281250\u001b[0m\n",
            "     | > loss: 4.25723  (4.39484)\n",
            "     | > grad_norm: 2160.25317  (3858.97632)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.55750  (0.67235)\n",
            "     | > loader_time: 3.87160  (0.88506)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 123/131 -- GLOBAL_STEP: 281275\u001b[0m\n",
            "     | > loss: 4.03075  (4.35160)\n",
            "     | > grad_norm: 2467.78516  (3754.93579)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.61390  (0.66929)\n",
            "     | > loader_time: 0.00360  (0.86917)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.33762  (2.33762)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.70820  (3.70820)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 6.72233  (5.21526)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.87829  (5.10294)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.81094  (4.77994)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.33627  (4.49121)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.41304  (4.64484)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 7.29495  (5.02343)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.68387  (4.98099)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.69617  (4.72712)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00064 \u001b[0m(+0.00007)\n",
            "     | > avg_loss:\u001b[91m 4.72712 \u001b[0m(+1.08405)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 237/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:11:05) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/131 -- GLOBAL_STEP: 281300\u001b[0m\n",
            "     | > loss: 3.83564  (4.13280)\n",
            "     | > grad_norm: 1401.83020  (3593.58545)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65680  (0.68941)\n",
            "     | > loader_time: 3.06630  (0.78363)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 41/131 -- GLOBAL_STEP: 281325\u001b[0m\n",
            "     | > loss: 4.23677  (4.09519)\n",
            "     | > grad_norm: 5616.56836  (3771.18994)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70190  (0.68453)\n",
            "     | > loader_time: 0.01900  (0.78367)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 66/131 -- GLOBAL_STEP: 281350\u001b[0m\n",
            "     | > loss: 4.19251  (4.09929)\n",
            "     | > grad_norm: 2504.34814  (3560.44458)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65140  (0.68788)\n",
            "     | > loader_time: 0.10350  (0.77939)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 91/131 -- GLOBAL_STEP: 281375\u001b[0m\n",
            "     | > loss: 4.25798  (4.20537)\n",
            "     | > grad_norm: 3159.75195  (3798.26685)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65080  (0.69190)\n",
            "     | > loader_time: 0.00290  (0.79632)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 116/131 -- GLOBAL_STEP: 281400\u001b[0m\n",
            "     | > loss: 4.62435  (4.21811)\n",
            "     | > grad_norm: 3958.83569  (3674.04150)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67200  (0.68897)\n",
            "     | > loader_time: 2.80690  (0.80371)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.29751  (3.29751)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.85884  (3.85884)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.43791  (3.64837)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 3.79035  (3.69570)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.12942  (3.55413)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.94018  (3.43134)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.61668  (3.79556)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 3.90699  (3.81148)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 5.36966  (4.00625)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.07257  (3.90251)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00046 \u001b[0m(-0.00018)\n",
            "     | > avg_loss:\u001b[92m 3.90251 \u001b[0m(-0.82461)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 238/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:14:41) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 9/131 -- GLOBAL_STEP: 281425\u001b[0m\n",
            "     | > loss: 4.08886  (4.19286)\n",
            "     | > grad_norm: 2631.70679  (3154.06421)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64740  (0.66923)\n",
            "     | > loader_time: 0.00620  (0.71050)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 34/131 -- GLOBAL_STEP: 281450\u001b[0m\n",
            "     | > loss: 4.26809  (4.20061)\n",
            "     | > grad_norm: 2350.44971  (3248.73560)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70590  (0.67768)\n",
            "     | > loader_time: 0.02370  (0.78122)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 59/131 -- GLOBAL_STEP: 281475\u001b[0m\n",
            "     | > loss: 4.11237  (4.19513)\n",
            "     | > grad_norm: 3583.04224  (3280.20093)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65190  (0.68018)\n",
            "     | > loader_time: 0.00840  (0.76806)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 84/131 -- GLOBAL_STEP: 281500\u001b[0m\n",
            "     | > loss: 3.83306  (4.19302)\n",
            "     | > grad_norm: 886.77814  (3263.73926)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68240  (0.67784)\n",
            "     | > loader_time: 2.66920  (0.80517)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 109/131 -- GLOBAL_STEP: 281525\u001b[0m\n",
            "     | > loss: 4.45550  (4.20095)\n",
            "     | > grad_norm: 4028.64111  (3290.64551)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69730  (0.67945)\n",
            "     | > loader_time: 0.00960  (0.78555)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.04579  (3.04579)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.30566  (3.30566)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.96172  (3.63369)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.12600  (4.13113)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.64578  (4.00979)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.81542  (3.97092)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.57317  (4.23796)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.33269  (4.39435)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.03774  (4.22477)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 5.80634  (4.40050)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.5 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00042 \u001b[0m(-0.00005)\n",
            "     | > avg_loss:\u001b[91m 4.40050 \u001b[0m(+0.49799)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 239/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:18:15) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/131 -- GLOBAL_STEP: 281550\u001b[0m\n",
            "     | > loss: 4.56059  (4.42095)\n",
            "     | > grad_norm: 3543.76562  (3701.89111)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70500  (0.71416)\n",
            "     | > loader_time: 0.01110  (0.00797)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 27/131 -- GLOBAL_STEP: 281575\u001b[0m\n",
            "     | > loss: 4.39906  (4.19491)\n",
            "     | > grad_norm: 2104.49902  (3171.66797)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.62900  (0.68939)\n",
            "     | > loader_time: 0.00470  (0.77871)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 52/131 -- GLOBAL_STEP: 281600\u001b[0m\n",
            "     | > loss: 4.81970  (4.20363)\n",
            "     | > grad_norm: 3901.20459  (3093.84375)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64770  (0.69183)\n",
            "     | > loader_time: 3.61050  (0.89233)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 77/131 -- GLOBAL_STEP: 281625\u001b[0m\n",
            "     | > loss: 4.10097  (4.21973)\n",
            "     | > grad_norm: 3440.60718  (3198.78271)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70760  (0.68583)\n",
            "     | > loader_time: 0.01650  (0.86775)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 102/131 -- GLOBAL_STEP: 281650\u001b[0m\n",
            "     | > loss: 3.76185  (4.22434)\n",
            "     | > grad_norm: 1049.28613  (3185.19507)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65520  (0.68272)\n",
            "     | > loader_time: 0.01210  (0.84685)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 127/131 -- GLOBAL_STEP: 281675\u001b[0m\n",
            "     | > loss: 4.29703  (4.22726)\n",
            "     | > grad_norm: 2821.29761  (3247.23047)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.54570  (0.67884)\n",
            "     | > loader_time: 0.00080  (0.83481)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.80664  (3.80664)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.27922  (3.27922)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.35382  (3.81652)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.24642  (4.29316)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.29803  (4.04437)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.25442  (3.88638)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.80649  (4.20640)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.23868  (4.21101)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.35220  (4.10366)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.68693  (4.05736)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00036 \u001b[0m(-0.00006)\n",
            "     | > avg_loss:\u001b[92m 4.05736 \u001b[0m(-0.34315)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 240/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:21:56) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/131 -- GLOBAL_STEP: 281700\u001b[0m\n",
            "     | > loss: 4.62856  (4.27221)\n",
            "     | > grad_norm: 5242.96973  (3115.95752)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65470  (0.68456)\n",
            "     | > loader_time: 3.16510  (0.83731)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 45/131 -- GLOBAL_STEP: 281725\u001b[0m\n",
            "     | > loss: 4.07695  (4.25130)\n",
            "     | > grad_norm: 3187.83716  (3194.95312)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68800  (0.68982)\n",
            "     | > loader_time: 0.00900  (0.80381)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/131 -- GLOBAL_STEP: 281750\u001b[0m\n",
            "     | > loss: 3.93498  (4.25391)\n",
            "     | > grad_norm: 2274.61450  (3269.18359)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65600  (0.68681)\n",
            "     | > loader_time: 0.00910  (0.81332)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 95/131 -- GLOBAL_STEP: 281775\u001b[0m\n",
            "     | > loss: 4.08898  (4.24699)\n",
            "     | > grad_norm: 3276.15894  (3321.41309)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72980  (0.68380)\n",
            "     | > loader_time: 0.00450  (0.81303)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/131 -- GLOBAL_STEP: 281800\u001b[0m\n",
            "     | > loss: 3.80231  (4.24178)\n",
            "     | > grad_norm: 359.51212  (3344.00122)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65770  (0.68594)\n",
            "     | > loader_time: 2.81460  (0.81254)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.87204  (2.87204)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.24570  (3.24570)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 5.01325  (4.12948)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.83978  (4.36625)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.64537  (4.18603)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.17267  (3.98336)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.12975  (4.17442)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.24262  (4.18416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.19802  (4.18590)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.58058  (4.11864)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00258 \u001b[0m(+0.00221)\n",
            "     | > avg_loss:\u001b[91m 4.11864 \u001b[0m(+0.06128)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 241/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:25:32) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 13/131 -- GLOBAL_STEP: 281825\u001b[0m\n",
            "     | > loss: 4.37102  (4.24467)\n",
            "     | > grad_norm: 4451.01953  (3143.25586)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65480  (0.66871)\n",
            "     | > loader_time: 0.00560  (0.75377)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 38/131 -- GLOBAL_STEP: 281850\u001b[0m\n",
            "     | > loss: 4.46752  (4.35842)\n",
            "     | > grad_norm: 3922.68066  (3799.31177)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64990  (0.67292)\n",
            "     | > loader_time: 0.00320  (0.76096)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 63/131 -- GLOBAL_STEP: 281875\u001b[0m\n",
            "     | > loss: 4.13124  (4.31446)\n",
            "     | > grad_norm: 1758.55457  (3492.82690)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65570  (0.67474)\n",
            "     | > loader_time: 0.00330  (0.76094)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/131 -- GLOBAL_STEP: 281900\u001b[0m\n",
            "     | > loss: 4.41754  (4.31059)\n",
            "     | > grad_norm: 951.53516  (3326.42456)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66510  (0.67862)\n",
            "     | > loader_time: 3.22670  (0.79510)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 113/131 -- GLOBAL_STEP: 281925\u001b[0m\n",
            "     | > loss: 5.27619  (4.29990)\n",
            "     | > grad_norm: 5633.33057  (3206.78516)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69790  (0.67945)\n",
            "     | > loader_time: 0.01220  (0.77040)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.93260  (3.93260)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.05605  (4.05605)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.29292  (4.17448)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.56014  (4.30303)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.81202  (4.18028)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.91301  (4.12683)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 6.09829  (4.45540)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.47067  (4.60044)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.78169  (4.49810)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 4.11871  (4.45595)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00051 \u001b[0m(-0.00207)\n",
            "     | > avg_loss:\u001b[91m 4.45595 \u001b[0m(+0.33731)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 242/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:29:04) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 6/131 -- GLOBAL_STEP: 281950\u001b[0m\n",
            "     | > loss: 3.83138  (4.38852)\n",
            "     | > grad_norm: 4009.99219  (3257.56396)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69270  (0.70361)\n",
            "     | > loader_time: 0.00580  (0.49212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 31/131 -- GLOBAL_STEP: 281975\u001b[0m\n",
            "     | > loss: 3.68755  (4.28279)\n",
            "     | > grad_norm: 2752.06152  (3626.95068)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67580  (0.69510)\n",
            "     | > loader_time: 0.00280  (0.72450)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 56/131 -- GLOBAL_STEP: 282000\u001b[0m\n",
            "     | > loss: 3.89555  (4.26095)\n",
            "     | > grad_norm: 2294.19043  (3887.84766)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66510  (0.68813)\n",
            "     | > loader_time: 2.96140  (0.80419)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 81/131 -- GLOBAL_STEP: 282025\u001b[0m\n",
            "     | > loss: 3.99025  (4.26978)\n",
            "     | > grad_norm: 2726.25781  (3966.73047)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69840  (0.68762)\n",
            "     | > loader_time: 0.00590  (0.80299)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 106/131 -- GLOBAL_STEP: 282050\u001b[0m\n",
            "     | > loss: 4.64728  (4.32153)\n",
            "     | > grad_norm: 7031.78564  (4198.05176)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68950  (0.68516)\n",
            "     | > loader_time: 0.00460  (0.78177)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 131/131 -- GLOBAL_STEP: 282075\u001b[0m\n",
            "     | > loss: 4.82303  (4.31618)\n",
            "     | > grad_norm: 6038.40967  (4265.10449)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.41110  (0.67735)\n",
            "     | > loader_time: 0.00050  (0.75627)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 4.46862  (4.46862)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.69756  (3.69756)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.63131  (4.16443)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.51899  (4.28262)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.92713  (4.19375)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 4.04705  (4.16441)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 3.53172  (4.05896)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 6.27160  (4.37505)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.05576  (4.33514)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.74789  (4.26989)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.5 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00049 \u001b[0m(-0.00002)\n",
            "     | > avg_loss:\u001b[92m 4.26989 \u001b[0m(-0.18606)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 243/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:32:38) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 24/131 -- GLOBAL_STEP: 282100\u001b[0m\n",
            "     | > loss: 4.10607  (4.26669)\n",
            "     | > grad_norm: 2715.32275  (3673.63257)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.61170  (0.68503)\n",
            "     | > loader_time: 3.53720  (0.84072)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 49/131 -- GLOBAL_STEP: 282125\u001b[0m\n",
            "     | > loss: 4.22722  (4.23572)\n",
            "     | > grad_norm: 2567.63550  (3459.23047)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66620  (0.68740)\n",
            "     | > loader_time: 0.00850  (0.81737)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 74/131 -- GLOBAL_STEP: 282150\u001b[0m\n",
            "     | > loss: 4.38540  (4.24479)\n",
            "     | > grad_norm: 4016.41870  (3271.71484)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72210  (0.69111)\n",
            "     | > loader_time: 0.00370  (0.80240)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 99/131 -- GLOBAL_STEP: 282175\u001b[0m\n",
            "     | > loss: 4.31311  (4.24067)\n",
            "     | > grad_norm: 2653.81323  (3231.17651)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70120  (0.68787)\n",
            "     | > loader_time: 0.00600  (0.78357)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 124/131 -- GLOBAL_STEP: 282200\u001b[0m\n",
            "     | > loss: 4.11290  (4.24748)\n",
            "     | > grad_norm: 3040.63696  (3294.37622)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66990  (0.68584)\n",
            "     | > loader_time: 1.09200  (0.79323)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.03297  (3.03297)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.43419  (4.43419)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.17002  (4.30210)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.13473  (4.57965)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.43666  (4.04390)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 4.48097  (4.13131)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 2.99599  (3.94209)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.28040  (3.99042)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.70689  (3.82998)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.55869  (3.79984)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00050 \u001b[0m(+0.00001)\n",
            "     | > avg_loss:\u001b[92m 3.79984 \u001b[0m(-0.47005)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 244/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:36:16) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 17/131 -- GLOBAL_STEP: 282225\u001b[0m\n",
            "     | > loss: 4.48799  (4.30622)\n",
            "     | > grad_norm: 3016.06372  (3926.39941)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.73660  (0.68851)\n",
            "     | > loader_time: 0.00640  (0.74207)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 42/131 -- GLOBAL_STEP: 282250\u001b[0m\n",
            "     | > loss: 3.82102  (4.27591)\n",
            "     | > grad_norm: 930.97699  (3544.35864)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64970  (0.68290)\n",
            "     | > loader_time: 0.00980  (0.75198)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 67/131 -- GLOBAL_STEP: 282275\u001b[0m\n",
            "     | > loss: 4.24290  (4.25549)\n",
            "     | > grad_norm: 4127.86963  (3554.02026)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66580  (0.68864)\n",
            "     | > loader_time: 0.01830  (0.75465)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 92/131 -- GLOBAL_STEP: 282300\u001b[0m\n",
            "     | > loss: 4.72399  (4.26996)\n",
            "     | > grad_norm: 4111.61914  (3452.62646)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66840  (0.68909)\n",
            "     | > loader_time: 2.64320  (0.79485)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 117/131 -- GLOBAL_STEP: 282325\u001b[0m\n",
            "     | > loss: 3.86347  (4.27359)\n",
            "     | > grad_norm: 695.31592  (3336.11255)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67310  (0.68633)\n",
            "     | > loader_time: 0.02000  (0.77639)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.69190  (2.69190)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 5.79418  (5.79418)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.88711  (4.84065)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.47885  (4.72005)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 4.25916  (4.60483)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 6.12516  (4.90889)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.37583  (4.82005)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.33318  (4.89335)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.88599  (4.76743)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 4.09122  (4.69230)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00264 \u001b[0m(+0.00214)\n",
            "     | > avg_loss:\u001b[91m 4.69230 \u001b[0m(+0.89246)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 245/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:39:51) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/131 -- GLOBAL_STEP: 282350\u001b[0m\n",
            "     | > loss: 4.08038  (4.05393)\n",
            "     | > grad_norm: 4205.81738  (3433.26245)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69660  (0.69756)\n",
            "     | > loader_time: 0.00580  (0.63996)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 35/131 -- GLOBAL_STEP: 282375\u001b[0m\n",
            "     | > loss: 3.79415  (4.02718)\n",
            "     | > grad_norm: 3547.19507  (3472.39746)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63310  (0.68868)\n",
            "     | > loader_time: 0.00740  (0.70745)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/131 -- GLOBAL_STEP: 282400\u001b[0m\n",
            "     | > loss: 3.87566  (4.02928)\n",
            "     | > grad_norm: 993.37567  (3357.43945)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.73480  (0.68484)\n",
            "     | > loader_time: 3.07260  (0.76893)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 85/131 -- GLOBAL_STEP: 282425\u001b[0m\n",
            "     | > loss: 4.50385  (4.03854)\n",
            "     | > grad_norm: 5755.33887  (3462.70386)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64840  (0.68216)\n",
            "     | > loader_time: 0.00480  (0.76490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 110/131 -- GLOBAL_STEP: 282450\u001b[0m\n",
            "     | > loss: 3.96092  (4.05133)\n",
            "     | > grad_norm: 5972.90283  (3598.32690)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69310  (0.68393)\n",
            "     | > loader_time: 0.00630  (0.75564)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.54574  (3.54574)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.10807  (4.10807)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.90853  (4.00830)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.20659  (4.07440)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.63565  (3.96471)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.52117  (3.67600)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.36307  (3.79051)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 6.26757  (4.14438)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.53486  (3.94319)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.29006  (3.87062)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.5 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00105 \u001b[0m(-0.00158)\n",
            "     | > avg_loss:\u001b[92m 3.87062 \u001b[0m(-0.82168)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 246/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:43:23) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 3/131 -- GLOBAL_STEP: 282475\u001b[0m\n",
            "     | > loss: 3.67191  (3.93685)\n",
            "     | > grad_norm: 1567.71558  (4426.20312)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70620  (0.70531)\n",
            "     | > loader_time: 0.01650  (0.00909)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 28/131 -- GLOBAL_STEP: 282500\u001b[0m\n",
            "     | > loss: 4.03804  (4.13784)\n",
            "     | > grad_norm: 3890.19458  (4141.24951)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65170  (0.67148)\n",
            "     | > loader_time: 3.18800  (0.80213)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 53/131 -- GLOBAL_STEP: 282525\u001b[0m\n",
            "     | > loss: 4.19744  (4.12072)\n",
            "     | > grad_norm: 5300.79541  (3992.63013)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68300  (0.66529)\n",
            "     | > loader_time: 1.27580  (0.82196)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 78/131 -- GLOBAL_STEP: 282550\u001b[0m\n",
            "     | > loss: 4.10640  (4.09485)\n",
            "     | > grad_norm: 6276.60742  (3887.38062)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68580  (0.66326)\n",
            "     | > loader_time: 0.02000  (0.83647)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 103/131 -- GLOBAL_STEP: 282575\u001b[0m\n",
            "     | > loss: 4.16904  (4.09125)\n",
            "     | > grad_norm: 1520.80847  (3838.51953)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63970  (0.66644)\n",
            "     | > loader_time: 0.00220  (0.83249)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 128/131 -- GLOBAL_STEP: 282600\u001b[0m\n",
            "     | > loss: 3.83784  (4.07058)\n",
            "     | > grad_norm: 2148.68066  (3822.07910)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.57640  (0.66479)\n",
            "     | > loader_time: 0.00070  (0.81691)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.85362  (2.85362)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.61516  (3.61516)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.86007  (3.73762)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.05692  (4.17739)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.59525  (3.78185)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.45090  (3.51566)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.52782  (3.85102)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.86767  (4.13912)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.47134  (3.93064)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.50473  (3.77221)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00060 \u001b[0m(-0.00046)\n",
            "     | > avg_loss:\u001b[92m 3.77221 \u001b[0m(-0.09841)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 247/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:47:02) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 21/131 -- GLOBAL_STEP: 282625\u001b[0m\n",
            "     | > loss: 4.40571  (3.97909)\n",
            "     | > grad_norm: 5541.35400  (3711.32007)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66160  (0.66950)\n",
            "     | > loader_time: 0.21220  (0.79609)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 46/131 -- GLOBAL_STEP: 282650\u001b[0m\n",
            "     | > loss: 4.28064  (3.99879)\n",
            "     | > grad_norm: 5622.78809  (3581.01709)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69780  (0.67300)\n",
            "     | > loader_time: 0.00500  (0.79304)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 71/131 -- GLOBAL_STEP: 282675\u001b[0m\n",
            "     | > loss: 4.02878  (4.00760)\n",
            "     | > grad_norm: 4596.64941  (3491.11816)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69630  (0.67419)\n",
            "     | > loader_time: 0.00450  (0.79169)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 96/131 -- GLOBAL_STEP: 282700\u001b[0m\n",
            "     | > loss: 3.75759  (3.99184)\n",
            "     | > grad_norm: 3112.77197  (3496.51807)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69540  (0.67568)\n",
            "     | > loader_time: 2.36300  (0.80547)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 121/131 -- GLOBAL_STEP: 282725\u001b[0m\n",
            "     | > loss: 4.04005  (3.98648)\n",
            "     | > grad_norm: 4970.99463  (3501.45483)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.62020  (0.67456)\n",
            "     | > loader_time: 2.26740  (0.80615)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.23536  (3.23536)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.24923  (3.24923)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.98992  (3.61958)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 6.80570  (4.68162)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.00221  (4.26177)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.68588  (3.94659)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.70255  (4.07258)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 3.76012  (4.02795)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.65040  (3.85575)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.82048  (3.74072)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00048 \u001b[0m(-0.00012)\n",
            "     | > avg_loss:\u001b[92m 3.74072 \u001b[0m(-0.03149)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 248/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:50:38) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 14/131 -- GLOBAL_STEP: 282750\u001b[0m\n",
            "     | > loss: 3.79929  (4.00618)\n",
            "     | > grad_norm: 5192.70508  (3426.25415)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.76080  (0.68593)\n",
            "     | > loader_time: 0.01210  (0.72968)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 39/131 -- GLOBAL_STEP: 282775\u001b[0m\n",
            "     | > loss: 3.94048  (3.99217)\n",
            "     | > grad_norm: 2418.52417  (3377.59863)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72550  (0.68382)\n",
            "     | > loader_time: 0.01590  (0.72462)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 64/131 -- GLOBAL_STEP: 282800\u001b[0m\n",
            "     | > loss: 4.25298  (4.00491)\n",
            "     | > grad_norm: 3895.58496  (3377.41699)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.61290  (0.68107)\n",
            "     | > loader_time: 3.12660  (0.78291)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 89/131 -- GLOBAL_STEP: 282825\u001b[0m\n",
            "     | > loss: 4.22608  (4.01405)\n",
            "     | > grad_norm: 4327.11328  (3396.17725)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71970  (0.68271)\n",
            "     | > loader_time: 0.01380  (0.78431)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 114/131 -- GLOBAL_STEP: 282850\u001b[0m\n",
            "     | > loss: 4.11082  (4.02121)\n",
            "     | > grad_norm: 4050.91479  (3401.85474)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68620  (0.68360)\n",
            "     | > loader_time: 0.00450  (0.76059)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 4.13542  (4.13542)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.17570  (4.17570)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.18792  (4.18181)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.57969  (4.31444)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.78536  (3.93217)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 4.62824  (4.07138)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 6.37771  (4.45577)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.29080  (4.43220)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.69868  (4.46551)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 4.08681  (4.42343)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00251 \u001b[0m(+0.00203)\n",
            "     | > avg_loss:\u001b[91m 4.42343 \u001b[0m(+0.68271)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 249/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:54:10) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 7/131 -- GLOBAL_STEP: 282875\u001b[0m\n",
            "     | > loss: 3.70120  (3.99448)\n",
            "     | > grad_norm: 2947.05029  (3885.89331)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68880  (0.69123)\n",
            "     | > loader_time: 0.00660  (0.37646)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 32/131 -- GLOBAL_STEP: 282900\u001b[0m\n",
            "     | > loss: 4.04260  (4.00899)\n",
            "     | > grad_norm: 2760.72681  (3652.52368)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63950  (0.69129)\n",
            "     | > loader_time: 3.07480  (0.78520)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 57/131 -- GLOBAL_STEP: 282925\u001b[0m\n",
            "     | > loss: 4.09687  (4.00142)\n",
            "     | > grad_norm: 4883.69727  (3591.17017)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.62560  (0.69056)\n",
            "     | > loader_time: 0.00320  (0.75153)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 82/131 -- GLOBAL_STEP: 282950\u001b[0m\n",
            "     | > loss: 3.80836  (3.99599)\n",
            "     | > grad_norm: 4192.17285  (3500.58032)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65360  (0.68757)\n",
            "     | > loader_time: 0.00630  (0.74493)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 107/131 -- GLOBAL_STEP: 282975\u001b[0m\n",
            "     | > loss: 3.86219  (3.99205)\n",
            "     | > grad_norm: 1929.19092  (3462.03687)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63690  (0.68706)\n",
            "     | > loader_time: 0.02070  (0.73955)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.75355  (2.75355)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.06636  (4.06636)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.92999  (3.99818)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.12026  (4.03887)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.34980  (3.61660)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.32111  (3.35750)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 3.62843  (3.40266)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.65220  (3.58116)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.50990  (3.57226)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.67612  (3.47268)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00043 \u001b[0m(-0.00209)\n",
            "     | > avg_loss:\u001b[92m 3.47268 \u001b[0m(-0.95075)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 250/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 10:57:41) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/131 -- GLOBAL_STEP: 283000\u001b[0m\n",
            "     | > loss: 4.12210  (4.12210)\n",
            "     | > grad_norm: 3402.00879  (3402.00879)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.73030  (0.73032)\n",
            "     | > loader_time: 6.75470  (6.75470)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 25/131 -- GLOBAL_STEP: 283025\u001b[0m\n",
            "     | > loss: 4.01773  (4.00000)\n",
            "     | > grad_norm: 5136.66406  (3429.97949)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.73830  (0.68508)\n",
            "     | > loader_time: 0.01700  (0.82460)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/131 -- GLOBAL_STEP: 283050\u001b[0m\n",
            "     | > loss: 3.92756  (4.00113)\n",
            "     | > grad_norm: 4307.15088  (3428.67578)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67020  (0.68159)\n",
            "     | > loader_time: 0.01140  (0.82427)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 75/131 -- GLOBAL_STEP: 283075\u001b[0m\n",
            "     | > loss: 3.95401  (3.99546)\n",
            "     | > grad_norm: 2524.67334  (3415.63159)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67110  (0.68316)\n",
            "     | > loader_time: 0.00420  (0.84122)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/131 -- GLOBAL_STEP: 283100\u001b[0m\n",
            "     | > loss: 4.00743  (4.14795)\n",
            "     | > grad_norm: 2912.34839  (3414.06909)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65460  (0.68282)\n",
            "     | > loader_time: 3.03240  (0.85465)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 125/131 -- GLOBAL_STEP: 283125\u001b[0m\n",
            "     | > loss: 4.06025  (4.12424)\n",
            "     | > grad_norm: 4526.07520  (3376.23438)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.55000  (0.68297)\n",
            "     | > loader_time: 0.00070  (0.83431)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.36092  (3.36092)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.26096  (3.26096)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.05533  (3.15814)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.21927  (3.51185)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.82688  (3.34061)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.71028  (3.41454)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.96396  (3.83945)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 3.78663  (3.83190)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.13272  (3.86950)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.63750  (3.73261)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00043 \u001b[0m(+0.00001)\n",
            "     | > avg_loss:\u001b[91m 3.73261 \u001b[0m(+0.25993)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 251/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:01:20) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 18/131 -- GLOBAL_STEP: 283150\u001b[0m\n",
            "     | > loss: 4.04739  (4.03669)\n",
            "     | > grad_norm: 4387.55029  (3522.86694)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70270  (0.66793)\n",
            "     | > loader_time: 0.01480  (0.75079)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 43/131 -- GLOBAL_STEP: 283175\u001b[0m\n",
            "     | > loss: 3.94842  (4.02627)\n",
            "     | > grad_norm: 2591.75269  (3566.36523)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67730  (0.67515)\n",
            "     | > loader_time: 0.00500  (0.75602)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 68/131 -- GLOBAL_STEP: 283200\u001b[0m\n",
            "     | > loss: 3.69382  (4.02120)\n",
            "     | > grad_norm: 1389.38062  (3503.03809)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67140  (0.67654)\n",
            "     | > loader_time: 3.74230  (0.81554)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 93/131 -- GLOBAL_STEP: 283225\u001b[0m\n",
            "     | > loss: 4.18137  (4.01934)\n",
            "     | > grad_norm: 4397.82373  (3507.73047)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67710  (0.67842)\n",
            "     | > loader_time: 0.88190  (0.81086)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 118/131 -- GLOBAL_STEP: 283250\u001b[0m\n",
            "     | > loss: 3.94623  (4.02174)\n",
            "     | > grad_norm: 4346.58447  (3507.78784)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67350  (0.67938)\n",
            "     | > loader_time: 0.00520  (0.79806)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.94300  (2.94300)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.95966  (4.95966)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.86363  (4.41164)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.23322  (4.68550)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.17471  (4.30781)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 4.50927  (4.34810)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.13768  (4.47970)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.76440  (4.52037)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.31461  (4.36965)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.73206  (4.29880)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00044 \u001b[0m(+0.00001)\n",
            "     | > avg_loss:\u001b[91m 4.29880 \u001b[0m(+0.56619)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 252/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:04:56) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 11/131 -- GLOBAL_STEP: 283275\u001b[0m\n",
            "     | > loss: 4.25149  (3.94649)\n",
            "     | > grad_norm: 2985.72363  (3528.53467)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68250  (0.68445)\n",
            "     | > loader_time: 0.00400  (0.57447)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 36/131 -- GLOBAL_STEP: 283300\u001b[0m\n",
            "     | > loss: 3.76694  (3.95068)\n",
            "     | > grad_norm: 2039.11987  (3463.28125)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67340  (0.69110)\n",
            "     | > loader_time: 2.61650  (0.77952)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 61/131 -- GLOBAL_STEP: 283325\u001b[0m\n",
            "     | > loss: 4.09350  (3.97341)\n",
            "     | > grad_norm: 2854.07007  (3508.16113)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71060  (0.68978)\n",
            "     | > loader_time: 0.01200  (0.77487)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 86/131 -- GLOBAL_STEP: 283350\u001b[0m\n",
            "     | > loss: 3.89575  (3.98447)\n",
            "     | > grad_norm: 4861.05811  (3525.31177)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69120  (0.68591)\n",
            "     | > loader_time: 0.00100  (0.77611)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 111/131 -- GLOBAL_STEP: 283375\u001b[0m\n",
            "     | > loss: 3.74042  (3.99177)\n",
            "     | > grad_norm: 3534.20874  (3547.23438)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65400  (0.68500)\n",
            "     | > loader_time: 0.01940  (0.76492)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.58270  (2.58270)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 5.86629  (5.86629)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.50442  (4.68535)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.35152  (4.57407)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.27256  (4.24870)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 6.07930  (4.61482)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 6.57610  (4.94170)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.76093  (4.91588)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.62060  (4.62897)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.66171  (4.41038)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00041 \u001b[0m(-0.00003)\n",
            "     | > avg_loss:\u001b[91m 4.41038 \u001b[0m(+0.11158)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 253/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:08:28) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 4/131 -- GLOBAL_STEP: 283400\u001b[0m\n",
            "     | > loss: 4.21398  (4.03255)\n",
            "     | > grad_norm: 1420.72241  (3464.74365)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65610  (0.69219)\n",
            "     | > loader_time: 3.02260  (0.75805)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 29/131 -- GLOBAL_STEP: 283425\u001b[0m\n",
            "     | > loss: 4.19681  (3.98870)\n",
            "     | > grad_norm: 4380.91309  (3550.97168)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71860  (0.69047)\n",
            "     | > loader_time: 0.01350  (0.81251)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 54/131 -- GLOBAL_STEP: 283450\u001b[0m\n",
            "     | > loss: 4.24131  (4.00266)\n",
            "     | > grad_norm: 4441.58350  (3521.19556)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68330  (0.68590)\n",
            "     | > loader_time: 0.00620  (0.81777)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 79/131 -- GLOBAL_STEP: 283475\u001b[0m\n",
            "     | > loss: 3.75514  (4.00160)\n",
            "     | > grad_norm: 3560.11597  (3503.05005)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65250  (0.68481)\n",
            "     | > loader_time: 0.00350  (0.82601)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 104/131 -- GLOBAL_STEP: 283500\u001b[0m\n",
            "     | > loss: 3.59860  (4.00819)\n",
            "     | > grad_norm: 2093.32983  (3421.08105)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66410  (0.68454)\n",
            "     | > loader_time: 2.61010  (0.82829)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 129/131 -- GLOBAL_STEP: 283525\u001b[0m\n",
            "     | > loss: 4.38920  (4.01781)\n",
            "     | > grad_norm: 4237.45703  (3426.38354)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.54440  (0.68348)\n",
            "     | > loader_time: 0.00060  (0.80154)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.68595  (3.68595)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.26427  (4.26427)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.91714  (4.59071)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.32081  (4.50074)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.35436  (3.96415)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.74874  (3.92107)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.39290  (3.99971)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.19186  (4.17001)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.31414  (4.06303)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 4.72627  (4.13672)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00055 \u001b[0m(+0.00014)\n",
            "     | > avg_loss:\u001b[92m 4.13672 \u001b[0m(-0.27366)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 254/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:12:07) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/131 -- GLOBAL_STEP: 283550\u001b[0m\n",
            "     | > loss: 4.20616  (4.04817)\n",
            "     | > grad_norm: 4215.51904  (3613.32812)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71170  (0.68534)\n",
            "     | > loader_time: 0.02540  (0.71769)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 47/131 -- GLOBAL_STEP: 283575\u001b[0m\n",
            "     | > loss: 3.96052  (4.02554)\n",
            "     | > grad_norm: 5052.01562  (3583.78491)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68140  (0.67871)\n",
            "     | > loader_time: 0.00510  (0.74932)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 72/131 -- GLOBAL_STEP: 283600\u001b[0m\n",
            "     | > loss: 3.84206  (4.02414)\n",
            "     | > grad_norm: 1218.48242  (3553.32324)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65440  (0.68043)\n",
            "     | > loader_time: 3.61540  (0.81134)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 97/131 -- GLOBAL_STEP: 283625\u001b[0m\n",
            "     | > loss: 4.08095  (4.01611)\n",
            "     | > grad_norm: 2712.16113  (3556.70898)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63290  (0.68143)\n",
            "     | > loader_time: 0.00740  (0.79583)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 122/131 -- GLOBAL_STEP: 283650\u001b[0m\n",
            "     | > loss: 4.09585  (4.01549)\n",
            "     | > grad_norm: 2764.96558  (3529.91895)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66310  (0.68308)\n",
            "     | > loader_time: 0.01350  (0.77447)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.95156  (2.95156)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 5.61770  (5.61770)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.60512  (4.61141)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.88071  (4.70117)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.94843  (4.26299)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.84778  (3.97995)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.99790  (4.31627)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.63011  (4.50396)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.26825  (4.47450)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.72055  (4.27961)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00044 \u001b[0m(-0.00011)\n",
            "     | > avg_loss:\u001b[91m 4.27961 \u001b[0m(+0.14289)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 255/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:15:43) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/131 -- GLOBAL_STEP: 283675\u001b[0m\n",
            "     | > loss: 4.19728  (4.02551)\n",
            "     | > grad_norm: 3171.85596  (3809.65308)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.74380  (0.68301)\n",
            "     | > loader_time: 0.00670  (0.61146)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 40/131 -- GLOBAL_STEP: 283700\u001b[0m\n",
            "     | > loss: 3.94192  (4.01851)\n",
            "     | > grad_norm: 2064.52490  (3638.77808)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67700  (0.68314)\n",
            "     | > loader_time: 3.44840  (0.78158)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 65/131 -- GLOBAL_STEP: 283725\u001b[0m\n",
            "     | > loss: 4.05944  (4.02028)\n",
            "     | > grad_norm: 4440.38428  (3592.79663)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72990  (0.68165)\n",
            "     | > loader_time: 0.00620  (0.77202)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/131 -- GLOBAL_STEP: 283750\u001b[0m\n",
            "     | > loss: 4.30462  (4.01148)\n",
            "     | > grad_norm: 4614.99512  (3583.85645)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.73550  (0.68014)\n",
            "     | > loader_time: 0.01280  (0.78168)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 115/131 -- GLOBAL_STEP: 283775\u001b[0m\n",
            "     | > loss: 4.00642  (4.00982)\n",
            "     | > grad_norm: 2384.73096  (3552.98633)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72450  (0.67980)\n",
            "     | > loader_time: 0.01630  (0.77261)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.86046  (3.86046)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 2.94325  (2.94325)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.77036  (3.85680)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.09695  (4.27019)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.62870  (3.85981)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.19777  (3.72741)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.54867  (4.03095)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.70757  (4.12761)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.02188  (3.98939)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.59438  (3.94550)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.5 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00110 \u001b[0m(+0.00067)\n",
            "     | > avg_loss:\u001b[92m 3.94550 \u001b[0m(-0.33411)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 256/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:19:16) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 8/131 -- GLOBAL_STEP: 283800\u001b[0m\n",
            "     | > loss: 3.53560  (3.98892)\n",
            "     | > grad_norm: 753.86255  (3421.68115)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67520  (0.66665)\n",
            "     | > loader_time: 3.76200  (0.79314)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 33/131 -- GLOBAL_STEP: 283825\u001b[0m\n",
            "     | > loss: 4.22143  (4.02006)\n",
            "     | > grad_norm: 4368.25049  (3673.96240)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66690  (0.68129)\n",
            "     | > loader_time: 0.00320  (0.75447)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 58/131 -- GLOBAL_STEP: 283850\u001b[0m\n",
            "     | > loss: 4.48971  (4.00613)\n",
            "     | > grad_norm: 3802.94971  (3638.34497)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72780  (0.68005)\n",
            "     | > loader_time: 0.03370  (0.74449)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 83/131 -- GLOBAL_STEP: 283875\u001b[0m\n",
            "     | > loss: 3.62333  (4.01322)\n",
            "     | > grad_norm: 2569.76001  (3550.35522)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70160  (0.68149)\n",
            "     | > loader_time: 0.01530  (0.75905)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 108/131 -- GLOBAL_STEP: 283900\u001b[0m\n",
            "     | > loss: 3.88423  (4.00259)\n",
            "     | > grad_norm: 2111.86987  (3482.05396)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71630  (0.68274)\n",
            "     | > loader_time: 1.97260  (0.77208)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.46021  (2.46021)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 2.63011  (2.63011)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 5.32613  (3.97812)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 3.43000  (3.79541)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 4.43646  (3.95567)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.47731  (3.66000)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.94621  (3.87437)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.15407  (4.05718)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.83384  (3.90426)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.59508  (3.86991)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00041 \u001b[0m(-0.00069)\n",
            "     | > avg_loss:\u001b[92m 3.86991 \u001b[0m(-0.07559)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 257/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:22:48) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 1/131 -- GLOBAL_STEP: 283925\u001b[0m\n",
            "     | > loss: 3.93204  (3.93204)\n",
            "     | > grad_norm: 4494.83154  (4494.83154)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64480  (0.64483)\n",
            "     | > loader_time: 0.00310  (0.00309)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 26/131 -- GLOBAL_STEP: 283950\u001b[0m\n",
            "     | > loss: 4.23194  (3.97121)\n",
            "     | > grad_norm: 3502.71533  (3396.25024)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68130  (0.68852)\n",
            "     | > loader_time: 0.00270  (0.74289)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 51/131 -- GLOBAL_STEP: 283975\u001b[0m\n",
            "     | > loss: 3.89687  (3.97863)\n",
            "     | > grad_norm: 4819.97510  (3365.42749)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.74790  (0.69517)\n",
            "     | > loader_time: 0.00460  (0.76440)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 76/131 -- GLOBAL_STEP: 284000\u001b[0m\n",
            "     | > loss: 3.90311  (3.98974)\n",
            "     | > grad_norm: 1789.44800  (3307.66821)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69170  (0.69307)\n",
            "     | > loader_time: 3.20900  (0.82656)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 101/131 -- GLOBAL_STEP: 284025\u001b[0m\n",
            "     | > loss: 4.19904  (3.99501)\n",
            "     | > grad_norm: 5556.75537  (3297.07471)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71530  (0.69198)\n",
            "     | > loader_time: 0.00230  (0.80999)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 126/131 -- GLOBAL_STEP: 284050\u001b[0m\n",
            "     | > loss: 3.94043  (3.99730)\n",
            "     | > grad_norm: 5559.16895  (3336.56470)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.59820  (0.68707)\n",
            "     | > loader_time: 0.00080  (0.79879)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.50615  (2.50615)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.18409  (3.18409)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.48943  (3.33676)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.34767  (3.67373)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.41261  (3.60845)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.23269  (3.53330)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.15271  (3.80320)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.77613  (3.94219)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.34329  (3.99233)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.60829  (3.83855)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00051 \u001b[0m(+0.00010)\n",
            "     | > avg_loss:\u001b[92m 3.83855 \u001b[0m(-0.03137)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 258/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:26:25) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 19/131 -- GLOBAL_STEP: 284075\u001b[0m\n",
            "     | > loss: 4.21645  (4.03171)\n",
            "     | > grad_norm: 3514.03784  (3542.56323)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66360  (0.68430)\n",
            "     | > loader_time: 0.00490  (0.71645)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 44/131 -- GLOBAL_STEP: 284100\u001b[0m\n",
            "     | > loss: 3.72369  (4.00018)\n",
            "     | > grad_norm: 1891.47327  (3351.02173)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65430  (0.68379)\n",
            "     | > loader_time: 3.07940  (0.80908)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 69/131 -- GLOBAL_STEP: 284125\u001b[0m\n",
            "     | > loss: 4.15598  (4.02745)\n",
            "     | > grad_norm: 3924.33325  (3419.63379)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64470  (0.67993)\n",
            "     | > loader_time: 0.15720  (0.80604)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 94/131 -- GLOBAL_STEP: 284150\u001b[0m\n",
            "     | > loss: 4.14870  (4.02407)\n",
            "     | > grad_norm: 4190.09961  (3424.46558)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65360  (0.68123)\n",
            "     | > loader_time: 0.00550  (0.79810)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 119/131 -- GLOBAL_STEP: 284175\u001b[0m\n",
            "     | > loss: 4.32774  (4.09454)\n",
            "     | > grad_norm: 2197.62280  (3546.55347)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69280  (0.68120)\n",
            "     | > loader_time: 0.00920  (0.78567)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.99496  (2.99496)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 2.60439  (2.60439)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.30830  (2.95635)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.21130  (3.70800)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.61947  (3.68586)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.87725  (3.52414)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 6.31816  (3.98981)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 3.91706  (3.97942)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.92582  (3.84772)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.73811  (3.83554)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00041 \u001b[0m(-0.00010)\n",
            "     | > avg_loss:\u001b[92m 3.83554 \u001b[0m(-0.00301)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 259/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:30:02) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 12/131 -- GLOBAL_STEP: 284200\u001b[0m\n",
            "     | > loss: 4.00931  (4.05096)\n",
            "     | > grad_norm: 1389.49792  (3464.30566)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69840  (0.67953)\n",
            "     | > loader_time: 3.29410  (0.78308)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 37/131 -- GLOBAL_STEP: 284225\u001b[0m\n",
            "     | > loss: 4.08913  (4.03144)\n",
            "     | > grad_norm: 5259.52393  (3516.09326)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69080  (0.67717)\n",
            "     | > loader_time: 0.01290  (0.76441)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 62/131 -- GLOBAL_STEP: 284250\u001b[0m\n",
            "     | > loss: 4.12293  (4.01437)\n",
            "     | > grad_norm: 3587.07397  (3584.51318)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66700  (0.67977)\n",
            "     | > loader_time: 0.00250  (0.77950)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 87/131 -- GLOBAL_STEP: 284275\u001b[0m\n",
            "     | > loss: 3.89583  (3.99847)\n",
            "     | > grad_norm: 3046.30371  (3526.80957)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70020  (0.68035)\n",
            "     | > loader_time: 0.01060  (0.77141)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 112/131 -- GLOBAL_STEP: 284300\u001b[0m\n",
            "     | > loss: 3.81773  (3.99121)\n",
            "     | > grad_norm: 1401.14807  (3494.46777)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68900  (0.68277)\n",
            "     | > loader_time: 2.67480  (0.77686)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.94769  (2.94769)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.62991  (3.62991)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.66902  (4.14946)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.10404  (4.46765)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.75361  (4.28914)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.79068  (3.98945)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.00904  (4.15938)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 7.67024  (4.66093)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.86124  (4.56097)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.76276  (4.36117)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.5 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00043 \u001b[0m(+0.00002)\n",
            "     | > avg_loss:\u001b[91m 4.36117 \u001b[0m(+0.52563)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 260/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:33:35) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/131 -- GLOBAL_STEP: 284325\u001b[0m\n",
            "     | > loss: 4.20404  (4.05536)\n",
            "     | > grad_norm: 4580.67480  (3894.54761)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65230  (0.67769)\n",
            "     | > loader_time: 0.09510  (0.62743)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/131 -- GLOBAL_STEP: 284350\u001b[0m\n",
            "     | > loss: 3.77847  (4.00411)\n",
            "     | > grad_norm: 4465.62646  (3514.65894)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69450  (0.68589)\n",
            "     | > loader_time: 0.01620  (0.82075)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/131 -- GLOBAL_STEP: 284375\u001b[0m\n",
            "     | > loss: 4.05680  (3.99903)\n",
            "     | > grad_norm: 1238.34656  (3486.57129)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71060  (0.68339)\n",
            "     | > loader_time: 0.00950  (0.84065)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/131 -- GLOBAL_STEP: 284400\u001b[0m\n",
            "     | > loss: 3.78827  (3.99073)\n",
            "     | > grad_norm: 3243.31201  (3480.54077)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65240  (0.67335)\n",
            "     | > loader_time: 3.50890  (0.89203)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/131 -- GLOBAL_STEP: 284425\u001b[0m\n",
            "     | > loss: 4.21517  (3.99135)\n",
            "     | > grad_norm: 4301.33789  (3462.52295)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65900  (0.67209)\n",
            "     | > loader_time: 0.00520  (0.86943)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/131 -- GLOBAL_STEP: 284450\u001b[0m\n",
            "     | > loss: 4.19278  (3.99059)\n",
            "     | > grad_norm: 4368.33057  (3468.60547)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.54770  (0.66760)\n",
            "     | > loader_time: 0.00060  (0.82939)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 2.47429  (2.47429)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.14976  (3.14976)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.62821  (3.38899)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.06827  (3.94875)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.08573  (3.73299)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.62870  (3.51214)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 6.01712  (3.92963)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.95626  (4.07629)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.15783  (3.96148)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 4.10485  (3.97741)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00049 \u001b[0m(+0.00006)\n",
            "     | > avg_loss:\u001b[92m 3.97741 \u001b[0m(-0.38376)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 261/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:37:16) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 23/131 -- GLOBAL_STEP: 284475\u001b[0m\n",
            "     | > loss: 3.94669  (4.04221)\n",
            "     | > grad_norm: 5282.83350  (3843.22485)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71050  (0.68853)\n",
            "     | > loader_time: 0.00930  (0.67179)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 48/131 -- GLOBAL_STEP: 284500\u001b[0m\n",
            "     | > loss: 3.74572  (4.00523)\n",
            "     | > grad_norm: 2195.83740  (3643.96973)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68070  (0.68437)\n",
            "     | > loader_time: 3.24250  (0.79523)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 73/131 -- GLOBAL_STEP: 284525\u001b[0m\n",
            "     | > loss: 4.30080  (4.01958)\n",
            "     | > grad_norm: 3653.63623  (3578.73682)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66150  (0.68448)\n",
            "     | > loader_time: 0.00570  (0.79616)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 98/131 -- GLOBAL_STEP: 284550\u001b[0m\n",
            "     | > loss: 4.15494  (4.01339)\n",
            "     | > grad_norm: 3767.85205  (3562.28784)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68940  (0.68691)\n",
            "     | > loader_time: 0.00710  (0.77785)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 123/131 -- GLOBAL_STEP: 284575\u001b[0m\n",
            "     | > loss: 3.83913  (4.01279)\n",
            "     | > grad_norm: 852.99182  (3582.16309)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64660  (0.68500)\n",
            "     | > loader_time: 0.00690  (0.77031)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.19031  (3.19031)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.43651  (3.43651)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.59373  (3.51512)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.06794  (4.03272)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 5.58845  (4.42165)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 6.47003  (4.83133)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 3.61019  (4.62781)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.98709  (4.67913)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.48286  (4.52960)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.90433  (4.34901)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00043 \u001b[0m(-0.00005)\n",
            "     | > avg_loss:\u001b[91m 4.34901 \u001b[0m(+0.37160)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 262/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:40:50) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/131 -- GLOBAL_STEP: 284600\u001b[0m\n",
            "     | > loss: 3.99091  (4.01033)\n",
            "     | > grad_norm: 1521.27490  (3624.26147)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68020  (0.69331)\n",
            "     | > loader_time: 3.41050  (0.80538)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 41/131 -- GLOBAL_STEP: 284625\u001b[0m\n",
            "     | > loss: 4.16670  (3.99473)\n",
            "     | > grad_norm: 5533.96973  (3617.82080)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66410  (0.69382)\n",
            "     | > loader_time: 0.00600  (0.75556)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 66/131 -- GLOBAL_STEP: 284650\u001b[0m\n",
            "     | > loss: 4.30449  (4.01094)\n",
            "     | > grad_norm: 4526.20557  (3539.34546)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71830  (0.69552)\n",
            "     | > loader_time: 0.01040  (0.75448)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 91/131 -- GLOBAL_STEP: 284675\u001b[0m\n",
            "     | > loss: 3.88541  (4.00959)\n",
            "     | > grad_norm: 5393.07275  (3477.71069)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.74520  (0.69477)\n",
            "     | > loader_time: 0.00400  (0.76371)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 116/131 -- GLOBAL_STEP: 284700\u001b[0m\n",
            "     | > loss: 4.34166  (4.09730)\n",
            "     | > grad_norm: 1548.93677  (3352.63721)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65210  (0.69462)\n",
            "     | > loader_time: 2.91100  (0.76739)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 4.53530  (4.53530)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.53605  (4.53605)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.23372  (4.38489)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.74349  (4.50442)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.44956  (4.24071)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 5.29631  (4.45183)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 3.68173  (4.32348)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.65792  (4.37126)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.70739  (4.28827)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 5.16109  (4.38525)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00051 \u001b[0m(+0.00008)\n",
            "     | > avg_loss:\u001b[91m 4.38525 \u001b[0m(+0.03624)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 263/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:44:22) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 9/131 -- GLOBAL_STEP: 284725\u001b[0m\n",
            "     | > loss: 4.87604  (4.36901)\n",
            "     | > grad_norm: 2265.10181  (2972.85278)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67710  (0.68800)\n",
            "     | > loader_time: 0.00460  (0.70827)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 34/131 -- GLOBAL_STEP: 284750\u001b[0m\n",
            "     | > loss: 4.36118  (4.37476)\n",
            "     | > grad_norm: 5474.95703  (2868.36255)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66610  (0.67945)\n",
            "     | > loader_time: 0.00500  (0.78715)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 59/131 -- GLOBAL_STEP: 284775\u001b[0m\n",
            "     | > loss: 4.29379  (4.25161)\n",
            "     | > grad_norm: 5699.05029  (2991.53662)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67710  (0.67838)\n",
            "     | > loader_time: 0.00740  (0.77037)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 84/131 -- GLOBAL_STEP: 284800\u001b[0m\n",
            "     | > loss: 4.05976  (4.20469)\n",
            "     | > grad_norm: 5444.50000  (3159.17188)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67620  (0.67745)\n",
            "     | > loader_time: 3.08030  (0.80505)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 109/131 -- GLOBAL_STEP: 284825\u001b[0m\n",
            "     | > loss: 4.24004  (4.15889)\n",
            "     | > grad_norm: 1764.73413  (3231.41992)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70270  (0.67827)\n",
            "     | > loader_time: 0.00350  (0.78993)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.35414  (3.35414)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.91484  (3.91484)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 5.04841  (4.48162)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.78582  (4.91636)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.15506  (4.47603)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.92531  (4.36589)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.64789  (4.41289)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.77902  (4.46519)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.83220  (4.51107)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.16531  (4.36154)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00050 \u001b[0m(-0.00001)\n",
            "     | > avg_loss:\u001b[92m 4.36154 \u001b[0m(-0.02371)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 264/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:47:55) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/131 -- GLOBAL_STEP: 284850\u001b[0m\n",
            "     | > loss: 4.02638  (4.06632)\n",
            "     | > grad_norm: 3646.99072  (4178.74756)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67470  (0.69895)\n",
            "     | > loader_time: 0.00430  (0.00418)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 27/131 -- GLOBAL_STEP: 284875\u001b[0m\n",
            "     | > loss: 4.09687  (4.02376)\n",
            "     | > grad_norm: 2149.51514  (3417.85352)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66990  (0.67969)\n",
            "     | > loader_time: 0.00470  (0.83665)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 52/131 -- GLOBAL_STEP: 284900\u001b[0m\n",
            "     | > loss: 4.30816  (4.00895)\n",
            "     | > grad_norm: 3875.94165  (3423.49463)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.62840  (0.68006)\n",
            "     | > loader_time: 2.98840  (0.87579)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 77/131 -- GLOBAL_STEP: 284925\u001b[0m\n",
            "     | > loss: 4.00824  (4.01514)\n",
            "     | > grad_norm: 6697.42090  (3454.23950)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68960  (0.67601)\n",
            "     | > loader_time: 0.01580  (0.84499)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 102/131 -- GLOBAL_STEP: 284950\u001b[0m\n",
            "     | > loss: 4.25920  (4.05274)\n",
            "     | > grad_norm: 4411.95459  (3588.18018)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.64430  (0.67722)\n",
            "     | > loader_time: 0.00340  (0.81120)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 127/131 -- GLOBAL_STEP: 284975\u001b[0m\n",
            "     | > loss: 4.52492  (4.07726)\n",
            "     | > grad_norm: 4817.76758  (3461.20020)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.57140  (0.67599)\n",
            "     | > loader_time: 0.00070  (0.80004)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.91819  (3.91819)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.36472  (3.36472)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 6.39740  (4.88106)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.72457  (4.82890)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 4.12433  (4.65276)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 2.60061  (4.24233)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.50465  (4.45272)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 4.81097  (4.50389)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.93286  (4.30752)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.03788  (4.16645)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00254 \u001b[0m(+0.00204)\n",
            "     | > avg_loss:\u001b[92m 4.16645 \u001b[0m(-0.19509)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 265/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:51:33) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/131 -- GLOBAL_STEP: 285000\u001b[0m\n",
            "     | > loss: 4.05875  (4.19564)\n",
            "     | > grad_norm: 2157.95776  (3723.10400)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69760  (0.67423)\n",
            "     | > loader_time: 3.29200  (0.81066)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 45/131 -- GLOBAL_STEP: 285025\u001b[0m\n",
            "     | > loss: 4.49001  (4.23352)\n",
            "     | > grad_norm: 7972.87744  (4155.13379)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65660  (0.67825)\n",
            "     | > loader_time: 0.01010  (0.78293)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/131 -- GLOBAL_STEP: 285050\u001b[0m\n",
            "     | > loss: 3.76324  (4.24425)\n",
            "     | > grad_norm: 504.16666  (4134.19189)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.61930  (0.67773)\n",
            "     | > loader_time: 0.59760  (0.80263)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 95/131 -- GLOBAL_STEP: 285075\u001b[0m\n",
            "     | > loss: 3.84614  (4.27318)\n",
            "     | > grad_norm: 2384.60278  (4020.58154)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71060  (0.67350)\n",
            "     | > loader_time: 0.00560  (0.79910)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/131 -- GLOBAL_STEP: 285100\u001b[0m\n",
            "     | > loss: 3.59060  (4.23122)\n",
            "     | > grad_norm: 1317.56909  (3991.93140)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.69260  (0.67417)\n",
            "     | > loader_time: 0.00840  (0.80360)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 5.35156  (5.35156)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 2.81512  (2.81512)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.87380  (3.34446)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.17119  (3.95337)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 2.18807  (3.51205)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 5.74387  (3.95841)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 3.72414  (3.91937)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.32350  (4.11996)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 2.40926  (3.90612)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 2.79584  (3.78275)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00059 \u001b[0m(-0.00195)\n",
            "     | > avg_loss:\u001b[92m 3.78275 \u001b[0m(-0.38369)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 266/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:55:08) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 13/131 -- GLOBAL_STEP: 285125\u001b[0m\n",
            "     | > loss: 4.18825  (4.13629)\n",
            "     | > grad_norm: 6991.40088  (4282.19434)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72420  (0.68052)\n",
            "     | > loader_time: 0.00560  (0.73350)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 38/131 -- GLOBAL_STEP: 285150\u001b[0m\n",
            "     | > loss: 4.25528  (4.06952)\n",
            "     | > grad_norm: 4969.32959  (4306.34570)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68500  (0.67019)\n",
            "     | > loader_time: 0.00300  (0.75531)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 63/131 -- GLOBAL_STEP: 285175\u001b[0m\n",
            "     | > loss: 3.59309  (4.07090)\n",
            "     | > grad_norm: 2401.68555  (4127.46631)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68600  (0.67094)\n",
            "     | > loader_time: 0.00360  (0.75751)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/131 -- GLOBAL_STEP: 285200\u001b[0m\n",
            "     | > loss: 3.80534  (4.05811)\n",
            "     | > grad_norm: 295.10104  (4021.78662)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66500  (0.66918)\n",
            "     | > loader_time: 3.23520  (0.80125)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 113/131 -- GLOBAL_STEP: 285225\u001b[0m\n",
            "     | > loss: 4.24959  (4.17058)\n",
            "     | > grad_norm: 368.27161  (4114.35596)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.73890  (0.66977)\n",
            "     | > loader_time: 0.01200  (0.78215)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.53624  (3.53624)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.33627  (3.33627)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.35233  (3.84430)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.64535  (4.11132)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.91323  (4.06180)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 4.72952  (4.19534)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.39954  (4.22937)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 6.37699  (4.53618)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.12045  (4.48421)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.72387  (4.39973)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.5 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00043 \u001b[0m(-0.00016)\n",
            "     | > avg_loss:\u001b[91m 4.39973 \u001b[0m(+0.61697)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 267/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 11:58:41) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 6/131 -- GLOBAL_STEP: 285250\u001b[0m\n",
            "     | > loss: 3.90981  (4.21685)\n",
            "     | > grad_norm: 3369.39673  (3694.95776)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63600  (0.67816)\n",
            "     | > loader_time: 0.00520  (0.50965)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 31/131 -- GLOBAL_STEP: 285275\u001b[0m\n",
            "     | > loss: 4.10343  (4.24229)\n",
            "     | > grad_norm: 4130.94580  (3580.41064)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66280  (0.67968)\n",
            "     | > loader_time: 0.00910  (0.78140)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 56/131 -- GLOBAL_STEP: 285300\u001b[0m\n",
            "     | > loss: 4.07158  (4.22696)\n",
            "     | > grad_norm: 1452.90881  (3338.51147)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65200  (0.67566)\n",
            "     | > loader_time: 3.37330  (0.88597)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 81/131 -- GLOBAL_STEP: 285325\u001b[0m\n",
            "     | > loss: 4.25136  (4.23243)\n",
            "     | > grad_norm: 2530.04175  (3249.32178)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70310  (0.67212)\n",
            "     | > loader_time: 0.00830  (0.87634)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 106/131 -- GLOBAL_STEP: 285350\u001b[0m\n",
            "     | > loss: 4.29365  (4.22081)\n",
            "     | > grad_norm: 3497.29761  (3190.99951)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67230  (0.66889)\n",
            "     | > loader_time: 0.00480  (0.85784)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 131/131 -- GLOBAL_STEP: 285375\u001b[0m\n",
            "     | > loss: 3.90728  (4.21564)\n",
            "     | > grad_norm: 924.31818  (3153.56104)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.41010  (0.66376)\n",
            "     | > loader_time: 0.00040  (0.82601)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.53361  (3.53361)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.05168  (4.05168)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.93037  (3.99103)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.60407  (4.19538)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 4.02844  (4.15364)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.98589  (4.12009)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 6.84591  (4.57440)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.84909  (4.75649)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 3.81371  (4.63865)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.98188  (4.56567)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00063 \u001b[0m(+0.00020)\n",
            "     | > avg_loss:\u001b[91m 4.56567 \u001b[0m(+0.16594)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 268/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 12:02:23) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 24/131 -- GLOBAL_STEP: 285400\u001b[0m\n",
            "     | > loss: 4.54443  (4.24826)\n",
            "     | > grad_norm: 4900.26367  (3274.08936)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.63490  (0.67235)\n",
            "     | > loader_time: 3.03000  (0.82374)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 49/131 -- GLOBAL_STEP: 285425\u001b[0m\n",
            "     | > loss: 4.80095  (4.29772)\n",
            "     | > grad_norm: 5393.40967  (3622.04785)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65810  (0.67020)\n",
            "     | > loader_time: 0.01060  (0.80224)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 74/131 -- GLOBAL_STEP: 285450\u001b[0m\n",
            "     | > loss: 4.07856  (4.29058)\n",
            "     | > grad_norm: 3166.10474  (3527.45239)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67700  (0.67714)\n",
            "     | > loader_time: 0.01680  (0.79143)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 99/131 -- GLOBAL_STEP: 285475\u001b[0m\n",
            "     | > loss: 4.05016  (4.28320)\n",
            "     | > grad_norm: 2380.22217  (3476.23608)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.67530  (0.67962)\n",
            "     | > loader_time: 0.00470  (0.77519)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 124/131 -- GLOBAL_STEP: 285500\u001b[0m\n",
            "     | > loss: 4.33113  (4.34765)\n",
            "     | > grad_norm: 2806.09399  (3746.59351)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70700  (0.68152)\n",
            "     | > loader_time: 2.13030  (0.78149)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.97674  (3.97674)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 4.34368  (4.34368)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 4.57147  (4.45758)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 5.28542  (4.73353)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.81215  (4.50318)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.97392  (4.39733)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 5.00812  (4.49913)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 5.05741  (4.57888)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 4.20551  (4.53221)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.97646  (4.47046)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00048 \u001b[0m(-0.00015)\n",
            "     | > avg_loss:\u001b[92m 4.47046 \u001b[0m(-0.09521)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 269/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 12:05:57) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 17/131 -- GLOBAL_STEP: 285525\u001b[0m\n",
            "     | > loss: 3.96043  (4.24543)\n",
            "     | > grad_norm: 851.90527  (3751.05298)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66970  (0.69398)\n",
            "     | > loader_time: 0.00370  (0.71803)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 42/131 -- GLOBAL_STEP: 285550\u001b[0m\n",
            "     | > loss: 4.02104  (4.12753)\n",
            "     | > grad_norm: 701.96094  (3027.78198)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.71850  (0.68551)\n",
            "     | > loader_time: 0.00760  (0.73801)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 67/131 -- GLOBAL_STEP: 285575\u001b[0m\n",
            "     | > loss: 3.80000  (4.03746)\n",
            "     | > grad_norm: 1393.67151  (2899.70898)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.70650  (0.68327)\n",
            "     | > loader_time: 0.00440  (0.75149)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 92/131 -- GLOBAL_STEP: 285600\u001b[0m\n",
            "     | > loss: 3.74123  (3.97817)\n",
            "     | > grad_norm: 3831.43945  (2910.41333)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.60530  (0.68539)\n",
            "     | > loader_time: 2.98790  (0.78767)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 117/131 -- GLOBAL_STEP: 285625\u001b[0m\n",
            "     | > loss: 4.00606  (4.03373)\n",
            "     | > grad_norm: 4353.48193  (2966.85913)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.66390  (0.68281)\n",
            "     | > loader_time: 0.00370  (0.76799)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 3.77607  (3.77607)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 3.69438  (3.69438)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 3.64684  (3.67061)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 4.95758  (4.09960)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 3.23381  (3.88315)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 3.41737  (3.79000)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 4.85600  (3.96766)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 6.96533  (4.39590)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 5.64785  (4.55240)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss: 3.40265  (4.42465)\n",
            "\n",
            " [!] Instance is too short! : /content/drive/MyDrive/Emergent/recorded_dataset/dataset/newData/LJSpeech-1.1/wavs/5bf90c2ddba1236fd1bbd767cb64a437.wav\n",
            "108000/108900 -- batch_size: 9 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00262 \u001b[0m(+0.00215)\n",
            "     | > avg_loss:\u001b[92m 4.42465 \u001b[0m(-0.04582)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 270/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-02 12:09:30) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/131 -- GLOBAL_STEP: 285650\u001b[0m\n",
            "     | > loss: 3.78219  (3.97150)\n",
            "     | > grad_norm: 1201.04700  (3492.64648)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.68750  (0.67923)\n",
            "     | > loader_time: 0.00340  (0.65593)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 35/131 -- GLOBAL_STEP: 285675\u001b[0m\n",
            "     | > loss: 4.35579  (4.02699)\n",
            "     | > grad_norm: 6367.74561  (3722.13843)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.72600  (0.67272)\n",
            "     | > loader_time: 0.01720  (0.74926)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/131 -- GLOBAL_STEP: 285700\u001b[0m\n",
            "     | > loss: 4.37221  (4.06166)\n",
            "     | > grad_norm: 5164.09473  (3871.09644)\n",
            "     | > current_lr: 0.00010 \n",
            "     | > step_time: 0.65260  (0.67842)\n",
            "     | > loader_time: 3.26130  (0.80915)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Emergent/train/Day11_newData/train_wavernn.py \\\n",
        "    --continue_path /content/drive/MyDrive/Emergent/train/Day11_newData/wavernn/coqui_tts-November-25-2021_08+36AM-33aa27e2/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzvW3ufW3zoh",
        "outputId": "89edb9d8-8da1-4c67-8486-409e35da903b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Using model: tacotron2\n",
            " > Model's reduction rate `r` is set to: 2\n",
            " > Vocoder Model: hifigan\n",
            " > Generator Model: hifigan_generator\n",
            " > Discriminator Model: hifigan_discriminator\n",
            "Removing weight norm...\n",
            " > Text: Tolya, wadde okubaaga ekinyonyi ekifudde oba ekirwadde.\n",
            " > Text splitted to sentences.\n",
            "['Tolya, wadde okubaaga ekinyonyi ekifudde oba ekirwadde.']\n",
            " > Processing time: 3.78263521194458\n",
            " > Real-time factor: 0.8043425631015467\n",
            " > Saving output to ../output1.wav\n"
          ]
        }
      ],
      "source": [
        "!tts --text \"Tolya, wadde okubaaga ekinyonyi ekifudde oba ekirwadde.\" \\\n",
        "      --model_path /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/best_model_103422.pth.tar \\\n",
        "      --config_path /content/drive/MyDrive/Emergent/train/Day15_24dB/coqui_tts-December-22-2021_07+06PM-7f1a2378/config.json \\\n",
        "      --out_path ../output1.wav \\\n",
        "      --vocoder_path /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2/checkpoint_190000.pth.tar \\\n",
        "      --vocoder_config_path /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2/config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2dEigBCd45e",
        "outputId": "890442b4-9088-4e03-a933-8dab2f8d1561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.80860  (3.53389)\n",
            "     | > loader_time: 0.03940  (0.02899)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.29035  (2.29035)\n",
            "     | > loss_kl: 1.75746  (1.75746)\n",
            "     | > loss_feat: 4.45355  (4.45355)\n",
            "     | > loss_mel: 18.49261  (18.49261)\n",
            "     | > loss_duration: 1.78930  (1.78930)\n",
            "     | > loss_0: 28.78328  (28.78328)\n",
            "     | > loss_disc: 2.51580  (2.51580)\n",
            "     | > loss_1: 2.51580  (2.51580)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.19703  (2.19703)\n",
            "     | > loss_kl: 1.99258  (1.99258)\n",
            "     | > loss_feat: 4.36736  (4.36736)\n",
            "     | > loss_mel: 18.23780  (18.23780)\n",
            "     | > loss_duration: 1.76237  (1.76237)\n",
            "     | > loss_0: 28.55714  (28.55714)\n",
            "     | > loss_disc: 2.60873  (2.60873)\n",
            "     | > loss_1: 2.60873  (2.60873)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.13658  (2.16680)\n",
            "     | > loss_kl: 1.87673  (1.93466)\n",
            "     | > loss_feat: 4.55824  (4.46280)\n",
            "     | > loss_mel: 19.30425  (18.77103)\n",
            "     | > loss_duration: 1.76412  (1.76325)\n",
            "     | > loss_0: 29.63992  (29.09853)\n",
            "     | > loss_disc: 2.47473  (2.54173)\n",
            "     | > loss_1: 2.47473  (2.54173)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.13758  (2.15706)\n",
            "     | > loss_kl: 2.13805  (2.00245)\n",
            "     | > loss_feat: 5.11258  (4.67939)\n",
            "     | > loss_mel: 18.83145  (18.79117)\n",
            "     | > loss_duration: 1.72512  (1.75054)\n",
            "     | > loss_0: 29.94478  (29.38062)\n",
            "     | > loss_disc: 2.53249  (2.53865)\n",
            "     | > loss_1: 2.53249  (2.53865)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.26097  (2.18304)\n",
            "     | > loss_kl: 1.89946  (1.97671)\n",
            "     | > loss_feat: 4.24553  (4.57093)\n",
            "     | > loss_mel: 18.30219  (18.66892)\n",
            "     | > loss_duration: 1.76185  (1.75336)\n",
            "     | > loss_0: 28.46999  (29.15296)\n",
            "     | > loss_disc: 2.57529  (2.54781)\n",
            "     | > loss_1: 2.57529  (2.54781)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.23238  (2.19291)\n",
            "     | > loss_kl: 2.14283  (2.00993)\n",
            "     | > loss_feat: 4.17603  (4.49195)\n",
            "     | > loss_mel: 19.02174  (18.73948)\n",
            "     | > loss_duration: 1.73237  (1.74917)\n",
            "     | > loss_0: 29.30536  (29.18344)\n",
            "     | > loss_disc: 2.53058  (2.54436)\n",
            "     | > loss_1: 2.53058  (2.54436)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01654 \u001b[0m(-0.00104)\n",
            "     | > avg_loss_gen:\u001b[91m 2.19291 \u001b[0m(+0.19462)\n",
            "     | > avg_loss_kl:\u001b[91m 2.00993 \u001b[0m(+0.00961)\n",
            "     | > avg_loss_feat:\u001b[91m 4.49195 \u001b[0m(+0.25004)\n",
            "     | > avg_loss_mel:\u001b[91m 18.73948 \u001b[0m(+0.26617)\n",
            "     | > avg_loss_duration:\u001b[91m 1.74917 \u001b[0m(+0.02515)\n",
            "     | > avg_loss_0:\u001b[91m 29.18344 \u001b[0m(+0.74559)\n",
            "     | > avg_loss_disc:\u001b[92m 2.54436 \u001b[0m(-0.01139)\n",
            "     | > avg_loss_1:\u001b[92m 2.54436 \u001b[0m(-0.01139)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 13/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 08:41:22) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 13/173 -- GLOBAL_STEP: 59275\u001b[0m\n",
            "     | > loss_gen: 2.10651  (2.14291)\n",
            "     | > loss_kl: 1.27012  (1.17446)\n",
            "     | > loss_feat: 5.01538  (4.95356)\n",
            "     | > loss_mel: 18.04900  (18.19696)\n",
            "     | > loss_duration: 1.67446  (1.69781)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.11547  (28.16569)\n",
            "     | > grad_norm_0: 299.40189  (342.59729)\n",
            "     | > loss_disc: 2.53928  (2.56149)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.53928  (2.56149)\n",
            "     | > grad_norm_1: 25.96906  (30.22982)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.36500  (3.29150)\n",
            "     | > loader_time: 0.02110  (0.01899)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 38/173 -- GLOBAL_STEP: 59300\u001b[0m\n",
            "     | > loss_gen: 2.15107  (2.12681)\n",
            "     | > loss_kl: 1.32488  (1.23040)\n",
            "     | > loss_feat: 5.21172  (4.74861)\n",
            "     | > loss_mel: 18.39794  (18.20310)\n",
            "     | > loss_duration: 1.62838  (1.66924)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.71399  (27.97815)\n",
            "     | > grad_norm_0: 250.19061  (265.23227)\n",
            "     | > loss_disc: 2.58238  (2.55172)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.58238  (2.55172)\n",
            "     | > grad_norm_1: 6.61540  (30.97219)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.39080  (3.33537)\n",
            "     | > loader_time: 0.02330  (0.02119)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 63/173 -- GLOBAL_STEP: 59325\u001b[0m\n",
            "     | > loss_gen: 2.13993  (2.12211)\n",
            "     | > loss_kl: 1.11333  (1.23594)\n",
            "     | > loss_feat: 5.17078  (4.69981)\n",
            "     | > loss_mel: 18.33546  (18.22037)\n",
            "     | > loss_duration: 1.64786  (1.66318)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.40736  (27.94141)\n",
            "     | > grad_norm_0: 83.52215  (248.42461)\n",
            "     | > loss_disc: 2.51353  (2.55626)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.51353  (2.55626)\n",
            "     | > grad_norm_1: 12.38938  (30.08468)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.40320  (3.36858)\n",
            "     | > loader_time: 0.02550  (0.02276)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/173 -- GLOBAL_STEP: 59350\u001b[0m\n",
            "     | > loss_gen: 2.24174  (2.11676)\n",
            "     | > loss_kl: 1.31240  (1.24900)\n",
            "     | > loss_feat: 4.75671  (4.65720)\n",
            "     | > loss_mel: 17.97148  (18.18684)\n",
            "     | > loss_duration: 1.65733  (1.65711)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.93966  (27.86691)\n",
            "     | > grad_norm_0: 80.18571  (212.61296)\n",
            "     | > loss_disc: 2.53319  (2.55760)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.53319  (2.55760)\n",
            "     | > grad_norm_1: 17.70273  (27.76192)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.49510  (3.40407)\n",
            "     | > loader_time: 0.02800  (0.02431)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 113/173 -- GLOBAL_STEP: 59375\u001b[0m\n",
            "     | > loss_gen: 1.90385  (2.11453)\n",
            "     | > loss_kl: 1.27234  (1.26172)\n",
            "     | > loss_feat: 4.02567  (4.64540)\n",
            "     | > loss_mel: 17.82590  (18.17942)\n",
            "     | > loss_duration: 1.64937  (1.65277)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 26.67714  (27.85384)\n",
            "     | > grad_norm_0: 284.59393  (211.84354)\n",
            "     | > loss_disc: 2.53503  (2.55973)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.53503  (2.55973)\n",
            "     | > grad_norm_1: 61.84272  (27.75786)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.55830  (3.44101)\n",
            "     | > loader_time: 0.03050  (0.02572)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 138/173 -- GLOBAL_STEP: 59400\u001b[0m\n",
            "     | > loss_gen: 2.04028  (2.11783)\n",
            "     | > loss_kl: 1.38799  (1.27529)\n",
            "     | > loss_feat: 5.18261  (4.63529)\n",
            "     | > loss_mel: 18.17282  (18.15943)\n",
            "     | > loss_duration: 1.64951  (1.65004)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.43321  (27.83788)\n",
            "     | > grad_norm_0: 121.26515  (208.30095)\n",
            "     | > loss_disc: 2.49924  (2.55304)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49924  (2.55304)\n",
            "     | > grad_norm_1: 7.90982  (28.49107)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.68320  (3.47850)\n",
            "     | > loader_time: 0.03850  (0.02706)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 163/173 -- GLOBAL_STEP: 59425\u001b[0m\n",
            "     | > loss_gen: 2.13094  (2.11804)\n",
            "     | > loss_kl: 1.47557  (1.28541)\n",
            "     | > loss_feat: 4.64413  (4.62435)\n",
            "     | > loss_mel: 18.37239  (18.14298)\n",
            "     | > loss_duration: 1.63851  (1.64915)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.26155  (27.81993)\n",
            "     | > grad_norm_0: 85.56881  (212.21547)\n",
            "     | > loss_disc: 2.47355  (2.55278)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.47355  (2.55278)\n",
            "     | > grad_norm_1: 16.28396  (28.36212)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.85160  (3.52732)\n",
            "     | > loader_time: 0.03970  (0.02868)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.00874  (2.00874)\n",
            "     | > loss_kl: 1.72666  (1.72666)\n",
            "     | > loss_feat: 4.58776  (4.58776)\n",
            "     | > loss_mel: 18.18934  (18.18934)\n",
            "     | > loss_duration: 1.77912  (1.77912)\n",
            "     | > loss_0: 28.29162  (28.29162)\n",
            "     | > loss_disc: 2.52821  (2.52821)\n",
            "     | > loss_1: 2.52821  (2.52821)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 1.99512  (1.99512)\n",
            "     | > loss_kl: 2.23318  (2.23318)\n",
            "     | > loss_feat: 4.84898  (4.84898)\n",
            "     | > loss_mel: 18.36835  (18.36835)\n",
            "     | > loss_duration: 1.74353  (1.74353)\n",
            "     | > loss_0: 29.18916  (29.18916)\n",
            "     | > loss_disc: 2.60349  (2.60349)\n",
            "     | > loss_1: 2.60349  (2.60349)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.02662  (2.01087)\n",
            "     | > loss_kl: 1.76739  (2.00029)\n",
            "     | > loss_feat: 3.93335  (4.39116)\n",
            "     | > loss_mel: 17.78659  (18.07747)\n",
            "     | > loss_duration: 1.76111  (1.75232)\n",
            "     | > loss_0: 27.27505  (28.23211)\n",
            "     | > loss_disc: 2.51325  (2.55837)\n",
            "     | > loss_1: 2.51325  (2.55837)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.05154  (2.02443)\n",
            "     | > loss_kl: 2.14776  (2.04944)\n",
            "     | > loss_feat: 4.68813  (4.49015)\n",
            "     | > loss_mel: 17.96447  (18.03980)\n",
            "     | > loss_duration: 1.69007  (1.73157)\n",
            "     | > loss_0: 28.54197  (28.33539)\n",
            "     | > loss_disc: 2.54670  (2.55448)\n",
            "     | > loss_1: 2.54670  (2.55448)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 1.97046  (2.01094)\n",
            "     | > loss_kl: 1.84460  (1.99823)\n",
            "     | > loss_feat: 3.85579  (4.33156)\n",
            "     | > loss_mel: 17.98422  (18.02591)\n",
            "     | > loss_duration: 1.73597  (1.73267)\n",
            "     | > loss_0: 27.39105  (28.09931)\n",
            "     | > loss_disc: 2.53516  (2.54965)\n",
            "     | > loss_1: 2.53516  (2.54965)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.09461  (2.02767)\n",
            "     | > loss_kl: 2.20773  (2.04013)\n",
            "     | > loss_feat: 4.50063  (4.36538)\n",
            "     | > loss_mel: 18.94431  (18.20959)\n",
            "     | > loss_duration: 1.69655  (1.72545)\n",
            "     | > loss_0: 29.44383  (28.36821)\n",
            "     | > loss_disc: 2.58519  (2.55676)\n",
            "     | > loss_1: 2.58519  (2.55676)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01538 \u001b[0m(-0.00116)\n",
            "     | > avg_loss_gen:\u001b[92m 2.02767 \u001b[0m(-0.16524)\n",
            "     | > avg_loss_kl:\u001b[91m 2.04013 \u001b[0m(+0.03020)\n",
            "     | > avg_loss_feat:\u001b[92m 4.36538 \u001b[0m(-0.12657)\n",
            "     | > avg_loss_mel:\u001b[92m 18.20959 \u001b[0m(-0.52990)\n",
            "     | > avg_loss_duration:\u001b[92m 1.72545 \u001b[0m(-0.02372)\n",
            "     | > avg_loss_0:\u001b[92m 28.36821 \u001b[0m(-0.81523)\n",
            "     | > avg_loss_disc:\u001b[91m 2.55676 \u001b[0m(+0.01240)\n",
            "     | > avg_loss_1:\u001b[91m 2.55676 \u001b[0m(+0.01240)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 14/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 08:51:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 14/173 -- GLOBAL_STEP: 59450\u001b[0m\n",
            "     | > loss_gen: 2.28703  (2.19341)\n",
            "     | > loss_kl: 1.23189  (1.21555)\n",
            "     | > loss_feat: 5.01011  (4.87450)\n",
            "     | > loss_mel: 17.94119  (18.04939)\n",
            "     | > loss_duration: 1.67652  (1.68924)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.14674  (28.02209)\n",
            "     | > grad_norm_0: 635.36328  (339.20453)\n",
            "     | > loss_disc: 2.51533  (2.50469)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.51533  (2.50469)\n",
            "     | > grad_norm_1: 21.27576  (36.58419)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.30710  (3.28679)\n",
            "     | > loader_time: 0.02040  (0.02152)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 39/173 -- GLOBAL_STEP: 59475\u001b[0m\n",
            "     | > loss_gen: 2.11451  (2.14960)\n",
            "     | > loss_kl: 1.32638  (1.24636)\n",
            "     | > loss_feat: 4.60423  (4.76424)\n",
            "     | > loss_mel: 18.16148  (18.03223)\n",
            "     | > loss_duration: 1.62705  (1.66502)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.83366  (27.85744)\n",
            "     | > grad_norm_0: 473.91220  (348.86267)\n",
            "     | > loss_disc: 2.49979  (2.52214)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49979  (2.52214)\n",
            "     | > grad_norm_1: 44.11492  (37.72042)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.35790  (3.34627)\n",
            "     | > loader_time: 0.02280  (0.02212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 64/173 -- GLOBAL_STEP: 59500\u001b[0m\n",
            "     | > loss_gen: 2.10402  (2.15733)\n",
            "     | > loss_kl: 1.19015  (1.26575)\n",
            "     | > loss_feat: 4.43785  (4.73897)\n",
            "     | > loss_mel: 18.19553  (18.03379)\n",
            "     | > loss_duration: 1.66217  (1.66120)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.58971  (27.85704)\n",
            "     | > grad_norm_0: 335.94534  (328.63004)\n",
            "     | > loss_disc: 2.65862  (2.52436)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.65862  (2.52436)\n",
            "     | > grad_norm_1: 20.49719  (35.60232)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.42620  (3.38278)\n",
            "     | > loader_time: 0.02680  (0.02355)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 89/173 -- GLOBAL_STEP: 59525\u001b[0m\n",
            "     | > loss_gen: 2.08697  (2.14095)\n",
            "     | > loss_kl: 1.39397  (1.27717)\n",
            "     | > loss_feat: 5.02605  (4.69980)\n",
            "     | > loss_mel: 18.40157  (18.04920)\n",
            "     | > loss_duration: 1.65292  (1.65577)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.56147  (27.82289)\n",
            "     | > grad_norm_0: 275.44583  (306.84961)\n",
            "     | > loss_disc: 2.54708  (2.53266)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.54708  (2.53266)\n",
            "     | > grad_norm_1: 42.42387  (36.00302)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.56380  (3.41350)\n",
            "     | > loader_time: 0.02680  (0.02491)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 114/173 -- GLOBAL_STEP: 59550\u001b[0m\n",
            "     | > loss_gen: 2.15030  (2.14292)\n",
            "     | > loss_kl: 1.37040  (1.28442)\n",
            "     | > loss_feat: 5.17839  (4.68906)\n",
            "     | > loss_mel: 17.78854  (18.04263)\n",
            "     | > loss_duration: 1.62770  (1.65119)\n",
            "     | > amp_scaler: 512.00000  (305.40351)\n",
            "     | > loss_0: 28.11534  (27.81021)\n",
            "     | > grad_norm_0: 223.24614  (284.03787)\n",
            "     | > loss_disc: 2.50824  (2.53112)\n",
            "     | > amp_scaler-1: 512.00000  (305.40351)\n",
            "     | > loss_1: 2.50824  (2.53112)\n",
            "     | > grad_norm_1: 11.52690  (33.48105)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.54390  (3.44793)\n",
            "     | > loader_time: 0.02790  (0.02608)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 139/173 -- GLOBAL_STEP: 59575\u001b[0m\n",
            "     | > loss_gen: 1.84024  (2.13784)\n",
            "     | > loss_kl: 1.38892  (1.29144)\n",
            "     | > loss_feat: 4.61056  (4.66978)\n",
            "     | > loss_mel: 17.89730  (18.02322)\n",
            "     | > loss_duration: 1.64049  (1.64809)\n",
            "     | > amp_scaler: 256.00000  (337.03597)\n",
            "     | > loss_0: 27.37752  (27.77038)\n",
            "     | > grad_norm_0: 138.63765  (263.48746)\n",
            "     | > loss_disc: 2.65352  (2.53128)\n",
            "     | > amp_scaler-1: 256.00000  (337.03597)\n",
            "     | > loss_1: 2.65352  (2.53128)\n",
            "     | > grad_norm_1: 26.28524  (31.21342)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.75150  (3.48626)\n",
            "     | > loader_time: 0.03520  (0.02730)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 164/173 -- GLOBAL_STEP: 59600\u001b[0m\n",
            "     | > loss_gen: 1.98383  (2.13455)\n",
            "     | > loss_kl: 1.33924  (1.29974)\n",
            "     | > loss_feat: 4.41684  (4.64803)\n",
            "     | > loss_mel: 18.36691  (18.02986)\n",
            "     | > loss_duration: 1.65083  (1.64763)\n",
            "     | > amp_scaler: 256.00000  (324.68293)\n",
            "     | > loss_0: 27.75766  (27.75982)\n",
            "     | > grad_norm_0: 275.08542  (261.83176)\n",
            "     | > loss_disc: 2.54256  (2.53525)\n",
            "     | > amp_scaler-1: 256.00000  (324.68293)\n",
            "     | > loss_1: 2.54256  (2.53525)\n",
            "     | > grad_norm_1: 47.62254  (30.90696)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.83920  (3.53217)\n",
            "     | > loader_time: 0.04260  (0.02897)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.23397  (2.23397)\n",
            "     | > loss_kl: 2.02553  (2.02553)\n",
            "     | > loss_feat: 4.53217  (4.53217)\n",
            "     | > loss_mel: 17.85140  (17.85140)\n",
            "     | > loss_duration: 1.81650  (1.81650)\n",
            "     | > loss_0: 28.45957  (28.45957)\n",
            "     | > loss_disc: 2.46314  (2.46314)\n",
            "     | > loss_1: 2.46314  (2.46314)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.17465  (2.17465)\n",
            "     | > loss_kl: 2.15561  (2.15561)\n",
            "     | > loss_feat: 4.40369  (4.40369)\n",
            "     | > loss_mel: 17.97192  (17.97192)\n",
            "     | > loss_duration: 1.78289  (1.78289)\n",
            "     | > loss_0: 28.48877  (28.48877)\n",
            "     | > loss_disc: 2.62027  (2.62027)\n",
            "     | > loss_1: 2.62027  (2.62027)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.24737  (2.21101)\n",
            "     | > loss_kl: 1.92874  (2.04217)\n",
            "     | > loss_feat: 3.92623  (4.16496)\n",
            "     | > loss_mel: 18.51098  (18.24145)\n",
            "     | > loss_duration: 1.77978  (1.78134)\n",
            "     | > loss_0: 28.39311  (28.44094)\n",
            "     | > loss_disc: 2.50453  (2.56240)\n",
            "     | > loss_1: 2.50453  (2.56240)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.18402  (2.20201)\n",
            "     | > loss_kl: 2.56577  (2.21670)\n",
            "     | > loss_feat: 4.55809  (4.29600)\n",
            "     | > loss_mel: 18.04102  (18.17464)\n",
            "     | > loss_duration: 1.73304  (1.76524)\n",
            "     | > loss_0: 29.08193  (28.65460)\n",
            "     | > loss_disc: 2.51456  (2.54645)\n",
            "     | > loss_1: 2.51456  (2.54645)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.33043  (2.23412)\n",
            "     | > loss_kl: 2.06078  (2.17772)\n",
            "     | > loss_feat: 4.30991  (4.29948)\n",
            "     | > loss_mel: 18.38967  (18.22840)\n",
            "     | > loss_duration: 1.74365  (1.75984)\n",
            "     | > loss_0: 28.83444  (28.69956)\n",
            "     | > loss_disc: 2.51643  (2.53895)\n",
            "     | > loss_1: 2.51643  (2.53895)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.37375  (2.26204)\n",
            "     | > loss_kl: 2.15444  (2.17307)\n",
            "     | > loss_feat: 4.02055  (4.24370)\n",
            "     | > loss_mel: 18.34028  (18.25078)\n",
            "     | > loss_duration: 1.74684  (1.75724)\n",
            "     | > loss_0: 28.63586  (28.68682)\n",
            "     | > loss_disc: 2.39199  (2.50955)\n",
            "     | > loss_1: 2.39199  (2.50955)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01426 \u001b[0m(-0.00112)\n",
            "     | > avg_loss_gen:\u001b[91m 2.26204 \u001b[0m(+0.23437)\n",
            "     | > avg_loss_kl:\u001b[91m 2.17307 \u001b[0m(+0.13294)\n",
            "     | > avg_loss_feat:\u001b[92m 4.24370 \u001b[0m(-0.12168)\n",
            "     | > avg_loss_mel:\u001b[91m 18.25078 \u001b[0m(+0.04119)\n",
            "     | > avg_loss_duration:\u001b[91m 1.75724 \u001b[0m(+0.03179)\n",
            "     | > avg_loss_0:\u001b[91m 28.68682 \u001b[0m(+0.31861)\n",
            "     | > avg_loss_disc:\u001b[92m 2.50955 \u001b[0m(-0.04720)\n",
            "     | > avg_loss_1:\u001b[92m 2.50955 \u001b[0m(-0.04720)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 15/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 09:02:36) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/173 -- GLOBAL_STEP: 59625\u001b[0m\n",
            "     | > loss_gen: 2.20313  (2.17528)\n",
            "     | > loss_kl: 1.19391  (1.20726)\n",
            "     | > loss_feat: 4.55724  (4.74709)\n",
            "     | > loss_mel: 17.92061  (18.08761)\n",
            "     | > loss_duration: 1.68139  (1.69163)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.55628  (27.90887)\n",
            "     | > grad_norm_0: 489.86203  (340.00296)\n",
            "     | > loss_disc: 2.51099  (2.52681)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.51099  (2.52681)\n",
            "     | > grad_norm_1: 55.49464  (37.46701)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.29320  (3.29737)\n",
            "     | > loader_time: 0.02170  (0.01864)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 40/173 -- GLOBAL_STEP: 59650\u001b[0m\n",
            "     | > loss_gen: 2.17355  (2.13717)\n",
            "     | > loss_kl: 1.42778  (1.24614)\n",
            "     | > loss_feat: 5.37887  (4.79561)\n",
            "     | > loss_mel: 17.96924  (18.11787)\n",
            "     | > loss_duration: 1.69870  (1.66679)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.64815  (27.96357)\n",
            "     | > grad_norm_0: 134.58508  (293.78281)\n",
            "     | > loss_disc: 2.45945  (2.53802)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.45945  (2.53802)\n",
            "     | > grad_norm_1: 17.15557  (35.23495)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.35620  (3.34274)\n",
            "     | > loader_time: 0.02320  (0.02082)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 65/173 -- GLOBAL_STEP: 59675\u001b[0m\n",
            "     | > loss_gen: 2.25576  (2.13864)\n",
            "     | > loss_kl: 1.47023  (1.25694)\n",
            "     | > loss_feat: 5.01513  (4.74180)\n",
            "     | > loss_mel: 18.37305  (18.12200)\n",
            "     | > loss_duration: 1.63510  (1.66029)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.74927  (27.91967)\n",
            "     | > grad_norm_0: 431.58392  (244.59602)\n",
            "     | > loss_disc: 2.55369  (2.54526)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.55369  (2.54526)\n",
            "     | > grad_norm_1: 67.82087  (33.70153)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.47420  (3.37532)\n",
            "     | > loader_time: 0.02630  (0.02215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/173 -- GLOBAL_STEP: 59700\u001b[0m\n",
            "     | > loss_gen: 2.05625  (2.11765)\n",
            "     | > loss_kl: 1.24128  (1.25953)\n",
            "     | > loss_feat: 4.47578  (4.65898)\n",
            "     | > loss_mel: 17.93607  (18.11089)\n",
            "     | > loss_duration: 1.63145  (1.65496)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.34083  (27.80201)\n",
            "     | > grad_norm_0: 63.01002  (225.38150)\n",
            "     | > loss_disc: 2.49748  (2.55542)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49748  (2.55542)\n",
            "     | > grad_norm_1: 14.57711  (31.53556)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.51330  (3.40875)\n",
            "     | > loader_time: 0.02940  (0.02378)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 115/173 -- GLOBAL_STEP: 59725\u001b[0m\n",
            "     | > loss_gen: 2.12591  (2.12373)\n",
            "     | > loss_kl: 1.33741  (1.27240)\n",
            "     | > loss_feat: 5.06875  (4.66753)\n",
            "     | > loss_mel: 18.08642  (18.11454)\n",
            "     | > loss_duration: 1.61762  (1.65130)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.23611  (27.82951)\n",
            "     | > grad_norm_0: 181.09871  (212.12613)\n",
            "     | > loss_disc: 2.47503  (2.54638)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.47503  (2.54638)\n",
            "     | > grad_norm_1: 26.42049  (29.00541)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.60190  (3.44283)\n",
            "     | > loader_time: 0.03020  (0.02506)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 140/173 -- GLOBAL_STEP: 59750\u001b[0m\n",
            "     | > loss_gen: 2.18692  (2.12373)\n",
            "     | > loss_kl: 1.34791  (1.28610)\n",
            "     | > loss_feat: 5.26457  (4.65593)\n",
            "     | > loss_mel: 18.15138  (18.09556)\n",
            "     | > loss_duration: 1.62985  (1.64838)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.58062  (27.80971)\n",
            "     | > grad_norm_0: 111.26208  (204.54793)\n",
            "     | > loss_disc: 2.59614  (2.54726)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.59614  (2.54726)\n",
            "     | > grad_norm_1: 12.08817  (29.53255)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.73690  (3.47885)\n",
            "     | > loader_time: 0.03690  (0.02637)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 165/173 -- GLOBAL_STEP: 59775\u001b[0m\n",
            "     | > loss_gen: 2.08476  (2.12286)\n",
            "     | > loss_kl: 1.44579  (1.29954)\n",
            "     | > loss_feat: 4.91130  (4.63237)\n",
            "     | > loss_mel: 18.14794  (18.08441)\n",
            "     | > loss_duration: 1.65709  (1.64801)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.24687  (27.78719)\n",
            "     | > grad_norm_0: 100.59772  (191.79811)\n",
            "     | > loss_disc: 2.47859  (2.54637)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.47859  (2.54637)\n",
            "     | > grad_norm_1: 24.82969  (27.82226)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.89180  (3.52512)\n",
            "     | > loader_time: 0.04240  (0.02801)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.00479  (2.00479)\n",
            "     | > loss_kl: 1.42479  (1.42479)\n",
            "     | > loss_feat: 4.75159  (4.75159)\n",
            "     | > loss_mel: 18.45435  (18.45435)\n",
            "     | > loss_duration: 1.76825  (1.76825)\n",
            "     | > loss_0: 28.40376  (28.40376)\n",
            "     | > loss_disc: 2.42224  (2.42224)\n",
            "     | > loss_1: 2.42224  (2.42224)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 1.82717  (1.82717)\n",
            "     | > loss_kl: 2.41090  (2.41090)\n",
            "     | > loss_feat: 3.80046  (3.80046)\n",
            "     | > loss_mel: 17.75105  (17.75105)\n",
            "     | > loss_duration: 1.73440  (1.73440)\n",
            "     | > loss_0: 27.52398  (27.52398)\n",
            "     | > loss_disc: 2.58699  (2.58699)\n",
            "     | > loss_1: 2.58699  (2.58699)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 1.86461  (1.84589)\n",
            "     | > loss_kl: 1.92021  (2.16556)\n",
            "     | > loss_feat: 3.99636  (3.89841)\n",
            "     | > loss_mel: 18.63235  (18.19170)\n",
            "     | > loss_duration: 1.75653  (1.74547)\n",
            "     | > loss_0: 28.17006  (27.84702)\n",
            "     | > loss_disc: 2.59635  (2.59167)\n",
            "     | > loss_1: 2.59635  (2.59167)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.00526  (1.89901)\n",
            "     | > loss_kl: 2.31107  (2.21406)\n",
            "     | > loss_feat: 4.93949  (4.24543)\n",
            "     | > loss_mel: 18.62001  (18.33447)\n",
            "     | > loss_duration: 1.73171  (1.74088)\n",
            "     | > loss_0: 29.60753  (28.43386)\n",
            "     | > loss_disc: 2.50495  (2.56277)\n",
            "     | > loss_1: 2.50495  (2.56277)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 1.99159  (1.92216)\n",
            "     | > loss_kl: 1.86424  (2.12661)\n",
            "     | > loss_feat: 4.02443  (4.19018)\n",
            "     | > loss_mel: 18.04452  (18.26198)\n",
            "     | > loss_duration: 1.75619  (1.74471)\n",
            "     | > loss_0: 27.68097  (28.24563)\n",
            "     | > loss_disc: 2.50631  (2.54865)\n",
            "     | > loss_1: 2.50631  (2.54865)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 1.71517  (1.88076)\n",
            "     | > loss_kl: 1.94606  (2.09050)\n",
            "     | > loss_feat: 4.45403  (4.24295)\n",
            "     | > loss_mel: 19.92982  (18.59555)\n",
            "     | > loss_duration: 1.68362  (1.73249)\n",
            "     | > loss_0: 29.72869  (28.54225)\n",
            "     | > loss_disc: 2.55046  (2.54901)\n",
            "     | > loss_1: 2.55046  (2.54901)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01453 \u001b[0m(+0.00026)\n",
            "     | > avg_loss_gen:\u001b[92m 1.88076 \u001b[0m(-0.38129)\n",
            "     | > avg_loss_kl:\u001b[92m 2.09050 \u001b[0m(-0.08257)\n",
            "     | > avg_loss_feat:\u001b[92m 4.24295 \u001b[0m(-0.00074)\n",
            "     | > avg_loss_mel:\u001b[91m 18.59555 \u001b[0m(+0.34477)\n",
            "     | > avg_loss_duration:\u001b[92m 1.73249 \u001b[0m(-0.02475)\n",
            "     | > avg_loss_0:\u001b[92m 28.54225 \u001b[0m(-0.14458)\n",
            "     | > avg_loss_disc:\u001b[91m 2.54901 \u001b[0m(+0.03946)\n",
            "     | > avg_loss_1:\u001b[91m 2.54901 \u001b[0m(+0.03946)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 16/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 09:13:10) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/173 -- GLOBAL_STEP: 59800\u001b[0m\n",
            "     | > loss_gen: 2.13246  (2.13749)\n",
            "     | > loss_kl: 1.20384  (1.17419)\n",
            "     | > loss_feat: 4.49986  (4.83294)\n",
            "     | > loss_mel: 18.20995  (18.14429)\n",
            "     | > loss_duration: 1.66896  (1.68853)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.71507  (27.97743)\n",
            "     | > grad_norm_0: 43.46599  (135.55470)\n",
            "     | > loss_disc: 2.53256  (2.52737)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.53256  (2.52737)\n",
            "     | > grad_norm_1: 7.68863  (15.12376)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.32550  (3.31086)\n",
            "     | > loader_time: 0.02000  (0.01966)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 41/173 -- GLOBAL_STEP: 59825\u001b[0m\n",
            "     | > loss_gen: 1.97284  (2.13006)\n",
            "     | > loss_kl: 1.13392  (1.21550)\n",
            "     | > loss_feat: 4.79957  (4.69437)\n",
            "     | > loss_mel: 18.21540  (18.15528)\n",
            "     | > loss_duration: 1.69245  (1.66749)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.81417  (27.86270)\n",
            "     | > grad_norm_0: 209.29172  (212.51187)\n",
            "     | > loss_disc: 2.55271  (2.53668)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.55271  (2.53668)\n",
            "     | > grad_norm_1: 27.57843  (23.18946)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.35570  (3.34800)\n",
            "     | > loader_time: 0.02150  (0.02157)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 66/173 -- GLOBAL_STEP: 59850\u001b[0m\n",
            "     | > loss_gen: 2.61137  (2.14596)\n",
            "     | > loss_kl: 1.24858  (1.24717)\n",
            "     | > loss_feat: 4.63285  (4.70567)\n",
            "     | > loss_mel: 17.89875  (18.16142)\n",
            "     | > loss_duration: 1.65787  (1.66042)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.04943  (27.92063)\n",
            "     | > grad_norm_0: 533.87384  (263.83554)\n",
            "     | > loss_disc: 2.62239  (2.53100)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.62239  (2.53100)\n",
            "     | > grad_norm_1: 60.13435  (28.42500)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.48920  (3.37952)\n",
            "     | > loader_time: 0.02530  (0.02280)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 91/173 -- GLOBAL_STEP: 59875\u001b[0m\n",
            "     | > loss_gen: 2.21769  (2.13958)\n",
            "     | > loss_kl: 1.34287  (1.26158)\n",
            "     | > loss_feat: 4.83856  (4.69001)\n",
            "     | > loss_mel: 18.20348  (18.15356)\n",
            "     | > loss_duration: 1.66580  (1.65617)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.26840  (27.90091)\n",
            "     | > grad_norm_0: 253.19034  (271.37650)\n",
            "     | > loss_disc: 2.51475  (2.53458)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.51475  (2.53458)\n",
            "     | > grad_norm_1: 18.24344  (31.42921)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.56010  (3.41367)\n",
            "     | > loader_time: 0.03020  (0.02403)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 116/173 -- GLOBAL_STEP: 59900\u001b[0m\n",
            "     | > loss_gen: 2.02832  (2.13818)\n",
            "     | > loss_kl: 1.34813  (1.27316)\n",
            "     | > loss_feat: 4.17923  (4.68315)\n",
            "     | > loss_mel: 18.31534  (18.14048)\n",
            "     | > loss_duration: 1.63397  (1.65267)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.50499  (27.88765)\n",
            "     | > grad_norm_0: 292.86313  (268.12265)\n",
            "     | > loss_disc: 2.57637  (2.53242)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.57637  (2.53242)\n",
            "     | > grad_norm_1: 42.89442  (30.96798)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.64040  (3.45176)\n",
            "     | > loader_time: 0.03030  (0.02550)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 141/173 -- GLOBAL_STEP: 59925\u001b[0m\n",
            "     | > loss_gen: 2.03409  (2.13360)\n",
            "     | > loss_kl: 1.35758  (1.28512)\n",
            "     | > loss_feat: 4.73847  (4.66068)\n",
            "     | > loss_mel: 18.31085  (18.12010)\n",
            "     | > loss_duration: 1.62917  (1.64942)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.07014  (27.84893)\n",
            "     | > grad_norm_0: 184.70006  (276.47635)\n",
            "     | > loss_disc: 2.48193  (2.53545)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.48193  (2.53545)\n",
            "     | > grad_norm_1: 18.83335  (32.18273)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.78020  (3.48539)\n",
            "     | > loader_time: 0.03540  (0.02693)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 166/173 -- GLOBAL_STEP: 59950\u001b[0m\n",
            "     | > loss_gen: 2.43479  (2.13534)\n",
            "     | > loss_kl: 1.35880  (1.30247)\n",
            "     | > loss_feat: 4.44821  (4.63776)\n",
            "     | > loss_mel: 18.22874  (18.12068)\n",
            "     | > loss_duration: 1.69273  (1.64900)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.16326  (27.84523)\n",
            "     | > grad_norm_0: 122.65395  (268.91541)\n",
            "     | > loss_disc: 2.59836  (2.53847)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.59836  (2.53847)\n",
            "     | > grad_norm_1: 39.66698  (31.70538)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.85320  (3.53081)\n",
            "     | > loader_time: 0.04040  (0.02859)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.04649  (2.04649)\n",
            "     | > loss_kl: 1.61769  (1.61769)\n",
            "     | > loss_feat: 3.85566  (3.85566)\n",
            "     | > loss_mel: 18.60426  (18.60426)\n",
            "     | > loss_duration: 1.82328  (1.82328)\n",
            "     | > loss_0: 27.94738  (27.94738)\n",
            "     | > loss_disc: 2.49212  (2.49212)\n",
            "     | > loss_1: 2.49212  (2.49212)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 1.90979  (1.90979)\n",
            "     | > loss_kl: 2.34662  (2.34662)\n",
            "     | > loss_feat: 4.67752  (4.67752)\n",
            "     | > loss_mel: 18.36734  (18.36734)\n",
            "     | > loss_duration: 1.75892  (1.75892)\n",
            "     | > loss_0: 29.06019  (29.06019)\n",
            "     | > loss_disc: 2.59906  (2.59906)\n",
            "     | > loss_1: 2.59906  (2.59906)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.03954  (1.97467)\n",
            "     | > loss_kl: 1.95843  (2.15253)\n",
            "     | > loss_feat: 3.88563  (4.28157)\n",
            "     | > loss_mel: 18.66194  (18.51464)\n",
            "     | > loss_duration: 1.78231  (1.77062)\n",
            "     | > loss_0: 28.32786  (28.69403)\n",
            "     | > loss_disc: 2.53677  (2.56792)\n",
            "     | > loss_1: 2.53677  (2.56792)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.04368  (1.99767)\n",
            "     | > loss_kl: 2.42131  (2.24212)\n",
            "     | > loss_feat: 4.57637  (4.37984)\n",
            "     | > loss_mel: 18.47681  (18.50203)\n",
            "     | > loss_duration: 1.73974  (1.76033)\n",
            "     | > loss_0: 29.25792  (28.88199)\n",
            "     | > loss_disc: 2.53510  (2.55698)\n",
            "     | > loss_1: 2.53510  (2.55698)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 1.99073  (1.99594)\n",
            "     | > loss_kl: 2.05522  (2.19540)\n",
            "     | > loss_feat: 3.95746  (4.27424)\n",
            "     | > loss_mel: 17.93898  (18.36127)\n",
            "     | > loss_duration: 1.73955  (1.75513)\n",
            "     | > loss_0: 27.68194  (28.58198)\n",
            "     | > loss_disc: 2.60697  (2.56947)\n",
            "     | > loss_1: 2.60697  (2.56947)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.03054  (2.00286)\n",
            "     | > loss_kl: 2.27473  (2.21126)\n",
            "     | > loss_feat: 4.30229  (4.27985)\n",
            "     | > loss_mel: 19.01744  (18.49250)\n",
            "     | > loss_duration: 1.69800  (1.74371)\n",
            "     | > loss_0: 29.32300  (28.73018)\n",
            "     | > loss_disc: 2.41282  (2.53814)\n",
            "     | > loss_1: 2.41282  (2.53814)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01772 \u001b[0m(+0.00320)\n",
            "     | > avg_loss_gen:\u001b[91m 2.00286 \u001b[0m(+0.12210)\n",
            "     | > avg_loss_kl:\u001b[91m 2.21126 \u001b[0m(+0.12077)\n",
            "     | > avg_loss_feat:\u001b[91m 4.27985 \u001b[0m(+0.03690)\n",
            "     | > avg_loss_mel:\u001b[92m 18.49250 \u001b[0m(-0.10304)\n",
            "     | > avg_loss_duration:\u001b[91m 1.74371 \u001b[0m(+0.01122)\n",
            "     | > avg_loss_0:\u001b[91m 28.73018 \u001b[0m(+0.18794)\n",
            "     | > avg_loss_disc:\u001b[92m 2.53814 \u001b[0m(-0.01087)\n",
            "     | > avg_loss_1:\u001b[92m 2.53814 \u001b[0m(-0.01087)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 17/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 09:23:46) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 17/173 -- GLOBAL_STEP: 59975\u001b[0m\n",
            "     | > loss_gen: 2.03005  (2.13684)\n",
            "     | > loss_kl: 1.30074  (1.22382)\n",
            "     | > loss_feat: 4.58646  (4.83601)\n",
            "     | > loss_mel: 18.14748  (18.16610)\n",
            "     | > loss_duration: 1.66303  (1.68218)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.72776  (28.04496)\n",
            "     | > grad_norm_0: 124.59071  (202.94646)\n",
            "     | > loss_disc: 2.54000  (2.50420)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.54000  (2.50420)\n",
            "     | > grad_norm_1: 22.53358  (21.92482)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.30040  (3.30139)\n",
            "     | > loader_time: 0.01990  (0.02028)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 42/173 -- GLOBAL_STEP: 60000\u001b[0m\n",
            "     | > loss_gen: 2.08907  (2.15142)\n",
            "     | > loss_kl: 1.17863  (1.24106)\n",
            "     | > loss_feat: 4.42564  (4.77024)\n",
            "     | > loss_mel: 17.98094  (18.09002)\n",
            "     | > loss_duration: 1.68307  (1.66623)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.35734  (27.91896)\n",
            "     | > grad_norm_0: 80.92274  (237.52000)\n",
            "     | > loss_disc: 2.61664  (2.51496)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.61664  (2.51496)\n",
            "     | > grad_norm_1: 22.91636  (27.26628)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.37070  (3.34081)\n",
            "     | > loader_time: 0.02500  (0.02157)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2/checkpoint_60000.pth.tar\n",
            "\n",
            "\u001b[1m   --> STEP: 67/173 -- GLOBAL_STEP: 60025\u001b[0m\n",
            "     | > loss_gen: 2.12991  (2.13869)\n",
            "     | > loss_kl: 1.29545  (1.25123)\n",
            "     | > loss_feat: 5.36012  (4.75131)\n",
            "     | > loss_mel: 18.43482  (18.11469)\n",
            "     | > loss_duration: 1.63975  (1.65816)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.86005  (27.91407)\n",
            "     | > grad_norm_0: 304.95810  (249.01410)\n",
            "     | > loss_disc: 2.49008  (2.52225)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49008  (2.52225)\n",
            "     | > grad_norm_1: 36.92243  (27.74617)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.48730  (3.38122)\n",
            "     | > loader_time: 0.02790  (0.02350)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 92/173 -- GLOBAL_STEP: 60050\u001b[0m\n",
            "     | > loss_gen: 1.99853  (2.13875)\n",
            "     | > loss_kl: 1.34544  (1.26268)\n",
            "     | > loss_feat: 4.70800  (4.71801)\n",
            "     | > loss_mel: 17.86885  (18.12247)\n",
            "     | > loss_duration: 1.63417  (1.65411)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.55500  (27.89602)\n",
            "     | > grad_norm_0: 402.07022  (260.53952)\n",
            "     | > loss_disc: 2.50437  (2.52741)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.50437  (2.52741)\n",
            "     | > grad_norm_1: 34.32738  (30.88449)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.59500  (3.41539)\n",
            "     | > loader_time: 0.02970  (0.02463)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 117/173 -- GLOBAL_STEP: 60075\u001b[0m\n",
            "     | > loss_gen: 2.18906  (2.14173)\n",
            "     | > loss_kl: 1.30907  (1.27360)\n",
            "     | > loss_feat: 4.73274  (4.71735)\n",
            "     | > loss_mel: 17.97320  (18.11916)\n",
            "     | > loss_duration: 1.63874  (1.65099)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.84282  (27.90283)\n",
            "     | > grad_norm_0: 92.51547  (271.53323)\n",
            "     | > loss_disc: 2.44455  (2.52520)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.44455  (2.52520)\n",
            "     | > grad_norm_1: 9.44075  (31.84022)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.63010  (3.45250)\n",
            "     | > loader_time: 0.03010  (0.02602)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 142/173 -- GLOBAL_STEP: 60100\u001b[0m\n",
            "     | > loss_gen: 2.14376  (2.14217)\n",
            "     | > loss_kl: 1.34863  (1.28253)\n",
            "     | > loss_feat: 5.19504  (4.71362)\n",
            "     | > loss_mel: 18.29163  (18.10815)\n",
            "     | > loss_duration: 1.63585  (1.64842)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.61491  (27.89488)\n",
            "     | > grad_norm_0: 107.60915  (264.22992)\n",
            "     | > loss_disc: 2.46243  (2.52823)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.46243  (2.52823)\n",
            "     | > grad_norm_1: 16.81915  (30.95520)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.76770  (3.49127)\n",
            "     | > loader_time: 0.03470  (0.02724)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 167/173 -- GLOBAL_STEP: 60125\u001b[0m\n",
            "     | > loss_gen: 1.84902  (2.13475)\n",
            "     | > loss_kl: 1.36252  (1.29562)\n",
            "     | > loss_feat: 4.29563  (4.65949)\n",
            "     | > loss_mel: 18.13213  (18.09722)\n",
            "     | > loss_duration: 1.67704  (1.64866)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.31633  (27.83575)\n",
            "     | > grad_norm_0: 198.12006  (256.90811)\n",
            "     | > loss_disc: 2.65710  (2.53383)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.65710  (2.53383)\n",
            "     | > grad_norm_1: 34.81007  (31.64362)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.86670  (3.53816)\n",
            "     | > loader_time: 0.03970  (0.02882)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 1.87504  (1.87504)\n",
            "     | > loss_kl: 1.83701  (1.83701)\n",
            "     | > loss_feat: 4.27169  (4.27169)\n",
            "     | > loss_mel: 18.31719  (18.31719)\n",
            "     | > loss_duration: 1.78110  (1.78110)\n",
            "     | > loss_0: 28.08202  (28.08202)\n",
            "     | > loss_disc: 2.60038  (2.60038)\n",
            "     | > loss_1: 2.60038  (2.60038)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 1.90865  (1.90865)\n",
            "     | > loss_kl: 2.07506  (2.07506)\n",
            "     | > loss_feat: 4.75149  (4.75149)\n",
            "     | > loss_mel: 18.95033  (18.95033)\n",
            "     | > loss_duration: 1.71894  (1.71894)\n",
            "     | > loss_0: 29.40447  (29.40447)\n",
            "     | > loss_disc: 2.53325  (2.53325)\n",
            "     | > loss_1: 2.53325  (2.53325)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 1.88985  (1.89925)\n",
            "     | > loss_kl: 1.80652  (1.94079)\n",
            "     | > loss_feat: 4.59698  (4.67424)\n",
            "     | > loss_mel: 18.99854  (18.97443)\n",
            "     | > loss_duration: 1.76749  (1.74322)\n",
            "     | > loss_0: 29.05938  (29.23193)\n",
            "     | > loss_disc: 2.61564  (2.57445)\n",
            "     | > loss_1: 2.61564  (2.57445)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 1.79389  (1.86413)\n",
            "     | > loss_kl: 2.20531  (2.02896)\n",
            "     | > loss_feat: 4.90356  (4.75068)\n",
            "     | > loss_mel: 17.67477  (18.54121)\n",
            "     | > loss_duration: 1.71627  (1.73423)\n",
            "     | > loss_0: 28.29380  (28.91922)\n",
            "     | > loss_disc: 2.58832  (2.57907)\n",
            "     | > loss_1: 2.58832  (2.57907)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 1.78910  (1.84537)\n",
            "     | > loss_kl: 1.93366  (2.00514)\n",
            "     | > loss_feat: 3.78838  (4.51010)\n",
            "     | > loss_mel: 18.28793  (18.47789)\n",
            "     | > loss_duration: 1.73730  (1.73500)\n",
            "     | > loss_0: 27.53637  (28.57350)\n",
            "     | > loss_disc: 2.67831  (2.60388)\n",
            "     | > loss_1: 2.67831  (2.60388)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 1.83270  (1.84284)\n",
            "     | > loss_kl: 2.02869  (2.00985)\n",
            "     | > loss_feat: 5.38685  (4.68545)\n",
            "     | > loss_mel: 20.38867  (18.86005)\n",
            "     | > loss_duration: 1.72112  (1.73223)\n",
            "     | > loss_0: 31.35803  (29.13041)\n",
            "     | > loss_disc: 2.73156  (2.62942)\n",
            "     | > loss_1: 2.73156  (2.62942)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01520 \u001b[0m(-0.00252)\n",
            "     | > avg_loss_gen:\u001b[92m 1.84284 \u001b[0m(-0.16002)\n",
            "     | > avg_loss_kl:\u001b[92m 2.00985 \u001b[0m(-0.20142)\n",
            "     | > avg_loss_feat:\u001b[91m 4.68545 \u001b[0m(+0.40560)\n",
            "     | > avg_loss_mel:\u001b[91m 18.86005 \u001b[0m(+0.36754)\n",
            "     | > avg_loss_duration:\u001b[92m 1.73223 \u001b[0m(-0.01148)\n",
            "     | > avg_loss_0:\u001b[91m 29.13041 \u001b[0m(+0.40023)\n",
            "     | > avg_loss_disc:\u001b[91m 2.62942 \u001b[0m(+0.09127)\n",
            "     | > avg_loss_1:\u001b[91m 2.62942 \u001b[0m(+0.09127)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 18/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 09:34:27) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 18/173 -- GLOBAL_STEP: 60150\u001b[0m\n",
            "     | > loss_gen: 1.93032  (2.16819)\n",
            "     | > loss_kl: 1.12627  (1.21715)\n",
            "     | > loss_feat: 4.73268  (4.89388)\n",
            "     | > loss_mel: 18.25624  (18.11279)\n",
            "     | > loss_duration: 1.63942  (1.68323)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.68493  (28.07523)\n",
            "     | > grad_norm_0: 349.75098  (294.34879)\n",
            "     | > loss_disc: 2.62169  (2.49848)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.62169  (2.49848)\n",
            "     | > grad_norm_1: 42.83515  (29.42458)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.31630  (3.30402)\n",
            "     | > loader_time: 0.02390  (0.02061)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 43/173 -- GLOBAL_STEP: 60175\u001b[0m\n",
            "     | > loss_gen: 1.88001  (2.15478)\n",
            "     | > loss_kl: 1.22304  (1.23079)\n",
            "     | > loss_feat: 4.67160  (4.74990)\n",
            "     | > loss_mel: 18.08681  (18.10153)\n",
            "     | > loss_duration: 1.67027  (1.66794)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.53174  (27.90494)\n",
            "     | > grad_norm_0: 105.21513  (298.50095)\n",
            "     | > loss_disc: 2.53404  (2.53322)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.53404  (2.53322)\n",
            "     | > grad_norm_1: 34.01911  (35.18346)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.37940  (3.34736)\n",
            "     | > loader_time: 0.02430  (0.02191)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 68/173 -- GLOBAL_STEP: 60200\u001b[0m\n",
            "     | > loss_gen: 2.17493  (2.14585)\n",
            "     | > loss_kl: 1.22903  (1.24318)\n",
            "     | > loss_feat: 4.77789  (4.72703)\n",
            "     | > loss_mel: 17.85532  (18.11119)\n",
            "     | > loss_duration: 1.61730  (1.66056)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.65447  (27.88781)\n",
            "     | > grad_norm_0: 68.61525  (250.93086)\n",
            "     | > loss_disc: 2.47087  (2.53214)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.47087  (2.53214)\n",
            "     | > grad_norm_1: 18.44391  (33.34251)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.55840  (3.38044)\n",
            "     | > loader_time: 0.02560  (0.02319)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 93/173 -- GLOBAL_STEP: 60225\u001b[0m\n",
            "     | > loss_gen: 1.98509  (2.14153)\n",
            "     | > loss_kl: 1.38416  (1.26834)\n",
            "     | > loss_feat: 4.03698  (4.70317)\n",
            "     | > loss_mel: 18.42155  (18.11101)\n",
            "     | > loss_duration: 1.66160  (1.65648)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.48937  (27.88054)\n",
            "     | > grad_norm_0: 119.76329  (251.22131)\n",
            "     | > loss_disc: 2.50915  (2.53753)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.50915  (2.53753)\n",
            "     | > grad_norm_1: 18.37197  (32.49912)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.56590  (3.41801)\n",
            "     | > loader_time: 0.03100  (0.02466)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 118/173 -- GLOBAL_STEP: 60250\u001b[0m\n",
            "     | > loss_gen: 2.15279  (2.13987)\n",
            "     | > loss_kl: 1.47319  (1.28378)\n",
            "     | > loss_feat: 5.10849  (4.69733)\n",
            "     | > loss_mel: 18.48852  (18.08995)\n",
            "     | > loss_duration: 1.63077  (1.65250)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.85376  (27.86343)\n",
            "     | > grad_norm_0: 352.17715  (229.08907)\n",
            "     | > loss_disc: 2.45210  (2.52968)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.45210  (2.52968)\n",
            "     | > grad_norm_1: 30.85496  (30.02005)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.57900  (3.45613)\n",
            "     | > loader_time: 0.03340  (0.02585)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 143/173 -- GLOBAL_STEP: 60275\u001b[0m\n",
            "     | > loss_gen: 2.23650  (2.13935)\n",
            "     | > loss_kl: 1.14535  (1.28819)\n",
            "     | > loss_feat: 5.34233  (4.70025)\n",
            "     | > loss_mel: 17.88255  (18.07591)\n",
            "     | > loss_duration: 1.63380  (1.64917)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.24054  (27.85288)\n",
            "     | > grad_norm_0: 437.65527  (233.32567)\n",
            "     | > loss_disc: 2.47999  (2.52995)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.47999  (2.52995)\n",
            "     | > grad_norm_1: 40.63366  (29.04670)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.71320  (3.49411)\n",
            "     | > loader_time: 0.03260  (0.02695)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 168/173 -- GLOBAL_STEP: 60300\u001b[0m\n",
            "     | > loss_gen: 2.08752  (2.13773)\n",
            "     | > loss_kl: 1.38707  (1.29969)\n",
            "     | > loss_feat: 4.20187  (4.65267)\n",
            "     | > loss_mel: 17.96623  (18.06352)\n",
            "     | > loss_duration: 1.67991  (1.64972)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.32260  (27.80333)\n",
            "     | > grad_norm_0: 185.28845  (225.36635)\n",
            "     | > loss_disc: 2.57500  (2.53073)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.57500  (2.53073)\n",
            "     | > grad_norm_1: 19.86965  (28.55449)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.95530  (3.54632)\n",
            "     | > loader_time: 0.04220  (0.02883)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 1.95437  (1.95437)\n",
            "     | > loss_kl: 1.63189  (1.63189)\n",
            "     | > loss_feat: 4.61624  (4.61624)\n",
            "     | > loss_mel: 19.26646  (19.26646)\n",
            "     | > loss_duration: 1.77534  (1.77534)\n",
            "     | > loss_0: 29.24430  (29.24430)\n",
            "     | > loss_disc: 2.62049  (2.62049)\n",
            "     | > loss_1: 2.62049  (2.62049)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 1.99388  (1.99388)\n",
            "     | > loss_kl: 2.20128  (2.20128)\n",
            "     | > loss_feat: 4.23310  (4.23310)\n",
            "     | > loss_mel: 17.70959  (17.70959)\n",
            "     | > loss_duration: 1.73162  (1.73162)\n",
            "     | > loss_0: 27.86946  (27.86946)\n",
            "     | > loss_disc: 2.61862  (2.61862)\n",
            "     | > loss_1: 2.61862  (2.61862)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.11726  (2.05557)\n",
            "     | > loss_kl: 1.82129  (2.01129)\n",
            "     | > loss_feat: 4.23266  (4.23288)\n",
            "     | > loss_mel: 18.28370  (17.99664)\n",
            "     | > loss_duration: 1.75532  (1.74347)\n",
            "     | > loss_0: 28.21023  (28.03985)\n",
            "     | > loss_disc: 2.47375  (2.54618)\n",
            "     | > loss_1: 2.47375  (2.54618)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 1.92290  (2.01135)\n",
            "     | > loss_kl: 2.38239  (2.13499)\n",
            "     | > loss_feat: 4.77277  (4.41284)\n",
            "     | > loss_mel: 18.70360  (18.23230)\n",
            "     | > loss_duration: 1.71824  (1.73506)\n",
            "     | > loss_0: 29.49989  (28.52653)\n",
            "     | > loss_disc: 2.47210  (2.52149)\n",
            "     | > loss_1: 2.47210  (2.52149)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.11786  (2.03797)\n",
            "     | > loss_kl: 2.11725  (2.13055)\n",
            "     | > loss_feat: 4.21957  (4.36452)\n",
            "     | > loss_mel: 18.69111  (18.34700)\n",
            "     | > loss_duration: 1.73718  (1.73559)\n",
            "     | > loss_0: 28.88296  (28.61564)\n",
            "     | > loss_disc: 2.44670  (2.50279)\n",
            "     | > loss_1: 2.44670  (2.50279)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.12723  (2.05582)\n",
            "     | > loss_kl: 2.13490  (2.13142)\n",
            "     | > loss_feat: 3.66828  (4.22527)\n",
            "     | > loss_mel: 18.65341  (18.40828)\n",
            "     | > loss_duration: 1.72178  (1.73283)\n",
            "     | > loss_0: 28.30559  (28.55363)\n",
            "     | > loss_disc: 2.70448  (2.54313)\n",
            "     | > loss_1: 2.70448  (2.54313)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01467 \u001b[0m(-0.00054)\n",
            "     | > avg_loss_gen:\u001b[91m 2.05582 \u001b[0m(+0.21299)\n",
            "     | > avg_loss_kl:\u001b[91m 2.13142 \u001b[0m(+0.12158)\n",
            "     | > avg_loss_feat:\u001b[92m 4.22527 \u001b[0m(-0.46018)\n",
            "     | > avg_loss_mel:\u001b[92m 18.40828 \u001b[0m(-0.45177)\n",
            "     | > avg_loss_duration:\u001b[91m 1.73283 \u001b[0m(+0.00061)\n",
            "     | > avg_loss_0:\u001b[92m 28.55363 \u001b[0m(-0.57678)\n",
            "     | > avg_loss_disc:\u001b[92m 2.54313 \u001b[0m(-0.08629)\n",
            "     | > avg_loss_1:\u001b[92m 2.54313 \u001b[0m(-0.08629)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 19/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 09:45:05) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 19/173 -- GLOBAL_STEP: 60325\u001b[0m\n",
            "     | > loss_gen: 1.96886  (2.14165)\n",
            "     | > loss_kl: 1.28477  (1.20896)\n",
            "     | > loss_feat: 4.48434  (4.75971)\n",
            "     | > loss_mel: 17.81313  (18.21484)\n",
            "     | > loss_duration: 1.64542  (1.67533)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.19651  (28.00048)\n",
            "     | > grad_norm_0: 40.29265  (225.31685)\n",
            "     | > loss_disc: 2.56543  (2.54353)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.56543  (2.54353)\n",
            "     | > grad_norm_1: 29.35910  (32.16744)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.32740  (3.30902)\n",
            "     | > loader_time: 0.02040  (0.02084)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 44/173 -- GLOBAL_STEP: 60350\u001b[0m\n",
            "     | > loss_gen: 2.09211  (2.15988)\n",
            "     | > loss_kl: 1.31128  (1.25288)\n",
            "     | > loss_feat: 4.48080  (4.78996)\n",
            "     | > loss_mel: 18.37175  (18.24194)\n",
            "     | > loss_duration: 1.68519  (1.66663)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.94114  (28.11129)\n",
            "     | > grad_norm_0: 217.56943  (218.65819)\n",
            "     | > loss_disc: 2.60323  (2.53685)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.60323  (2.53685)\n",
            "     | > grad_norm_1: 48.23501  (31.80536)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.39930  (3.35572)\n",
            "     | > loader_time: 0.02470  (0.02235)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 69/173 -- GLOBAL_STEP: 60375\u001b[0m\n",
            "     | > loss_gen: 2.06556  (2.14447)\n",
            "     | > loss_kl: 1.22371  (1.24776)\n",
            "     | > loss_feat: 4.19452  (4.73618)\n",
            "     | > loss_mel: 18.07627  (18.21985)\n",
            "     | > loss_duration: 1.63793  (1.65927)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.19798  (28.00753)\n",
            "     | > grad_norm_0: 81.08942  (199.86308)\n",
            "     | > loss_disc: 2.64390  (2.54116)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.64390  (2.54116)\n",
            "     | > grad_norm_1: 16.85448  (28.52473)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.42320  (3.38619)\n",
            "     | > loader_time: 0.02550  (0.02360)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 94/173 -- GLOBAL_STEP: 60400\u001b[0m\n",
            "     | > loss_gen: 2.00508  (2.14964)\n",
            "     | > loss_kl: 1.30593  (1.26271)\n",
            "     | > loss_feat: 4.20901  (4.72587)\n",
            "     | > loss_mel: 17.97803  (18.21205)\n",
            "     | > loss_duration: 1.62930  (1.65414)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.12736  (28.00441)\n",
            "     | > grad_norm_0: 32.10673  (189.22467)\n",
            "     | > loss_disc: 2.56433  (2.55306)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.56433  (2.55306)\n",
            "     | > grad_norm_1: 15.38856  (26.22567)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.50280  (3.42166)\n",
            "     | > loader_time: 0.02750  (0.02489)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 119/173 -- GLOBAL_STEP: 60425\u001b[0m\n",
            "     | > loss_gen: 1.98556  (2.14079)\n",
            "     | > loss_kl: 1.36797  (1.27713)\n",
            "     | > loss_feat: 4.15861  (4.70698)\n",
            "     | > loss_mel: 18.10189  (18.18387)\n",
            "     | > loss_duration: 1.61495  (1.65044)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.22898  (27.95921)\n",
            "     | > grad_norm_0: 64.50016  (181.72961)\n",
            "     | > loss_disc: 2.59859  (2.54727)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.59859  (2.54727)\n",
            "     | > grad_norm_1: 15.13109  (25.17681)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.59410  (3.45665)\n",
            "     | > loader_time: 0.03100  (0.02617)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 144/173 -- GLOBAL_STEP: 60450\u001b[0m\n",
            "     | > loss_gen: 2.10607  (2.13556)\n",
            "     | > loss_kl: 1.35504  (1.28640)\n",
            "     | > loss_feat: 5.17931  (4.68119)\n",
            "     | > loss_mel: 17.76771  (18.15716)\n",
            "     | > loss_duration: 1.64884  (1.64726)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.05696  (27.90756)\n",
            "     | > grad_norm_0: 397.13046  (183.98857)\n",
            "     | > loss_disc: 2.49529  (2.54720)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49529  (2.54720)\n",
            "     | > grad_norm_1: 27.42533  (24.50703)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.72120  (3.49309)\n",
            "     | > loader_time: 0.03740  (0.02760)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 169/173 -- GLOBAL_STEP: 60475\u001b[0m\n",
            "     | > loss_gen: 2.19318  (2.13577)\n",
            "     | > loss_kl: 1.35857  (1.29537)\n",
            "     | > loss_feat: 4.93367  (4.65435)\n",
            "     | > loss_mel: 18.18279  (18.13071)\n",
            "     | > loss_duration: 1.67855  (1.64791)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.34677  (27.86412)\n",
            "     | > grad_norm_0: 356.32605  (188.11890)\n",
            "     | > loss_disc: 2.43194  (2.54249)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.43194  (2.54249)\n",
            "     | > grad_norm_1: 24.12134  (25.18504)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.85380  (3.54259)\n",
            "     | > loader_time: 0.04310  (0.02928)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.44397  (2.44397)\n",
            "     | > loss_kl: 2.10534  (2.10534)\n",
            "     | > loss_feat: 4.88473  (4.88473)\n",
            "     | > loss_mel: 18.41478  (18.41478)\n",
            "     | > loss_duration: 1.76790  (1.76790)\n",
            "     | > loss_0: 29.61673  (29.61673)\n",
            "     | > loss_disc: 2.57585  (2.57585)\n",
            "     | > loss_1: 2.57585  (2.57585)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.48916  (2.48916)\n",
            "     | > loss_kl: 1.95008  (1.95008)\n",
            "     | > loss_feat: 4.10444  (4.10444)\n",
            "     | > loss_mel: 18.17287  (18.17287)\n",
            "     | > loss_duration: 1.73649  (1.73649)\n",
            "     | > loss_0: 28.45305  (28.45305)\n",
            "     | > loss_disc: 2.50769  (2.50769)\n",
            "     | > loss_1: 2.50769  (2.50769)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.44092  (2.46504)\n",
            "     | > loss_kl: 1.88890  (1.91949)\n",
            "     | > loss_feat: 4.03551  (4.06998)\n",
            "     | > loss_mel: 18.65219  (18.41253)\n",
            "     | > loss_duration: 1.75076  (1.74363)\n",
            "     | > loss_0: 28.76828  (28.61067)\n",
            "     | > loss_disc: 2.51366  (2.51067)\n",
            "     | > loss_1: 2.51366  (2.51067)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.55225  (2.49411)\n",
            "     | > loss_kl: 2.33681  (2.05860)\n",
            "     | > loss_feat: 4.70647  (4.28214)\n",
            "     | > loss_mel: 18.57451  (18.46652)\n",
            "     | > loss_duration: 1.71281  (1.73335)\n",
            "     | > loss_0: 29.88285  (29.03473)\n",
            "     | > loss_disc: 2.55214  (2.52450)\n",
            "     | > loss_1: 2.55214  (2.52450)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.50194  (2.49607)\n",
            "     | > loss_kl: 2.12159  (2.07434)\n",
            "     | > loss_feat: 4.78086  (4.40682)\n",
            "     | > loss_mel: 18.25010  (18.41242)\n",
            "     | > loss_duration: 1.72787  (1.73198)\n",
            "     | > loss_0: 29.38235  (29.12163)\n",
            "     | > loss_disc: 2.46578  (2.50982)\n",
            "     | > loss_1: 2.46578  (2.50982)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.59700  (2.51625)\n",
            "     | > loss_kl: 2.32418  (2.12431)\n",
            "     | > loss_feat: 4.06063  (4.33758)\n",
            "     | > loss_mel: 19.78043  (18.68602)\n",
            "     | > loss_duration: 1.73656  (1.73290)\n",
            "     | > loss_0: 30.49880  (29.39707)\n",
            "     | > loss_disc: 2.65690  (2.53923)\n",
            "     | > loss_1: 2.65690  (2.53923)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01789 \u001b[0m(+0.00322)\n",
            "     | > avg_loss_gen:\u001b[91m 2.51625 \u001b[0m(+0.46043)\n",
            "     | > avg_loss_kl:\u001b[92m 2.12431 \u001b[0m(-0.00711)\n",
            "     | > avg_loss_feat:\u001b[91m 4.33758 \u001b[0m(+0.11231)\n",
            "     | > avg_loss_mel:\u001b[91m 18.68602 \u001b[0m(+0.27774)\n",
            "     | > avg_loss_duration:\u001b[91m 1.73290 \u001b[0m(+0.00007)\n",
            "     | > avg_loss_0:\u001b[91m 29.39707 \u001b[0m(+0.84344)\n",
            "     | > avg_loss_disc:\u001b[92m 2.53923 \u001b[0m(-0.00390)\n",
            "     | > avg_loss_1:\u001b[92m 2.53923 \u001b[0m(-0.00390)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 20/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 09:55:41) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/173 -- GLOBAL_STEP: 60500\u001b[0m\n",
            "     | > loss_gen: 2.18316  (2.11747)\n",
            "     | > loss_kl: 1.37231  (1.19777)\n",
            "     | > loss_feat: 4.87626  (4.74732)\n",
            "     | > loss_mel: 18.20654  (18.17785)\n",
            "     | > loss_duration: 1.64317  (1.67875)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.28144  (27.91915)\n",
            "     | > grad_norm_0: 76.97713  (105.84176)\n",
            "     | > loss_disc: 2.45469  (2.58071)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.45469  (2.58071)\n",
            "     | > grad_norm_1: 17.43247  (26.81696)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.42130  (3.31508)\n",
            "     | > loader_time: 0.02200  (0.01985)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 45/173 -- GLOBAL_STEP: 60525\u001b[0m\n",
            "     | > loss_gen: 2.13497  (2.13056)\n",
            "     | > loss_kl: 1.25702  (1.23171)\n",
            "     | > loss_feat: 5.09359  (4.70762)\n",
            "     | > loss_mel: 18.11344  (18.07395)\n",
            "     | > loss_duration: 1.62851  (1.66513)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.22753  (27.80898)\n",
            "     | > grad_norm_0: 172.25523  (131.67668)\n",
            "     | > loss_disc: 2.44629  (2.54928)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.44629  (2.54928)\n",
            "     | > grad_norm_1: 14.26834  (22.58825)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.47760  (3.35662)\n",
            "     | > loader_time: 0.02530  (0.02176)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/173 -- GLOBAL_STEP: 60550\u001b[0m\n",
            "     | > loss_gen: 2.27284  (2.13813)\n",
            "     | > loss_kl: 1.33151  (1.24372)\n",
            "     | > loss_feat: 4.54694  (4.71983)\n",
            "     | > loss_mel: 18.29325  (18.11588)\n",
            "     | > loss_duration: 1.66404  (1.65934)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.10859  (27.87690)\n",
            "     | > grad_norm_0: 88.36356  (151.41426)\n",
            "     | > loss_disc: 2.51500  (2.53628)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.51500  (2.53628)\n",
            "     | > grad_norm_1: 10.63042  (20.26505)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.53530  (3.39257)\n",
            "     | > loader_time: 0.02630  (0.02334)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 95/173 -- GLOBAL_STEP: 60575\u001b[0m\n",
            "     | > loss_gen: 2.37657  (2.14318)\n",
            "     | > loss_kl: 1.19639  (1.25509)\n",
            "     | > loss_feat: 4.67190  (4.71597)\n",
            "     | > loss_mel: 18.09288  (18.12692)\n",
            "     | > loss_duration: 1.61468  (1.65449)\n",
            "     | > amp_scaler: 512.00000  (264.08421)\n",
            "     | > loss_0: 27.95242  (27.89566)\n",
            "     | > grad_norm_0: 457.60385  (168.31181)\n",
            "     | > loss_disc: 2.56285  (2.53525)\n",
            "     | > amp_scaler-1: 512.00000  (264.08421)\n",
            "     | > loss_1: 2.56285  (2.53525)\n",
            "     | > grad_norm_1: 49.41422  (21.66927)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.66060  (3.42586)\n",
            "     | > loader_time: 0.03060  (0.02488)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/173 -- GLOBAL_STEP: 60600\u001b[0m\n",
            "     | > loss_gen: 2.08345  (2.14108)\n",
            "     | > loss_kl: 1.26190  (1.26387)\n",
            "     | > loss_feat: 4.39538  (4.72041)\n",
            "     | > loss_mel: 18.00973  (18.13232)\n",
            "     | > loss_duration: 1.65987  (1.65182)\n",
            "     | > amp_scaler: 512.00000  (315.73333)\n",
            "     | > loss_0: 27.41033  (27.90950)\n",
            "     | > grad_norm_0: 277.29758  (184.60129)\n",
            "     | > loss_disc: 2.43826  (2.53122)\n",
            "     | > amp_scaler-1: 512.00000  (315.73333)\n",
            "     | > loss_1: 2.43826  (2.53122)\n",
            "     | > grad_norm_1: 31.95253  (23.00944)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.64040  (3.46222)\n",
            "     | > loader_time: 0.02830  (0.02614)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 145/173 -- GLOBAL_STEP: 60625\u001b[0m\n",
            "     | > loss_gen: 2.00370  (2.14077)\n",
            "     | > loss_kl: 1.27581  (1.27313)\n",
            "     | > loss_feat: 4.27937  (4.69734)\n",
            "     | > loss_mel: 18.26587  (18.10516)\n",
            "     | > loss_duration: 1.63243  (1.64927)\n",
            "     | > amp_scaler: 256.00000  (335.44828)\n",
            "     | > loss_0: 27.45718  (27.86566)\n",
            "     | > grad_norm_0: 256.53391  (189.95155)\n",
            "     | > loss_disc: 2.57190  (2.53124)\n",
            "     | > amp_scaler-1: 256.00000  (335.44828)\n",
            "     | > loss_1: 2.57190  (2.53124)\n",
            "     | > grad_norm_1: 15.57259  (24.37141)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.70540  (3.49771)\n",
            "     | > loader_time: 0.03320  (0.02729)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 170/173 -- GLOBAL_STEP: 60650\u001b[0m\n",
            "     | > loss_gen: 2.08150  (2.13873)\n",
            "     | > loss_kl: 1.29065  (1.28376)\n",
            "     | > loss_feat: 4.08426  (4.66794)\n",
            "     | > loss_mel: 17.92135  (18.10603)\n",
            "     | > loss_duration: 1.66315  (1.64993)\n",
            "     | > amp_scaler: 256.00000  (323.76471)\n",
            "     | > loss_0: 27.04092  (27.84641)\n",
            "     | > grad_norm_0: 106.08525  (201.18289)\n",
            "     | > loss_disc: 2.59672  (2.53355)\n",
            "     | > amp_scaler-1: 256.00000  (323.76471)\n",
            "     | > loss_1: 2.59672  (2.53355)\n",
            "     | > grad_norm_1: 22.55635  (25.66877)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.89800  (3.54871)\n",
            "     | > loader_time: 0.04230  (0.02903)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.23317  (2.23317)\n",
            "     | > loss_kl: 1.96368  (1.96368)\n",
            "     | > loss_feat: 4.45367  (4.45367)\n",
            "     | > loss_mel: 18.44732  (18.44732)\n",
            "     | > loss_duration: 1.79714  (1.79714)\n",
            "     | > loss_0: 28.89498  (28.89498)\n",
            "     | > loss_disc: 2.55820  (2.55820)\n",
            "     | > loss_1: 2.55820  (2.55820)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.13184  (2.13184)\n",
            "     | > loss_kl: 1.93998  (1.93998)\n",
            "     | > loss_feat: 4.74265  (4.74265)\n",
            "     | > loss_mel: 18.47959  (18.47959)\n",
            "     | > loss_duration: 1.74796  (1.74796)\n",
            "     | > loss_0: 29.04202  (29.04202)\n",
            "     | > loss_disc: 2.53917  (2.53917)\n",
            "     | > loss_1: 2.53917  (2.53917)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.17945  (2.15564)\n",
            "     | > loss_kl: 1.76710  (1.85354)\n",
            "     | > loss_feat: 4.21696  (4.47980)\n",
            "     | > loss_mel: 18.95540  (18.71749)\n",
            "     | > loss_duration: 1.76891  (1.75844)\n",
            "     | > loss_0: 28.88782  (28.96492)\n",
            "     | > loss_disc: 2.61228  (2.57573)\n",
            "     | > loss_1: 2.61228  (2.57573)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.12374  (2.14501)\n",
            "     | > loss_kl: 2.34287  (2.01665)\n",
            "     | > loss_feat: 4.38524  (4.44828)\n",
            "     | > loss_mel: 18.31862  (18.58454)\n",
            "     | > loss_duration: 1.72026  (1.74571)\n",
            "     | > loss_0: 28.89073  (28.94019)\n",
            "     | > loss_disc: 2.54290  (2.56478)\n",
            "     | > loss_1: 2.54290  (2.56478)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.22517  (2.16505)\n",
            "     | > loss_kl: 1.91021  (1.99004)\n",
            "     | > loss_feat: 4.72985  (4.51867)\n",
            "     | > loss_mel: 18.79640  (18.63750)\n",
            "     | > loss_duration: 1.75739  (1.74863)\n",
            "     | > loss_0: 29.41901  (29.05990)\n",
            "     | > loss_disc: 2.42670  (2.53026)\n",
            "     | > loss_1: 2.42670  (2.53026)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 1.99115  (2.13027)\n",
            "     | > loss_kl: 2.15482  (2.02300)\n",
            "     | > loss_feat: 3.22491  (4.25992)\n",
            "     | > loss_mel: 20.04750  (18.91950)\n",
            "     | > loss_duration: 1.72675  (1.74426)\n",
            "     | > loss_0: 29.14513  (29.07694)\n",
            "     | > loss_disc: 2.62126  (2.54846)\n",
            "     | > loss_1: 2.62126  (2.54846)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01802 \u001b[0m(+0.00013)\n",
            "     | > avg_loss_gen:\u001b[92m 2.13027 \u001b[0m(-0.38599)\n",
            "     | > avg_loss_kl:\u001b[92m 2.02300 \u001b[0m(-0.10131)\n",
            "     | > avg_loss_feat:\u001b[92m 4.25992 \u001b[0m(-0.07766)\n",
            "     | > avg_loss_mel:\u001b[91m 18.91950 \u001b[0m(+0.23348)\n",
            "     | > avg_loss_duration:\u001b[91m 1.74426 \u001b[0m(+0.01136)\n",
            "     | > avg_loss_0:\u001b[92m 29.07694 \u001b[0m(-0.32012)\n",
            "     | > avg_loss_disc:\u001b[91m 2.54846 \u001b[0m(+0.00923)\n",
            "     | > avg_loss_1:\u001b[91m 2.54846 \u001b[0m(+0.00923)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 21/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 10:06:19) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 21/173 -- GLOBAL_STEP: 60675\u001b[0m\n",
            "     | > loss_gen: 2.16061  (2.19323)\n",
            "     | > loss_kl: 1.16428  (1.23765)\n",
            "     | > loss_feat: 4.88913  (4.81109)\n",
            "     | > loss_mel: 18.03200  (18.16392)\n",
            "     | > loss_duration: 1.64023  (1.67426)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.88625  (28.08016)\n",
            "     | > grad_norm_0: 44.02758  (273.06604)\n",
            "     | > loss_disc: 2.56541  (2.55020)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.56541  (2.55020)\n",
            "     | > grad_norm_1: 16.52504  (41.29765)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.34070  (3.31952)\n",
            "     | > loader_time: 0.02170  (0.02051)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 46/173 -- GLOBAL_STEP: 60700\u001b[0m\n",
            "     | > loss_gen: 2.12149  (2.16530)\n",
            "     | > loss_kl: 1.41790  (1.24900)\n",
            "     | > loss_feat: 4.45991  (4.80476)\n",
            "     | > loss_mel: 18.65087  (18.16282)\n",
            "     | > loss_duration: 1.67685  (1.66275)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.32703  (28.04463)\n",
            "     | > grad_norm_0: 108.53738  (181.43527)\n",
            "     | > loss_disc: 2.58586  (2.53384)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.58586  (2.53384)\n",
            "     | > grad_norm_1: 9.08418  (28.09733)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.45220  (3.34583)\n",
            "     | > loader_time: 0.02580  (0.02238)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 71/173 -- GLOBAL_STEP: 60725\u001b[0m\n",
            "     | > loss_gen: 2.14677  (2.15244)\n",
            "     | > loss_kl: 1.30613  (1.25659)\n",
            "     | > loss_feat: 4.43480  (4.77058)\n",
            "     | > loss_mel: 17.65629  (18.13959)\n",
            "     | > loss_duration: 1.65724  (1.65709)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.20123  (27.97627)\n",
            "     | > grad_norm_0: 200.65532  (190.87201)\n",
            "     | > loss_disc: 2.55343  (2.53750)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.55343  (2.53750)\n",
            "     | > grad_norm_1: 17.14043  (26.11370)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.46370  (3.38855)\n",
            "     | > loader_time: 0.02590  (0.02391)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 96/173 -- GLOBAL_STEP: 60750\u001b[0m\n",
            "     | > loss_gen: 2.17347  (2.15292)\n",
            "     | > loss_kl: 1.22522  (1.26489)\n",
            "     | > loss_feat: 4.12886  (4.76114)\n",
            "     | > loss_mel: 17.82645  (18.11614)\n",
            "     | > loss_duration: 1.61145  (1.65212)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 26.96545  (27.94722)\n",
            "     | > grad_norm_0: 147.64171  (176.93291)\n",
            "     | > loss_disc: 2.52943  (2.52786)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.52943  (2.52786)\n",
            "     | > grad_norm_1: 12.43698  (24.06097)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.56700  (3.42296)\n",
            "     | > loader_time: 0.03130  (0.02531)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 121/173 -- GLOBAL_STEP: 60775\u001b[0m\n",
            "     | > loss_gen: 2.11174  (2.14734)\n",
            "     | > loss_kl: 1.19587  (1.27201)\n",
            "     | > loss_feat: 4.51131  (4.74199)\n",
            "     | > loss_mel: 17.83579  (18.12103)\n",
            "     | > loss_duration: 1.62151  (1.64924)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.27621  (27.93161)\n",
            "     | > grad_norm_0: 445.13135  (186.44424)\n",
            "     | > loss_disc: 2.49275  (2.53104)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49275  (2.53104)\n",
            "     | > grad_norm_1: 28.91661  (23.52423)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.71930  (3.46090)\n",
            "     | > loader_time: 0.03260  (0.02662)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 146/173 -- GLOBAL_STEP: 60800\u001b[0m\n",
            "     | > loss_gen: 2.05097  (2.14381)\n",
            "     | > loss_kl: 1.36686  (1.27877)\n",
            "     | > loss_feat: 4.81910  (4.71707)\n",
            "     | > loss_mel: 18.35217  (18.10201)\n",
            "     | > loss_duration: 1.63133  (1.64669)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.22044  (27.88835)\n",
            "     | > grad_norm_0: 449.77817  (195.66133)\n",
            "     | > loss_disc: 2.51611  (2.52951)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.51611  (2.52951)\n",
            "     | > grad_norm_1: 24.09221  (24.46865)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.80890  (3.49881)\n",
            "     | > loader_time: 0.03690  (0.02790)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 171/173 -- GLOBAL_STEP: 60825\u001b[0m\n",
            "     | > loss_gen: 2.07910  (2.14075)\n",
            "     | > loss_kl: 1.46187  (1.28950)\n",
            "     | > loss_feat: 4.23857  (4.68745)\n",
            "     | > loss_mel: 17.96098  (18.09122)\n",
            "     | > loss_duration: 1.69760  (1.64806)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.43812  (27.85698)\n",
            "     | > grad_norm_0: 406.58792  (201.36140)\n",
            "     | > loss_disc: 2.49348  (2.52885)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49348  (2.52885)\n",
            "     | > grad_norm_1: 32.49656  (25.09219)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 4.04750  (3.54736)\n",
            "     | > loader_time: 0.04280  (0.02947)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.39424  (2.39424)\n",
            "     | > loss_kl: 1.56314  (1.56314)\n",
            "     | > loss_feat: 4.56187  (4.56187)\n",
            "     | > loss_mel: 18.08007  (18.08007)\n",
            "     | > loss_duration: 1.79778  (1.79778)\n",
            "     | > loss_0: 28.39711  (28.39711)\n",
            "     | > loss_disc: 2.47327  (2.47327)\n",
            "     | > loss_1: 2.47327  (2.47327)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.17869  (2.17869)\n",
            "     | > loss_kl: 2.01419  (2.01419)\n",
            "     | > loss_feat: 3.98727  (3.98727)\n",
            "     | > loss_mel: 17.87682  (17.87682)\n",
            "     | > loss_duration: 1.73532  (1.73532)\n",
            "     | > loss_0: 27.79229  (27.79229)\n",
            "     | > loss_disc: 2.50080  (2.50080)\n",
            "     | > loss_1: 2.50080  (2.50080)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.49891  (2.33880)\n",
            "     | > loss_kl: 1.96266  (1.98842)\n",
            "     | > loss_feat: 4.54467  (4.26597)\n",
            "     | > loss_mel: 18.21986  (18.04834)\n",
            "     | > loss_duration: 1.74888  (1.74210)\n",
            "     | > loss_0: 28.97498  (28.38363)\n",
            "     | > loss_disc: 2.43707  (2.46894)\n",
            "     | > loss_1: 2.43707  (2.46894)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.28233  (2.31998)\n",
            "     | > loss_kl: 2.38875  (2.12186)\n",
            "     | > loss_feat: 4.48293  (4.33829)\n",
            "     | > loss_mel: 17.82931  (17.97533)\n",
            "     | > loss_duration: 1.74244  (1.74221)\n",
            "     | > loss_0: 28.72576  (28.49767)\n",
            "     | > loss_disc: 2.45888  (2.46559)\n",
            "     | > loss_1: 2.45888  (2.46559)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.34698  (2.32673)\n",
            "     | > loss_kl: 1.97056  (2.08404)\n",
            "     | > loss_feat: 4.87023  (4.47127)\n",
            "     | > loss_mel: 18.48248  (18.10212)\n",
            "     | > loss_duration: 1.72749  (1.73853)\n",
            "     | > loss_0: 29.39774  (28.72269)\n",
            "     | > loss_disc: 2.45324  (2.46250)\n",
            "     | > loss_1: 2.45324  (2.46250)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.85593  (2.43257)\n",
            "     | > loss_kl: 2.01914  (2.07106)\n",
            "     | > loss_feat: 5.90640  (4.75830)\n",
            "     | > loss_mel: 20.07132  (18.49596)\n",
            "     | > loss_duration: 1.73437  (1.73770)\n",
            "     | > loss_0: 32.58717  (29.49559)\n",
            "     | > loss_disc: 2.39382  (2.44876)\n",
            "     | > loss_1: 2.39382  (2.44876)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01400 \u001b[0m(-0.00402)\n",
            "     | > avg_loss_gen:\u001b[91m 2.43257 \u001b[0m(+0.30230)\n",
            "     | > avg_loss_kl:\u001b[91m 2.07106 \u001b[0m(+0.04806)\n",
            "     | > avg_loss_feat:\u001b[91m 4.75830 \u001b[0m(+0.49838)\n",
            "     | > avg_loss_mel:\u001b[92m 18.49596 \u001b[0m(-0.42354)\n",
            "     | > avg_loss_duration:\u001b[92m 1.73770 \u001b[0m(-0.00655)\n",
            "     | > avg_loss_0:\u001b[91m 29.49559 \u001b[0m(+0.41864)\n",
            "     | > avg_loss_disc:\u001b[92m 2.44876 \u001b[0m(-0.09970)\n",
            "     | > avg_loss_1:\u001b[92m 2.44876 \u001b[0m(-0.09970)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 22/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 10:16:56) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/173 -- GLOBAL_STEP: 60850\u001b[0m\n",
            "     | > loss_gen: 2.15339  (2.18859)\n",
            "     | > loss_kl: 1.26083  (1.18999)\n",
            "     | > loss_feat: 4.76506  (4.93871)\n",
            "     | > loss_mel: 17.92492  (18.12973)\n",
            "     | > loss_duration: 1.66187  (1.67788)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.76607  (28.12490)\n",
            "     | > grad_norm_0: 146.93747  (329.36960)\n",
            "     | > loss_disc: 2.55309  (2.50337)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.55309  (2.50337)\n",
            "     | > grad_norm_1: 16.43618  (29.70872)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.32150  (3.30163)\n",
            "     | > loader_time: 0.02270  (0.02005)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 47/173 -- GLOBAL_STEP: 60875\u001b[0m\n",
            "     | > loss_gen: 2.54475  (2.16966)\n",
            "     | > loss_kl: 1.30024  (1.19451)\n",
            "     | > loss_feat: 4.86406  (4.85541)\n",
            "     | > loss_mel: 17.93378  (18.07982)\n",
            "     | > loss_duration: 1.64046  (1.66383)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.28328  (27.96323)\n",
            "     | > grad_norm_0: 574.06372  (312.46329)\n",
            "     | > loss_disc: 2.54785  (2.50145)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.54785  (2.50145)\n",
            "     | > grad_norm_1: 29.14403  (29.27865)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.45360  (3.34163)\n",
            "     | > loader_time: 0.02590  (0.02195)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 72/173 -- GLOBAL_STEP: 60900\u001b[0m\n",
            "     | > loss_gen: 2.20237  (2.16637)\n",
            "     | > loss_kl: 1.31099  (1.22224)\n",
            "     | > loss_feat: 5.24007  (4.83890)\n",
            "     | > loss_mel: 18.67089  (18.10363)\n",
            "     | > loss_duration: 1.66204  (1.65730)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 29.08637  (27.98844)\n",
            "     | > grad_norm_0: 153.27190  (311.29446)\n",
            "     | > loss_disc: 2.56807  (2.51464)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.56807  (2.51464)\n",
            "     | > grad_norm_1: 23.68563  (30.62152)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.41510  (3.37763)\n",
            "     | > loader_time: 0.02550  (0.02372)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 97/173 -- GLOBAL_STEP: 60925\u001b[0m\n",
            "     | > loss_gen: 2.08074  (2.15242)\n",
            "     | > loss_kl: 1.30944  (1.23606)\n",
            "     | > loss_feat: 5.02874  (4.77053)\n",
            "     | > loss_mel: 18.08172  (18.08782)\n",
            "     | > loss_duration: 1.61953  (1.65283)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.12017  (27.89966)\n",
            "     | > grad_norm_0: 178.89799  (261.89874)\n",
            "     | > loss_disc: 2.55896  (2.52317)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.55896  (2.52317)\n",
            "     | > grad_norm_1: 13.45392  (28.18749)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.60280  (3.41684)\n",
            "     | > loader_time: 0.02760  (0.02495)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 122/173 -- GLOBAL_STEP: 60950\u001b[0m\n",
            "     | > loss_gen: 2.19710  (2.15005)\n",
            "     | > loss_kl: 1.38715  (1.25190)\n",
            "     | > loss_feat: 4.55245  (4.77217)\n",
            "     | > loss_mel: 17.90703  (18.06856)\n",
            "     | > loss_duration: 1.64872  (1.64961)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.69244  (27.89229)\n",
            "     | > grad_norm_0: 62.23388  (235.58569)\n",
            "     | > loss_disc: 2.66961  (2.52770)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.66961  (2.52770)\n",
            "     | > grad_norm_1: 22.43447  (26.13379)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.70700  (3.45353)\n",
            "     | > loader_time: 0.03260  (0.02626)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 147/173 -- GLOBAL_STEP: 60975\u001b[0m\n",
            "     | > loss_gen: 2.21455  (2.14503)\n",
            "     | > loss_kl: 1.38309  (1.26839)\n",
            "     | > loss_feat: 4.77818  (4.73847)\n",
            "     | > loss_mel: 18.23281  (18.04875)\n",
            "     | > loss_duration: 1.64035  (1.64678)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.24898  (27.84742)\n",
            "     | > grad_norm_0: 75.72737  (222.23505)\n",
            "     | > loss_disc: 2.50449  (2.53001)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.50449  (2.53001)\n",
            "     | > grad_norm_1: 12.57497  (24.93286)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.71430  (3.49182)\n",
            "     | > loader_time: 0.03440  (0.02754)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 172/173 -- GLOBAL_STEP: 61000\u001b[0m\n",
            "     | > loss_gen: 2.07233  (2.13734)\n",
            "     | > loss_kl: 1.28159  (1.28413)\n",
            "     | > loss_feat: 4.20766  (4.67803)\n",
            "     | > loss_mel: 17.97665  (18.05209)\n",
            "     | > loss_duration: 1.70956  (1.64850)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.24780  (27.80008)\n",
            "     | > grad_norm_0: 76.26101  (217.87766)\n",
            "     | > loss_disc: 2.49910  (2.53498)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49910  (2.53498)\n",
            "     | > grad_norm_1: 11.06868  (24.99775)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 4.02800  (3.54287)\n",
            "     | > loader_time: 0.04630  (0.02935)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2/checkpoint_61000.pth.tar\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.02398  (2.02398)\n",
            "     | > loss_kl: 1.78263  (1.78263)\n",
            "     | > loss_feat: 4.26545  (4.26545)\n",
            "     | > loss_mel: 18.59576  (18.59576)\n",
            "     | > loss_duration: 1.78451  (1.78451)\n",
            "     | > loss_0: 28.45234  (28.45234)\n",
            "     | > loss_disc: 2.48932  (2.48932)\n",
            "     | > loss_1: 2.48932  (2.48932)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.04501  (2.04501)\n",
            "     | > loss_kl: 2.14310  (2.14310)\n",
            "     | > loss_feat: 4.82071  (4.82071)\n",
            "     | > loss_mel: 17.96066  (17.96066)\n",
            "     | > loss_duration: 1.72673  (1.72673)\n",
            "     | > loss_0: 28.69621  (28.69621)\n",
            "     | > loss_disc: 2.42997  (2.42997)\n",
            "     | > loss_1: 2.42997  (2.42997)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.11159  (2.07830)\n",
            "     | > loss_kl: 1.96165  (2.05238)\n",
            "     | > loss_feat: 4.55461  (4.68766)\n",
            "     | > loss_mel: 19.09213  (18.52639)\n",
            "     | > loss_duration: 1.74802  (1.73737)\n",
            "     | > loss_0: 29.46800  (29.08211)\n",
            "     | > loss_disc: 2.40392  (2.41695)\n",
            "     | > loss_1: 2.40392  (2.41695)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.03546  (2.06402)\n",
            "     | > loss_kl: 2.24395  (2.11623)\n",
            "     | > loss_feat: 4.92361  (4.76631)\n",
            "     | > loss_mel: 18.44477  (18.49918)\n",
            "     | > loss_duration: 1.72455  (1.73310)\n",
            "     | > loss_0: 29.37233  (29.17885)\n",
            "     | > loss_disc: 2.55469  (2.46286)\n",
            "     | > loss_1: 2.55469  (2.46286)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.08433  (2.06910)\n",
            "     | > loss_kl: 1.92717  (2.06897)\n",
            "     | > loss_feat: 3.68538  (4.49608)\n",
            "     | > loss_mel: 17.38354  (18.22027)\n",
            "     | > loss_duration: 1.74611  (1.73635)\n",
            "     | > loss_0: 26.82654  (28.59077)\n",
            "     | > loss_disc: 2.59716  (2.49644)\n",
            "     | > loss_1: 2.59716  (2.49644)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.27176  (2.10963)\n",
            "     | > loss_kl: 2.36792  (2.12876)\n",
            "     | > loss_feat: 4.40737  (4.47834)\n",
            "     | > loss_mel: 19.10669  (18.39756)\n",
            "     | > loss_duration: 1.70306  (1.72969)\n",
            "     | > loss_0: 29.85681  (28.84398)\n",
            "     | > loss_disc: 2.51974  (2.50110)\n",
            "     | > loss_1: 2.51974  (2.50110)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01655 \u001b[0m(+0.00255)\n",
            "     | > avg_loss_gen:\u001b[92m 2.10963 \u001b[0m(-0.32294)\n",
            "     | > avg_loss_kl:\u001b[91m 2.12876 \u001b[0m(+0.05770)\n",
            "     | > avg_loss_feat:\u001b[92m 4.47834 \u001b[0m(-0.27996)\n",
            "     | > avg_loss_mel:\u001b[92m 18.39756 \u001b[0m(-0.09840)\n",
            "     | > avg_loss_duration:\u001b[92m 1.72969 \u001b[0m(-0.00801)\n",
            "     | > avg_loss_0:\u001b[92m 28.84398 \u001b[0m(-0.65161)\n",
            "     | > avg_loss_disc:\u001b[91m 2.50110 \u001b[0m(+0.05233)\n",
            "     | > avg_loss_1:\u001b[91m 2.50110 \u001b[0m(+0.05233)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 23/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 10:27:36) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 23/173 -- GLOBAL_STEP: 61025\u001b[0m\n",
            "     | > loss_gen: 2.16927  (2.15441)\n",
            "     | > loss_kl: 1.07208  (1.20242)\n",
            "     | > loss_feat: 4.39198  (4.78118)\n",
            "     | > loss_mel: 18.01719  (18.12739)\n",
            "     | > loss_duration: 1.67538  (1.67467)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.32590  (27.94008)\n",
            "     | > grad_norm_0: 424.72443  (237.74216)\n",
            "     | > loss_disc: 2.44779  (2.51204)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.44779  (2.51204)\n",
            "     | > grad_norm_1: 42.35949  (21.61043)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.33210  (3.33973)\n",
            "     | > loader_time: 0.02130  (0.02084)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 48/173 -- GLOBAL_STEP: 61050\u001b[0m\n",
            "     | > loss_gen: 2.03837  (2.13501)\n",
            "     | > loss_kl: 1.26525  (1.22546)\n",
            "     | > loss_feat: 4.35573  (4.71784)\n",
            "     | > loss_mel: 17.95188  (18.16380)\n",
            "     | > loss_duration: 1.66391  (1.66382)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.27514  (27.90593)\n",
            "     | > grad_norm_0: 86.51546  (211.11920)\n",
            "     | > loss_disc: 2.53648  (2.54262)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.53648  (2.54262)\n",
            "     | > grad_norm_1: 7.50199  (24.25902)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.46360  (3.36938)\n",
            "     | > loader_time: 0.02410  (0.02276)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 73/173 -- GLOBAL_STEP: 61075\u001b[0m\n",
            "     | > loss_gen: 2.07787  (2.13322)\n",
            "     | > loss_kl: 1.27374  (1.23164)\n",
            "     | > loss_feat: 4.40620  (4.72563)\n",
            "     | > loss_mel: 17.95050  (18.13334)\n",
            "     | > loss_duration: 1.65423  (1.65799)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.36255  (27.88183)\n",
            "     | > grad_norm_0: 80.97102  (202.76799)\n",
            "     | > loss_disc: 2.55869  (2.54638)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.55869  (2.54638)\n",
            "     | > grad_norm_1: 18.64904  (23.16180)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.45640  (3.40392)\n",
            "     | > loader_time: 0.02860  (0.02391)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 98/173 -- GLOBAL_STEP: 61100\u001b[0m\n",
            "     | > loss_gen: 2.21531  (2.12677)\n",
            "     | > loss_kl: 1.27536  (1.25207)\n",
            "     | > loss_feat: 4.54969  (4.69501)\n",
            "     | > loss_mel: 18.49857  (18.11344)\n",
            "     | > loss_duration: 1.62788  (1.65170)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.16682  (27.83898)\n",
            "     | > grad_norm_0: 102.13437  (193.51823)\n",
            "     | > loss_disc: 2.47862  (2.54594)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.47862  (2.54594)\n",
            "     | > grad_norm_1: 14.89494  (22.79978)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.52800  (3.43379)\n",
            "     | > loader_time: 0.02820  (0.02537)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 123/173 -- GLOBAL_STEP: 61125\u001b[0m\n",
            "     | > loss_gen: 2.01319  (2.12642)\n",
            "     | > loss_kl: 1.36555  (1.27324)\n",
            "     | > loss_feat: 4.57982  (4.68042)\n",
            "     | > loss_mel: 18.19772  (18.10302)\n",
            "     | > loss_duration: 1.64672  (1.64967)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.80300  (27.83277)\n",
            "     | > grad_norm_0: 172.35381  (188.31175)\n",
            "     | > loss_disc: 2.58083  (2.54777)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.58083  (2.54777)\n",
            "     | > grad_norm_1: 38.88483  (22.73408)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.60420  (3.46927)\n",
            "     | > loader_time: 0.03320  (0.02682)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 148/173 -- GLOBAL_STEP: 61150\u001b[0m\n",
            "     | > loss_gen: 2.02432  (2.12278)\n",
            "     | > loss_kl: 1.38600  (1.28693)\n",
            "     | > loss_feat: 4.43943  (4.65959)\n",
            "     | > loss_mel: 18.30461  (18.06851)\n",
            "     | > loss_duration: 1.65061  (1.64696)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.80497  (27.78477)\n",
            "     | > grad_norm_0: 65.86501  (184.45056)\n",
            "     | > loss_disc: 2.51783  (2.54607)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.51783  (2.54607)\n",
            "     | > grad_norm_1: 16.00994  (22.64482)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.74240  (3.50486)\n",
            "     | > loader_time: 0.03280  (0.02792)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 173/173 -- GLOBAL_STEP: 61175\u001b[0m\n",
            "     | > loss_gen: 1.97476  (2.11948)\n",
            "     | > loss_kl: 1.35721  (1.30304)\n",
            "     | > loss_feat: 4.42489  (4.62259)\n",
            "     | > loss_mel: 18.20492  (18.05726)\n",
            "     | > loss_duration: 1.72168  (1.64882)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.68346  (27.75119)\n",
            "     | > grad_norm_0: 323.74112  (179.29523)\n",
            "     | > loss_disc: 2.71277  (2.54870)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.71277  (2.54870)\n",
            "     | > grad_norm_1: 43.34780  (22.62670)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 2.69420  (3.54549)\n",
            "     | > loader_time: 0.03790  (0.02967)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.19075  (2.19075)\n",
            "     | > loss_kl: 1.65218  (1.65218)\n",
            "     | > loss_feat: 4.97111  (4.97111)\n",
            "     | > loss_mel: 18.59583  (18.59583)\n",
            "     | > loss_duration: 1.79957  (1.79957)\n",
            "     | > loss_0: 29.20945  (29.20945)\n",
            "     | > loss_disc: 2.59234  (2.59234)\n",
            "     | > loss_1: 2.59234  (2.59234)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.36640  (2.36640)\n",
            "     | > loss_kl: 2.24027  (2.24027)\n",
            "     | > loss_feat: 4.51050  (4.51050)\n",
            "     | > loss_mel: 18.13318  (18.13318)\n",
            "     | > loss_duration: 1.76136  (1.76136)\n",
            "     | > loss_0: 29.01171  (29.01171)\n",
            "     | > loss_disc: 2.63429  (2.63429)\n",
            "     | > loss_1: 2.63429  (2.63429)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.10079  (2.23360)\n",
            "     | > loss_kl: 1.94132  (2.09080)\n",
            "     | > loss_feat: 4.27939  (4.39494)\n",
            "     | > loss_mel: 18.62687  (18.38003)\n",
            "     | > loss_duration: 1.79136  (1.77636)\n",
            "     | > loss_0: 28.73974  (28.87573)\n",
            "     | > loss_disc: 2.62712  (2.63070)\n",
            "     | > loss_1: 2.62712  (2.63070)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.24796  (2.23839)\n",
            "     | > loss_kl: 2.35278  (2.17813)\n",
            "     | > loss_feat: 4.94105  (4.57698)\n",
            "     | > loss_mel: 18.13228  (18.29745)\n",
            "     | > loss_duration: 1.72143  (1.75805)\n",
            "     | > loss_0: 29.39550  (29.04898)\n",
            "     | > loss_disc: 2.61049  (2.62397)\n",
            "     | > loss_1: 2.61049  (2.62397)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.37380  (2.27224)\n",
            "     | > loss_kl: 1.76069  (2.07377)\n",
            "     | > loss_feat: 3.98476  (4.42892)\n",
            "     | > loss_mel: 18.12877  (18.25528)\n",
            "     | > loss_duration: 1.74750  (1.75541)\n",
            "     | > loss_0: 27.99552  (28.78562)\n",
            "     | > loss_disc: 2.54515  (2.60426)\n",
            "     | > loss_1: 2.54515  (2.60426)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.27082  (2.27196)\n",
            "     | > loss_kl: 2.55277  (2.16957)\n",
            "     | > loss_feat: 4.55094  (4.45333)\n",
            "     | > loss_mel: 19.54887  (18.51400)\n",
            "     | > loss_duration: 1.72902  (1.75013)\n",
            "     | > loss_0: 30.65242  (29.15898)\n",
            "     | > loss_disc: 2.51049  (2.58551)\n",
            "     | > loss_1: 2.51049  (2.58551)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01629 \u001b[0m(-0.00026)\n",
            "     | > avg_loss_gen:\u001b[91m 2.27196 \u001b[0m(+0.16233)\n",
            "     | > avg_loss_kl:\u001b[91m 2.16957 \u001b[0m(+0.04081)\n",
            "     | > avg_loss_feat:\u001b[92m 4.45333 \u001b[0m(-0.02501)\n",
            "     | > avg_loss_mel:\u001b[91m 18.51400 \u001b[0m(+0.11644)\n",
            "     | > avg_loss_duration:\u001b[91m 1.75013 \u001b[0m(+0.02044)\n",
            "     | > avg_loss_0:\u001b[91m 29.15898 \u001b[0m(+0.31500)\n",
            "     | > avg_loss_disc:\u001b[91m 2.58551 \u001b[0m(+0.08441)\n",
            "     | > avg_loss_1:\u001b[91m 2.58551 \u001b[0m(+0.08441)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 24/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 10:38:13) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 24/173 -- GLOBAL_STEP: 61200\u001b[0m\n",
            "     | > loss_gen: 1.84288  (2.10504)\n",
            "     | > loss_kl: 1.31721  (1.21376)\n",
            "     | > loss_feat: 5.09855  (4.75220)\n",
            "     | > loss_mel: 18.43279  (18.15115)\n",
            "     | > loss_duration: 1.67209  (1.67895)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.36352  (27.90109)\n",
            "     | > grad_norm_0: 177.66653  (211.65881)\n",
            "     | > loss_disc: 2.60923  (2.55877)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.60923  (2.55877)\n",
            "     | > grad_norm_1: 24.64331  (32.44788)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.28650  (3.32018)\n",
            "     | > loader_time: 0.02290  (0.02029)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 49/173 -- GLOBAL_STEP: 61225\u001b[0m\n",
            "     | > loss_gen: 2.08821  (2.14232)\n",
            "     | > loss_kl: 1.26775  (1.23943)\n",
            "     | > loss_feat: 4.32429  (4.74411)\n",
            "     | > loss_mel: 17.69708  (18.10830)\n",
            "     | > loss_duration: 1.65096  (1.66793)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.02830  (27.90209)\n",
            "     | > grad_norm_0: 274.48468  (192.83502)\n",
            "     | > loss_disc: 2.58332  (2.52719)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.58332  (2.52719)\n",
            "     | > grad_norm_1: 29.77131  (25.44151)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.37550  (3.35632)\n",
            "     | > loader_time: 0.02430  (0.02250)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 74/173 -- GLOBAL_STEP: 61250\u001b[0m\n",
            "     | > loss_gen: 1.92560  (2.13835)\n",
            "     | > loss_kl: 1.24443  (1.25211)\n",
            "     | > loss_feat: 4.56369  (4.76700)\n",
            "     | > loss_mel: 18.32776  (18.12745)\n",
            "     | > loss_duration: 1.63976  (1.66173)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.70124  (27.94663)\n",
            "     | > grad_norm_0: 232.73599  (219.03568)\n",
            "     | > loss_disc: 2.60552  (2.52883)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.60552  (2.52883)\n",
            "     | > grad_norm_1: 34.19178  (26.16586)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.44360  (3.39686)\n",
            "     | > loader_time: 0.02830  (0.02402)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 99/173 -- GLOBAL_STEP: 61275\u001b[0m\n",
            "     | > loss_gen: 2.01627  (2.14753)\n",
            "     | > loss_kl: 1.29484  (1.26073)\n",
            "     | > loss_feat: 4.34370  (4.74589)\n",
            "     | > loss_mel: 17.92611  (18.11365)\n",
            "     | > loss_duration: 1.65825  (1.65603)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.23918  (27.92383)\n",
            "     | > grad_norm_0: 207.36732  (234.46719)\n",
            "     | > loss_disc: 2.55795  (2.53645)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.55795  (2.53645)\n",
            "     | > grad_norm_1: 34.43430  (28.80309)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.61290  (3.43721)\n",
            "     | > loader_time: 0.03020  (0.02548)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 124/173 -- GLOBAL_STEP: 61300\u001b[0m\n",
            "     | > loss_gen: 2.20361  (2.14237)\n",
            "     | > loss_kl: 1.28865  (1.26866)\n",
            "     | > loss_feat: 4.54759  (4.73228)\n",
            "     | > loss_mel: 18.08307  (18.09963)\n",
            "     | > loss_duration: 1.61197  (1.65228)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.73489  (27.89521)\n",
            "     | > grad_norm_0: 345.82422  (226.36992)\n",
            "     | > loss_disc: 2.44441  (2.53616)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.44441  (2.53616)\n",
            "     | > grad_norm_1: 28.79750  (28.00130)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.58400  (3.47341)\n",
            "     | > loader_time: 0.03320  (0.02687)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 149/173 -- GLOBAL_STEP: 61325\u001b[0m\n",
            "     | > loss_gen: 2.16537  (2.14071)\n",
            "     | > loss_kl: 1.33473  (1.28142)\n",
            "     | > loss_feat: 4.24407  (4.71207)\n",
            "     | > loss_mel: 18.17881  (18.08188)\n",
            "     | > loss_duration: 1.62669  (1.64885)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.54967  (27.86492)\n",
            "     | > grad_norm_0: 189.20830  (235.57713)\n",
            "     | > loss_disc: 2.52437  (2.53460)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.52437  (2.53460)\n",
            "     | > grad_norm_1: 6.98385  (28.07766)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.91720  (3.51342)\n",
            "     | > loader_time: 0.03540  (0.02824)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.02907  (2.02907)\n",
            "     | > loss_kl: 2.06934  (2.06934)\n",
            "     | > loss_feat: 4.50947  (4.50947)\n",
            "     | > loss_mel: 18.65529  (18.65529)\n",
            "     | > loss_duration: 1.79567  (1.79567)\n",
            "     | > loss_0: 29.05884  (29.05884)\n",
            "     | > loss_disc: 2.48364  (2.48364)\n",
            "     | > loss_1: 2.48364  (2.48364)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.07704  (2.07704)\n",
            "     | > loss_kl: 2.40904  (2.40904)\n",
            "     | > loss_feat: 4.60669  (4.60669)\n",
            "     | > loss_mel: 17.96824  (17.96824)\n",
            "     | > loss_duration: 1.74687  (1.74687)\n",
            "     | > loss_0: 28.80789  (28.80789)\n",
            "     | > loss_disc: 2.50316  (2.50316)\n",
            "     | > loss_1: 2.50316  (2.50316)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.11118  (2.09411)\n",
            "     | > loss_kl: 1.88339  (2.14622)\n",
            "     | > loss_feat: 4.87768  (4.74218)\n",
            "     | > loss_mel: 18.21478  (18.09151)\n",
            "     | > loss_duration: 1.73626  (1.74156)\n",
            "     | > loss_0: 28.82329  (28.81559)\n",
            "     | > loss_disc: 2.48318  (2.49317)\n",
            "     | > loss_1: 2.48318  (2.49317)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.07023  (2.08615)\n",
            "     | > loss_kl: 2.06279  (2.11841)\n",
            "     | > loss_feat: 5.07706  (4.85381)\n",
            "     | > loss_mel: 18.48354  (18.22219)\n",
            "     | > loss_duration: 1.71388  (1.73233)\n",
            "     | > loss_0: 29.40750  (29.01289)\n",
            "     | > loss_disc: 2.40963  (2.46532)\n",
            "     | > loss_1: 2.40963  (2.46532)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 1.96904  (2.05687)\n",
            "     | > loss_kl: 2.05635  (2.10290)\n",
            "     | > loss_feat: 4.52404  (4.77137)\n",
            "     | > loss_mel: 18.41689  (18.27086)\n",
            "     | > loss_duration: 1.73886  (1.73396)\n",
            "     | > loss_0: 28.70518  (28.93596)\n",
            "     | > loss_disc: 2.49181  (2.47194)\n",
            "     | > loss_1: 2.49181  (2.47194)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 1.81729  (2.00896)\n",
            "     | > loss_kl: 2.16304  (2.11492)\n",
            "     | > loss_feat: 4.95836  (4.80877)\n",
            "     | > loss_mel: 18.37627  (18.29194)\n",
            "     | > loss_duration: 1.67927  (1.72303)\n",
            "     | > loss_0: 28.99422  (28.94762)\n",
            "     | > loss_disc: 2.69253  (2.51606)\n",
            "     | > loss_1: 2.69253  (2.51606)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.02463 \u001b[0m(+0.00834)\n",
            "     | > avg_loss_gen:\u001b[92m 2.00896 \u001b[0m(-0.26300)\n",
            "     | > avg_loss_kl:\u001b[92m 2.11492 \u001b[0m(-0.05464)\n",
            "     | > avg_loss_feat:\u001b[91m 4.80877 \u001b[0m(+0.35544)\n",
            "     | > avg_loss_mel:\u001b[92m 18.29194 \u001b[0m(-0.22205)\n",
            "     | > avg_loss_duration:\u001b[92m 1.72303 \u001b[0m(-0.02711)\n",
            "     | > avg_loss_0:\u001b[92m 28.94762 \u001b[0m(-0.21136)\n",
            "     | > avg_loss_disc:\u001b[92m 2.51606 \u001b[0m(-0.06945)\n",
            "     | > avg_loss_1:\u001b[92m 2.51606 \u001b[0m(-0.06945)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 25/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 10:48:51) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/173 -- GLOBAL_STEP: 61350\u001b[0m\n",
            "     | > loss_gen: 1.94669  (1.94669)\n",
            "     | > loss_kl: 1.21069  (1.21069)\n",
            "     | > loss_feat: 4.71186  (4.71186)\n",
            "     | > loss_mel: 18.11160  (18.11160)\n",
            "     | > loss_duration: 1.80268  (1.80268)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.78352  (27.78352)\n",
            "     | > grad_norm_0: 568.05640  (568.05640)\n",
            "     | > loss_disc: 2.54921  (2.54921)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.54921  (2.54921)\n",
            "     | > grad_norm_1: 73.38448  (73.38448)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.76950  (3.76948)\n",
            "     | > loader_time: 2.70980  (2.70983)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 25/173 -- GLOBAL_STEP: 61375\u001b[0m\n",
            "     | > loss_gen: 1.90183  (2.13856)\n",
            "     | > loss_kl: 1.40773  (1.21646)\n",
            "     | > loss_feat: 4.38368  (4.76830)\n",
            "     | > loss_mel: 18.21103  (18.11056)\n",
            "     | > loss_duration: 1.68896  (1.67573)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.59323  (27.90960)\n",
            "     | > grad_norm_0: 270.65427  (423.21890)\n",
            "     | > loss_disc: 2.54811  (2.56587)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.54811  (2.56587)\n",
            "     | > grad_norm_1: 32.24648  (49.86184)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.32360  (3.32518)\n",
            "     | > loader_time: 0.02230  (0.02110)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/173 -- GLOBAL_STEP: 61400\u001b[0m\n",
            "     | > loss_gen: 2.24737  (2.13890)\n",
            "     | > loss_kl: 1.39719  (1.23449)\n",
            "     | > loss_feat: 4.58597  (4.73797)\n",
            "     | > loss_mel: 18.12691  (18.10087)\n",
            "     | > loss_duration: 1.67092  (1.66589)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.02835  (27.87812)\n",
            "     | > grad_norm_0: 434.89633  (314.44312)\n",
            "     | > loss_disc: 2.53286  (2.54326)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.53286  (2.54326)\n",
            "     | > grad_norm_1: 33.93457  (34.94942)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.44720  (3.37059)\n",
            "     | > loader_time: 0.02510  (0.02273)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 75/173 -- GLOBAL_STEP: 61425\u001b[0m\n",
            "     | > loss_gen: 2.19550  (2.13441)\n",
            "     | > loss_kl: 1.16472  (1.24177)\n",
            "     | > loss_feat: 4.29956  (4.73674)\n",
            "     | > loss_mel: 18.17766  (18.10447)\n",
            "     | > loss_duration: 1.65610  (1.65949)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.49354  (27.87689)\n",
            "     | > grad_norm_0: 494.06882  (275.62958)\n",
            "     | > loss_disc: 2.50158  (2.54456)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.50158  (2.54456)\n",
            "     | > grad_norm_1: 33.57255  (30.14666)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.52160  (3.39963)\n",
            "     | > loader_time: 0.02980  (0.02430)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/173 -- GLOBAL_STEP: 61450\u001b[0m\n",
            "     | > loss_gen: 2.00479  (2.13946)\n",
            "     | > loss_kl: 1.29652  (1.24942)\n",
            "     | > loss_feat: 4.69585  (4.71940)\n",
            "     | > loss_mel: 18.18192  (18.09816)\n",
            "     | > loss_duration: 1.65445  (1.65243)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.83352  (27.85887)\n",
            "     | > grad_norm_0: 435.54559  (277.55008)\n",
            "     | > loss_disc: 2.59124  (2.54469)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.59124  (2.54469)\n",
            "     | > grad_norm_1: 40.99431  (29.71627)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.57880  (3.43407)\n",
            "     | > loader_time: 0.03460  (0.02590)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 125/173 -- GLOBAL_STEP: 61475\u001b[0m\n",
            "     | > loss_gen: 2.12171  (2.13599)\n",
            "     | > loss_kl: 1.35624  (1.26471)\n",
            "     | > loss_feat: 4.82741  (4.70933)\n",
            "     | > loss_mel: 17.86087  (18.10973)\n",
            "     | > loss_duration: 1.62044  (1.64905)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.78666  (27.86880)\n",
            "     | > grad_norm_0: 96.99960  (270.28500)\n",
            "     | > loss_disc: 2.56769  (2.54643)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.56769  (2.54643)\n",
            "     | > grad_norm_1: 16.61847  (29.03388)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.64120  (3.46989)\n",
            "     | > loader_time: 0.03360  (0.02718)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 150/173 -- GLOBAL_STEP: 61500\u001b[0m\n",
            "     | > loss_gen: 2.14492  (2.13569)\n",
            "     | > loss_kl: 1.38382  (1.27901)\n",
            "     | > loss_feat: 4.17778  (4.68076)\n",
            "     | > loss_mel: 18.28897  (18.09127)\n",
            "     | > loss_duration: 1.63551  (1.64651)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.63100  (27.83323)\n",
            "     | > grad_norm_0: 390.60016  (263.40155)\n",
            "     | > loss_disc: 2.67898  (2.54720)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.67898  (2.54720)\n",
            "     | > grad_norm_1: 58.79297  (28.43044)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.78050  (3.51025)\n",
            "     | > loader_time: 0.04090  (0.02859)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.02797  (2.02797)\n",
            "     | > loss_kl: 1.89924  (1.89924)\n",
            "     | > loss_feat: 4.12542  (4.12542)\n",
            "     | > loss_mel: 18.10952  (18.10952)\n",
            "     | > loss_duration: 1.81193  (1.81193)\n",
            "     | > loss_0: 27.97409  (27.97409)\n",
            "     | > loss_disc: 2.55790  (2.55790)\n",
            "     | > loss_1: 2.55790  (2.55790)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 1.95017  (1.95017)\n",
            "     | > loss_kl: 2.42011  (2.42011)\n",
            "     | > loss_feat: 4.97748  (4.97748)\n",
            "     | > loss_mel: 18.56931  (18.56931)\n",
            "     | > loss_duration: 1.73930  (1.73930)\n",
            "     | > loss_0: 29.65637  (29.65637)\n",
            "     | > loss_disc: 2.56239  (2.56239)\n",
            "     | > loss_1: 2.56239  (2.56239)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.06028  (2.00523)\n",
            "     | > loss_kl: 2.04453  (2.23232)\n",
            "     | > loss_feat: 3.61353  (4.29551)\n",
            "     | > loss_mel: 17.37469  (17.97200)\n",
            "     | > loss_duration: 1.72638  (1.73284)\n",
            "     | > loss_0: 26.81942  (28.23789)\n",
            "     | > loss_disc: 2.58970  (2.57605)\n",
            "     | > loss_1: 2.58970  (2.57605)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 1.98148  (1.99731)\n",
            "     | > loss_kl: 2.09328  (2.18597)\n",
            "     | > loss_feat: 5.27655  (4.62252)\n",
            "     | > loss_mel: 18.18234  (18.04211)\n",
            "     | > loss_duration: 1.69051  (1.71873)\n",
            "     | > loss_0: 29.22417  (28.56665)\n",
            "     | > loss_disc: 2.51870  (2.55693)\n",
            "     | > loss_1: 2.51870  (2.55693)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.07343  (2.01634)\n",
            "     | > loss_kl: 1.91558  (2.11837)\n",
            "     | > loss_feat: 4.53878  (4.60159)\n",
            "     | > loss_mel: 18.22996  (18.08908)\n",
            "     | > loss_duration: 1.74535  (1.72538)\n",
            "     | > loss_0: 28.50310  (28.55076)\n",
            "     | > loss_disc: 2.39356  (2.51609)\n",
            "     | > loss_1: 2.39356  (2.51609)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 1.86336  (1.98575)\n",
            "     | > loss_kl: 2.04483  (2.10367)\n",
            "     | > loss_feat: 6.03579  (4.88843)\n",
            "     | > loss_mel: 19.88632  (18.44852)\n",
            "     | > loss_duration: 1.70597  (1.72150)\n",
            "     | > loss_0: 31.53627  (29.14786)\n",
            "     | > loss_disc: 2.63783  (2.54044)\n",
            "     | > loss_1: 2.63783  (2.54044)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01805 \u001b[0m(-0.00658)\n",
            "     | > avg_loss_gen:\u001b[92m 1.98575 \u001b[0m(-0.02321)\n",
            "     | > avg_loss_kl:\u001b[92m 2.10367 \u001b[0m(-0.01126)\n",
            "     | > avg_loss_feat:\u001b[91m 4.88843 \u001b[0m(+0.07966)\n",
            "     | > avg_loss_mel:\u001b[91m 18.44852 \u001b[0m(+0.15658)\n",
            "     | > avg_loss_duration:\u001b[92m 1.72150 \u001b[0m(-0.00152)\n",
            "     | > avg_loss_0:\u001b[91m 29.14786 \u001b[0m(+0.20025)\n",
            "     | > avg_loss_disc:\u001b[91m 2.54044 \u001b[0m(+0.02437)\n",
            "     | > avg_loss_1:\u001b[91m 2.54044 \u001b[0m(+0.02437)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 26/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 10:59:29) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 1/173 -- GLOBAL_STEP: 61525\u001b[0m\n",
            "     | > loss_gen: 2.20010  (2.20010)\n",
            "     | > loss_kl: 1.08021  (1.08021)\n",
            "     | > loss_feat: 4.70035  (4.70035)\n",
            "     | > loss_mel: 18.29371  (18.29371)\n",
            "     | > loss_duration: 1.78933  (1.78933)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.06369  (28.06369)\n",
            "     | > grad_norm_0: 101.29671  (101.29671)\n",
            "     | > loss_disc: 2.58433  (2.58433)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.58433  (2.58433)\n",
            "     | > grad_norm_1: 15.74373  (15.74373)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.23880  (3.23885)\n",
            "     | > loader_time: 0.01490  (0.01490)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 26/173 -- GLOBAL_STEP: 61550\u001b[0m\n",
            "     | > loss_gen: 2.11578  (2.17020)\n",
            "     | > loss_kl: 1.31987  (1.23025)\n",
            "     | > loss_feat: 4.62730  (4.90046)\n",
            "     | > loss_mel: 17.80880  (18.18941)\n",
            "     | > loss_duration: 1.64719  (1.66830)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.51895  (28.15862)\n",
            "     | > grad_norm_0: 61.19223  (305.95502)\n",
            "     | > loss_disc: 2.50189  (2.51599)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.50189  (2.51599)\n",
            "     | > grad_norm_1: 14.24499  (31.37442)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.33490  (3.34348)\n",
            "     | > loader_time: 0.02550  (0.02065)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 51/173 -- GLOBAL_STEP: 61575\u001b[0m\n",
            "     | > loss_gen: 2.03357  (2.15688)\n",
            "     | > loss_kl: 1.26336  (1.25443)\n",
            "     | > loss_feat: 4.70992  (4.80308)\n",
            "     | > loss_mel: 18.00548  (18.20218)\n",
            "     | > loss_duration: 1.63443  (1.66115)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.64675  (28.07773)\n",
            "     | > grad_norm_0: 137.81093  (227.41495)\n",
            "     | > loss_disc: 2.49365  (2.51693)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49365  (2.51693)\n",
            "     | > grad_norm_1: 16.57256  (26.05851)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.43240  (3.36912)\n",
            "     | > loader_time: 0.02520  (0.02254)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 76/173 -- GLOBAL_STEP: 61600\u001b[0m\n",
            "     | > loss_gen: 2.16551  (2.15137)\n",
            "     | > loss_kl: 1.21733  (1.25597)\n",
            "     | > loss_feat: 4.26926  (4.80714)\n",
            "     | > loss_mel: 18.28771  (18.19351)\n",
            "     | > loss_duration: 1.63873  (1.65545)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.57854  (28.06345)\n",
            "     | > grad_norm_0: 79.07122  (200.88533)\n",
            "     | > loss_disc: 2.57690  (2.52004)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.57690  (2.52004)\n",
            "     | > grad_norm_1: 11.20812  (23.91847)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.53740  (3.40529)\n",
            "     | > loader_time: 0.02740  (0.02407)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 101/173 -- GLOBAL_STEP: 61625\u001b[0m\n",
            "     | > loss_gen: 2.19935  (2.14196)\n",
            "     | > loss_kl: 1.29377  (1.25751)\n",
            "     | > loss_feat: 5.21406  (4.73239)\n",
            "     | > loss_mel: 18.10703  (18.15774)\n",
            "     | > loss_duration: 1.64732  (1.65167)\n",
            "     | > amp_scaler: 512.00000  (276.27723)\n",
            "     | > loss_0: 28.46152  (27.94127)\n",
            "     | > grad_norm_0: 139.37105  (175.46123)\n",
            "     | > loss_disc: 2.48795  (2.53090)\n",
            "     | > amp_scaler-1: 512.00000  (276.27723)\n",
            "     | > loss_1: 2.48795  (2.53090)\n",
            "     | > grad_norm_1: 6.92696  (21.66446)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.60740  (3.43882)\n",
            "     | > loader_time: 0.03160  (0.02530)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 126/173 -- GLOBAL_STEP: 61650\u001b[0m\n",
            "     | > loss_gen: 2.03347  (2.13429)\n",
            "     | > loss_kl: 1.46435  (1.26553)\n",
            "     | > loss_feat: 5.10716  (4.70373)\n",
            "     | > loss_mel: 18.40531  (18.14022)\n",
            "     | > loss_duration: 1.60806  (1.64806)\n",
            "     | > amp_scaler: 512.00000  (323.04762)\n",
            "     | > loss_0: 28.61834  (27.89182)\n",
            "     | > grad_norm_0: 149.87614  (168.19896)\n",
            "     | > loss_disc: 2.59394  (2.53409)\n",
            "     | > amp_scaler-1: 512.00000  (323.04762)\n",
            "     | > loss_1: 2.59394  (2.53409)\n",
            "     | > grad_norm_1: 28.00637  (21.86755)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.77970  (3.47589)\n",
            "     | > loader_time: 0.03330  (0.02660)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 151/173 -- GLOBAL_STEP: 61675\u001b[0m\n",
            "     | > loss_gen: 2.04184  (2.13131)\n",
            "     | > loss_kl: 1.35660  (1.27946)\n",
            "     | > loss_feat: 4.02077  (4.67751)\n",
            "     | > loss_mel: 17.87793  (18.11305)\n",
            "     | > loss_duration: 1.64250  (1.64646)\n",
            "     | > amp_scaler: 512.00000  (354.33113)\n",
            "     | > loss_0: 26.93964  (27.84779)\n",
            "     | > grad_norm_0: 69.03320  (170.44675)\n",
            "     | > loss_disc: 2.60501  (2.54088)\n",
            "     | > amp_scaler-1: 512.00000  (354.33113)\n",
            "     | > loss_1: 2.60501  (2.54088)\n",
            "     | > grad_norm_1: 10.42404  (22.85831)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.93730  (3.51655)\n",
            "     | > loader_time: 0.03880  (0.02810)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.02442  (2.02442)\n",
            "     | > loss_kl: 1.27835  (1.27835)\n",
            "     | > loss_feat: 4.25850  (4.25850)\n",
            "     | > loss_mel: 17.77224  (17.77224)\n",
            "     | > loss_duration: 1.79230  (1.79230)\n",
            "     | > loss_0: 27.12582  (27.12582)\n",
            "     | > loss_disc: 2.60441  (2.60441)\n",
            "     | > loss_1: 2.60441  (2.60441)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.05529  (2.05529)\n",
            "     | > loss_kl: 2.25234  (2.25234)\n",
            "     | > loss_feat: 4.65447  (4.65447)\n",
            "     | > loss_mel: 18.27569  (18.27569)\n",
            "     | > loss_duration: 1.72906  (1.72906)\n",
            "     | > loss_0: 28.96685  (28.96685)\n",
            "     | > loss_disc: 2.47595  (2.47595)\n",
            "     | > loss_1: 2.47595  (2.47595)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 1.98616  (2.02072)\n",
            "     | > loss_kl: 1.91855  (2.08545)\n",
            "     | > loss_feat: 4.15213  (4.40330)\n",
            "     | > loss_mel: 17.96531  (18.12050)\n",
            "     | > loss_duration: 1.74636  (1.73771)\n",
            "     | > loss_0: 27.76852  (28.36768)\n",
            "     | > loss_disc: 2.42880  (2.45238)\n",
            "     | > loss_1: 2.42880  (2.45238)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 1.89848  (1.97998)\n",
            "     | > loss_kl: 2.26767  (2.14619)\n",
            "     | > loss_feat: 4.55863  (4.45508)\n",
            "     | > loss_mel: 17.60536  (17.94879)\n",
            "     | > loss_duration: 1.72382  (1.73308)\n",
            "     | > loss_0: 28.05396  (28.26311)\n",
            "     | > loss_disc: 2.59911  (2.50129)\n",
            "     | > loss_1: 2.59911  (2.50129)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.08445  (2.00609)\n",
            "     | > loss_kl: 1.97899  (2.10439)\n",
            "     | > loss_feat: 4.16273  (4.38199)\n",
            "     | > loss_mel: 17.75003  (17.89910)\n",
            "     | > loss_duration: 1.75071  (1.73749)\n",
            "     | > loss_0: 27.72691  (28.12906)\n",
            "     | > loss_disc: 2.54855  (2.51310)\n",
            "     | > loss_1: 2.54855  (2.51310)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 1.97471  (1.99982)\n",
            "     | > loss_kl: 2.18344  (2.12020)\n",
            "     | > loss_feat: 5.15034  (4.53566)\n",
            "     | > loss_mel: 20.21671  (18.36262)\n",
            "     | > loss_duration: 1.72704  (1.73540)\n",
            "     | > loss_0: 31.25223  (28.75369)\n",
            "     | > loss_disc: 2.41472  (2.49343)\n",
            "     | > loss_1: 2.41472  (2.49343)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01710 \u001b[0m(-0.00094)\n",
            "     | > avg_loss_gen:\u001b[91m 1.99982 \u001b[0m(+0.01407)\n",
            "     | > avg_loss_kl:\u001b[91m 2.12020 \u001b[0m(+0.01653)\n",
            "     | > avg_loss_feat:\u001b[92m 4.53566 \u001b[0m(-0.35277)\n",
            "     | > avg_loss_mel:\u001b[92m 18.36262 \u001b[0m(-0.08590)\n",
            "     | > avg_loss_duration:\u001b[91m 1.73540 \u001b[0m(+0.01390)\n",
            "     | > avg_loss_0:\u001b[92m 28.75369 \u001b[0m(-0.39417)\n",
            "     | > avg_loss_disc:\u001b[92m 2.49343 \u001b[0m(-0.04701)\n",
            "     | > avg_loss_1:\u001b[92m 2.49343 \u001b[0m(-0.04701)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 27/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 11:10:07) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/173 -- GLOBAL_STEP: 61700\u001b[0m\n",
            "     | > loss_gen: 2.07202  (2.18985)\n",
            "     | > loss_kl: 1.19026  (1.12066)\n",
            "     | > loss_feat: 4.73177  (4.83523)\n",
            "     | > loss_mel: 18.21965  (18.27800)\n",
            "     | > loss_duration: 1.71902  (1.73613)\n",
            "     | > amp_scaler: 256.00000  (384.00000)\n",
            "     | > loss_0: 27.93273  (28.15987)\n",
            "     | > grad_norm_0: 0.00000  (209.23111)\n",
            "     | > loss_disc: 2.49640  (2.50913)\n",
            "     | > amp_scaler-1: 256.00000  (384.00000)\n",
            "     | > loss_1: 2.49640  (2.50913)\n",
            "     | > grad_norm_1: 52.27499  (40.57268)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.18810  (3.19590)\n",
            "     | > loader_time: 0.01470  (0.01866)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 27/173 -- GLOBAL_STEP: 61725\u001b[0m\n",
            "     | > loss_gen: 2.04620  (2.17866)\n",
            "     | > loss_kl: 1.22289  (1.22807)\n",
            "     | > loss_feat: 4.83884  (4.83005)\n",
            "     | > loss_mel: 18.55255  (18.17209)\n",
            "     | > loss_duration: 1.66479  (1.67665)\n",
            "     | > amp_scaler: 256.00000  (265.48148)\n",
            "     | > loss_0: 28.32528  (28.08552)\n",
            "     | > grad_norm_0: 161.78880  (352.56229)\n",
            "     | > loss_disc: 2.47785  (2.52876)\n",
            "     | > amp_scaler-1: 256.00000  (265.48148)\n",
            "     | > loss_1: 2.47785  (2.52876)\n",
            "     | > grad_norm_1: 10.71681  (44.46258)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.38440  (3.33490)\n",
            "     | > loader_time: 0.02320  (0.02094)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 52/173 -- GLOBAL_STEP: 61750\u001b[0m\n",
            "     | > loss_gen: 2.35460  (2.16450)\n",
            "     | > loss_kl: 1.16990  (1.23688)\n",
            "     | > loss_feat: 4.82697  (4.77268)\n",
            "     | > loss_mel: 18.05511  (18.11012)\n",
            "     | > loss_duration: 1.62447  (1.66703)\n",
            "     | > amp_scaler: 256.00000  (260.92308)\n",
            "     | > loss_0: 28.03105  (27.95121)\n",
            "     | > grad_norm_0: 298.78360  (281.30246)\n",
            "     | > loss_disc: 2.50970  (2.50911)\n",
            "     | > amp_scaler-1: 256.00000  (260.92308)\n",
            "     | > loss_1: 2.50970  (2.50911)\n",
            "     | > grad_norm_1: 23.16211  (31.65228)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.42260  (3.36956)\n",
            "     | > loader_time: 0.02480  (0.02238)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 77/173 -- GLOBAL_STEP: 61775\u001b[0m\n",
            "     | > loss_gen: 2.17564  (2.15159)\n",
            "     | > loss_kl: 1.19036  (1.24570)\n",
            "     | > loss_feat: 4.87370  (4.75811)\n",
            "     | > loss_mel: 18.09479  (18.13444)\n",
            "     | > loss_duration: 1.62998  (1.65996)\n",
            "     | > amp_scaler: 256.00000  (259.32468)\n",
            "     | > loss_0: 27.96447  (27.94980)\n",
            "     | > grad_norm_0: 384.21417  (258.32727)\n",
            "     | > loss_disc: 2.50936  (2.52228)\n",
            "     | > amp_scaler-1: 256.00000  (259.32468)\n",
            "     | > loss_1: 2.50936  (2.52228)\n",
            "     | > grad_norm_1: 42.18391  (29.25529)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.48760  (3.40821)\n",
            "     | > loader_time: 0.02650  (0.02392)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 102/173 -- GLOBAL_STEP: 61800\u001b[0m\n",
            "     | > loss_gen: 1.99636  (2.14353)\n",
            "     | > loss_kl: 1.31706  (1.25726)\n",
            "     | > loss_feat: 4.48529  (4.72700)\n",
            "     | > loss_mel: 18.04137  (18.10733)\n",
            "     | > loss_duration: 1.65229  (1.65420)\n",
            "     | > amp_scaler: 256.00000  (258.50980)\n",
            "     | > loss_0: 27.49236  (27.88933)\n",
            "     | > grad_norm_0: 270.26862  (233.53229)\n",
            "     | > loss_disc: 2.55332  (2.52444)\n",
            "     | > amp_scaler-1: 256.00000  (258.50980)\n",
            "     | > loss_1: 2.55332  (2.52444)\n",
            "     | > grad_norm_1: 34.63065  (27.01058)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.54320  (3.44356)\n",
            "     | > loader_time: 0.03460  (0.02534)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 127/173 -- GLOBAL_STEP: 61825\u001b[0m\n",
            "     | > loss_gen: 2.27181  (2.14108)\n",
            "     | > loss_kl: 1.36031  (1.27022)\n",
            "     | > loss_feat: 4.55921  (4.71466)\n",
            "     | > loss_mel: 18.03622  (18.10371)\n",
            "     | > loss_duration: 1.65029  (1.65082)\n",
            "     | > amp_scaler: 256.00000  (258.01575)\n",
            "     | > loss_0: 27.87784  (27.88050)\n",
            "     | > grad_norm_0: 108.04916  (220.09264)\n",
            "     | > loss_disc: 2.48042  (2.52844)\n",
            "     | > amp_scaler-1: 256.00000  (258.01575)\n",
            "     | > loss_1: 2.48042  (2.52844)\n",
            "     | > grad_norm_1: 22.36751  (26.60931)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.72670  (3.48092)\n",
            "     | > loader_time: 0.03600  (0.02668)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 152/173 -- GLOBAL_STEP: 61850\u001b[0m\n",
            "     | > loss_gen: 2.13904  (2.14076)\n",
            "     | > loss_kl: 1.31777  (1.28424)\n",
            "     | > loss_feat: 4.29106  (4.68227)\n",
            "     | > loss_mel: 17.96796  (18.08277)\n",
            "     | > loss_duration: 1.63257  (1.64836)\n",
            "     | > amp_scaler: 256.00000  (257.68421)\n",
            "     | > loss_0: 27.34839  (27.83840)\n",
            "     | > grad_norm_0: 48.73615  (212.02534)\n",
            "     | > loss_disc: 2.51946  (2.52710)\n",
            "     | > amp_scaler-1: 256.00000  (257.68421)\n",
            "     | > loss_1: 2.51946  (2.52710)\n",
            "     | > grad_norm_1: 8.43773  (25.95045)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.74080  (3.51783)\n",
            "     | > loader_time: 0.03750  (0.02797)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.13174  (2.13174)\n",
            "     | > loss_kl: 1.70185  (1.70185)\n",
            "     | > loss_feat: 4.26332  (4.26332)\n",
            "     | > loss_mel: 18.24677  (18.24677)\n",
            "     | > loss_duration: 1.79875  (1.79875)\n",
            "     | > loss_0: 28.14244  (28.14244)\n",
            "     | > loss_disc: 2.42849  (2.42849)\n",
            "     | > loss_1: 2.42849  (2.42849)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.20927  (2.20927)\n",
            "     | > loss_kl: 2.19309  (2.19309)\n",
            "     | > loss_feat: 4.55541  (4.55541)\n",
            "     | > loss_mel: 18.25782  (18.25782)\n",
            "     | > loss_duration: 1.73878  (1.73878)\n",
            "     | > loss_0: 28.95437  (28.95437)\n",
            "     | > loss_disc: 2.46898  (2.46898)\n",
            "     | > loss_1: 2.46898  (2.46898)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.25672  (2.23300)\n",
            "     | > loss_kl: 1.94223  (2.06766)\n",
            "     | > loss_feat: 4.47220  (4.51380)\n",
            "     | > loss_mel: 18.36693  (18.31238)\n",
            "     | > loss_duration: 1.75386  (1.74632)\n",
            "     | > loss_0: 28.79194  (28.87316)\n",
            "     | > loss_disc: 2.48933  (2.47915)\n",
            "     | > loss_1: 2.48933  (2.47915)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.34045  (2.26881)\n",
            "     | > loss_kl: 2.01842  (2.05125)\n",
            "     | > loss_feat: 4.80135  (4.60965)\n",
            "     | > loss_mel: 18.04228  (18.22234)\n",
            "     | > loss_duration: 1.70772  (1.73345)\n",
            "     | > loss_0: 28.91022  (28.88551)\n",
            "     | > loss_disc: 2.50497  (2.48776)\n",
            "     | > loss_1: 2.50497  (2.48776)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.20752  (2.25349)\n",
            "     | > loss_kl: 1.90790  (2.01541)\n",
            "     | > loss_feat: 4.17908  (4.50201)\n",
            "     | > loss_mel: 17.84135  (18.12709)\n",
            "     | > loss_duration: 1.72830  (1.73217)\n",
            "     | > loss_0: 27.86416  (28.63017)\n",
            "     | > loss_disc: 2.35888  (2.45554)\n",
            "     | > loss_1: 2.35888  (2.45554)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.01357  (2.20551)\n",
            "     | > loss_kl: 2.13288  (2.03891)\n",
            "     | > loss_feat: 3.82290  (4.36619)\n",
            "     | > loss_mel: 17.95771  (18.09322)\n",
            "     | > loss_duration: 1.70575  (1.72688)\n",
            "     | > loss_0: 27.63283  (28.43070)\n",
            "     | > loss_disc: 2.54589  (2.47361)\n",
            "     | > loss_1: 2.54589  (2.47361)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01526 \u001b[0m(-0.00185)\n",
            "     | > avg_loss_gen:\u001b[91m 2.20551 \u001b[0m(+0.20569)\n",
            "     | > avg_loss_kl:\u001b[92m 2.03891 \u001b[0m(-0.08129)\n",
            "     | > avg_loss_feat:\u001b[92m 4.36619 \u001b[0m(-0.16947)\n",
            "     | > avg_loss_mel:\u001b[92m 18.09322 \u001b[0m(-0.26940)\n",
            "     | > avg_loss_duration:\u001b[92m 1.72688 \u001b[0m(-0.00851)\n",
            "     | > avg_loss_0:\u001b[92m 28.43070 \u001b[0m(-0.32299)\n",
            "     | > avg_loss_disc:\u001b[92m 2.47361 \u001b[0m(-0.01982)\n",
            "     | > avg_loss_1:\u001b[92m 2.47361 \u001b[0m(-0.01982)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 28/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 11:20:46) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 3/173 -- GLOBAL_STEP: 61875\u001b[0m\n",
            "     | > loss_gen: 2.08133  (2.14068)\n",
            "     | > loss_kl: 1.14076  (1.11521)\n",
            "     | > loss_feat: 4.40717  (4.79553)\n",
            "     | > loss_mel: 18.09426  (18.02722)\n",
            "     | > loss_duration: 1.73803  (1.75360)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.46156  (27.83224)\n",
            "     | > grad_norm_0: 262.05054  (157.39015)\n",
            "     | > loss_disc: 2.47511  (2.49017)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.47511  (2.49017)\n",
            "     | > grad_norm_1: 9.32718  (14.95381)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.19230  (3.23335)\n",
            "     | > loader_time: 0.01340  (0.01889)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 28/173 -- GLOBAL_STEP: 61900\u001b[0m\n",
            "     | > loss_gen: 1.92262  (2.15951)\n",
            "     | > loss_kl: 1.34743  (1.22152)\n",
            "     | > loss_feat: 5.10571  (4.85517)\n",
            "     | > loss_mel: 18.29956  (18.08082)\n",
            "     | > loss_duration: 1.65818  (1.66955)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.33349  (27.98657)\n",
            "     | > grad_norm_0: 292.53607  (278.48132)\n",
            "     | > loss_disc: 2.62808  (2.54221)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.62808  (2.54221)\n",
            "     | > grad_norm_1: 32.11057  (27.23972)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.40090  (3.31379)\n",
            "     | > loader_time: 0.02060  (0.02061)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 53/173 -- GLOBAL_STEP: 61925\u001b[0m\n",
            "     | > loss_gen: 2.24575  (2.14709)\n",
            "     | > loss_kl: 1.39998  (1.23559)\n",
            "     | > loss_feat: 4.67401  (4.80547)\n",
            "     | > loss_mel: 18.15924  (18.09807)\n",
            "     | > loss_duration: 1.66701  (1.66257)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.14598  (27.94878)\n",
            "     | > grad_norm_0: 399.91156  (275.53391)\n",
            "     | > loss_disc: 2.63523  (2.53386)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.63523  (2.53386)\n",
            "     | > grad_norm_1: 42.92596  (29.79618)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.43890  (3.35030)\n",
            "     | > loader_time: 0.02740  (0.02224)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 78/173 -- GLOBAL_STEP: 61950\u001b[0m\n",
            "     | > loss_gen: 2.10203  (2.14313)\n",
            "     | > loss_kl: 1.26848  (1.25453)\n",
            "     | > loss_feat: 4.61814  (4.77869)\n",
            "     | > loss_mel: 18.07076  (18.11219)\n",
            "     | > loss_duration: 1.62762  (1.65771)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.68703  (27.94624)\n",
            "     | > grad_norm_0: 161.21895  (250.73961)\n",
            "     | > loss_disc: 2.55503  (2.54136)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.55503  (2.54136)\n",
            "     | > grad_norm_1: 9.66494  (28.13802)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.44740  (3.38361)\n",
            "     | > loader_time: 0.02550  (0.02336)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 103/173 -- GLOBAL_STEP: 61975\u001b[0m\n",
            "     | > loss_gen: 2.14387  (2.14015)\n",
            "     | > loss_kl: 1.24372  (1.26682)\n",
            "     | > loss_feat: 4.99924  (4.73536)\n",
            "     | > loss_mel: 17.88407  (18.08032)\n",
            "     | > loss_duration: 1.64038  (1.65293)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.91127  (27.87559)\n",
            "     | > grad_norm_0: 43.85471  (228.92528)\n",
            "     | > loss_disc: 2.50201  (2.53508)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.50201  (2.53508)\n",
            "     | > grad_norm_1: 19.75164  (26.69691)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.59970  (3.42177)\n",
            "     | > loader_time: 0.03020  (0.02475)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 128/173 -- GLOBAL_STEP: 62000\u001b[0m\n",
            "     | > loss_gen: 1.91763  (2.13714)\n",
            "     | > loss_kl: 1.36506  (1.28250)\n",
            "     | > loss_feat: 4.93409  (4.73453)\n",
            "     | > loss_mel: 17.71596  (18.07817)\n",
            "     | > loss_duration: 1.61129  (1.64985)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.54403  (27.88220)\n",
            "     | > grad_norm_0: 248.01363  (208.81422)\n",
            "     | > loss_disc: 2.48759  (2.53917)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.48759  (2.53917)\n",
            "     | > grad_norm_1: 30.85268  (25.20242)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.65600  (3.45609)\n",
            "     | > loader_time: 0.02820  (0.02600)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2/checkpoint_62000.pth.tar\n",
            "\n",
            "\u001b[1m   --> STEP: 153/173 -- GLOBAL_STEP: 62025\u001b[0m\n",
            "     | > loss_gen: 2.31714  (2.13390)\n",
            "     | > loss_kl: 1.29005  (1.28796)\n",
            "     | > loss_feat: 4.80452  (4.69757)\n",
            "     | > loss_mel: 17.92265  (18.06975)\n",
            "     | > loss_duration: 1.63387  (1.64735)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.96822  (27.83652)\n",
            "     | > grad_norm_0: 228.15866  (197.32303)\n",
            "     | > loss_disc: 2.48817  (2.53948)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.48817  (2.53948)\n",
            "     | > grad_norm_1: 18.30442  (24.38667)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.75570  (3.49895)\n",
            "     | > loader_time: 0.03350  (0.02775)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.00612  (2.00612)\n",
            "     | > loss_kl: 1.74773  (1.74773)\n",
            "     | > loss_feat: 4.87737  (4.87737)\n",
            "     | > loss_mel: 18.70211  (18.70211)\n",
            "     | > loss_duration: 1.77796  (1.77796)\n",
            "     | > loss_0: 29.11129  (29.11129)\n",
            "     | > loss_disc: 2.45698  (2.45698)\n",
            "     | > loss_1: 2.45698  (2.45698)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.02296  (2.02296)\n",
            "     | > loss_kl: 2.13023  (2.13023)\n",
            "     | > loss_feat: 4.89767  (4.89767)\n",
            "     | > loss_mel: 18.46062  (18.46062)\n",
            "     | > loss_duration: 1.73966  (1.73966)\n",
            "     | > loss_0: 29.25114  (29.25114)\n",
            "     | > loss_disc: 2.55705  (2.55705)\n",
            "     | > loss_1: 2.55705  (2.55705)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 1.99566  (2.00931)\n",
            "     | > loss_kl: 1.99754  (2.06389)\n",
            "     | > loss_feat: 3.82446  (4.36107)\n",
            "     | > loss_mel: 18.56853  (18.51457)\n",
            "     | > loss_duration: 1.76829  (1.75397)\n",
            "     | > loss_0: 28.15447  (28.70280)\n",
            "     | > loss_disc: 2.61388  (2.58546)\n",
            "     | > loss_1: 2.61388  (2.58546)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.12317  (2.04726)\n",
            "     | > loss_kl: 2.08411  (2.07063)\n",
            "     | > loss_feat: 4.75700  (4.49305)\n",
            "     | > loss_mel: 17.89510  (18.30808)\n",
            "     | > loss_duration: 1.71949  (1.74248)\n",
            "     | > loss_0: 28.57886  (28.66149)\n",
            "     | > loss_disc: 2.43245  (2.53446)\n",
            "     | > loss_1: 2.43245  (2.53446)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 1.96619  (2.02699)\n",
            "     | > loss_kl: 1.85823  (2.01753)\n",
            "     | > loss_feat: 3.85397  (4.33328)\n",
            "     | > loss_mel: 18.04459  (18.24221)\n",
            "     | > loss_duration: 1.73069  (1.73953)\n",
            "     | > loss_0: 27.45367  (28.35954)\n",
            "     | > loss_disc: 2.59618  (2.54989)\n",
            "     | > loss_1: 2.59618  (2.54989)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 1.97436  (2.01647)\n",
            "     | > loss_kl: 2.32023  (2.07807)\n",
            "     | > loss_feat: 4.45310  (4.35724)\n",
            "     | > loss_mel: 20.09181  (18.61213)\n",
            "     | > loss_duration: 1.72343  (1.73631)\n",
            "     | > loss_0: 30.56292  (28.80021)\n",
            "     | > loss_disc: 2.42480  (2.52487)\n",
            "     | > loss_1: 2.42480  (2.52487)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01517 \u001b[0m(-0.00009)\n",
            "     | > avg_loss_gen:\u001b[92m 2.01647 \u001b[0m(-0.18904)\n",
            "     | > avg_loss_kl:\u001b[91m 2.07807 \u001b[0m(+0.03916)\n",
            "     | > avg_loss_feat:\u001b[92m 4.35724 \u001b[0m(-0.00895)\n",
            "     | > avg_loss_mel:\u001b[91m 18.61213 \u001b[0m(+0.51891)\n",
            "     | > avg_loss_duration:\u001b[91m 1.73631 \u001b[0m(+0.00943)\n",
            "     | > avg_loss_0:\u001b[91m 28.80021 \u001b[0m(+0.36951)\n",
            "     | > avg_loss_disc:\u001b[91m 2.52487 \u001b[0m(+0.05126)\n",
            "     | > avg_loss_1:\u001b[91m 2.52487 \u001b[0m(+0.05126)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 29/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 11:31:25) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 4/173 -- GLOBAL_STEP: 62050\u001b[0m\n",
            "     | > loss_gen: 2.43859  (2.24756)\n",
            "     | > loss_kl: 0.95725  (1.08368)\n",
            "     | > loss_feat: 4.81840  (4.61879)\n",
            "     | > loss_mel: 18.35870  (18.20852)\n",
            "     | > loss_duration: 1.72856  (1.74154)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.30149  (27.90007)\n",
            "     | > grad_norm_0: 382.41193  (244.49406)\n",
            "     | > loss_disc: 2.54941  (2.53957)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.54941  (2.53957)\n",
            "     | > grad_norm_1: 43.67815  (39.36415)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.24980  (3.24480)\n",
            "     | > loader_time: 0.02080  (0.01628)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 29/173 -- GLOBAL_STEP: 62075\u001b[0m\n",
            "     | > loss_gen: 2.14982  (2.18364)\n",
            "     | > loss_kl: 1.11165  (1.22417)\n",
            "     | > loss_feat: 4.70537  (4.77827)\n",
            "     | > loss_mel: 18.29458  (18.14411)\n",
            "     | > loss_duration: 1.65943  (1.66852)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.92086  (27.99871)\n",
            "     | > grad_norm_0: 221.00696  (229.37326)\n",
            "     | > loss_disc: 2.49159  (2.50985)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49159  (2.50985)\n",
            "     | > grad_norm_1: 11.95770  (28.36114)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.34820  (3.31058)\n",
            "     | > loader_time: 0.02080  (0.01988)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 54/173 -- GLOBAL_STEP: 62100\u001b[0m\n",
            "     | > loss_gen: 2.02844  (2.15578)\n",
            "     | > loss_kl: 1.28610  (1.23780)\n",
            "     | > loss_feat: 4.90658  (4.74389)\n",
            "     | > loss_mel: 17.97116  (18.13013)\n",
            "     | > loss_duration: 1.62099  (1.66103)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.81326  (27.92862)\n",
            "     | > grad_norm_0: 262.65970  (218.63182)\n",
            "     | > loss_disc: 2.52720  (2.51940)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.52720  (2.51940)\n",
            "     | > grad_norm_1: 13.41752  (26.46216)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.39300  (3.35645)\n",
            "     | > loader_time: 0.02320  (0.02177)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 79/173 -- GLOBAL_STEP: 62125\u001b[0m\n",
            "     | > loss_gen: 2.17083  (2.14936)\n",
            "     | > loss_kl: 1.31402  (1.25603)\n",
            "     | > loss_feat: 4.99427  (4.73203)\n",
            "     | > loss_mel: 18.23547  (18.13746)\n",
            "     | > loss_duration: 1.63610  (1.65546)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.35069  (27.93035)\n",
            "     | > grad_norm_0: 186.60980  (230.38687)\n",
            "     | > loss_disc: 2.57481  (2.52896)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.57481  (2.52896)\n",
            "     | > grad_norm_1: 22.32333  (27.06208)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.43330  (3.39187)\n",
            "     | > loader_time: 0.03040  (0.02364)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 104/173 -- GLOBAL_STEP: 62150\u001b[0m\n",
            "     | > loss_gen: 2.21655  (2.14343)\n",
            "     | > loss_kl: 1.24614  (1.27017)\n",
            "     | > loss_feat: 4.40214  (4.72483)\n",
            "     | > loss_mel: 17.96790  (18.11181)\n",
            "     | > loss_duration: 1.62311  (1.65115)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.45585  (27.90139)\n",
            "     | > grad_norm_0: 173.41313  (225.70992)\n",
            "     | > loss_disc: 2.42245  (2.52590)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.42245  (2.52590)\n",
            "     | > grad_norm_1: 6.80572  (27.06091)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.58130  (3.42644)\n",
            "     | > loader_time: 0.03090  (0.02494)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 129/173 -- GLOBAL_STEP: 62175\u001b[0m\n",
            "     | > loss_gen: 2.14174  (2.14754)\n",
            "     | > loss_kl: 1.23348  (1.28239)\n",
            "     | > loss_feat: 4.21462  (4.73579)\n",
            "     | > loss_mel: 18.06814  (18.09311)\n",
            "     | > loss_duration: 1.61160  (1.64720)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.26958  (27.90602)\n",
            "     | > grad_norm_0: 244.29414  (227.94449)\n",
            "     | > loss_disc: 2.60007  (2.52916)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.60007  (2.52916)\n",
            "     | > grad_norm_1: 17.20860  (27.55618)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.62750  (3.46737)\n",
            "     | > loader_time: 0.03490  (0.02640)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 154/173 -- GLOBAL_STEP: 62200\u001b[0m\n",
            "     | > loss_gen: 2.01274  (2.13833)\n",
            "     | > loss_kl: 1.30125  (1.29279)\n",
            "     | > loss_feat: 4.46112  (4.69169)\n",
            "     | > loss_mel: 17.97726  (18.07624)\n",
            "     | > loss_duration: 1.64981  (1.64582)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.40218  (27.84486)\n",
            "     | > grad_norm_0: 90.15402  (233.43784)\n",
            "     | > loss_disc: 2.64580  (2.53745)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.64580  (2.53745)\n",
            "     | > grad_norm_1: 24.54589  (29.08011)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.74190  (3.50964)\n",
            "     | > loader_time: 0.03690  (0.02811)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.31471  (2.31471)\n",
            "     | > loss_kl: 1.56816  (1.56816)\n",
            "     | > loss_feat: 4.57742  (4.57742)\n",
            "     | > loss_mel: 18.21353  (18.21353)\n",
            "     | > loss_duration: 1.78826  (1.78826)\n",
            "     | > loss_0: 28.46208  (28.46208)\n",
            "     | > loss_disc: 2.49576  (2.49576)\n",
            "     | > loss_1: 2.49576  (2.49576)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.26720  (2.26720)\n",
            "     | > loss_kl: 1.93605  (1.93605)\n",
            "     | > loss_feat: 4.05147  (4.05147)\n",
            "     | > loss_mel: 17.90203  (17.90203)\n",
            "     | > loss_duration: 1.75130  (1.75130)\n",
            "     | > loss_0: 27.90804  (27.90804)\n",
            "     | > loss_disc: 2.57286  (2.57286)\n",
            "     | > loss_1: 2.57286  (2.57286)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.20574  (2.23647)\n",
            "     | > loss_kl: 1.78534  (1.86069)\n",
            "     | > loss_feat: 4.04597  (4.04872)\n",
            "     | > loss_mel: 18.39883  (18.15043)\n",
            "     | > loss_duration: 1.79400  (1.77265)\n",
            "     | > loss_0: 28.22987  (28.06895)\n",
            "     | > loss_disc: 2.62659  (2.59972)\n",
            "     | > loss_1: 2.62659  (2.59972)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.19160  (2.22151)\n",
            "     | > loss_kl: 1.98758  (1.90299)\n",
            "     | > loss_feat: 4.62858  (4.24200)\n",
            "     | > loss_mel: 18.22005  (18.17364)\n",
            "     | > loss_duration: 1.70897  (1.75142)\n",
            "     | > loss_0: 28.73678  (28.29156)\n",
            "     | > loss_disc: 2.65670  (2.61871)\n",
            "     | > loss_1: 2.65670  (2.61871)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.32579  (2.24758)\n",
            "     | > loss_kl: 1.92301  (1.90799)\n",
            "     | > loss_feat: 3.98267  (4.17717)\n",
            "     | > loss_mel: 18.17740  (18.17458)\n",
            "     | > loss_duration: 1.75744  (1.75293)\n",
            "     | > loss_0: 28.16630  (28.26024)\n",
            "     | > loss_disc: 2.59016  (2.61157)\n",
            "     | > loss_1: 2.59016  (2.61157)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.16609  (2.23128)\n",
            "     | > loss_kl: 1.94142  (1.91468)\n",
            "     | > loss_feat: 3.92169  (4.12607)\n",
            "     | > loss_mel: 18.75184  (18.29003)\n",
            "     | > loss_duration: 1.69882  (1.74210)\n",
            "     | > loss_0: 28.47985  (28.30417)\n",
            "     | > loss_disc: 2.71114  (2.63149)\n",
            "     | > loss_1: 2.71114  (2.63149)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01676 \u001b[0m(+0.00159)\n",
            "     | > avg_loss_gen:\u001b[91m 2.23128 \u001b[0m(+0.21481)\n",
            "     | > avg_loss_kl:\u001b[92m 1.91468 \u001b[0m(-0.16339)\n",
            "     | > avg_loss_feat:\u001b[92m 4.12607 \u001b[0m(-0.23117)\n",
            "     | > avg_loss_mel:\u001b[92m 18.29003 \u001b[0m(-0.32210)\n",
            "     | > avg_loss_duration:\u001b[91m 1.74210 \u001b[0m(+0.00579)\n",
            "     | > avg_loss_0:\u001b[92m 28.30417 \u001b[0m(-0.49605)\n",
            "     | > avg_loss_disc:\u001b[91m 2.63149 \u001b[0m(+0.10662)\n",
            "     | > avg_loss_1:\u001b[91m 2.63149 \u001b[0m(+0.10662)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 30/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 11:42:00) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/173 -- GLOBAL_STEP: 62225\u001b[0m\n",
            "     | > loss_gen: 2.15842  (2.14106)\n",
            "     | > loss_kl: 1.27819  (1.11824)\n",
            "     | > loss_feat: 4.93920  (4.88338)\n",
            "     | > loss_mel: 18.34172  (18.16483)\n",
            "     | > loss_duration: 1.69129  (1.72156)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.40882  (28.02907)\n",
            "     | > grad_norm_0: 325.39594  (360.84689)\n",
            "     | > loss_disc: 2.51502  (2.56862)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.51502  (2.56862)\n",
            "     | > grad_norm_1: 19.76265  (23.70834)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.25690  (3.21616)\n",
            "     | > loader_time: 0.01950  (0.01877)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/173 -- GLOBAL_STEP: 62250\u001b[0m\n",
            "     | > loss_gen: 2.31501  (2.15105)\n",
            "     | > loss_kl: 0.98295  (1.19070)\n",
            "     | > loss_feat: 4.59635  (4.79536)\n",
            "     | > loss_mel: 17.69836  (18.09268)\n",
            "     | > loss_duration: 1.60623  (1.66499)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.19890  (27.89478)\n",
            "     | > grad_norm_0: 76.67877  (198.14864)\n",
            "     | > loss_disc: 2.48502  (2.53371)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.48502  (2.53371)\n",
            "     | > grad_norm_1: 31.54885  (20.81345)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.42630  (3.32219)\n",
            "     | > loader_time: 0.02360  (0.02065)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/173 -- GLOBAL_STEP: 62275\u001b[0m\n",
            "     | > loss_gen: 2.11961  (2.15028)\n",
            "     | > loss_kl: 1.19040  (1.21077)\n",
            "     | > loss_feat: 4.80464  (4.76270)\n",
            "     | > loss_mel: 18.20565  (18.12206)\n",
            "     | > loss_duration: 1.64776  (1.66016)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.96807  (27.90597)\n",
            "     | > grad_norm_0: 388.52927  (228.15820)\n",
            "     | > loss_disc: 2.46834  (2.51809)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.46834  (2.51809)\n",
            "     | > grad_norm_1: 14.77420  (24.90743)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.44290  (3.35679)\n",
            "     | > loader_time: 0.02700  (0.02258)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/173 -- GLOBAL_STEP: 62300\u001b[0m\n",
            "     | > loss_gen: 2.18543  (2.15621)\n",
            "     | > loss_kl: 1.29886  (1.23243)\n",
            "     | > loss_feat: 4.26482  (4.75288)\n",
            "     | > loss_mel: 18.03723  (18.11645)\n",
            "     | > loss_duration: 1.64772  (1.65464)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.43406  (27.91260)\n",
            "     | > grad_norm_0: 68.92643  (228.98396)\n",
            "     | > loss_disc: 2.50825  (2.51738)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.50825  (2.51738)\n",
            "     | > grad_norm_1: 14.07660  (26.49993)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.46150  (3.38701)\n",
            "     | > loader_time: 0.02800  (0.02397)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/173 -- GLOBAL_STEP: 62325\u001b[0m\n",
            "     | > loss_gen: 2.13706  (2.15038)\n",
            "     | > loss_kl: 1.25157  (1.25535)\n",
            "     | > loss_feat: 4.52806  (4.75425)\n",
            "     | > loss_mel: 18.01861  (18.10008)\n",
            "     | > loss_duration: 1.62617  (1.65014)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.56147  (27.91020)\n",
            "     | > grad_norm_0: 58.69553  (209.34615)\n",
            "     | > loss_disc: 2.51927  (2.51865)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.51927  (2.51865)\n",
            "     | > grad_norm_1: 19.65839  (24.68030)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.56640  (3.42253)\n",
            "     | > loader_time: 0.02690  (0.02521)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/173 -- GLOBAL_STEP: 62350\u001b[0m\n",
            "     | > loss_gen: 2.01527  (2.14802)\n",
            "     | > loss_kl: 1.28660  (1.26720)\n",
            "     | > loss_feat: 4.63657  (4.74212)\n",
            "     | > loss_mel: 18.07324  (18.10071)\n",
            "     | > loss_duration: 1.62118  (1.64658)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.63285  (27.90464)\n",
            "     | > grad_norm_0: 164.92285  (217.95189)\n",
            "     | > loss_disc: 2.54161  (2.51976)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.54161  (2.51976)\n",
            "     | > grad_norm_1: 38.32265  (25.73561)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.60570  (3.46021)\n",
            "     | > loader_time: 0.03490  (0.02658)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 155/173 -- GLOBAL_STEP: 62375\u001b[0m\n",
            "     | > loss_gen: 2.05506  (2.14926)\n",
            "     | > loss_kl: 1.45183  (1.28279)\n",
            "     | > loss_feat: 4.42947  (4.71405)\n",
            "     | > loss_mel: 17.71788  (18.08603)\n",
            "     | > loss_duration: 1.65208  (1.64529)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.30633  (27.87742)\n",
            "     | > grad_norm_0: 142.97000  (218.03528)\n",
            "     | > loss_disc: 2.50315  (2.51902)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.50315  (2.51902)\n",
            "     | > grad_norm_1: 13.02435  (26.88822)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.82340  (3.50441)\n",
            "     | > loader_time: 0.04500  (0.02819)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.07600  (2.07600)\n",
            "     | > loss_kl: 1.83529  (1.83529)\n",
            "     | > loss_feat: 4.44881  (4.44881)\n",
            "     | > loss_mel: 17.46182  (17.46182)\n",
            "     | > loss_duration: 1.79687  (1.79687)\n",
            "     | > loss_0: 27.61879  (27.61879)\n",
            "     | > loss_disc: 2.51434  (2.51434)\n",
            "     | > loss_1: 2.51434  (2.51434)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.11820  (2.11820)\n",
            "     | > loss_kl: 2.34221  (2.34221)\n",
            "     | > loss_feat: 4.71711  (4.71711)\n",
            "     | > loss_mel: 18.00427  (18.00427)\n",
            "     | > loss_duration: 1.76627  (1.76627)\n",
            "     | > loss_0: 28.94807  (28.94807)\n",
            "     | > loss_disc: 2.51684  (2.51684)\n",
            "     | > loss_1: 2.51684  (2.51684)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.13376  (2.12598)\n",
            "     | > loss_kl: 2.03529  (2.18875)\n",
            "     | > loss_feat: 4.57812  (4.64761)\n",
            "     | > loss_mel: 18.48997  (18.24712)\n",
            "     | > loss_duration: 1.76708  (1.76668)\n",
            "     | > loss_0: 29.00422  (28.97614)\n",
            "     | > loss_disc: 2.46875  (2.49279)\n",
            "     | > loss_1: 2.46875  (2.49279)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.11175  (2.12124)\n",
            "     | > loss_kl: 2.29825  (2.22525)\n",
            "     | > loss_feat: 4.60758  (4.63427)\n",
            "     | > loss_mel: 18.08178  (18.19201)\n",
            "     | > loss_duration: 1.71019  (1.74785)\n",
            "     | > loss_0: 28.80956  (28.92062)\n",
            "     | > loss_disc: 2.54865  (2.51141)\n",
            "     | > loss_1: 2.54865  (2.51141)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.15709  (2.13020)\n",
            "     | > loss_kl: 1.97681  (2.16314)\n",
            "     | > loss_feat: 3.90008  (4.45072)\n",
            "     | > loss_mel: 17.88279  (18.11470)\n",
            "     | > loss_duration: 1.73210  (1.74391)\n",
            "     | > loss_0: 27.64887  (28.60268)\n",
            "     | > loss_disc: 2.58316  (2.52935)\n",
            "     | > loss_1: 2.58316  (2.52935)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.04771  (2.11370)\n",
            "     | > loss_kl: 2.23253  (2.17702)\n",
            "     | > loss_feat: 3.29492  (4.21956)\n",
            "     | > loss_mel: 19.35601  (18.36296)\n",
            "     | > loss_duration: 1.73277  (1.74168)\n",
            "     | > loss_0: 28.66393  (28.61493)\n",
            "     | > loss_disc: 2.48253  (2.51998)\n",
            "     | > loss_1: 2.48253  (2.51998)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01633 \u001b[0m(-0.00043)\n",
            "     | > avg_loss_gen:\u001b[92m 2.11370 \u001b[0m(-0.11758)\n",
            "     | > avg_loss_kl:\u001b[91m 2.17702 \u001b[0m(+0.26234)\n",
            "     | > avg_loss_feat:\u001b[91m 4.21956 \u001b[0m(+0.09349)\n",
            "     | > avg_loss_mel:\u001b[91m 18.36296 \u001b[0m(+0.07293)\n",
            "     | > avg_loss_duration:\u001b[92m 1.74168 \u001b[0m(-0.00042)\n",
            "     | > avg_loss_0:\u001b[91m 28.61493 \u001b[0m(+0.31076)\n",
            "     | > avg_loss_disc:\u001b[92m 2.51998 \u001b[0m(-0.11150)\n",
            "     | > avg_loss_1:\u001b[92m 2.51998 \u001b[0m(-0.11150)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 31/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 11:52:35) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 6/173 -- GLOBAL_STEP: 62400\u001b[0m\n",
            "     | > loss_gen: 2.21720  (2.19364)\n",
            "     | > loss_kl: 1.10484  (1.20818)\n",
            "     | > loss_feat: 4.78739  (5.00101)\n",
            "     | > loss_mel: 18.01563  (18.09768)\n",
            "     | > loss_duration: 1.70968  (1.72447)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.83474  (28.22498)\n",
            "     | > grad_norm_0: 433.47708  (368.73422)\n",
            "     | > loss_disc: 2.49785  (2.49319)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49785  (2.49319)\n",
            "     | > grad_norm_1: 48.16591  (30.72439)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.32430  (3.24934)\n",
            "     | > loader_time: 0.02030  (0.01656)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 31/173 -- GLOBAL_STEP: 62425\u001b[0m\n",
            "     | > loss_gen: 2.09638  (2.14410)\n",
            "     | > loss_kl: 1.24085  (1.25215)\n",
            "     | > loss_feat: 4.62718  (4.81236)\n",
            "     | > loss_mel: 18.05058  (18.12558)\n",
            "     | > loss_duration: 1.64861  (1.66986)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.66361  (28.00405)\n",
            "     | > grad_norm_0: 69.42424  (280.87726)\n",
            "     | > loss_disc: 2.60830  (2.54933)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.60830  (2.54933)\n",
            "     | > grad_norm_1: 13.92614  (32.23254)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.38150  (3.33777)\n",
            "     | > loader_time: 0.02230  (0.02044)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 56/173 -- GLOBAL_STEP: 62450\u001b[0m\n",
            "     | > loss_gen: 2.14796  (2.14428)\n",
            "     | > loss_kl: 1.29128  (1.24987)\n",
            "     | > loss_feat: 4.52452  (4.76707)\n",
            "     | > loss_mel: 18.50150  (18.15336)\n",
            "     | > loss_duration: 1.68096  (1.66247)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.14622  (27.97706)\n",
            "     | > grad_norm_0: 64.46368  (242.55070)\n",
            "     | > loss_disc: 2.53099  (2.53887)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.53099  (2.53887)\n",
            "     | > grad_norm_1: 5.99060  (27.42617)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.38140  (3.37703)\n",
            "     | > loader_time: 0.02770  (0.02276)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 81/173 -- GLOBAL_STEP: 62475\u001b[0m\n",
            "     | > loss_gen: 2.38939  (2.14794)\n",
            "     | > loss_kl: 1.13287  (1.25174)\n",
            "     | > loss_feat: 5.04546  (4.78031)\n",
            "     | > loss_mel: 17.94527  (18.12895)\n",
            "     | > loss_duration: 1.62097  (1.65665)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.13398  (27.96559)\n",
            "     | > grad_norm_0: 435.20486  (220.34766)\n",
            "     | > loss_disc: 2.50119  (2.53360)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.50119  (2.53360)\n",
            "     | > grad_norm_1: 32.11902  (25.75011)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.51700  (3.40483)\n",
            "     | > loader_time: 0.02550  (0.02410)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 106/173 -- GLOBAL_STEP: 62500\u001b[0m\n",
            "     | > loss_gen: 2.34054  (2.14673)\n",
            "     | > loss_kl: 1.30024  (1.26098)\n",
            "     | > loss_feat: 4.29283  (4.75639)\n",
            "     | > loss_mel: 17.82739  (18.10933)\n",
            "     | > loss_duration: 1.63149  (1.65073)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.39249  (27.92415)\n",
            "     | > grad_norm_0: 425.31580  (223.65543)\n",
            "     | > loss_disc: 2.59139  (2.54004)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.59139  (2.54004)\n",
            "     | > grad_norm_1: 55.06991  (27.06631)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.59650  (3.43656)\n",
            "     | > loader_time: 0.03600  (0.02538)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 131/173 -- GLOBAL_STEP: 62525\u001b[0m\n",
            "     | > loss_gen: 1.96225  (2.13998)\n",
            "     | > loss_kl: 1.32439  (1.27392)\n",
            "     | > loss_feat: 4.19227  (4.72838)\n",
            "     | > loss_mel: 17.47058  (18.08055)\n",
            "     | > loss_duration: 1.62185  (1.64690)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 26.57135  (27.86974)\n",
            "     | > grad_norm_0: 110.80411  (226.60841)\n",
            "     | > loss_disc: 2.60072  (2.54211)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.60072  (2.54211)\n",
            "     | > grad_norm_1: 45.71991  (28.19475)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.81270  (3.46953)\n",
            "     | > loader_time: 0.03400  (0.02662)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 156/173 -- GLOBAL_STEP: 62550\u001b[0m\n",
            "     | > loss_gen: 2.33091  (2.13812)\n",
            "     | > loss_kl: 1.35814  (1.28251)\n",
            "     | > loss_feat: 4.45064  (4.69744)\n",
            "     | > loss_mel: 18.10467  (18.06398)\n",
            "     | > loss_duration: 1.65665  (1.64515)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.90101  (27.82721)\n",
            "     | > grad_norm_0: 260.24811  (229.24895)\n",
            "     | > loss_disc: 2.44931  (2.53837)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.44931  (2.53837)\n",
            "     | > grad_norm_1: 29.60789  (28.23587)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.77850  (3.50974)\n",
            "     | > loader_time: 0.03910  (0.02807)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.44174  (2.44174)\n",
            "     | > loss_kl: 1.88569  (1.88569)\n",
            "     | > loss_feat: 4.45622  (4.45622)\n",
            "     | > loss_mel: 18.48412  (18.48412)\n",
            "     | > loss_duration: 1.79834  (1.79834)\n",
            "     | > loss_0: 29.06611  (29.06611)\n",
            "     | > loss_disc: 2.49893  (2.49893)\n",
            "     | > loss_1: 2.49893  (2.49893)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.41817  (2.41817)\n",
            "     | > loss_kl: 2.18159  (2.18159)\n",
            "     | > loss_feat: 4.92875  (4.92875)\n",
            "     | > loss_mel: 17.89162  (17.89162)\n",
            "     | > loss_duration: 1.73370  (1.73370)\n",
            "     | > loss_0: 29.15384  (29.15384)\n",
            "     | > loss_disc: 2.56239  (2.56239)\n",
            "     | > loss_1: 2.56239  (2.56239)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.45559  (2.43688)\n",
            "     | > loss_kl: 2.17501  (2.17830)\n",
            "     | > loss_feat: 4.35433  (4.64154)\n",
            "     | > loss_mel: 18.06138  (17.97650)\n",
            "     | > loss_duration: 1.75731  (1.74550)\n",
            "     | > loss_0: 28.80361  (28.97873)\n",
            "     | > loss_disc: 2.47310  (2.51775)\n",
            "     | > loss_1: 2.47310  (2.51775)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.63112  (2.50163)\n",
            "     | > loss_kl: 2.27369  (2.21010)\n",
            "     | > loss_feat: 5.11724  (4.80011)\n",
            "     | > loss_mel: 18.22976  (18.06092)\n",
            "     | > loss_duration: 1.72069  (1.73723)\n",
            "     | > loss_0: 29.97250  (29.30999)\n",
            "     | > loss_disc: 2.40156  (2.47902)\n",
            "     | > loss_1: 2.40156  (2.47902)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.40982  (2.47868)\n",
            "     | > loss_kl: 1.94519  (2.14387)\n",
            "     | > loss_feat: 4.08682  (4.62178)\n",
            "     | > loss_mel: 18.05813  (18.06022)\n",
            "     | > loss_duration: 1.72874  (1.73511)\n",
            "     | > loss_0: 28.22870  (29.03966)\n",
            "     | > loss_disc: 2.47122  (2.47707)\n",
            "     | > loss_1: 2.47122  (2.47707)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.44975  (2.47289)\n",
            "     | > loss_kl: 2.18982  (2.15306)\n",
            "     | > loss_feat: 4.35793  (4.56901)\n",
            "     | > loss_mel: 18.40562  (18.12930)\n",
            "     | > loss_duration: 1.69789  (1.72766)\n",
            "     | > loss_0: 29.10101  (29.05193)\n",
            "     | > loss_disc: 2.81662  (2.54498)\n",
            "     | > loss_1: 2.81662  (2.54498)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01650 \u001b[0m(+0.00017)\n",
            "     | > avg_loss_gen:\u001b[91m 2.47289 \u001b[0m(+0.35919)\n",
            "     | > avg_loss_kl:\u001b[92m 2.15306 \u001b[0m(-0.02396)\n",
            "     | > avg_loss_feat:\u001b[91m 4.56901 \u001b[0m(+0.34945)\n",
            "     | > avg_loss_mel:\u001b[92m 18.12930 \u001b[0m(-0.23366)\n",
            "     | > avg_loss_duration:\u001b[92m 1.72766 \u001b[0m(-0.01402)\n",
            "     | > avg_loss_0:\u001b[91m 29.05193 \u001b[0m(+0.43700)\n",
            "     | > avg_loss_disc:\u001b[91m 2.54498 \u001b[0m(+0.02499)\n",
            "     | > avg_loss_1:\u001b[91m 2.54498 \u001b[0m(+0.02499)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 32/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 12:03:10) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 7/173 -- GLOBAL_STEP: 62575\u001b[0m\n",
            "     | > loss_gen: 2.23753  (2.15840)\n",
            "     | > loss_kl: 1.04010  (1.19542)\n",
            "     | > loss_feat: 4.78115  (4.86647)\n",
            "     | > loss_mel: 18.27474  (18.13111)\n",
            "     | > loss_duration: 1.68356  (1.71520)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.01708  (28.06661)\n",
            "     | > grad_norm_0: 543.38916  (289.95947)\n",
            "     | > loss_disc: 2.47481  (2.53571)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.47481  (2.53571)\n",
            "     | > grad_norm_1: 68.84008  (34.44552)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.25360  (3.25132)\n",
            "     | > loader_time: 0.01340  (0.01617)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 32/173 -- GLOBAL_STEP: 62600\u001b[0m\n",
            "     | > loss_gen: 2.34858  (2.15542)\n",
            "     | > loss_kl: 1.31797  (1.24189)\n",
            "     | > loss_feat: 4.95719  (4.82507)\n",
            "     | > loss_mel: 18.24780  (18.10142)\n",
            "     | > loss_duration: 1.64812  (1.66583)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.51966  (27.98962)\n",
            "     | > grad_norm_0: 90.27233  (297.50946)\n",
            "     | > loss_disc: 2.53625  (2.52815)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.53625  (2.52815)\n",
            "     | > grad_norm_1: 34.42081  (34.22891)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.31790  (3.31595)\n",
            "     | > loader_time: 0.02050  (0.01982)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 57/173 -- GLOBAL_STEP: 62625\u001b[0m\n",
            "     | > loss_gen: 1.93933  (2.13712)\n",
            "     | > loss_kl: 1.19920  (1.24726)\n",
            "     | > loss_feat: 4.56856  (4.73439)\n",
            "     | > loss_mel: 18.29254  (18.13198)\n",
            "     | > loss_duration: 1.65705  (1.65878)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.65668  (27.90954)\n",
            "     | > grad_norm_0: 129.22319  (282.58881)\n",
            "     | > loss_disc: 2.62803  (2.54643)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.62803  (2.54643)\n",
            "     | > grad_norm_1: 20.72022  (33.61134)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.40670  (3.35538)\n",
            "     | > loader_time: 0.02940  (0.02221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 82/173 -- GLOBAL_STEP: 62650\u001b[0m\n",
            "     | > loss_gen: 2.33649  (2.13979)\n",
            "     | > loss_kl: 1.36216  (1.25800)\n",
            "     | > loss_feat: 5.03163  (4.72268)\n",
            "     | > loss_mel: 18.11800  (18.10243)\n",
            "     | > loss_duration: 1.64931  (1.65328)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 28.49758  (27.87618)\n",
            "     | > grad_norm_0: 496.53006  (281.84833)\n",
            "     | > loss_disc: 2.49053  (2.53972)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.49053  (2.53972)\n",
            "     | > grad_norm_1: 32.86926  (33.34344)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.51740  (3.39012)\n",
            "     | > loader_time: 0.02810  (0.02376)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 107/173 -- GLOBAL_STEP: 62675\u001b[0m\n",
            "     | > loss_gen: 2.14196  (2.13912)\n",
            "     | > loss_kl: 1.36608  (1.26738)\n",
            "     | > loss_feat: 4.41742  (4.71185)\n",
            "     | > loss_mel: 18.14237  (18.09709)\n",
            "     | > loss_duration: 1.62618  (1.64975)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.69401  (27.86520)\n",
            "     | > grad_norm_0: 317.91394  (281.56894)\n",
            "     | > loss_disc: 2.58602  (2.53844)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.58602  (2.53844)\n",
            "     | > grad_norm_1: 33.75547  (33.19175)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.60220  (3.42969)\n",
            "     | > loader_time: 0.03150  (0.02531)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 132/173 -- GLOBAL_STEP: 62700\u001b[0m\n",
            "     | > loss_gen: 1.96172  (2.13876)\n",
            "     | > loss_kl: 1.28508  (1.27946)\n",
            "     | > loss_feat: 4.68838  (4.71850)\n",
            "     | > loss_mel: 17.93699  (18.07527)\n",
            "     | > loss_duration: 1.64432  (1.64620)\n",
            "     | > amp_scaler: 512.00000  (257.93939)\n",
            "     | > loss_0: 27.51648  (27.85818)\n",
            "     | > grad_norm_0: 200.87074  (275.97412)\n",
            "     | > loss_disc: 2.59098  (2.53720)\n",
            "     | > amp_scaler-1: 512.00000  (257.93939)\n",
            "     | > loss_1: 2.59098  (2.53720)\n",
            "     | > grad_norm_1: 45.33095  (32.88731)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.69700  (3.46604)\n",
            "     | > loader_time: 0.03440  (0.02671)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 157/173 -- GLOBAL_STEP: 62725\u001b[0m\n",
            "     | > loss_gen: 2.21045  (2.13029)\n",
            "     | > loss_kl: 1.47682  (1.29151)\n",
            "     | > loss_feat: 4.27533  (4.66778)\n",
            "     | > loss_mel: 18.10524  (18.06666)\n",
            "     | > loss_duration: 1.65814  (1.64535)\n",
            "     | > amp_scaler: 512.00000  (298.39490)\n",
            "     | > loss_0: 27.72597  (27.80159)\n",
            "     | > grad_norm_0: 147.56567  (254.82370)\n",
            "     | > loss_disc: 2.46429  (2.54554)\n",
            "     | > amp_scaler-1: 512.00000  (298.39490)\n",
            "     | > loss_1: 2.46429  (2.54554)\n",
            "     | > grad_norm_1: 6.97092  (31.30308)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.78520  (3.50513)\n",
            "     | > loader_time: 0.03880  (0.02825)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 2.08766  (2.08766)\n",
            "     | > loss_kl: 2.07223  (2.07223)\n",
            "     | > loss_feat: 4.44942  (4.44942)\n",
            "     | > loss_mel: 18.51146  (18.51146)\n",
            "     | > loss_duration: 1.82186  (1.82186)\n",
            "     | > loss_0: 28.94263  (28.94263)\n",
            "     | > loss_disc: 2.60607  (2.60607)\n",
            "     | > loss_1: 2.60607  (2.60607)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.02474  (2.02474)\n",
            "     | > loss_kl: 2.13822  (2.13822)\n",
            "     | > loss_feat: 4.28163  (4.28163)\n",
            "     | > loss_mel: 19.04598  (19.04598)\n",
            "     | > loss_duration: 1.73653  (1.73653)\n",
            "     | > loss_0: 29.22709  (29.22709)\n",
            "     | > loss_disc: 2.56380  (2.56380)\n",
            "     | > loss_1: 2.56380  (2.56380)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 2.23372  (2.12923)\n",
            "     | > loss_kl: 1.97225  (2.05524)\n",
            "     | > loss_feat: 4.34455  (4.31309)\n",
            "     | > loss_mel: 18.69574  (18.87086)\n",
            "     | > loss_duration: 1.74756  (1.74204)\n",
            "     | > loss_0: 28.99382  (29.11046)\n",
            "     | > loss_disc: 2.52489  (2.54435)\n",
            "     | > loss_1: 2.52489  (2.54435)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 2.16448  (2.14098)\n",
            "     | > loss_kl: 2.22432  (2.11160)\n",
            "     | > loss_feat: 4.62855  (4.41824)\n",
            "     | > loss_mel: 18.39162  (18.71111)\n",
            "     | > loss_duration: 1.72255  (1.73555)\n",
            "     | > loss_0: 29.13153  (29.11748)\n",
            "     | > loss_disc: 2.54322  (2.54397)\n",
            "     | > loss_1: 2.54322  (2.54397)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 2.08819  (2.12778)\n",
            "     | > loss_kl: 2.01217  (2.08674)\n",
            "     | > loss_feat: 4.13617  (4.34772)\n",
            "     | > loss_mel: 18.32479  (18.61453)\n",
            "     | > loss_duration: 1.75981  (1.74161)\n",
            "     | > loss_0: 28.32113  (28.91839)\n",
            "     | > loss_disc: 2.58841  (2.55508)\n",
            "     | > loss_1: 2.58841  (2.55508)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 2.03744  (2.10971)\n",
            "     | > loss_kl: 2.15983  (2.10136)\n",
            "     | > loss_feat: 4.23066  (4.32431)\n",
            "     | > loss_mel: 19.43651  (18.77893)\n",
            "     | > loss_duration: 1.75693  (1.74468)\n",
            "     | > loss_0: 29.62136  (29.05899)\n",
            "     | > loss_disc: 2.72533  (2.58913)\n",
            "     | > loss_1: 2.72533  (2.58913)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01650 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_gen:\u001b[92m 2.10971 \u001b[0m(-0.36318)\n",
            "     | > avg_loss_kl:\u001b[92m 2.10136 \u001b[0m(-0.05170)\n",
            "     | > avg_loss_feat:\u001b[92m 4.32431 \u001b[0m(-0.24470)\n",
            "     | > avg_loss_mel:\u001b[91m 18.77893 \u001b[0m(+0.64963)\n",
            "     | > avg_loss_duration:\u001b[91m 1.74468 \u001b[0m(+0.01701)\n",
            "     | > avg_loss_0:\u001b[91m 29.05899 \u001b[0m(+0.00705)\n",
            "     | > avg_loss_disc:\u001b[91m 2.58913 \u001b[0m(+0.04415)\n",
            "     | > avg_loss_1:\u001b[91m 2.58913 \u001b[0m(+0.04415)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 33/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 12:13:44) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 8/173 -- GLOBAL_STEP: 62750\u001b[0m\n",
            "     | > loss_gen: 2.02981  (2.22376)\n",
            "     | > loss_kl: 1.16397  (1.18676)\n",
            "     | > loss_feat: 5.20168  (5.11926)\n",
            "     | > loss_mel: 18.27047  (18.23758)\n",
            "     | > loss_duration: 1.69141  (1.70870)\n",
            "     | > amp_scaler: 256.00000  (416.00000)\n",
            "     | > loss_0: 28.35734  (28.47606)\n",
            "     | > grad_norm_0: 163.91705  (287.46381)\n",
            "     | > loss_disc: 2.40160  (2.46300)\n",
            "     | > amp_scaler-1: 256.00000  (416.00000)\n",
            "     | > loss_1: 2.40160  (2.46300)\n",
            "     | > grad_norm_1: 31.56159  (32.47869)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.26060  (3.24740)\n",
            "     | > loader_time: 0.01900  (0.01846)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 33/173 -- GLOBAL_STEP: 62775\u001b[0m\n",
            "     | > loss_gen: 1.96584  (2.15435)\n",
            "     | > loss_kl: 1.40354  (1.24921)\n",
            "     | > loss_feat: 4.90198  (4.83178)\n",
            "     | > loss_mel: 18.05334  (18.08684)\n",
            "     | > loss_duration: 1.67076  (1.66962)\n",
            "     | > amp_scaler: 256.00000  (294.78788)\n",
            "     | > loss_0: 27.99544  (27.99180)\n",
            "     | > grad_norm_0: 419.07632  (282.28601)\n",
            "     | > loss_disc: 2.63492  (2.52630)\n",
            "     | > amp_scaler-1: 256.00000  (294.78788)\n",
            "     | > loss_1: 2.63492  (2.52630)\n",
            "     | > grad_norm_1: 48.92271  (29.70817)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.37070  (3.32558)\n",
            "     | > loader_time: 0.02130  (0.02099)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 58/173 -- GLOBAL_STEP: 62800\u001b[0m\n",
            "     | > loss_gen: 2.05757  (2.15768)\n",
            "     | > loss_kl: 1.30432  (1.25779)\n",
            "     | > loss_feat: 4.89872  (4.79433)\n",
            "     | > loss_mel: 18.14740  (18.08079)\n",
            "     | > loss_duration: 1.62814  (1.66141)\n",
            "     | > amp_scaler: 256.00000  (278.06897)\n",
            "     | > loss_0: 28.03615  (27.95200)\n",
            "     | > grad_norm_0: 476.90677  (307.62216)\n",
            "     | > loss_disc: 2.62739  (2.52936)\n",
            "     | > amp_scaler-1: 256.00000  (278.06897)\n",
            "     | > loss_1: 2.62739  (2.52936)\n",
            "     | > grad_norm_1: 44.49232  (32.55308)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.48500  (3.35924)\n",
            "     | > loader_time: 0.02830  (0.02280)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 83/173 -- GLOBAL_STEP: 62825\u001b[0m\n",
            "     | > loss_gen: 2.30275  (2.14862)\n",
            "     | > loss_kl: 1.26901  (1.26855)\n",
            "     | > loss_feat: 4.69837  (4.74640)\n",
            "     | > loss_mel: 17.89130  (18.07679)\n",
            "     | > loss_duration: 1.62689  (1.65431)\n",
            "     | > amp_scaler: 256.00000  (271.42169)\n",
            "     | > loss_0: 27.78832  (27.89467)\n",
            "     | > grad_norm_0: 513.31726  (288.10413)\n",
            "     | > loss_disc: 2.46423  (2.53133)\n",
            "     | > amp_scaler-1: 256.00000  (271.42169)\n",
            "     | > loss_1: 2.46423  (2.53133)\n",
            "     | > grad_norm_1: 54.08583  (32.20203)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.52990  (3.39245)\n",
            "     | > loader_time: 0.02760  (0.02399)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 108/173 -- GLOBAL_STEP: 62850\u001b[0m\n",
            "     | > loss_gen: 1.97251  (2.15074)\n",
            "     | > loss_kl: 1.26020  (1.27707)\n",
            "     | > loss_feat: 4.69592  (4.76129)\n",
            "     | > loss_mel: 17.96010  (18.08556)\n",
            "     | > loss_duration: 1.63381  (1.65096)\n",
            "     | > amp_scaler: 256.00000  (267.85185)\n",
            "     | > loss_0: 27.52254  (27.92563)\n",
            "     | > grad_norm_0: 65.21432  (267.18857)\n",
            "     | > loss_disc: 2.50727  (2.52542)\n",
            "     | > amp_scaler-1: 256.00000  (267.85185)\n",
            "     | > loss_1: 2.50727  (2.52542)\n",
            "     | > grad_norm_1: 31.82388  (29.78577)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.68060  (3.42805)\n",
            "     | > loader_time: 0.03060  (0.02531)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 133/173 -- GLOBAL_STEP: 62875\u001b[0m\n",
            "     | > loss_gen: 2.13723  (2.14805)\n",
            "     | > loss_kl: 1.35752  (1.28914)\n",
            "     | > loss_feat: 4.54210  (4.75034)\n",
            "     | > loss_mel: 17.84399  (18.06956)\n",
            "     | > loss_duration: 1.61963  (1.64791)\n",
            "     | > amp_scaler: 256.00000  (265.62406)\n",
            "     | > loss_0: 27.50046  (27.90499)\n",
            "     | > grad_norm_0: 146.66652  (248.19864)\n",
            "     | > loss_disc: 2.59902  (2.52928)\n",
            "     | > amp_scaler-1: 256.00000  (265.62406)\n",
            "     | > loss_1: 2.59902  (2.52928)\n",
            "     | > grad_norm_1: 13.69636  (28.61134)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.61070  (3.46621)\n",
            "     | > loader_time: 0.03370  (0.02652)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 158/173 -- GLOBAL_STEP: 62900\u001b[0m\n",
            "     | > loss_gen: 2.01904  (2.14462)\n",
            "     | > loss_kl: 1.28404  (1.29617)\n",
            "     | > loss_feat: 4.16590  (4.71357)\n",
            "     | > loss_mel: 17.78002  (18.06539)\n",
            "     | > loss_duration: 1.65930  (1.64703)\n",
            "     | > amp_scaler: 256.00000  (264.10127)\n",
            "     | > loss_0: 26.90831  (27.86679)\n",
            "     | > grad_norm_0: 101.86766  (228.55144)\n",
            "     | > loss_disc: 2.50373  (2.52896)\n",
            "     | > amp_scaler-1: 256.00000  (264.10127)\n",
            "     | > loss_1: 2.50373  (2.52896)\n",
            "     | > grad_norm_1: 21.32847  (27.73525)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.80920  (3.51353)\n",
            "     | > loader_time: 0.03520  (0.02816)\n",
            "\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 84\n",
            " | > Max length sequence: 191433.0\n",
            " | > Min length sequence: 43257.0\n",
            " | > Avg length sequence: 101256.65476190476\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_gen: 1.87616  (1.87616)\n",
            "     | > loss_kl: 1.80418  (1.80418)\n",
            "     | > loss_feat: 4.26183  (4.26183)\n",
            "     | > loss_mel: 18.21875  (18.21875)\n",
            "     | > loss_duration: 1.81338  (1.81338)\n",
            "     | > loss_0: 27.97430  (27.97430)\n",
            "     | > loss_disc: 2.72229  (2.72229)\n",
            "     | > loss_1: 2.72229  (2.72229)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_gen: 2.10896  (2.10896)\n",
            "     | > loss_kl: 2.11533  (2.11533)\n",
            "     | > loss_feat: 4.35512  (4.35512)\n",
            "     | > loss_mel: 18.20893  (18.20893)\n",
            "     | > loss_duration: 1.76666  (1.76666)\n",
            "     | > loss_0: 28.55500  (28.55500)\n",
            "     | > loss_disc: 2.65314  (2.65314)\n",
            "     | > loss_1: 2.65314  (2.65314)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_gen: 1.94247  (2.02572)\n",
            "     | > loss_kl: 1.66898  (1.89216)\n",
            "     | > loss_feat: 4.22255  (4.28884)\n",
            "     | > loss_mel: 18.81123  (18.51008)\n",
            "     | > loss_duration: 1.74566  (1.75616)\n",
            "     | > loss_0: 28.39090  (28.47295)\n",
            "     | > loss_disc: 2.62316  (2.63815)\n",
            "     | > loss_1: 2.62316  (2.63815)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_gen: 1.83236  (1.96126)\n",
            "     | > loss_kl: 2.25769  (2.01400)\n",
            "     | > loss_feat: 4.80225  (4.45998)\n",
            "     | > loss_mel: 18.38118  (18.46711)\n",
            "     | > loss_duration: 1.71825  (1.74352)\n",
            "     | > loss_0: 28.99173  (28.64588)\n",
            "     | > loss_disc: 2.75337  (2.67656)\n",
            "     | > loss_1: 2.75337  (2.67656)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_gen: 1.79036  (1.91854)\n",
            "     | > loss_kl: 1.84032  (1.97058)\n",
            "     | > loss_feat: 3.91787  (4.32445)\n",
            "     | > loss_mel: 18.70627  (18.52690)\n",
            "     | > loss_duration: 1.72001  (1.73765)\n",
            "     | > loss_0: 27.97483  (28.47812)\n",
            "     | > loss_disc: 2.73841  (2.69202)\n",
            "     | > loss_1: 2.73841  (2.69202)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_gen: 1.88887  (1.91260)\n",
            "     | > loss_kl: 2.01134  (1.97873)\n",
            "     | > loss_feat: 2.96761  (4.05308)\n",
            "     | > loss_mel: 17.07033  (18.23559)\n",
            "     | > loss_duration: 1.69768  (1.72965)\n",
            "     | > loss_0: 25.63583  (27.90966)\n",
            "     | > loss_disc: 2.60843  (2.67530)\n",
            "     | > loss_1: 2.60843  (2.67530)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.01615 \u001b[0m(-0.00035)\n",
            "     | > avg_loss_gen:\u001b[92m 1.91260 \u001b[0m(-0.19711)\n",
            "     | > avg_loss_kl:\u001b[92m 1.97873 \u001b[0m(-0.12262)\n",
            "     | > avg_loss_feat:\u001b[92m 4.05308 \u001b[0m(-0.27123)\n",
            "     | > avg_loss_mel:\u001b[92m 18.23559 \u001b[0m(-0.54334)\n",
            "     | > avg_loss_duration:\u001b[92m 1.72965 \u001b[0m(-0.01502)\n",
            "     | > avg_loss_0:\u001b[92m 27.90966 \u001b[0m(-1.14933)\n",
            "     | > avg_loss_disc:\u001b[91m 2.67530 \u001b[0m(+0.08617)\n",
            "     | > avg_loss_1:\u001b[91m 2.67530 \u001b[0m(+0.08617)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 34/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 8332\n",
            " | > Max length sequence: 259788.0\n",
            " | > Min length sequence: 29807.0\n",
            " | > Avg length sequence: 100979.94227076332\n",
            " | > Num. instances discarded by max-min (max=500000, min=0) seq limits: 0\n",
            " | > Batch group size: 240.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-11-25 12:24:19) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 9/173 -- GLOBAL_STEP: 62925\u001b[0m\n",
            "     | > loss_gen: 1.93284  (2.18427)\n",
            "     | > loss_kl: 1.28725  (1.23643)\n",
            "     | > loss_feat: 4.59818  (4.82163)\n",
            "     | > loss_mel: 18.25432  (18.17797)\n",
            "     | > loss_duration: 1.67479  (1.70527)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.74737  (28.12558)\n",
            "     | > grad_norm_0: 28.40260  (145.35611)\n",
            "     | > loss_disc: 2.59470  (2.54510)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.59470  (2.54510)\n",
            "     | > grad_norm_1: 16.12470  (22.78245)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.28120  (3.26713)\n",
            "     | > loader_time: 0.01990  (0.01735)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 34/173 -- GLOBAL_STEP: 62950\u001b[0m\n",
            "     | > loss_gen: 1.89667  (2.14218)\n",
            "     | > loss_kl: 1.34827  (1.26323)\n",
            "     | > loss_feat: 4.73950  (4.78871)\n",
            "     | > loss_mel: 17.82693  (18.09884)\n",
            "     | > loss_duration: 1.64213  (1.66718)\n",
            "     | > amp_scaler: 256.00000  (256.00000)\n",
            "     | > loss_0: 27.45350  (27.96014)\n",
            "     | > grad_norm_0: 62.44006  (135.31645)\n",
            "     | > loss_disc: 2.54726  (2.53291)\n",
            "     | > amp_scaler-1: 256.00000  (256.00000)\n",
            "     | > loss_1: 2.54726  (2.53291)\n",
            "     | > grad_norm_1: 21.59397  (20.02769)\n",
            "     | > current_lr_0: 0.00020 \n",
            "     | > current_lr_1: 0.00020 \n",
            "     | > step_time: 3.41180  (3.33646)\n",
            "     | > loader_time: 0.02250  (0.02120)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Emergent/train/Day11_newData/train_vits.py \\\n",
        "    --continue_path /content/drive/MyDrive/Emergent/train/Day11_newData/vits_ljspeech-November-21-2021_01+37PM-33aa27e2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4aRwhYnh64D"
      },
      "source": [
        "Hifigan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAlU3QnBh8Qt",
        "outputId": "606e289a-a90d-413f-80f3-c76eb9416d87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 254/263 -- GLOBAL_STEP: 17150\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05179  (1.04636)\n",
            "     | > G_mse_fake_loss: 1.00682  (1.00098)\n",
            "     | > G_feat_match_loss: 0.16503  (0.16419)\n",
            "     | > G_gen_loss: 47.33076  (47.08631)\n",
            "     | > G_adv_loss: 2.65707  (2.64287)\n",
            "     | > loss_0: 49.98783  (49.72918)\n",
            "     | > grad_norm_0: 371.31274  (427.21317)\n",
            "     | > D_mse_gan_loss: 0.00052  (0.00035)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.0487925667111995e-06  (1.8297400503088712e-06)\n",
            "     | > loss_1: 0.00052  (0.00035)\n",
            "     | > grad_norm_1: 4.99398  (4.99412)\n",
            "     | > current_lr_0: 3.5292998999914496e-13 \n",
            "     | > current_lr_1: 3.5292998999914496e-13 \n",
            "     | > step_time: 2.42890  (2.44301)\n",
            "     | > loader_time: 0.00250  (0.00272)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00192 \u001b[0m(-0.00003)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[91m 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.08590 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.17106 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.08590 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 65/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 07:38:06) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/263 -- GLOBAL_STEP: 17175\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03415  (1.07292)\n",
            "     | > G_mse_fake_loss: 0.99823  (0.99588)\n",
            "     | > G_feat_match_loss: 0.16394  (0.16369)\n",
            "     | > G_gen_loss: 46.53697  (48.28138)\n",
            "     | > G_adv_loss: 2.63766  (2.63277)\n",
            "     | > loss_0: 49.17463  (50.91415)\n",
            "     | > grad_norm_0: 442.04169  (554.54620)\n",
            "     | > D_mse_gan_loss: 0.00019  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.87377725069382e-06  (2.2656364251361083e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.556074494146742e-06  (1.846757190075247e-06)\n",
            "     | > loss_1: 0.00019  (0.00029)\n",
            "     | > grad_norm_1: 4.99512  (4.99579)\n",
            "     | > current_lr_0: 3.4421181195306463e-13 \n",
            "     | > current_lr_1: 3.4421181195306463e-13 \n",
            "     | > step_time: 2.43380  (2.47875)\n",
            "     | > loader_time: 0.00230  (0.00491)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 40/263 -- GLOBAL_STEP: 17200\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07323  (1.06677)\n",
            "     | > G_mse_fake_loss: 0.99726  (0.99644)\n",
            "     | > G_feat_match_loss: 0.16388  (0.16374)\n",
            "     | > G_gen_loss: 48.29545  (48.00485)\n",
            "     | > G_adv_loss: 2.63609  (2.63383)\n",
            "     | > loss_0: 50.93155  (50.63868)\n",
            "     | > grad_norm_0: 569.13855  (550.13470)\n",
            "     | > D_mse_gan_loss: 0.00019  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 8.44459634663508e-07  (2.0816549543667405e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9213780433346983e-06  (1.8349309186760366e-06)\n",
            "     | > loss_1: 0.00019  (0.00029)\n",
            "     | > grad_norm_1: 4.99699  (4.99576)\n",
            "     | > current_lr_0: 3.357089928467087e-13 \n",
            "     | > current_lr_1: 3.357089928467087e-13 \n",
            "     | > step_time: 2.44230  (2.45499)\n",
            "     | > loader_time: 0.00260  (0.00343)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 65/263 -- GLOBAL_STEP: 17225\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04898  (1.07014)\n",
            "     | > G_mse_fake_loss: 0.99674  (0.99651)\n",
            "     | > G_feat_match_loss: 0.16368  (0.16375)\n",
            "     | > G_gen_loss: 47.20421  (48.15627)\n",
            "     | > G_adv_loss: 2.63358  (2.63401)\n",
            "     | > loss_0: 49.83779  (50.79028)\n",
            "     | > grad_norm_0: 485.82889  (548.11218)\n",
            "     | > D_mse_gan_loss: 0.00022  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.133881596615538e-06  (2.2801452377489813e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.941622940648813e-06  (1.896478917947556e-06)\n",
            "     | > loss_1: 0.00022  (0.00031)\n",
            "     | > grad_norm_1: 4.99574  (4.99572)\n",
            "     | > current_lr_0: 3.2741621282165337e-13 \n",
            "     | > current_lr_1: 3.2741621282165337e-13 \n",
            "     | > step_time: 2.43280  (2.44985)\n",
            "     | > loader_time: 0.00250  (0.00308)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/263 -- GLOBAL_STEP: 17250\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06482  (1.07125)\n",
            "     | > G_mse_fake_loss: 0.99515  (0.99663)\n",
            "     | > G_feat_match_loss: 0.16354  (0.16376)\n",
            "     | > G_gen_loss: 47.91711  (48.20629)\n",
            "     | > G_adv_loss: 2.63051  (2.63423)\n",
            "     | > loss_0: 50.54762  (50.84052)\n",
            "     | > grad_norm_0: 598.44611  (557.43097)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 4.550790038138075e-07  (2.293750532405485e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.715877374408592e-06  (1.9352446170412603e-06)\n",
            "     | > loss_1: 0.00021  (0.00031)\n",
            "     | > grad_norm_1: 4.99731  (4.99578)\n",
            "     | > current_lr_0: 3.193282834321463e-13 \n",
            "     | > current_lr_1: 3.193282834321463e-13 \n",
            "     | > step_time: 2.44200  (2.44789)\n",
            "     | > loader_time: 0.00310  (0.00293)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 115/263 -- GLOBAL_STEP: 17275\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03488  (1.07127)\n",
            "     | > G_mse_fake_loss: 1.00161  (0.99662)\n",
            "     | > G_feat_match_loss: 0.16413  (0.16376)\n",
            "     | > G_gen_loss: 46.56946  (48.20711)\n",
            "     | > G_adv_loss: 2.64295  (2.63423)\n",
            "     | > loss_0: 49.21241  (50.84134)\n",
            "     | > grad_norm_0: 415.10327  (557.74042)\n",
            "     | > D_mse_gan_loss: 0.00031  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 2.3701702502876287e-06  (2.3916767657817176e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6003575638023904e-06  (1.946430928534899e-06)\n",
            "     | > loss_1: 0.00031  (0.00031)\n",
            "     | > grad_norm_1: 4.99442  (4.99577)\n",
            "     | > current_lr_0: 3.1144014439891366e-13 \n",
            "     | > current_lr_1: 3.1144014439891366e-13 \n",
            "     | > step_time: 2.43560  (2.44660)\n",
            "     | > loader_time: 0.00230  (0.00283)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 140/263 -- GLOBAL_STEP: 17300\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.23729  (1.07202)\n",
            "     | > G_mse_fake_loss: 0.98061  (0.99666)\n",
            "     | > G_feat_match_loss: 0.16232  (0.16376)\n",
            "     | > G_gen_loss: 55.67815  (48.24101)\n",
            "     | > G_adv_loss: 2.60378  (2.63429)\n",
            "     | > loss_0: 58.28193  (50.87529)\n",
            "     | > grad_norm_0: 1430.36316  (562.19775)\n",
            "     | > D_mse_gan_loss: 0.00035  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 6.815172923779755e-07  (2.2709305883137987e-06)\n",
            "     | > D_mse_gan_fake_loss: 4.760219326271908e-06  (1.982540040655426e-06)\n",
            "     | > loss_1: 0.00035  (0.00030)\n",
            "     | > grad_norm_1: 5.00305  (4.99576)\n",
            "     | > current_lr_0: 3.0374686044315447e-13 \n",
            "     | > current_lr_1: 3.0374686044315447e-13 \n",
            "     | > step_time: 2.44290  (2.44589)\n",
            "     | > loader_time: 0.00260  (0.00278)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 165/263 -- GLOBAL_STEP: 17325\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01736  (1.07127)\n",
            "     | > G_mse_fake_loss: 0.99809  (0.99664)\n",
            "     | > G_feat_match_loss: 0.16377  (0.16375)\n",
            "     | > G_gen_loss: 45.78106  (48.20712)\n",
            "     | > G_adv_loss: 2.63579  (2.63414)\n",
            "     | > loss_0: 48.41684  (50.84126)\n",
            "     | > grad_norm_0: 410.47894  (561.52112)\n",
            "     | > D_mse_gan_loss: 0.00031  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 8.661386345920619e-07  (2.2992587858108465e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.1336938996464596e-06  (1.9729322885227303e-06)\n",
            "     | > loss_1: 0.00031  (0.00030)\n",
            "     | > grad_norm_1: 4.99428  (4.99579)\n",
            "     | > current_lr_0: 2.9624361819874297e-13 \n",
            "     | > current_lr_1: 2.9624361819874297e-13 \n",
            "     | > step_time: 2.44050  (2.44534)\n",
            "     | > loader_time: 0.00250  (0.00277)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 190/263 -- GLOBAL_STEP: 17350\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06527  (1.07083)\n",
            "     | > G_mse_fake_loss: 0.99970  (0.99662)\n",
            "     | > G_feat_match_loss: 0.16392  (0.16375)\n",
            "     | > G_gen_loss: 47.93716  (48.18713)\n",
            "     | > G_adv_loss: 2.63887  (2.63411)\n",
            "     | > loss_0: 50.57602  (50.82125)\n",
            "     | > grad_norm_0: 494.21216  (564.25555)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 2.1488169750227826e-06  (2.207854256210485e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.140468723155209e-06  (1.9629994381519204e-06)\n",
            "     | > loss_1: 0.00029  (0.00030)\n",
            "     | > grad_norm_1: 4.99608  (4.99582)\n",
            "     | > current_lr_0: 2.8892572320070674e-13 \n",
            "     | > current_lr_1: 2.8892572320070674e-13 \n",
            "     | > step_time: 2.43710  (2.44454)\n",
            "     | > loader_time: 0.00260  (0.00274)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 215/263 -- GLOBAL_STEP: 17375\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03739  (1.07139)\n",
            "     | > G_mse_fake_loss: 0.99664  (0.99665)\n",
            "     | > G_feat_match_loss: 0.16375  (0.16375)\n",
            "     | > G_gen_loss: 46.68257  (48.21267)\n",
            "     | > G_adv_loss: 2.63409  (2.63416)\n",
            "     | > loss_0: 49.31666  (50.84683)\n",
            "     | > grad_norm_0: 495.13669  (565.91528)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.4915551673766458e-06  (2.126000530483379e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5350167359429179e-06  (1.9800878920588828e-06)\n",
            "     | > loss_1: 0.00020  (0.00030)\n",
            "     | > grad_norm_1: 4.99604  (4.99584)\n",
            "     | > current_lr_0: 2.8178859694809674e-13 \n",
            "     | > current_lr_1: 2.8178859694809674e-13 \n",
            "     | > step_time: 2.43770  (2.44392)\n",
            "     | > loader_time: 0.00240  (0.00272)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 240/263 -- GLOBAL_STEP: 17400\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01430  (1.07122)\n",
            "     | > G_mse_fake_loss: 0.99843  (0.99667)\n",
            "     | > G_feat_match_loss: 0.16388  (0.16375)\n",
            "     | > G_gen_loss: 45.64365  (48.20483)\n",
            "     | > G_adv_loss: 2.63723  (2.63420)\n",
            "     | > loss_0: 48.28088  (50.83903)\n",
            "     | > grad_norm_0: 414.12808  (564.59650)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.295472429774236e-06  (2.142271952484029e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.2893888197140768e-06  (1.9849589522872685e-06)\n",
            "     | > loss_1: 0.00020  (0.00030)\n",
            "     | > grad_norm_1: 4.99430  (4.99584)\n",
            "     | > current_lr_0: 2.748277740394098e-13 \n",
            "     | > current_lr_1: 2.748277740394098e-13 \n",
            "     | > step_time: 2.43580  (2.44336)\n",
            "     | > loader_time: 0.00290  (0.00270)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00161 \u001b[0m(-0.00032)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 1.09853 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 49.43375 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 52.23592 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 66/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 07:49:04) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 1/263 -- GLOBAL_STEP: 17425\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.10140  (1.10140)\n",
            "     | > G_mse_fake_loss: 0.99194  (0.99194)\n",
            "     | > G_feat_match_loss: 0.16342  (0.16342)\n",
            "     | > G_gen_loss: 49.56313  (49.56313)\n",
            "     | > G_adv_loss: 2.62614  (2.62614)\n",
            "     | > loss_0: 52.18927  (52.18927)\n",
            "     | > grad_norm_0: 673.29742  (673.29742)\n",
            "     | > D_mse_gan_loss: 0.00022  (0.00022)\n",
            "     | > D_mse_gan_real_loss: 5.38220149337576e-07  (5.38220149337576e-07)\n",
            "     | > D_mse_gan_fake_loss: 2.6530694867688e-06  (2.6530694867688e-06)\n",
            "     | > loss_1: 0.00022  (0.00022)\n",
            "     | > grad_norm_1: 4.99859  (4.99859)\n",
            "     | > current_lr_0: 2.680388993787743e-13 \n",
            "     | > current_lr_1: 2.680388993787743e-13 \n",
            "     | > step_time: 3.08020  (3.08018)\n",
            "     | > loader_time: 0.03550  (0.03550)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 26/263 -- GLOBAL_STEP: 17450\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09976  (1.06581)\n",
            "     | > G_mse_fake_loss: 0.98976  (0.99693)\n",
            "     | > G_feat_match_loss: 0.16311  (0.16378)\n",
            "     | > G_gen_loss: 49.48929  (47.96139)\n",
            "     | > G_adv_loss: 2.62081  (2.63474)\n",
            "     | > loss_0: 52.11010  (50.59613)\n",
            "     | > grad_norm_0: 679.06689  (521.64856)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 7.948366373966564e-07  (1.958421011548895e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.1302937511791242e-06  (2.0003362456359005e-06)\n",
            "     | > loss_1: 0.00020  (0.00028)\n",
            "     | > grad_norm_1: 4.99878  (4.99584)\n",
            "     | > current_lr_0: 2.6141772545114843e-13 \n",
            "     | > current_lr_1: 2.6141772545114843e-13 \n",
            "     | > step_time: 2.43490  (2.46491)\n",
            "     | > loader_time: 0.00250  (0.00343)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 51/263 -- GLOBAL_STEP: 17475\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05750  (1.06690)\n",
            "     | > G_mse_fake_loss: 0.99898  (0.99730)\n",
            "     | > G_feat_match_loss: 0.16382  (0.16383)\n",
            "     | > G_gen_loss: 47.58742  (48.01057)\n",
            "     | > G_adv_loss: 2.63716  (2.63561)\n",
            "     | > loss_0: 50.22459  (50.64618)\n",
            "     | > grad_norm_0: 550.00922  (518.05566)\n",
            "     | > D_mse_gan_loss: 0.00026  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.8236894447909435e-06  (2.237266282160513e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7101691582865897e-06  (2.0177769895371077e-06)\n",
            "     | > loss_1: 0.00026  (0.00029)\n",
            "     | > grad_norm_1: 4.99682  (4.99577)\n",
            "     | > current_lr_0: 2.5496010966482777e-13 \n",
            "     | > current_lr_1: 2.5496010966482777e-13 \n",
            "     | > step_time: 2.45180  (2.45237)\n",
            "     | > loader_time: 0.00280  (0.00300)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 76/263 -- GLOBAL_STEP: 17500\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01047  (1.06639)\n",
            "     | > G_mse_fake_loss: 1.00356  (0.99762)\n",
            "     | > G_feat_match_loss: 0.16421  (0.16385)\n",
            "     | > G_gen_loss: 45.47105  (47.98753)\n",
            "     | > G_adv_loss: 2.64565  (2.63612)\n",
            "     | > loss_0: 48.11670  (50.62365)\n",
            "     | > grad_norm_0: 317.00165  (500.15683)\n",
            "     | > D_mse_gan_loss: 0.00019  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.139759433499421e-06  (2.245686471867788e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3040905741945608e-06  (1.9692702048695008e-06)\n",
            "     | > loss_1: 0.00019  (0.00030)\n",
            "     | > grad_norm_1: 4.99084  (4.99547)\n",
            "     | > current_lr_0: 2.486620117595986e-13 \n",
            "     | > current_lr_1: 2.486620117595986e-13 \n",
            "     | > step_time: 2.44850  (2.44868)\n",
            "     | > loader_time: 0.00230  (0.00287)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 101/263 -- GLOBAL_STEP: 17525\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04473  (1.06437)\n",
            "     | > G_mse_fake_loss: 0.99532  (0.99755)\n",
            "     | > G_feat_match_loss: 0.16363  (0.16384)\n",
            "     | > G_gen_loss: 47.01278  (47.89644)\n",
            "     | > G_adv_loss: 2.63163  (2.63598)\n",
            "     | > loss_0: 49.64441  (50.53242)\n",
            "     | > grad_norm_0: 528.37054  (506.57843)\n",
            "     | > D_mse_gan_loss: 0.00018  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.0558279655015212e-06  (2.163724200171249e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5087995279827737e-06  (1.9541639107505894e-06)\n",
            "     | > loss_1: 0.00018  (0.00029)\n",
            "     | > grad_norm_1: 4.99658  (4.99562)\n",
            "     | > current_lr_0: 2.425194912789164e-13 \n",
            "     | > current_lr_1: 2.425194912789164e-13 \n",
            "     | > step_time: 2.44240  (2.44690)\n",
            "     | > loader_time: 0.00250  (0.00281)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 126/263 -- GLOBAL_STEP: 17550\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06157  (1.06281)\n",
            "     | > G_mse_fake_loss: 1.00019  (0.99773)\n",
            "     | > G_feat_match_loss: 0.16400  (0.16386)\n",
            "     | > G_gen_loss: 47.77075  (47.82623)\n",
            "     | > G_adv_loss: 2.64020  (2.63628)\n",
            "     | > loss_0: 50.41095  (50.46251)\n",
            "     | > grad_norm_0: 449.14197  (504.39209)\n",
            "     | > D_mse_gan_loss: 0.00033  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 2.375568101342651e-06  (2.24031510802953e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.939382400450995e-06  (1.934835580641447e-06)\n",
            "     | > loss_1: 0.00033  (0.00029)\n",
            "     | > grad_norm_1: 4.99523  (4.99559)\n",
            "     | > current_lr_0: 2.365287051045265e-13 \n",
            "     | > current_lr_1: 2.365287051045265e-13 \n",
            "     | > step_time: 2.44130  (2.44562)\n",
            "     | > loader_time: 0.00220  (0.00278)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 151/263 -- GLOBAL_STEP: 17575\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.97808  (1.06071)\n",
            "     | > G_mse_fake_loss: 1.00370  (0.99776)\n",
            "     | > G_feat_match_loss: 0.16430  (0.16386)\n",
            "     | > G_gen_loss: 44.01375  (47.73200)\n",
            "     | > G_adv_loss: 2.64670  (2.63633)\n",
            "     | > loss_0: 46.66045  (50.36833)\n",
            "     | > grad_norm_0: 284.49786  (504.49405)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.1753331818908919e-06  (2.1844042737936147e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.0361901559008402e-06  (1.9153242060503947e-06)\n",
            "     | > loss_1: 0.00024  (0.00029)\n",
            "     | > grad_norm_1: 4.98867  (4.99555)\n",
            "     | > current_lr_0: 2.306859050519861e-13 \n",
            "     | > current_lr_1: 2.306859050519861e-13 \n",
            "     | > step_time: 2.44460  (2.44544)\n",
            "     | > loader_time: 0.00250  (0.00274)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 176/263 -- GLOBAL_STEP: 17600\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03017  (1.05965)\n",
            "     | > G_mse_fake_loss: 0.99771  (0.99770)\n",
            "     | > G_feat_match_loss: 0.16382  (0.16385)\n",
            "     | > G_gen_loss: 46.35744  (47.68438)\n",
            "     | > G_adv_loss: 2.63593  (2.63618)\n",
            "     | > loss_0: 48.99337  (50.32057)\n",
            "     | > grad_norm_0: 394.63144  (504.79813)\n",
            "     | > D_mse_gan_loss: 0.00016  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 8.115924288176757e-07  (2.1223291544394188e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.0689872169677983e-06  (1.9169434023974015e-06)\n",
            "     | > loss_1: 0.00016  (0.00029)\n",
            "     | > grad_norm_1: 4.99373  (4.99555)\n",
            "     | > current_lr_0: 2.2498743552558154e-13 \n",
            "     | > current_lr_1: 2.2498743552558154e-13 \n",
            "     | > step_time: 2.43780  (2.44510)\n",
            "     | > loader_time: 0.00240  (0.00273)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 201/263 -- GLOBAL_STEP: 17625\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06921  (1.05955)\n",
            "     | > G_mse_fake_loss: 0.99513  (0.99772)\n",
            "     | > G_feat_match_loss: 0.16354  (0.16385)\n",
            "     | > G_gen_loss: 48.11433  (47.67953)\n",
            "     | > G_adv_loss: 2.63049  (2.63621)\n",
            "     | > loss_0: 50.74482  (50.31575)\n",
            "     | > grad_norm_0: 564.94873  (508.13666)\n",
            "     | > D_mse_gan_loss: 0.00019  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 8.00510065346316e-07  (2.1164437082269196e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6120152395160403e-06  (1.9261437336381138e-06)\n",
            "     | > loss_1: 0.00019  (0.00029)\n",
            "     | > grad_norm_1: 4.99699  (4.99558)\n",
            "     | > current_lr_0: 2.1942973123117515e-13 \n",
            "     | > current_lr_1: 2.1942973123117515e-13 \n",
            "     | > step_time: 2.44170  (2.44478)\n",
            "     | > loader_time: 0.00250  (0.00272)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 226/263 -- GLOBAL_STEP: 17650\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07116  (1.05857)\n",
            "     | > G_mse_fake_loss: 0.99246  (0.99776)\n",
            "     | > G_feat_match_loss: 0.16331  (0.16385)\n",
            "     | > G_gen_loss: 48.20227  (47.63560)\n",
            "     | > G_adv_loss: 2.62553  (2.63626)\n",
            "     | > loss_0: 50.82780  (50.27187)\n",
            "     | > grad_norm_0: 567.35413  (507.31793)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 6.50823551495705e-07  (2.1316404215728073e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7626854287300375e-06  (1.9204063546386237e-06)\n",
            "     | > loss_1: 0.00020  (0.00029)\n",
            "     | > grad_norm_1: 4.99723  (4.99559)\n",
            "     | > current_lr_0: 2.1400931494554988e-13 \n",
            "     | > current_lr_1: 2.1400931494554988e-13 \n",
            "     | > step_time: 2.44940  (2.44489)\n",
            "     | > loader_time: 0.00300  (0.00271)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 251/263 -- GLOBAL_STEP: 17675\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06277  (1.05825)\n",
            "     | > G_mse_fake_loss: 0.99342  (0.99775)\n",
            "     | > G_feat_match_loss: 0.16331  (0.16385)\n",
            "     | > G_gen_loss: 47.82451  (47.62111)\n",
            "     | > G_adv_loss: 2.62648  (2.63625)\n",
            "     | > loss_0: 50.45099  (50.25736)\n",
            "     | > grad_norm_0: 587.03766  (508.10388)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 6.469093705163687e-07  (2.130804419771972e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.1145922346477164e-06  (1.919351184342412e-06)\n",
            "     | > loss_1: 0.00021  (0.00029)\n",
            "     | > grad_norm_1: 4.99720  (4.99558)\n",
            "     | > current_lr_0: 2.0872279534085574e-13 \n",
            "     | > current_lr_1: 2.0872279534085574e-13 \n",
            "     | > step_time: 2.44830  (2.44456)\n",
            "     | > loader_time: 0.00280  (0.00269)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00156 \u001b[0m(-0.00005)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 1.09853 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 49.43375 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 52.23592 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 67/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 08:00:02) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 12/263 -- GLOBAL_STEP: 17700\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.10763  (1.05477)\n",
            "     | > G_mse_fake_loss: 0.99400  (0.99837)\n",
            "     | > G_feat_match_loss: 0.16343  (0.16394)\n",
            "     | > G_gen_loss: 49.84338  (47.46445)\n",
            "     | > G_adv_loss: 2.62826  (2.63775)\n",
            "     | > loss_0: 52.47163  (50.10220)\n",
            "     | > grad_norm_0: 604.91437  (469.99496)\n",
            "     | > D_mse_gan_loss: 0.00026  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 4.598175110004377e-06  (2.5885915064804976e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9534779767127475e-06  (1.7726134444728814e-06)\n",
            "     | > loss_1: 0.00026  (0.00028)\n",
            "     | > grad_norm_1: 4.99735  (4.99557)\n",
            "     | > current_lr_0: 2.0356686486279802e-13 \n",
            "     | > current_lr_1: 2.0356686486279802e-13 \n",
            "     | > step_time: 2.43600  (2.50570)\n",
            "     | > loader_time: 0.00090  (0.00563)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 37/263 -- GLOBAL_STEP: 17725\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05057  (1.05395)\n",
            "     | > G_mse_fake_loss: 0.99634  (0.99871)\n",
            "     | > G_feat_match_loss: 0.16369  (0.16396)\n",
            "     | > G_gen_loss: 47.27581  (47.42791)\n",
            "     | > G_adv_loss: 2.63329  (2.63828)\n",
            "     | > loss_0: 49.90909  (50.06618)\n",
            "     | > grad_norm_0: 624.61035  (497.24210)\n",
            "     | > D_mse_gan_loss: 0.00019  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 8.889798550626438e-07  (2.1890766482876053e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.459618488297565e-06  (1.9319828560263575e-06)\n",
            "     | > loss_1: 0.00019  (0.00028)\n",
            "     | > grad_norm_1: 4.99753  (4.99578)\n",
            "     | > current_lr_0: 1.98538297661239e-13 \n",
            "     | > current_lr_1: 1.98538297661239e-13 \n",
            "     | > step_time: 2.44890  (2.46282)\n",
            "     | > loader_time: 0.00260  (0.00364)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 62/263 -- GLOBAL_STEP: 17750\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08210  (1.05682)\n",
            "     | > G_mse_fake_loss: 0.99471  (0.99897)\n",
            "     | > G_feat_match_loss: 0.16360  (0.16401)\n",
            "     | > G_gen_loss: 48.69466  (47.55679)\n",
            "     | > G_adv_loss: 2.63069  (2.63905)\n",
            "     | > loss_0: 51.32535  (50.19583)\n",
            "     | > grad_norm_0: 460.04556  (491.01837)\n",
            "     | > D_mse_gan_loss: 0.00018  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.3074483149466687e-06  (3.013845505778033e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.4464237665379187e-06  (1.94596545879317e-06)\n",
            "     | > loss_1: 0.00018  (0.00031)\n",
            "     | > grad_norm_1: 4.99547  (4.99572)\n",
            "     | > current_lr_0: 1.9363394757191795e-13 \n",
            "     | > current_lr_1: 1.9363394757191795e-13 \n",
            "     | > step_time: 2.44490  (2.45327)\n",
            "     | > loader_time: 0.00260  (0.00320)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 87/263 -- GLOBAL_STEP: 17775\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03846  (1.05819)\n",
            "     | > G_mse_fake_loss: 1.00393  (0.99919)\n",
            "     | > G_feat_match_loss: 0.16467  (0.16403)\n",
            "     | > G_gen_loss: 46.73065  (47.61851)\n",
            "     | > G_adv_loss: 2.65065  (2.63947)\n",
            "     | > loss_0: 49.38130  (50.25798)\n",
            "     | > grad_norm_0: 455.24350  (485.18359)\n",
            "     | > D_mse_gan_loss: 0.00037  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 1.6648624523440958e-06  (1.9690418041094775e-06)\n",
            "     | > loss_1: 0.00037  (0.00032)\n",
            "     | > grad_norm_1: 4.99563  (4.99559)\n",
            "     | > current_lr_0: 1.88850746148028e-13 \n",
            "     | > current_lr_1: 1.88850746148028e-13 \n",
            "     | > step_time: 2.43530  (2.44990)\n",
            "     | > loader_time: 0.00240  (0.00302)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 112/263 -- GLOBAL_STEP: 17800\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05795  (1.05812)\n",
            "     | > G_mse_fake_loss: 0.99683  (0.99915)\n",
            "     | > G_feat_match_loss: 0.16382  (0.16401)\n",
            "     | > G_gen_loss: 47.60788  (47.61548)\n",
            "     | > G_adv_loss: 2.63508  (2.63926)\n",
            "     | > loss_0: 50.24296  (50.25474)\n",
            "     | > grad_norm_0: 457.94528  (480.96585)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.261650254491542e-06  (2.9209390093955426e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.2836420637540868e-06  (1.9378090751079746e-06)\n",
            "     | > loss_1: 0.00030  (0.00032)\n",
            "     | > grad_norm_1: 4.99531  (4.99541)\n",
            "     | > current_lr_0: 1.841857007404172e-13 \n",
            "     | > current_lr_1: 1.841857007404172e-13 \n",
            "     | > step_time: 2.44100  (2.44846)\n",
            "     | > loader_time: 0.00270  (0.00292)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 137/263 -- GLOBAL_STEP: 17825\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08353  (1.05651)\n",
            "     | > G_mse_fake_loss: 1.00438  (0.99934)\n",
            "     | > G_feat_match_loss: 0.16472  (0.16403)\n",
            "     | > G_gen_loss: 48.75905  (47.54288)\n",
            "     | > G_adv_loss: 2.65160  (2.63959)\n",
            "     | > loss_0: 51.41065  (50.18247)\n",
            "     | > grad_norm_0: 416.72110  (475.38315)\n",
            "     | > D_mse_gan_loss: 0.00054  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 3.747650907826028e-06  (2.826790314454567e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.5918352548615076e-06  (1.9300483442887812e-06)\n",
            "     | > loss_1: 0.00054  (0.00031)\n",
            "     | > grad_norm_1: 4.99492  (4.99529)\n",
            "     | > current_lr_0: 1.7963589262521305e-13 \n",
            "     | > current_lr_1: 1.7963589262521305e-13 \n",
            "     | > step_time: 2.43400  (2.44744)\n",
            "     | > loader_time: 0.00220  (0.00287)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 162/263 -- GLOBAL_STEP: 17850\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09461  (1.05532)\n",
            "     | > G_mse_fake_loss: 0.99932  (0.99926)\n",
            "     | > G_feat_match_loss: 0.16391  (0.16401)\n",
            "     | > G_gen_loss: 49.25748  (47.48947)\n",
            "     | > G_adv_loss: 2.63842  (2.63939)\n",
            "     | > loss_0: 51.89590  (50.12886)\n",
            "     | > grad_norm_0: 610.83136  (477.53476)\n",
            "     | > D_mse_gan_loss: 0.00040  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 4.3579243538260926e-06  (2.739170968071549e-06)\n",
            "     | > D_mse_gan_fake_loss: 3.2990285490086535e-06  (1.9233830012463288e-06)\n",
            "     | > loss_1: 0.00040  (0.00031)\n",
            "     | > grad_norm_1: 4.99752  (4.99530)\n",
            "     | > current_lr_0: 1.751984751776989e-13 \n",
            "     | > current_lr_1: 1.751984751776989e-13 \n",
            "     | > step_time: 2.43700  (2.44667)\n",
            "     | > loader_time: 0.00230  (0.00283)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 187/263 -- GLOBAL_STEP: 17875\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04242  (1.05337)\n",
            "     | > G_mse_fake_loss: 1.00666  (0.99940)\n",
            "     | > G_feat_match_loss: 0.16470  (0.16403)\n",
            "     | > G_gen_loss: 46.90889  (47.40163)\n",
            "     | > G_adv_loss: 2.65371  (2.63967)\n",
            "     | > loss_0: 49.56260  (50.04130)\n",
            "     | > grad_norm_0: 446.84393  (472.39151)\n",
            "     | > D_mse_gan_loss: 0.00042  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 2.8136823857494164e-06  (2.6257564972801565e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.7713997496903175e-06  (1.9104003820971507e-06)\n",
            "     | > loss_1: 0.00042  (0.00031)\n",
            "     | > grad_norm_1: 4.99576  (4.99521)\n",
            "     | > current_lr_0: 1.708706720913002e-13 \n",
            "     | > current_lr_1: 1.708706720913002e-13 \n",
            "     | > step_time: 2.44880  (2.44610)\n",
            "     | > loader_time: 0.00270  (0.00280)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 212/263 -- GLOBAL_STEP: 17900\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06532  (1.05322)\n",
            "     | > G_mse_fake_loss: 0.99551  (0.99942)\n",
            "     | > G_feat_match_loss: 0.16369  (0.16403)\n",
            "     | > G_gen_loss: 47.93941  (47.39511)\n",
            "     | > G_adv_loss: 2.63246  (2.63968)\n",
            "     | > loss_0: 50.57187  (50.03479)\n",
            "     | > grad_norm_0: 639.02844  (470.81366)\n",
            "     | > D_mse_gan_loss: 0.00032  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 3.4850525025831303e-06  (2.567540362893284e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.301359927514568e-06  (1.909743950132256e-06)\n",
            "     | > loss_1: 0.00032  (0.00030)\n",
            "     | > grad_norm_1: 4.99766  (4.99514)\n",
            "     | > current_lr_0: 1.6664977564056502e-13 \n",
            "     | > current_lr_1: 1.6664977564056502e-13 \n",
            "     | > step_time: 2.45000  (2.44614)\n",
            "     | > loader_time: 0.00370  (0.00279)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 237/263 -- GLOBAL_STEP: 17925\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05574  (1.05410)\n",
            "     | > G_mse_fake_loss: 1.00070  (0.99937)\n",
            "     | > G_feat_match_loss: 0.16425  (0.16402)\n",
            "     | > G_gen_loss: 47.50833  (47.43449)\n",
            "     | > G_adv_loss: 2.64322  (2.63961)\n",
            "     | > loss_0: 50.15154  (50.07410)\n",
            "     | > grad_norm_0: 406.49673  (471.82953)\n",
            "     | > D_mse_gan_loss: 0.00042  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 2.925312401202973e-06  (2.580502921496324e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6218158407355077e-06  (1.929362473816691e-06)\n",
            "     | > loss_1: 0.00042  (0.00031)\n",
            "     | > grad_norm_1: 4.99414  (4.99514)\n",
            "     | > current_lr_0: 1.625331449870541e-13 \n",
            "     | > current_lr_1: 1.625331449870541e-13 \n",
            "     | > step_time: 2.44440  (2.44571)\n",
            "     | > loader_time: 0.00250  (0.00277)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 262/263 -- GLOBAL_STEP: 17950\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00339  (1.05363)\n",
            "     | > G_mse_fake_loss: 1.00000  (0.99947)\n",
            "     | > G_feat_match_loss: 0.16401  (0.16403)\n",
            "     | > G_gen_loss: 45.15275  (47.41345)\n",
            "     | > G_adv_loss: 2.64013  (2.63980)\n",
            "     | > loss_0: 47.79288  (50.05325)\n",
            "     | > grad_norm_0: 552.42859  (470.54050)\n",
            "     | > D_mse_gan_loss: 0.00034  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.9018430066353176e-06  (2.84092097914889e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.4624172308685957e-06  (1.93575919420706e-06)\n",
            "     | > loss_1: 0.00034  (0.00031)\n",
            "     | > grad_norm_1: 4.99677  (4.99510)\n",
            "     | > current_lr_0: 1.58518204527078e-13 \n",
            "     | > current_lr_1: 1.58518204527078e-13 \n",
            "     | > step_time: 2.43090  (2.44513)\n",
            "     | > loader_time: 0.00240  (0.00275)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00171 \u001b[0m(+0.00015)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[91m 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 68/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 08:11:01) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 23/263 -- GLOBAL_STEP: 17975\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04229  (1.05152)\n",
            "     | > G_mse_fake_loss: 1.00028  (0.99931)\n",
            "     | > G_feat_match_loss: 0.16416  (0.16407)\n",
            "     | > G_gen_loss: 46.90317  (47.31818)\n",
            "     | > G_adv_loss: 2.64190  (2.64005)\n",
            "     | > loss_0: 49.54506  (49.95824)\n",
            "     | > grad_norm_0: 630.96649  (468.52637)\n",
            "     | > D_mse_gan_loss: 0.00027  (0.00037)\n",
            "     | > D_mse_gan_real_loss: 2.0267309537302935e-06  (5.6026852929414375e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.984004711630405e-06  (1.7875769764902107e-06)\n",
            "     | > loss_1: 0.00027  (0.00037)\n",
            "     | > grad_norm_1: 4.99754  (4.99447)\n",
            "     | > current_lr_0: 1.5460244228024996e-13 \n",
            "     | > current_lr_1: 1.5460244228024996e-13 \n",
            "     | > step_time: 2.45750  (2.44972)\n",
            "     | > loader_time: 0.00260  (0.04410)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 48/263 -- GLOBAL_STEP: 18000\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06344  (1.05387)\n",
            "     | > G_mse_fake_loss: 1.00948  (0.99993)\n",
            "     | > G_feat_match_loss: 0.16518  (0.16411)\n",
            "     | > G_gen_loss: 47.85492  (47.42399)\n",
            "     | > G_adv_loss: 2.66133  (2.64103)\n",
            "     | > loss_0: 50.51625  (50.06503)\n",
            "     | > grad_norm_0: 350.80203  (449.62460)\n",
            "     | > D_mse_gan_loss: 0.00050  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.9090954285493353e-06  (1.8467957462557176e-06)\n",
            "     | > loss_1: 0.00050  (0.00034)\n",
            "     | > grad_norm_1: 4.99337  (4.99437)\n",
            "     | > current_lr_0: 1.5078340831784474e-13 \n",
            "     | > current_lr_1: 1.5078340831784474e-13 \n",
            "     | > step_time: 2.43590  (2.44579)\n",
            "     | > loader_time: 0.00240  (0.02242)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 73/263 -- GLOBAL_STEP: 18025\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06894  (1.05647)\n",
            "     | > G_mse_fake_loss: 0.99698  (0.99973)\n",
            "     | > G_feat_match_loss: 0.16379  (0.16409)\n",
            "     | > G_gen_loss: 48.10236  (47.54096)\n",
            "     | > G_adv_loss: 2.63490  (2.64062)\n",
            "     | > loss_0: 50.73726  (50.18158)\n",
            "     | > grad_norm_0: 506.16330  (453.29651)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 1.398971789967618e-06  (3.5713914131794226e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9267763491370715e-06  (1.8789482240906293e-06)\n",
            "     | > loss_1: 0.00025  (0.00034)\n",
            "     | > grad_norm_1: 4.99609  (4.99441)\n",
            "     | > current_lr_0: 1.4705871322998047e-13 \n",
            "     | > current_lr_1: 1.4705871322998047e-13 \n",
            "     | > step_time: 2.45440  (2.44494)\n",
            "     | > loader_time: 0.00230  (0.01564)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 98/263 -- GLOBAL_STEP: 18050\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.99347  (1.05400)\n",
            "     | > G_mse_fake_loss: 0.99884  (0.99963)\n",
            "     | > G_feat_match_loss: 0.16382  (0.16407)\n",
            "     | > G_gen_loss: 44.70638  (47.43008)\n",
            "     | > G_adv_loss: 2.63706  (2.64029)\n",
            "     | > loss_0: 47.34343  (50.07038)\n",
            "     | > grad_norm_0: 380.25650  (449.82596)\n",
            "     | > D_mse_gan_loss: 0.00017  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 7.105605845936225e-07  (3.2219871042928253e-06)\n",
            "     | > D_mse_gan_fake_loss: 9.615122280592914e-07  (1.84791720675539e-06)\n",
            "     | > loss_1: 0.00017  (0.00032)\n",
            "     | > grad_norm_1: 4.99335  (4.99436)\n",
            "     | > current_lr_0: 1.4342602663066493e-13 \n",
            "     | > current_lr_1: 1.4342602663066493e-13 \n",
            "     | > step_time: 2.43700  (2.44427)\n",
            "     | > loader_time: 0.00230  (0.01230)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 123/263 -- GLOBAL_STEP: 18075\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.10114  (1.05310)\n",
            "     | > G_mse_fake_loss: 0.99354  (0.99974)\n",
            "     | > G_feat_match_loss: 0.16331  (0.16407)\n",
            "     | > G_gen_loss: 49.55142  (47.38967)\n",
            "     | > G_adv_loss: 2.62668  (2.64041)\n",
            "     | > loss_0: 52.17810  (50.03008)\n",
            "     | > grad_norm_0: 612.53278  (449.44043)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.3524396510765655e-06  (1.863190789208602e-06)\n",
            "     | > loss_1: 0.00029  (0.00033)\n",
            "     | > grad_norm_1: 4.99738  (4.99440)\n",
            "     | > current_lr_0: 1.3988307569977055e-13 \n",
            "     | > current_lr_1: 1.3988307569977055e-13 \n",
            "     | > step_time: 2.43410  (2.44395)\n",
            "     | > loader_time: 0.00240  (0.01030)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 148/263 -- GLOBAL_STEP: 18100\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00816  (1.05214)\n",
            "     | > G_mse_fake_loss: 1.00249  (0.99978)\n",
            "     | > G_feat_match_loss: 0.16423  (0.16407)\n",
            "     | > G_gen_loss: 45.36712  (47.34634)\n",
            "     | > G_adv_loss: 2.64484  (2.64053)\n",
            "     | > loss_0: 48.01196  (49.98687)\n",
            "     | > grad_norm_0: 339.01874  (447.60257)\n",
            "     | > D_mse_gan_loss: 0.00039  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.1391985026421025e-06  (3.29296032392707e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.8200739759777207e-06  (1.8599639123767947e-06)\n",
            "     | > loss_1: 0.00039  (0.00033)\n",
            "     | > grad_norm_1: 4.99188  (4.99438)\n",
            "     | > current_lr_0: 1.3642764376102573e-13 \n",
            "     | > current_lr_1: 1.3642764376102573e-13 \n",
            "     | > step_time: 2.43540  (2.44358)\n",
            "     | > loader_time: 0.00260  (0.00899)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 173/263 -- GLOBAL_STEP: 18125\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08877  (1.05367)\n",
            "     | > G_mse_fake_loss: 0.99208  (0.99969)\n",
            "     | > G_feat_match_loss: 0.16328  (0.16407)\n",
            "     | > G_gen_loss: 48.99460  (47.41526)\n",
            "     | > G_adv_loss: 2.62491  (2.64035)\n",
            "     | > loss_0: 51.61951  (50.05561)\n",
            "     | > grad_norm_0: 643.05811  (453.19760)\n",
            "     | > D_mse_gan_loss: 0.00018  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 3.5664089637066354e-07  (3.2150899299115164e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9581996184570016e-06  (1.870084072326969e-06)\n",
            "     | > loss_1: 0.00018  (0.00033)\n",
            "     | > grad_norm_1: 4.99795  (4.99447)\n",
            "     | > current_lr_0: 1.3305756889513312e-13 \n",
            "     | > current_lr_1: 1.3305756889513312e-13 \n",
            "     | > step_time: 2.44220  (2.44330)\n",
            "     | > loader_time: 0.00280  (0.00807)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 198/263 -- GLOBAL_STEP: 18150\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03427  (1.05270)\n",
            "     | > G_mse_fake_loss: 1.01017  (0.99975)\n",
            "     | > G_feat_match_loss: 0.16509  (0.16407)\n",
            "     | > G_gen_loss: 46.54225  (47.37140)\n",
            "     | > G_adv_loss: 2.66112  (2.64042)\n",
            "     | > loss_0: 49.20337  (50.01182)\n",
            "     | > grad_norm_0: 294.60434  (450.24417)\n",
            "     | > D_mse_gan_loss: 0.00035  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.453878551023081e-06  (1.8606099610513576e-06)\n",
            "     | > loss_1: 0.00035  (0.00034)\n",
            "     | > grad_norm_1: 4.99058  (4.99442)\n",
            "     | > current_lr_0: 1.2977074258714727e-13 \n",
            "     | > current_lr_1: 1.2977074258714727e-13 \n",
            "     | > step_time: 2.45170  (2.44263)\n",
            "     | > loader_time: 0.00230  (0.00737)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 223/263 -- GLOBAL_STEP: 18175\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06707  (1.05345)\n",
            "     | > G_mse_fake_loss: 0.99325  (0.99974)\n",
            "     | > G_feat_match_loss: 0.16341  (0.16407)\n",
            "     | > G_gen_loss: 48.01836  (47.40509)\n",
            "     | > G_adv_loss: 2.62734  (2.64046)\n",
            "     | > loss_0: 50.64570  (50.04555)\n",
            "     | > grad_norm_0: 482.45865  (449.10953)\n",
            "     | > D_mse_gan_loss: 0.00014  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 3.8315525330290257e-07  (3.132314959795026e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.1971837921009865e-06  (1.8539282779882e-06)\n",
            "     | > loss_1: 0.00014  (0.00034)\n",
            "     | > grad_norm_1: 4.99614  (4.99441)\n",
            "     | > current_lr_0: 1.2656510840726486e-13 \n",
            "     | > current_lr_1: 1.2656510840726486e-13 \n",
            "     | > step_time: 2.45280  (2.44243)\n",
            "     | > loader_time: 0.00260  (0.00685)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 248/263 -- GLOBAL_STEP: 18200\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02251  (1.05266)\n",
            "     | > G_mse_fake_loss: 1.00640  (0.99981)\n",
            "     | > G_feat_match_loss: 0.16488  (0.16408)\n",
            "     | > G_gen_loss: 46.01284  (47.36989)\n",
            "     | > G_adv_loss: 2.65524  (2.64060)\n",
            "     | > loss_0: 48.66808  (50.01049)\n",
            "     | > grad_norm_0: 316.76987  (447.98141)\n",
            "     | > D_mse_gan_loss: 0.00052  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 4.48500077254721e-06  (3.1198048289860665e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.220381475126487e-06  (1.8502053876087273e-06)\n",
            "     | > loss_1: 0.00052  (0.00034)\n",
            "     | > grad_norm_1: 4.99108  (4.99439)\n",
            "     | > current_lr_0: 1.2343866072420265e-13 \n",
            "     | > current_lr_1: 1.2343866072420265e-13 \n",
            "     | > step_time: 2.44600  (2.44253)\n",
            "     | > loader_time: 0.00220  (0.00642)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00145 \u001b[0m(-0.00026)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[92m 1.00113 \u001b[0m(-0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[91m 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.08590 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.17106 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.08590 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 69/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 08:22:01) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 9/263 -- GLOBAL_STEP: 18225\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02236  (1.04033)\n",
            "     | > G_mse_fake_loss: 1.00994  (1.00064)\n",
            "     | > G_feat_match_loss: 0.16484  (0.16421)\n",
            "     | > G_gen_loss: 46.00598  (46.81483)\n",
            "     | > G_adv_loss: 2.65834  (2.64277)\n",
            "     | > loss_0: 48.66432  (49.45760)\n",
            "     | > grad_norm_0: 256.11475  (406.90982)\n",
            "     | > D_mse_gan_loss: 0.00034  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 1.2747559594572522e-06  (2.5699674842548525e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.387285803706618e-06  (1.7375110575004428e-06)\n",
            "     | > loss_1: 0.00034  (0.00034)\n",
            "     | > grad_norm_1: 4.98752  (4.99370)\n",
            "     | > current_lr_0: 1.2038944345035777e-13 \n",
            "     | > current_lr_1: 1.2038944345035777e-13 \n",
            "     | > step_time: 2.43620  (2.52875)\n",
            "     | > loader_time: 0.00270  (0.00530)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 34/263 -- GLOBAL_STEP: 18250\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.11448  (1.04853)\n",
            "     | > G_mse_fake_loss: 0.99593  (0.99993)\n",
            "     | > G_feat_match_loss: 0.16390  (0.16410)\n",
            "     | > G_gen_loss: 50.15151  (47.18377)\n",
            "     | > G_adv_loss: 2.63490  (2.64095)\n",
            "     | > loss_0: 52.78640  (49.82473)\n",
            "     | > grad_norm_0: 524.87909  (434.60211)\n",
            "     | > D_mse_gan_loss: 0.00036  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 1.5078611568242195e-06  (2.4352332149767196e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.113340769938077e-06  (1.8458626815278394e-06)\n",
            "     | > loss_1: 0.00036  (0.00034)\n",
            "     | > grad_norm_1: 4.99661  (4.99450)\n",
            "     | > current_lr_0: 1.174155488179655e-13 \n",
            "     | > current_lr_1: 1.174155488179655e-13 \n",
            "     | > step_time: 2.43820  (2.46481)\n",
            "     | > loader_time: 0.00230  (0.00326)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 59/263 -- GLOBAL_STEP: 18275\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07088  (1.04870)\n",
            "     | > G_mse_fake_loss: 1.00170  (1.00023)\n",
            "     | > G_feat_match_loss: 0.16433  (0.16414)\n",
            "     | > G_gen_loss: 48.18955  (47.19143)\n",
            "     | > G_adv_loss: 2.64496  (2.64165)\n",
            "     | > loss_0: 50.83451  (49.83308)\n",
            "     | > grad_norm_0: 385.44165  (433.01007)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 2.0451036562008085e-06  (2.8676041998065646e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.652904529692023e-06  (1.830945285615537e-06)\n",
            "     | > loss_1: 0.00030  (0.00034)\n",
            "     | > grad_norm_1: 4.99347  (4.99446)\n",
            "     | > current_lr_0: 1.1451511618548858e-13 \n",
            "     | > current_lr_1: 1.1451511618548858e-13 \n",
            "     | > step_time: 2.43490  (2.45375)\n",
            "     | > loader_time: 0.00300  (0.00300)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 84/263 -- GLOBAL_STEP: 18300\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05540  (1.05096)\n",
            "     | > G_mse_fake_loss: 0.99908  (1.00005)\n",
            "     | > G_feat_match_loss: 0.16413  (0.16411)\n",
            "     | > G_gen_loss: 47.49321  (47.29307)\n",
            "     | > G_adv_loss: 2.64037  (2.64120)\n",
            "     | > loss_0: 50.13358  (49.93427)\n",
            "     | > grad_norm_0: 471.59369  (436.39282)\n",
            "     | > D_mse_gan_loss: 0.00027  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 2.4658031634317013e-06  (2.5851859292845886e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5269406503648497e-06  (1.8330085808992452e-06)\n",
            "     | > loss_1: 0.00027  (0.00034)\n",
            "     | > grad_norm_1: 4.99567  (4.99453)\n",
            "     | > current_lr_0: 1.1168633087349202e-13 \n",
            "     | > current_lr_1: 1.1168633087349202e-13 \n",
            "     | > step_time: 2.43820  (2.44915)\n",
            "     | > loader_time: 0.00250  (0.00289)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 109/263 -- GLOBAL_STEP: 18325\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06957  (1.04981)\n",
            "     | > G_mse_fake_loss: 0.99578  (0.99996)\n",
            "     | > G_feat_match_loss: 0.16347  (0.16410)\n",
            "     | > G_gen_loss: 48.13063  (47.24130)\n",
            "     | > G_adv_loss: 2.63053  (2.64098)\n",
            "     | > loss_0: 50.76115  (49.88228)\n",
            "     | > grad_norm_0: 516.67566  (436.89914)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.710733840809553e-06  (2.525463607584442e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7485228909208672e-06  (1.8029174061207788e-06)\n",
            "     | > loss_1: 0.00024  (0.00033)\n",
            "     | > grad_norm_1: 4.99643  (4.99456)\n",
            "     | > current_lr_0: 1.0892742302927364e-13 \n",
            "     | > current_lr_1: 1.0892742302927364e-13 \n",
            "     | > step_time: 2.45060  (2.44691)\n",
            "     | > loader_time: 0.00240  (0.00283)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 134/263 -- GLOBAL_STEP: 18350\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04120  (1.05036)\n",
            "     | > G_mse_fake_loss: 1.00254  (1.00017)\n",
            "     | > G_feat_match_loss: 0.16422  (0.16412)\n",
            "     | > G_gen_loss: 46.85401  (47.26603)\n",
            "     | > G_adv_loss: 2.64478  (2.64136)\n",
            "     | > loss_0: 49.49879  (49.90739)\n",
            "     | > grad_norm_0: 418.24176  (435.27979)\n",
            "     | > D_mse_gan_loss: 0.00036  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.637590569065651e-06  (1.8479643245821385e-06)\n",
            "     | > loss_1: 0.00036  (0.00033)\n",
            "     | > grad_norm_1: 4.99498  (4.99446)\n",
            "     | > current_lr_0: 1.0623666651954145e-13 \n",
            "     | > current_lr_1: 1.0623666651954145e-13 \n",
            "     | > step_time: 2.44440  (2.44606)\n",
            "     | > loader_time: 0.00230  (0.00278)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 159/263 -- GLOBAL_STEP: 18375\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02577  (1.04927)\n",
            "     | > G_mse_fake_loss: 1.00635  (1.00018)\n",
            "     | > G_feat_match_loss: 0.16456  (0.16412)\n",
            "     | > G_gen_loss: 46.15958  (47.21702)\n",
            "     | > G_adv_loss: 2.65198  (2.64136)\n",
            "     | > loss_0: 48.81156  (49.85838)\n",
            "     | > grad_norm_0: 305.90625  (431.45615)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 9.802258773561334e-07  (2.7861667759671545e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.750323121996189e-06  (1.822701329205782e-06)\n",
            "     | > loss_1: 0.00029  (0.00033)\n",
            "     | > grad_norm_1: 4.99079  (4.99439)\n",
            "     | > current_lr_0: 1.03612377850444e-13 \n",
            "     | > current_lr_1: 1.03612377850444e-13 \n",
            "     | > step_time: 2.44020  (2.44497)\n",
            "     | > loader_time: 0.00230  (0.00274)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 184/263 -- GLOBAL_STEP: 18400\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06159  (1.04891)\n",
            "     | > G_mse_fake_loss: 0.99463  (1.00011)\n",
            "     | > G_feat_match_loss: 0.16341  (0.16411)\n",
            "     | > G_gen_loss: 47.77135  (47.20112)\n",
            "     | > G_adv_loss: 2.62877  (2.64119)\n",
            "     | > loss_0: 50.40012  (49.84231)\n",
            "     | > grad_norm_0: 468.43945  (434.14438)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 8.322044209307933e-07  (2.734617696654101e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.2917578260385199e-06  (1.8180293840037334e-06)\n",
            "     | > loss_1: 0.00021  (0.00033)\n",
            "     | > grad_norm_1: 4.99552  (4.99442)\n",
            "     | > current_lr_0: 1.0105291511427889e-13 \n",
            "     | > current_lr_1: 1.0105291511427889e-13 \n",
            "     | > step_time: 2.43730  (2.44400)\n",
            "     | > loader_time: 0.00270  (0.00273)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 209/263 -- GLOBAL_STEP: 18425\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02705  (1.04933)\n",
            "     | > G_mse_fake_loss: 1.00750  (1.00020)\n",
            "     | > G_feat_match_loss: 0.16490  (0.16411)\n",
            "     | > G_gen_loss: 46.21724  (47.21992)\n",
            "     | > G_adv_loss: 2.65645  (2.64133)\n",
            "     | > loss_0: 48.87370  (49.86125)\n",
            "     | > grad_norm_0: 282.29684  (436.80173)\n",
            "     | > D_mse_gan_loss: 0.00033  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 3.4662880352698267e-06  (2.743868301079788e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9779351987381233e-06  (1.8485306082096174e-06)\n",
            "     | > loss_1: 0.00033  (0.00033)\n",
            "     | > grad_norm_1: 4.98872  (4.99445)\n",
            "     | > current_lr_0: 9.855667696221972e-14 \n",
            "     | > current_lr_1: 9.855667696221972e-14 \n",
            "     | > step_time: 2.43260  (2.44353)\n",
            "     | > loader_time: 0.00250  (0.00271)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 234/263 -- GLOBAL_STEP: 18450\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04277  (1.04924)\n",
            "     | > G_mse_fake_loss: 1.00026  (1.00025)\n",
            "     | > G_feat_match_loss: 0.16427  (0.16412)\n",
            "     | > G_gen_loss: 46.92455  (47.21565)\n",
            "     | > G_adv_loss: 2.64299  (2.64144)\n",
            "     | > loss_0: 49.56753  (49.85709)\n",
            "     | > grad_norm_0: 399.06140  (434.14462)\n",
            "     | > D_mse_gan_loss: 0.00032  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.6243557183770463e-06  (2.746824325905071e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.2785648095814395e-06  (1.8440286713294515e-06)\n",
            "     | > loss_1: 0.00032  (0.00032)\n",
            "     | > grad_norm_1: 4.99389  (4.99441)\n",
            "     | > current_lr_0: 9.612210160241891e-14 \n",
            "     | > current_lr_1: 9.612210160241891e-14 \n",
            "     | > step_time: 2.43190  (2.44282)\n",
            "     | > loader_time: 0.00300  (0.00269)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 259/263 -- GLOBAL_STEP: 18475\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01279  (1.04966)\n",
            "     | > G_mse_fake_loss: 1.00062  (1.00028)\n",
            "     | > G_feat_match_loss: 0.16417  (0.16413)\n",
            "     | > G_gen_loss: 45.57552  (47.23462)\n",
            "     | > G_adv_loss: 2.64229  (2.64154)\n",
            "     | > loss_0: 48.21781  (49.87616)\n",
            "     | > grad_norm_0: 497.41315  (434.89633)\n",
            "     | > D_mse_gan_loss: 0.00051  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.452588207939698e-06  (3.066364112219443e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9263684407633264e-06  (1.8610652452223937e-06)\n",
            "     | > loss_1: 0.00051  (0.00033)\n",
            "     | > grad_norm_1: 4.99605  (4.99444)\n",
            "     | > current_lr_0: 9.374766582286007e-14 \n",
            "     | > current_lr_1: 9.374766582286007e-14 \n",
            "     | > step_time: 2.43130  (2.44221)\n",
            "     | > loader_time: 0.00260  (0.00267)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00166 \u001b[0m(+0.00021)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[91m 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 70/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 08:32:58) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/263 -- GLOBAL_STEP: 18500\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04513  (1.07166)\n",
            "     | > G_mse_fake_loss: 1.00301  (0.99564)\n",
            "     | > G_feat_match_loss: 0.16443  (0.16364)\n",
            "     | > G_gen_loss: 47.03082  (48.22481)\n",
            "     | > G_adv_loss: 2.64734  (2.63205)\n",
            "     | > loss_0: 49.67816  (50.85686)\n",
            "     | > grad_norm_0: 360.23953  (670.93433)\n",
            "     | > D_mse_gan_loss: 0.00028  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 2.151825356122572e-06  (1.9533083914780036e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9769709069805685e-06  (2.3092765928822702e-06)\n",
            "     | > loss_1: 0.00028  (0.00028)\n",
            "     | > grad_norm_1: 4.99265  (4.99640)\n",
            "     | > current_lr_0: 9.143188403834776e-14 \n",
            "     | > current_lr_1: 9.143188403834776e-14 \n",
            "     | > step_time: 2.45660  (2.47573)\n",
            "     | > loader_time: 0.00280  (0.00788)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 45/263 -- GLOBAL_STEP: 18525\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.10017  (1.07729)\n",
            "     | > G_mse_fake_loss: 0.99900  (0.99606)\n",
            "     | > G_feat_match_loss: 0.16394  (0.16372)\n",
            "     | > G_gen_loss: 49.50761  (48.47810)\n",
            "     | > G_adv_loss: 2.63841  (2.63323)\n",
            "     | > loss_0: 52.14602  (51.11133)\n",
            "     | > grad_norm_0: 483.78671  (666.90723)\n",
            "     | > D_mse_gan_loss: 0.00075  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 2.4458863663312513e-06  (2.154203156123913e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.6493844416108914e-06  (2.4284274016079313e-06)\n",
            "     | > loss_1: 0.00075  (0.00031)\n",
            "     | > grad_norm_1: 4.99582  (4.99640)\n",
            "     | > current_lr_0: 8.917330736103897e-14 \n",
            "     | > current_lr_1: 8.917330736103897e-14 \n",
            "     | > step_time: 2.43590  (2.45589)\n",
            "     | > loader_time: 0.00230  (0.00492)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/263 -- GLOBAL_STEP: 18550\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.12381  (1.08781)\n",
            "     | > G_mse_fake_loss: 0.98995  (0.99547)\n",
            "     | > G_feat_match_loss: 0.16314  (0.16366)\n",
            "     | > G_gen_loss: 50.57149  (48.95143)\n",
            "     | > G_adv_loss: 2.62134  (2.63202)\n",
            "     | > loss_0: 53.19283  (51.58345)\n",
            "     | > grad_norm_0: 738.30823  (684.75183)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 4.997009455109946e-07  (2.279515875004888e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.244159077235963e-06  (2.4898289950670013e-06)\n",
            "     | > loss_1: 0.00021  (0.00031)\n",
            "     | > grad_norm_1: 4.99891  (4.99673)\n",
            "     | > current_lr_0: 8.697052269393469e-14 \n",
            "     | > current_lr_1: 8.697052269393469e-14 \n",
            "     | > step_time: 2.43080  (2.44977)\n",
            "     | > loader_time: 0.00260  (0.00405)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 95/263 -- GLOBAL_STEP: 18575\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09957  (1.08864)\n",
            "     | > G_mse_fake_loss: 0.99480  (0.99547)\n",
            "     | > G_feat_match_loss: 0.16369  (0.16366)\n",
            "     | > G_gen_loss: 49.48075  (48.98898)\n",
            "     | > G_adv_loss: 2.63171  (2.63203)\n",
            "     | > loss_0: 52.11247  (51.62101)\n",
            "     | > grad_norm_0: 720.38043  (683.77563)\n",
            "     | > D_mse_gan_loss: 0.00043  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.4255444966693176e-06  (2.1449357213417603e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.5697549972392153e-06  (2.5000241024779267e-06)\n",
            "     | > loss_1: 0.00043  (0.00032)\n",
            "     | > grad_norm_1: 4.99837  (4.99677)\n",
            "     | > current_lr_0: 8.482215184676402e-14 \n",
            "     | > current_lr_1: 8.482215184676402e-14 \n",
            "     | > step_time: 2.44990  (2.44617)\n",
            "     | > loader_time: 0.00300  (0.00365)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/263 -- GLOBAL_STEP: 18600\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.25373  (1.08861)\n",
            "     | > G_mse_fake_loss: 0.97714  (0.99553)\n",
            "     | > G_feat_match_loss: 0.16199  (0.16366)\n",
            "     | > G_gen_loss: 56.41798  (48.98734)\n",
            "     | > G_adv_loss: 2.59708  (2.63215)\n",
            "     | > loss_0: 59.01506  (51.61948)\n",
            "     | > grad_norm_0: 1936.59082  (686.15155)\n",
            "     | > D_mse_gan_loss: 0.00041  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 2.993159569086856e-07  (2.349028877309441e-06)\n",
            "     | > D_mse_gan_fake_loss: 0.00001  (0.00000)\n",
            "     | > loss_1: 0.00041  (0.00032)\n",
            "     | > grad_norm_1: 5.00453  (4.99678)\n",
            "     | > current_lr_0: 8.272685067370827e-14 \n",
            "     | > current_lr_1: 8.272685067370827e-14 \n",
            "     | > step_time: 2.45210  (2.44508)\n",
            "     | > loader_time: 0.00240  (0.00342)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 145/263 -- GLOBAL_STEP: 18625\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04891  (1.08692)\n",
            "     | > G_mse_fake_loss: 0.99954  (0.99569)\n",
            "     | > G_feat_match_loss: 0.16390  (0.16367)\n",
            "     | > G_gen_loss: 47.20104  (48.91132)\n",
            "     | > G_adv_loss: 2.63858  (2.63243)\n",
            "     | > loss_0: 49.83963  (51.54375)\n",
            "     | > grad_norm_0: 467.53247  (678.81030)\n",
            "     | > D_mse_gan_loss: 0.00026  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 2.4276978365378454e-06  (2.247184075627204e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.140966898878105e-06  (2.5106863843355886e-06)\n",
            "     | > loss_1: 0.00026  (0.00032)\n",
            "     | > grad_norm_1: 4.99562  (4.99668)\n",
            "     | > current_lr_0: 8.068330823242512e-14 \n",
            "     | > current_lr_1: 8.068330823242512e-14 \n",
            "     | > step_time: 2.43590  (2.44432)\n",
            "     | > loader_time: 0.00260  (0.00329)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 170/263 -- GLOBAL_STEP: 18650\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06175  (1.08646)\n",
            "     | > G_mse_fake_loss: 0.99660  (0.99567)\n",
            "     | > G_feat_match_loss: 0.16378  (0.16367)\n",
            "     | > G_gen_loss: 47.77879  (48.89048)\n",
            "     | > G_adv_loss: 2.63444  (2.63241)\n",
            "     | > loss_0: 50.41323  (51.52288)\n",
            "     | > grad_norm_0: 397.43134  (674.65118)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 7.9283205423053e-07  (2.2420855144138623e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6420103747805115e-06  (2.489436936310299e-06)\n",
            "     | > loss_1: 0.00029  (0.00032)\n",
            "     | > grad_norm_1: 4.99385  (4.99664)\n",
            "     | > current_lr_0: 7.869024596384667e-14 \n",
            "     | > current_lr_1: 7.869024596384667e-14 \n",
            "     | > step_time: 2.44540  (2.44375)\n",
            "     | > loader_time: 0.00250  (0.00319)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 195/263 -- GLOBAL_STEP: 18675\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07057  (1.08707)\n",
            "     | > G_mse_fake_loss: 1.00368  (0.99564)\n",
            "     | > G_feat_match_loss: 0.16442  (0.16366)\n",
            "     | > G_gen_loss: 48.17557  (48.91833)\n",
            "     | > G_adv_loss: 2.64792  (2.63227)\n",
            "     | > loss_0: 50.82349  (51.55061)\n",
            "     | > grad_norm_0: 350.69827  (680.14178)\n",
            "     | > D_mse_gan_loss: 0.00040  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 4.28569410360069e-06  (2.1985890982297205e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.2025619728083257e-06  (2.4975552413217465e-06)\n",
            "     | > loss_1: 0.00040  (0.00032)\n",
            "     | > grad_norm_1: 4.99275  (4.99663)\n",
            "     | > current_lr_0: 7.674641689223863e-14 \n",
            "     | > current_lr_1: 7.674641689223863e-14 \n",
            "     | > step_time: 2.43090  (2.44290)\n",
            "     | > loader_time: 0.00260  (0.00310)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 220/263 -- GLOBAL_STEP: 18700\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05514  (1.08639)\n",
            "     | > G_mse_fake_loss: 0.99718  (0.99563)\n",
            "     | > G_feat_match_loss: 0.16378  (0.16366)\n",
            "     | > G_gen_loss: 47.48143  (48.88734)\n",
            "     | > G_adv_loss: 2.63501  (2.63228)\n",
            "     | > loss_0: 50.11644  (51.51962)\n",
            "     | > grad_norm_0: 434.92050  (680.07874)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 5.943986138845503e-07  (2.2785047067682998e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.4765349760637037e-06  (2.5057514325122942e-06)\n",
            "     | > loss_1: 0.00020  (0.00033)\n",
            "     | > grad_norm_1: 4.99481  (4.99665)\n",
            "     | > current_lr_0: 7.485060484501966e-14 \n",
            "     | > current_lr_1: 7.485060484501966e-14 \n",
            "     | > step_time: 2.43730  (2.44252)\n",
            "     | > loader_time: 0.00250  (0.00305)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 245/263 -- GLOBAL_STEP: 18725\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04979  (1.08652)\n",
            "     | > G_mse_fake_loss: 1.00114  (0.99563)\n",
            "     | > G_feat_match_loss: 0.16410  (0.16367)\n",
            "     | > G_gen_loss: 47.24034  (48.89343)\n",
            "     | > G_adv_loss: 2.64219  (2.63229)\n",
            "     | > loss_0: 49.88252  (51.52572)\n",
            "     | > grad_norm_0: 367.08453  (678.93347)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 3.264547558501363e-06  (2.4594478270965013e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6916608274186729e-06  (2.504739361994529e-06)\n",
            "     | > loss_1: 0.00030  (0.00033)\n",
            "     | > grad_norm_1: 4.99289  (4.99665)\n",
            "     | > current_lr_0: 7.300162369185312e-14 \n",
            "     | > current_lr_1: 7.300162369185312e-14 \n",
            "     | > step_time: 2.43770  (2.44208)\n",
            "     | > loader_time: 0.00220  (0.00299)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00210 \u001b[0m(+0.00044)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 1.09853 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[92m 1.00113 \u001b[0m(-0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 49.43375 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 52.23593 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 71/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 08:43:56) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 6/263 -- GLOBAL_STEP: 18750\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09947  (1.04244)\n",
            "     | > G_mse_fake_loss: 0.99858  (0.99788)\n",
            "     | > G_feat_match_loss: 0.16418  (0.16395)\n",
            "     | > G_gen_loss: 49.47631  (46.90970)\n",
            "     | > G_adv_loss: 2.64037  (2.63738)\n",
            "     | > loss_0: 52.11668  (49.54708)\n",
            "     | > grad_norm_0: 455.48209  (457.69891)\n",
            "     | > D_mse_gan_loss: 0.00033  (0.00026)\n",
            "     | > D_mse_gan_real_loss: 2.7169564873474883e-06  (1.66800415020892e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9253764094173675e-06  (1.5518407205187639e-06)\n",
            "     | > loss_1: 0.00033  (0.00026)\n",
            "     | > grad_norm_1: 4.99542  (4.99460)\n",
            "     | > current_lr_0: 7.119831660253471e-14 \n",
            "     | > current_lr_1: 7.119831660253471e-14 \n",
            "     | > step_time: 2.43390  (2.57356)\n",
            "     | > loader_time: 0.00090  (0.01178)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 31/263 -- GLOBAL_STEP: 18775\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02954  (1.05201)\n",
            "     | > G_mse_fake_loss: 0.99925  (0.99787)\n",
            "     | > G_feat_match_loss: 0.16409  (0.16385)\n",
            "     | > G_gen_loss: 46.32920  (47.34054)\n",
            "     | > G_adv_loss: 2.64010  (2.63639)\n",
            "     | > loss_0: 48.96930  (49.97693)\n",
            "     | > grad_norm_0: 403.09348  (456.70428)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 1.568435777699051e-06  (2.3073667971256064e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.0915528037003241e-06  (1.7655709051899566e-06)\n",
            "     | > loss_1: 0.00029  (0.00028)\n",
            "     | > grad_norm_1: 4.99400  (4.99497)\n",
            "     | > current_lr_0: 6.943955532321247e-14 \n",
            "     | > current_lr_1: 6.943955532321247e-14 \n",
            "     | > step_time: 2.43940  (2.46666)\n",
            "     | > loader_time: 0.00220  (0.00433)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 56/263 -- GLOBAL_STEP: 18800\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02837  (1.05573)\n",
            "     | > G_mse_fake_loss: 1.00014  (0.99790)\n",
            "     | > G_feat_match_loss: 0.16416  (0.16388)\n",
            "     | > G_gen_loss: 46.27668  (47.50774)\n",
            "     | > G_adv_loss: 2.64176  (2.63669)\n",
            "     | > loss_0: 48.91845  (50.14443)\n",
            "     | > grad_norm_0: 400.68054  (472.76141)\n",
            "     | > D_mse_gan_loss: 0.00026  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.1916215498786187e-06  (2.3851715442089835e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7790075617085677e-06  (1.8299843519896188e-06)\n",
            "     | > loss_1: 0.00026  (0.00030)\n",
            "     | > grad_norm_1: 4.99391  (4.99530)\n",
            "     | > current_lr_0: 6.772423947048515e-14 \n",
            "     | > current_lr_1: 6.772423947048515e-14 \n",
            "     | > step_time: 2.43460  (2.45501)\n",
            "     | > loader_time: 0.00280  (0.00355)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 81/263 -- GLOBAL_STEP: 18825\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03925  (1.05379)\n",
            "     | > G_mse_fake_loss: 1.00226  (0.99802)\n",
            "     | > G_feat_match_loss: 0.16433  (0.16389)\n",
            "     | > G_gen_loss: 46.76637  (47.42076)\n",
            "     | > G_adv_loss: 2.64561  (2.63691)\n",
            "     | > loss_0: 49.41198  (50.05766)\n",
            "     | > grad_norm_0: 388.07761  (468.92984)\n",
            "     | > D_mse_gan_loss: 0.00027  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.740031052577251e-06  (2.3536546219604113e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3039682471571723e-06  (1.7906863677350807e-06)\n",
            "     | > loss_1: 0.00027  (0.00030)\n",
            "     | > grad_norm_1: 4.99362  (4.99517)\n",
            "     | > current_lr_0: 6.605129584293878e-14 \n",
            "     | > current_lr_1: 6.605129584293878e-14 \n",
            "     | > step_time: 2.43610  (2.45010)\n",
            "     | > loader_time: 0.00270  (0.00329)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 106/263 -- GLOBAL_STEP: 18850\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02858  (1.05410)\n",
            "     | > G_mse_fake_loss: 0.99854  (0.99804)\n",
            "     | > G_feat_match_loss: 0.16383  (0.16389)\n",
            "     | > G_gen_loss: 46.28597  (47.43444)\n",
            "     | > G_adv_loss: 2.63682  (2.63694)\n",
            "     | > loss_0: 48.92279  (50.07138)\n",
            "     | > grad_norm_0: 406.61395  (467.29175)\n",
            "     | > D_mse_gan_loss: 0.00027  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.2117159258195898e-06  (2.277860387054271e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3601013506558957e-06  (1.7852949929634495e-06)\n",
            "     | > loss_1: 0.00027  (0.00030)\n",
            "     | > grad_norm_1: 4.99400  (4.99512)\n",
            "     | > current_lr_0: 6.441967774968904e-14 \n",
            "     | > current_lr_1: 6.441967774968904e-14 \n",
            "     | > step_time: 2.45900  (2.44767)\n",
            "     | > loader_time: 0.00220  (0.00312)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 131/263 -- GLOBAL_STEP: 18875\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06804  (1.05373)\n",
            "     | > G_mse_fake_loss: 0.99911  (0.99826)\n",
            "     | > G_feat_match_loss: 0.16390  (0.16390)\n",
            "     | > G_gen_loss: 48.06165  (47.41776)\n",
            "     | > G_adv_loss: 2.63814  (2.63730)\n",
            "     | > loss_0: 50.69978  (50.05506)\n",
            "     | > grad_norm_0: 379.59113  (463.40695)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 2.1823227598360972e-06  (2.403661488175787e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7508251630715677e-06  (1.7974067731725561e-06)\n",
            "     | > loss_1: 0.00025  (0.00030)\n",
            "     | > grad_norm_1: 4.99317  (4.99504)\n",
            "     | > current_lr_0: 6.282836435551057e-14 \n",
            "     | > current_lr_1: 6.282836435551057e-14 \n",
            "     | > step_time: 2.44920  (2.44663)\n",
            "     | > loader_time: 0.00260  (0.00303)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 156/263 -- GLOBAL_STEP: 18900\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02487  (1.05336)\n",
            "     | > G_mse_fake_loss: 0.99859  (0.99823)\n",
            "     | > G_feat_match_loss: 0.16392  (0.16389)\n",
            "     | > G_gen_loss: 46.11919  (47.40119)\n",
            "     | > G_adv_loss: 2.63775  (2.63714)\n",
            "     | > loss_0: 48.75694  (50.03833)\n",
            "     | > grad_norm_0: 391.38046  (459.28552)\n",
            "     | > D_mse_gan_loss: 0.00019  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.3576624269262538e-06  (2.3523639839231114e-06)\n",
            "     | > D_mse_gan_fake_loss: 9.34893250814639e-07  (1.7830593190655445e-06)\n",
            "     | > loss_1: 0.00019  (0.00029)\n",
            "     | > grad_norm_1: 4.99364  (4.99494)\n",
            "     | > current_lr_0: 6.12763600421433e-14 \n",
            "     | > current_lr_1: 6.12763600421433e-14 \n",
            "     | > step_time: 2.44810  (2.44533)\n",
            "     | > loader_time: 0.00290  (0.00297)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 181/263 -- GLOBAL_STEP: 18925\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00549  (1.05201)\n",
            "     | > G_mse_fake_loss: 0.99958  (0.99826)\n",
            "     | > G_feat_match_loss: 0.16401  (0.16389)\n",
            "     | > G_gen_loss: 45.24697  (47.34059)\n",
            "     | > G_adv_loss: 2.63969  (2.63719)\n",
            "     | > loss_0: 47.88667  (49.97778)\n",
            "     | > grad_norm_0: 437.25549  (458.80368)\n",
            "     | > D_mse_gan_loss: 0.00022  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.3785062265014858e-06  (2.3613359158989837e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.1142370794914314e-06  (1.7680598370566383e-06)\n",
            "     | > loss_1: 0.00022  (0.00029)\n",
            "     | > grad_norm_1: 4.99489  (4.99492)\n",
            "     | > current_lr_0: 5.97626937853754e-14 \n",
            "     | > current_lr_1: 5.97626937853754e-14 \n",
            "     | > step_time: 2.45910  (2.44450)\n",
            "     | > loader_time: 0.00240  (0.00293)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 206/263 -- GLOBAL_STEP: 18950\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03844  (1.05216)\n",
            "     | > G_mse_fake_loss: 1.00103  (0.99831)\n",
            "     | > G_feat_match_loss: 0.16434  (0.16389)\n",
            "     | > G_gen_loss: 46.72971  (47.34717)\n",
            "     | > G_adv_loss: 2.64444  (2.63726)\n",
            "     | > loss_0: 49.37415  (49.98443)\n",
            "     | > grad_norm_0: 496.39719  (458.97870)\n",
            "     | > D_mse_gan_loss: 0.00072  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 2.312732476639212e-06  (2.308522085156071e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.879559135886666e-06  (1.7761940387448774e-06)\n",
            "     | > loss_1: 0.00072  (0.00029)\n",
            "     | > grad_norm_1: 4.99606  (4.99492)\n",
            "     | > current_lr_0: 5.828641854751435e-14 \n",
            "     | > current_lr_1: 5.828641854751435e-14 \n",
            "     | > step_time: 2.42980  (2.44436)\n",
            "     | > loader_time: 0.00230  (0.00288)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 231/263 -- GLOBAL_STEP: 18975\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04453  (1.05164)\n",
            "     | > G_mse_fake_loss: 0.99773  (0.99826)\n",
            "     | > G_feat_match_loss: 0.16390  (0.16389)\n",
            "     | > G_gen_loss: 47.00377  (47.32392)\n",
            "     | > G_adv_loss: 2.63672  (2.63717)\n",
            "     | > loss_0: 49.64049  (49.96109)\n",
            "     | > grad_norm_0: 462.02884  (460.18808)\n",
            "     | > D_mse_gan_loss: 0.00033  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.0207473880873295e-06  (1.7727824056296354e-06)\n",
            "     | > loss_1: 0.00033  (0.00029)\n",
            "     | > grad_norm_1: 4.99551  (4.99494)\n",
            "     | > current_lr_0: 5.684661068486479e-14 \n",
            "     | > current_lr_1: 5.684661068486479e-14 \n",
            "     | > step_time: 2.44890  (2.44422)\n",
            "     | > loader_time: 0.00250  (0.00285)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 256/263 -- GLOBAL_STEP: 19000\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07217  (1.05269)\n",
            "     | > G_mse_fake_loss: 0.99866  (0.99824)\n",
            "     | > G_feat_match_loss: 0.16383  (0.16389)\n",
            "     | > G_gen_loss: 48.24760  (47.37110)\n",
            "     | > G_adv_loss: 2.63694  (2.63717)\n",
            "     | > loss_0: 50.88453  (50.00827)\n",
            "     | > grad_norm_0: 450.92068  (459.65594)\n",
            "     | > D_mse_gan_loss: 0.00031  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.0029385723319137e-06  (1.7844186861459832e-06)\n",
            "     | > loss_1: 0.00031  (0.00029)\n",
            "     | > grad_norm_1: 4.99512  (4.99493)\n",
            "     | > current_lr_0: 5.544236936984347e-14 \n",
            "     | > current_lr_1: 5.544236936984347e-14 \n",
            "     | > step_time: 2.43550  (2.44387)\n",
            "     | > loader_time: 0.00240  (0.00282)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00146 \u001b[0m(-0.00064)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 1.09853 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.18010 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 49.43375 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 2.80218 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 52.23593 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 72/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 08:54:54) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 17/263 -- GLOBAL_STEP: 19025\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00333  (1.04030)\n",
            "     | > G_mse_fake_loss: 1.00744  (0.99990)\n",
            "     | > G_feat_match_loss: 0.16475  (0.16407)\n",
            "     | > G_gen_loss: 45.14980  (46.81337)\n",
            "     | > G_adv_loss: 2.65499  (2.64058)\n",
            "     | > loss_0: 47.80479  (49.45395)\n",
            "     | > grad_norm_0: 290.15067  (395.84277)\n",
            "     | > D_mse_gan_loss: 0.00031  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 1.9235592390032252e-06  (1.7249385597792485e-06)\n",
            "     | > loss_1: 0.00031  (0.00029)\n",
            "     | > grad_norm_1: 4.98957  (4.99353)\n",
            "     | > current_lr_0: 5.40728160273689e-14 \n",
            "     | > current_lr_1: 5.40728160273689e-14 \n",
            "     | > step_time: 2.45650  (2.47459)\n",
            "     | > loader_time: 0.00260  (0.00439)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 42/263 -- GLOBAL_STEP: 19050\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07841  (1.04632)\n",
            "     | > G_mse_fake_loss: 0.99662  (0.99984)\n",
            "     | > G_feat_match_loss: 0.16360  (0.16406)\n",
            "     | > G_gen_loss: 48.52862  (47.08450)\n",
            "     | > G_adv_loss: 2.63263  (2.64047)\n",
            "     | > loss_0: 51.16125  (49.72498)\n",
            "     | > grad_norm_0: 484.42825  (420.83496)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.0330435316063813e-06  (2.737807246857994e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7514676073915325e-06  (1.7919801244997438e-06)\n",
            "     | > loss_1: 0.00029  (0.00033)\n",
            "     | > grad_norm_1: 4.99580  (4.99400)\n",
            "     | > current_lr_0: 5.27370937851738e-14 \n",
            "     | > current_lr_1: 5.27370937851738e-14 \n",
            "     | > step_time: 2.44840  (2.45495)\n",
            "     | > loader_time: 0.00260  (0.00337)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 67/263 -- GLOBAL_STEP: 19075\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02646  (1.04733)\n",
            "     | > G_mse_fake_loss: 1.00219  (0.99966)\n",
            "     | > G_feat_match_loss: 0.16407  (0.16404)\n",
            "     | > G_gen_loss: 46.19057  (47.13000)\n",
            "     | > G_adv_loss: 2.64294  (2.64005)\n",
            "     | > loss_0: 48.83351  (49.77005)\n",
            "     | > grad_norm_0: 361.19086  (433.80939)\n",
            "     | > D_mse_gan_loss: 0.00031  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 7.56922702294105e-07  (2.489708961449461e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5893762110863463e-06  (1.7924979277865275e-06)\n",
            "     | > loss_1: 0.00031  (0.00032)\n",
            "     | > grad_norm_1: 4.99291  (4.99427)\n",
            "     | > current_lr_0: 5.143436693769591e-14 \n",
            "     | > current_lr_1: 5.143436693769591e-14 \n",
            "     | > step_time: 2.45490  (2.45000)\n",
            "     | > loader_time: 0.00260  (0.00311)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 92/263 -- GLOBAL_STEP: 19100\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03254  (1.04641)\n",
            "     | > G_mse_fake_loss: 0.99879  (0.99959)\n",
            "     | > G_feat_match_loss: 0.16379  (0.16403)\n",
            "     | > G_gen_loss: 46.46429  (47.08827)\n",
            "     | > G_adv_loss: 2.63671  (2.63985)\n",
            "     | > loss_0: 49.10101  (49.72812)\n",
            "     | > grad_norm_0: 388.54535  (433.13483)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 3.294918315077666e-06  (2.5722196101780716e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.2663637107834802e-06  (1.7850183967460038e-06)\n",
            "     | > loss_1: 0.00023  (0.00031)\n",
            "     | > grad_norm_1: 4.99357  (4.99429)\n",
            "     | > current_lr_0: 5.016382042321199e-14 \n",
            "     | > current_lr_1: 5.016382042321199e-14 \n",
            "     | > step_time: 2.44340  (2.44778)\n",
            "     | > loader_time: 0.00230  (0.00297)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 117/263 -- GLOBAL_STEP: 19125\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01689  (1.04647)\n",
            "     | > G_mse_fake_loss: 0.99729  (0.99961)\n",
            "     | > G_feat_match_loss: 0.16375  (0.16403)\n",
            "     | > G_gen_loss: 45.75993  (47.09094)\n",
            "     | > G_adv_loss: 2.63483  (2.63995)\n",
            "     | > loss_0: 48.39477  (49.73089)\n",
            "     | > grad_norm_0: 632.79742  (434.39880)\n",
            "     | > D_mse_gan_loss: 0.00026  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 2.1070184175187023e-06  (2.654202882110292e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.059500047835172e-06  (1.7939776098384568e-06)\n",
            "     | > loss_1: 0.00026  (0.00031)\n",
            "     | > grad_norm_1: 4.99750  (4.99428)\n",
            "     | > current_lr_0: 4.892465931388766e-14 \n",
            "     | > current_lr_1: 4.892465931388766e-14 \n",
            "     | > step_time: 2.43820  (2.44604)\n",
            "     | > loader_time: 0.00240  (0.00290)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 142/263 -- GLOBAL_STEP: 19150\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03238  (1.04572)\n",
            "     | > G_mse_fake_loss: 0.99715  (0.99966)\n",
            "     | > G_feat_match_loss: 0.16373  (0.16404)\n",
            "     | > G_gen_loss: 46.45717  (47.05731)\n",
            "     | > G_adv_loss: 2.63440  (2.64001)\n",
            "     | > loss_0: 49.09157  (49.69732)\n",
            "     | > grad_norm_0: 465.73511  (438.55695)\n",
            "     | > D_mse_gan_loss: 0.00027  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.391595560562564e-06  (2.5635698660338894e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7665287259660545e-06  (1.806102222295701e-06)\n",
            "     | > loss_1: 0.00027  (0.00031)\n",
            "     | > grad_norm_1: 4.99557  (4.99435)\n",
            "     | > current_lr_0: 4.771610831842443e-14 \n",
            "     | > current_lr_1: 4.771610831842443e-14 \n",
            "     | > step_time: 2.43980  (2.44531)\n",
            "     | > loader_time: 0.00280  (0.00285)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 167/263 -- GLOBAL_STEP: 19175\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03045  (1.04562)\n",
            "     | > G_mse_fake_loss: 1.00284  (0.99962)\n",
            "     | > G_feat_match_loss: 0.16459  (0.16403)\n",
            "     | > G_gen_loss: 46.37008  (47.05281)\n",
            "     | > G_adv_loss: 2.64872  (2.63992)\n",
            "     | > loss_0: 49.01880  (49.69272)\n",
            "     | > grad_norm_0: 341.40976  (438.17975)\n",
            "     | > D_mse_gan_loss: 0.00039  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 1.7092139614760526e-06  (1.7974742069488135e-06)\n",
            "     | > loss_1: 0.00039  (0.00031)\n",
            "     | > grad_norm_1: 4.99175  (4.99429)\n",
            "     | > current_lr_0: 4.653741129699226e-14 \n",
            "     | > current_lr_1: 4.653741129699226e-14 \n",
            "     | > step_time: 2.44340  (2.44458)\n",
            "     | > loader_time: 0.00260  (0.00281)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 192/263 -- GLOBAL_STEP: 19200\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05976  (1.04515)\n",
            "     | > G_mse_fake_loss: 0.99713  (0.99960)\n",
            "     | > G_feat_match_loss: 0.16385  (0.16403)\n",
            "     | > G_gen_loss: 47.68942  (47.03157)\n",
            "     | > G_adv_loss: 2.63561  (2.63987)\n",
            "     | > loss_0: 50.32504  (49.67144)\n",
            "     | > grad_norm_0: 550.58545  (438.38580)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 8.762171432863397e-07  (2.6245488814839283e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.4702949303900823e-06  (1.7928474207451472e-06)\n",
            "     | > loss_1: 0.00025  (0.00031)\n",
            "     | > grad_norm_1: 4.99673  (4.99427)\n",
            "     | > current_lr_0: 4.53878307881445e-14 \n",
            "     | > current_lr_1: 4.53878307881445e-14 \n",
            "     | > step_time: 2.44270  (2.44402)\n",
            "     | > loader_time: 0.00220  (0.00278)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 217/263 -- GLOBAL_STEP: 19225\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06586  (1.04481)\n",
            "     | > G_mse_fake_loss: 1.00609  (0.99967)\n",
            "     | > G_feat_match_loss: 0.16445  (0.16403)\n",
            "     | > G_gen_loss: 47.96392  (47.01648)\n",
            "     | > G_adv_loss: 2.65063  (2.63998)\n",
            "     | > loss_0: 50.61455  (49.65646)\n",
            "     | > grad_norm_0: 342.96774  (438.18546)\n",
            "     | > D_mse_gan_loss: 0.00036  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.869318111857865e-06  (2.5634139712328437e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9537330899765948e-06  (1.7942610976134947e-06)\n",
            "     | > loss_1: 0.00036  (0.00031)\n",
            "     | > grad_norm_1: 4.99287  (4.99429)\n",
            "     | > current_lr_0: 4.426664754741914e-14 \n",
            "     | > current_lr_1: 4.426664754741914e-14 \n",
            "     | > step_time: 2.44120  (2.44374)\n",
            "     | > loader_time: 0.00300  (0.00278)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 242/263 -- GLOBAL_STEP: 19250\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05859  (1.04556)\n",
            "     | > G_mse_fake_loss: 0.99850  (0.99960)\n",
            "     | > G_feat_match_loss: 0.16392  (0.16403)\n",
            "     | > G_gen_loss: 47.63654  (47.05029)\n",
            "     | > G_adv_loss: 2.63774  (2.63994)\n",
            "     | > loss_0: 50.27428  (49.69023)\n",
            "     | > grad_norm_0: 356.10352  (441.14862)\n",
            "     | > D_mse_gan_loss: 0.00027  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.3919157026975881e-06  (2.5636488378410863e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6976094912024564e-06  (1.797525538810709e-06)\n",
            "     | > loss_1: 0.00027  (0.00031)\n",
            "     | > grad_norm_1: 4.99223  (4.99435)\n",
            "     | > current_lr_0: 4.3173160097337534e-14 \n",
            "     | > current_lr_1: 4.3173160097337534e-14 \n",
            "     | > step_time: 2.43610  (2.44341)\n",
            "     | > loader_time: 0.00280  (0.00278)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00185 \u001b[0m(+0.00039)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 1.09853 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[92m 1.00113 \u001b[0m(-0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 49.43375 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 52.23593 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.08590 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.17106 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.08590 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 73/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 09:05:51) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 3/263 -- GLOBAL_STEP: 19275\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07475  (1.03362)\n",
            "     | > G_mse_fake_loss: 1.00483  (0.99939)\n",
            "     | > G_feat_match_loss: 0.16475  (0.16404)\n",
            "     | > G_gen_loss: 48.36364  (46.51285)\n",
            "     | > G_adv_loss: 2.65228  (2.63974)\n",
            "     | > loss_0: 51.01592  (49.15259)\n",
            "     | > grad_norm_0: 437.07175  (426.65161)\n",
            "     | > D_mse_gan_loss: 0.00052  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 3.152946646878263e-06  (1.3191446252373378e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9742544736800482e-06  (1.4073305768154871e-06)\n",
            "     | > loss_1: 0.00052  (0.00028)\n",
            "     | > grad_norm_1: 4.99542  (4.99484)\n",
            "     | > current_lr_0: 4.210668428851939e-14 \n",
            "     | > current_lr_1: 4.210668428851939e-14 \n",
            "     | > step_time: 2.45030  (2.63022)\n",
            "     | > loader_time: 0.00420  (0.01631)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 28/263 -- GLOBAL_STEP: 19300\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.97896  (1.04560)\n",
            "     | > G_mse_fake_loss: 1.00253  (0.99973)\n",
            "     | > G_feat_match_loss: 0.16427  (0.16404)\n",
            "     | > G_gen_loss: 44.05326  (47.05179)\n",
            "     | > G_adv_loss: 2.64523  (2.64014)\n",
            "     | > loss_0: 46.69849  (49.69193)\n",
            "     | > grad_norm_0: 448.41483  (456.63321)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 7.108691306711989e-07  (4.219284816388478e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.2342569561951677e-06  (1.7258376163096858e-06)\n",
            "     | > loss_1: 0.00020  (0.00031)\n",
            "     | > grad_norm_1: 4.99523  (4.99504)\n",
            "     | > current_lr_0: 4.1066552871638956e-14 \n",
            "     | > current_lr_1: 4.1066552871638956e-14 \n",
            "     | > step_time: 2.43640  (2.46092)\n",
            "     | > loader_time: 0.00290  (0.00386)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 53/263 -- GLOBAL_STEP: 19325\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08921  (1.04969)\n",
            "     | > G_mse_fake_loss: 1.00439  (0.99994)\n",
            "     | > G_feat_match_loss: 0.16469  (0.16407)\n",
            "     | > G_gen_loss: 49.01466  (47.23602)\n",
            "     | > G_adv_loss: 2.65126  (2.64062)\n",
            "     | > loss_0: 51.66592  (49.87663)\n",
            "     | > grad_norm_0: 410.62610  (450.94577)\n",
            "     | > D_mse_gan_loss: 0.00058  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 3.42708199241315e-06  (3.361555800309807e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.7133082767250016e-06  (1.7865637316373188e-06)\n",
            "     | > loss_1: 0.00058  (0.00033)\n",
            "     | > grad_norm_1: 4.99440  (4.99491)\n",
            "     | > current_lr_0: 4.005211507995515e-14 \n",
            "     | > current_lr_1: 4.005211507995515e-14 \n",
            "     | > step_time: 2.43330  (2.45188)\n",
            "     | > loader_time: 0.00240  (0.00326)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 78/263 -- GLOBAL_STEP: 19350\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02410  (1.05011)\n",
            "     | > G_mse_fake_loss: 0.99788  (0.99996)\n",
            "     | > G_feat_match_loss: 0.16395  (0.16407)\n",
            "     | > G_gen_loss: 46.08429  (47.25498)\n",
            "     | > G_adv_loss: 2.63740  (2.64067)\n",
            "     | > loss_0: 48.72169  (49.89565)\n",
            "     | > grad_norm_0: 607.64362  (449.24438)\n",
            "     | > D_mse_gan_loss: 0.00026  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.1451119235061924e-06  (3.1200858176980078e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7971822217077715e-06  (1.761297211872331e-06)\n",
            "     | > loss_1: 0.00026  (0.00033)\n",
            "     | > grad_norm_1: 4.99789  (4.99481)\n",
            "     | > current_lr_0: 3.9062736222153933e-14 \n",
            "     | > current_lr_1: 3.9062736222153933e-14 \n",
            "     | > step_time: 2.43700  (2.44818)\n",
            "     | > loader_time: 0.00270  (0.00303)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 103/263 -- GLOBAL_STEP: 19375\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07546  (1.05098)\n",
            "     | > G_mse_fake_loss: 0.99355  (0.99984)\n",
            "     | > G_feat_match_loss: 0.16343  (0.16406)\n",
            "     | > G_gen_loss: 48.39558  (47.29404)\n",
            "     | > G_adv_loss: 2.62783  (2.64044)\n",
            "     | > loss_0: 51.02341  (49.93448)\n",
            "     | > grad_norm_0: 547.68433  (448.88110)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.0590738384053111e-06  (3.2042236744990043e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.1264445422275458e-06  (1.7913523584618956e-06)\n",
            "     | > loss_1: 0.00025  (0.00032)\n",
            "     | > grad_norm_1: 4.99681  (4.99480)\n",
            "     | > current_lr_0: 3.809779728524853e-14 \n",
            "     | > current_lr_1: 3.809779728524853e-14 \n",
            "     | > step_time: 2.43110  (2.44595)\n",
            "     | > loader_time: 0.00250  (0.00290)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 128/263 -- GLOBAL_STEP: 19400\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05487  (1.05062)\n",
            "     | > G_mse_fake_loss: 0.99617  (1.00004)\n",
            "     | > G_feat_match_loss: 0.16366  (0.16408)\n",
            "     | > G_gen_loss: 47.46920  (47.27784)\n",
            "     | > G_adv_loss: 2.63278  (2.64081)\n",
            "     | > loss_0: 50.10197  (49.91865)\n",
            "     | > grad_norm_0: 439.03021  (446.31714)\n",
            "     | > D_mse_gan_loss: 0.00022  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 2.106991132677649e-06  (3.1904150450401403e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3321522374099004e-06  (1.8007088602445265e-06)\n",
            "     | > loss_1: 0.00022  (0.00032)\n",
            "     | > grad_norm_1: 4.99488  (4.99477)\n",
            "     | > current_lr_0: 3.715669454728886e-14 \n",
            "     | > current_lr_1: 3.715669454728886e-14 \n",
            "     | > step_time: 2.43890  (2.44524)\n",
            "     | > loader_time: 0.00250  (0.00282)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 153/263 -- GLOBAL_STEP: 19425\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01592  (1.04809)\n",
            "     | > G_mse_fake_loss: 1.00191  (1.00002)\n",
            "     | > G_feat_match_loss: 0.16438  (0.16407)\n",
            "     | > G_gen_loss: 45.71655  (47.16392)\n",
            "     | > G_adv_loss: 2.64567  (2.64074)\n",
            "     | > loss_0: 48.36221  (49.80466)\n",
            "     | > grad_norm_0: 331.31451  (443.25598)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.56217811309034e-06  (3.0225610331708475e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.4249694686441217e-06  (1.7736063327426676e-06)\n",
            "     | > loss_1: 0.00023  (0.00032)\n",
            "     | > grad_norm_1: 4.99124  (4.99472)\n",
            "     | > current_lr_0: 3.6238839199637975e-14 \n",
            "     | > current_lr_1: 3.6238839199637975e-14 \n",
            "     | > step_time: 2.44320  (2.44436)\n",
            "     | > loader_time: 0.00280  (0.00280)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 178/263 -- GLOBAL_STEP: 19450\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06255  (1.04729)\n",
            "     | > G_mse_fake_loss: 1.00035  (1.00001)\n",
            "     | > G_feat_match_loss: 0.16416  (0.16407)\n",
            "     | > G_gen_loss: 47.81462  (47.12819)\n",
            "     | > G_adv_loss: 2.64196  (2.64067)\n",
            "     | > loss_0: 50.45659  (49.76886)\n",
            "     | > grad_norm_0: 504.37051  (441.89221)\n",
            "     | > D_mse_gan_loss: 0.00031  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.2967716429557186e-06  (2.9266426100149877e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.7201167540624738e-06  (1.7521258746200239e-06)\n",
            "     | > loss_1: 0.00031  (0.00031)\n",
            "     | > grad_norm_1: 4.99613  (4.99468)\n",
            "     | > current_lr_0: 3.53436569785791e-14 \n",
            "     | > current_lr_1: 3.53436569785791e-14 \n",
            "     | > step_time: 2.43280  (2.44356)\n",
            "     | > loader_time: 0.00250  (0.00277)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 203/263 -- GLOBAL_STEP: 19475\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06486  (1.04727)\n",
            "     | > G_mse_fake_loss: 1.00437  (1.00011)\n",
            "     | > G_feat_match_loss: 0.16449  (0.16408)\n",
            "     | > G_gen_loss: 47.91869  (47.12713)\n",
            "     | > G_adv_loss: 2.64932  (2.64088)\n",
            "     | > loss_0: 50.56801  (49.76801)\n",
            "     | > grad_norm_0: 319.71960  (439.60043)\n",
            "     | > D_mse_gan_loss: 0.00034  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 3.4524423426773865e-06  (2.8885266564756976e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6322312603733735e-06  (1.7545075209078466e-06)\n",
            "     | > loss_1: 0.00034  (0.00032)\n",
            "     | > grad_norm_1: 4.99094  (4.99461)\n",
            "     | > current_lr_0: 3.4470587806022826e-14 \n",
            "     | > current_lr_1: 3.4470587806022826e-14 \n",
            "     | > step_time: 2.43700  (2.44311)\n",
            "     | > loader_time: 0.00250  (0.00275)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 228/263 -- GLOBAL_STEP: 19500\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.98309  (1.04729)\n",
            "     | > G_mse_fake_loss: 1.00411  (1.00012)\n",
            "     | > G_feat_match_loss: 0.16415  (0.16408)\n",
            "     | > G_gen_loss: 44.23919  (47.12826)\n",
            "     | > G_adv_loss: 2.64564  (2.64092)\n",
            "     | > loss_0: 46.88483  (49.76918)\n",
            "     | > grad_norm_0: 304.10117  (440.15137)\n",
            "     | > D_mse_gan_loss: 0.00026  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 1.1040249319194118e-06  (1.766473763366281e-06)\n",
            "     | > loss_1: 0.00026  (0.00032)\n",
            "     | > grad_norm_1: 4.99050  (4.99461)\n",
            "     | > current_lr_0: 3.3619085439089705e-14 \n",
            "     | > current_lr_1: 3.3619085439089705e-14 \n",
            "     | > step_time: 2.43040  (2.44249)\n",
            "     | > loader_time: 0.00230  (0.00273)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 253/263 -- GLOBAL_STEP: 19525\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.99826  (1.04704)\n",
            "     | > G_mse_fake_loss: 0.99910  (1.00011)\n",
            "     | > G_feat_match_loss: 0.16384  (0.16408)\n",
            "     | > G_gen_loss: 44.92184  (47.11681)\n",
            "     | > G_adv_loss: 2.63754  (2.64092)\n",
            "     | > loss_0: 47.55938  (49.75773)\n",
            "     | > grad_norm_0: 457.09009  (439.71252)\n",
            "     | > D_mse_gan_loss: 0.00018  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.0716117913034395e-06  (2.914532026595042e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.0120338629349135e-06  (1.7696083159175708e-06)\n",
            "     | > loss_1: 0.00018  (0.00032)\n",
            "     | > grad_norm_1: 4.99533  (4.99459)\n",
            "     | > current_lr_0: 3.2788617128348863e-14 \n",
            "     | > current_lr_1: 3.2788617128348863e-14 \n",
            "     | > step_time: 2.43270  (2.44214)\n",
            "     | > loader_time: 0.00210  (0.00271)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00153 \u001b[0m(-0.00032)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[91m 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 74/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 09:16:50) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 14/263 -- GLOBAL_STEP: 19550\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05699  (1.04695)\n",
            "     | > G_mse_fake_loss: 0.99619  (0.99814)\n",
            "     | > G_feat_match_loss: 0.16376  (0.16393)\n",
            "     | > G_gen_loss: 47.56470  (47.11261)\n",
            "     | > G_adv_loss: 2.63381  (2.63749)\n",
            "     | > loss_0: 50.19851  (49.75010)\n",
            "     | > grad_norm_0: 664.09851  (475.13385)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 2.563495399954263e-06  (6.393697055224428e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.1834641756868223e-06  (1.7112173062676966e-06)\n",
            "     | > loss_1: 0.00021  (0.00033)\n",
            "     | > grad_norm_1: 4.99795  (4.99535)\n",
            "     | > current_lr_0: 3.1978663284498975e-14 \n",
            "     | > current_lr_1: 3.1978663284498975e-14 \n",
            "     | > step_time: 2.44330  (2.50570)\n",
            "     | > loader_time: 0.00320  (0.00644)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 39/263 -- GLOBAL_STEP: 19575\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03123  (1.04837)\n",
            "     | > G_mse_fake_loss: 1.00292  (0.99868)\n",
            "     | > G_feat_match_loss: 0.16451  (0.16397)\n",
            "     | > G_gen_loss: 46.40555  (47.17687)\n",
            "     | > G_adv_loss: 2.64800  (2.63840)\n",
            "     | > loss_0: 49.05355  (49.81527)\n",
            "     | > grad_norm_0: 356.07251  (458.77927)\n",
            "     | > D_mse_gan_loss: 0.00050  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 2.125305400113575e-06  (3.8145982276278237e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6953816839304636e-06  (1.7655067993543455e-06)\n",
            "     | > loss_1: 0.00050  (0.00031)\n",
            "     | > grad_norm_1: 4.99233  (4.99504)\n",
            "     | > current_lr_0: 3.1188717153283e-14 \n",
            "     | > current_lr_1: 3.1188717153283e-14 \n",
            "     | > step_time: 2.43840  (2.46368)\n",
            "     | > loader_time: 0.00260  (0.00391)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 64/263 -- GLOBAL_STEP: 19600\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04980  (1.05340)\n",
            "     | > G_mse_fake_loss: 1.00268  (0.99856)\n",
            "     | > G_feat_match_loss: 0.16425  (0.16396)\n",
            "     | > G_gen_loss: 47.24110  (47.40293)\n",
            "     | > G_adv_loss: 2.64515  (2.63819)\n",
            "     | > loss_0: 49.88625  (50.04112)\n",
            "     | > grad_norm_0: 327.15768  (470.31247)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.4045037914911518e-06  (3.420249743424364e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.8058611885862774e-06  (1.8411553925901103e-06)\n",
            "     | > loss_1: 0.00029  (0.00030)\n",
            "     | > grad_norm_1: 4.99102  (4.99524)\n",
            "     | > current_lr_0: 3.0418284498433184e-14 \n",
            "     | > current_lr_1: 3.0418284498433184e-14 \n",
            "     | > step_time: 2.43280  (2.45434)\n",
            "     | > loader_time: 0.00260  (0.00338)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 89/263 -- GLOBAL_STEP: 19625\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03702  (1.05114)\n",
            "     | > G_mse_fake_loss: 0.99921  (0.99878)\n",
            "     | > G_feat_match_loss: 0.16389  (0.16397)\n",
            "     | > G_gen_loss: 46.66569  (47.30135)\n",
            "     | > G_adv_loss: 2.63807  (2.63846)\n",
            "     | > loss_0: 49.30376  (49.93982)\n",
            "     | > grad_norm_0: 494.67548  (460.81436)\n",
            "     | > D_mse_gan_loss: 0.00022  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.453029994991084e-06  (3.130981005387787e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.858525024545088e-06  (1.8047599648648275e-06)\n",
            "     | > loss_1: 0.00022  (0.00030)\n",
            "     | > grad_norm_1: 4.99601  (4.99503)\n",
            "     | > current_lr_0: 2.966688329244807e-14 \n",
            "     | > current_lr_1: 2.966688329244807e-14 \n",
            "     | > step_time: 2.45340  (2.45115)\n",
            "     | > loader_time: 0.00230  (0.00316)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 114/263 -- GLOBAL_STEP: 19650\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.98929  (1.04961)\n",
            "     | > G_mse_fake_loss: 1.00358  (0.99893)\n",
            "     | > G_feat_match_loss: 0.16431  (0.16398)\n",
            "     | > G_gen_loss: 44.51793  (47.23252)\n",
            "     | > G_adv_loss: 2.64669  (2.63869)\n",
            "     | > loss_0: 47.16462  (49.87121)\n",
            "     | > grad_norm_0: 333.57230  (456.78821)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.5217838154057972e-06  (3.281833258234544e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3839791108694044e-06  (1.7943900450502457e-06)\n",
            "     | > loss_1: 0.00024  (0.00030)\n",
            "     | > grad_norm_1: 4.99162  (4.99494)\n",
            "     | > current_lr_0: 2.8934043415008136e-14 \n",
            "     | > current_lr_1: 2.8934043415008136e-14 \n",
            "     | > step_time: 2.44570  (2.44844)\n",
            "     | > loader_time: 0.00260  (0.00302)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 139/263 -- GLOBAL_STEP: 19675\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.10515  (1.04965)\n",
            "     | > G_mse_fake_loss: 0.99735  (0.99904)\n",
            "     | > G_feat_match_loss: 0.16374  (0.16398)\n",
            "     | > G_gen_loss: 49.73179  (47.23425)\n",
            "     | > G_adv_loss: 2.63476  (2.63886)\n",
            "     | > loss_0: 52.36655  (49.87311)\n",
            "     | > grad_norm_0: 592.01691  (452.41241)\n",
            "     | > D_mse_gan_loss: 0.00037  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.819627073018637e-06  (3.066562296410882e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.3795696506567765e-06  (1.801970912513772e-06)\n",
            "     | > loss_1: 0.00037  (0.00030)\n",
            "     | > grad_norm_1: 4.99725  (4.99483)\n",
            "     | > current_lr_0: 2.8219306358841075e-14 \n",
            "     | > current_lr_1: 2.8219306358841075e-14 \n",
            "     | > step_time: 2.44380  (2.44672)\n",
            "     | > loader_time: 0.00270  (0.00294)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 164/263 -- GLOBAL_STEP: 19700\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00503  (1.04850)\n",
            "     | > G_mse_fake_loss: 1.00380  (0.99906)\n",
            "     | > G_feat_match_loss: 0.16444  (0.16398)\n",
            "     | > G_gen_loss: 45.22622  (47.18232)\n",
            "     | > G_adv_loss: 2.64823  (2.63882)\n",
            "     | > loss_0: 47.87445  (49.82114)\n",
            "     | > grad_norm_0: 323.01898  (450.23346)\n",
            "     | > D_mse_gan_loss: 0.00026  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.7451153553338372e-06  (2.88252878936269e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5755491631352925e-06  (1.783086113752089e-06)\n",
            "     | > loss_1: 0.00026  (0.00030)\n",
            "     | > grad_norm_1: 4.99089  (4.99476)\n",
            "     | > current_lr_0: 2.7522224942852994e-14 \n",
            "     | > current_lr_1: 2.7522224942852994e-14 \n",
            "     | > step_time: 2.43610  (2.44593)\n",
            "     | > loader_time: 0.00270  (0.00290)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 189/263 -- GLOBAL_STEP: 19725\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07048  (1.04680)\n",
            "     | > G_mse_fake_loss: 0.99632  (0.99912)\n",
            "     | > G_feat_match_loss: 0.16364  (0.16398)\n",
            "     | > G_gen_loss: 48.17143  (47.10613)\n",
            "     | > G_adv_loss: 2.63274  (2.63896)\n",
            "     | > loss_0: 50.80416  (49.74510)\n",
            "     | > grad_norm_0: 448.00925  (450.04483)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 2.041426341747865e-06  (2.8066672949154972e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5673334701205022e-06  (1.7769055269671192e-06)\n",
            "     | > loss_1: 0.00023  (0.00030)\n",
            "     | > grad_norm_1: 4.99502  (4.99471)\n",
            "     | > current_lr_0: 2.684236303234591e-14 \n",
            "     | > current_lr_1: 2.684236303234591e-14 \n",
            "     | > step_time: 2.43670  (2.44522)\n",
            "     | > loader_time: 0.00230  (0.00286)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 214/263 -- GLOBAL_STEP: 19750\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09797  (1.04662)\n",
            "     | > G_mse_fake_loss: 0.99731  (0.99916)\n",
            "     | > G_feat_match_loss: 0.16380  (0.16399)\n",
            "     | > G_gen_loss: 49.40866  (47.09803)\n",
            "     | > G_adv_loss: 2.63536  (2.63906)\n",
            "     | > loss_0: 52.04402  (49.73709)\n",
            "     | > grad_norm_0: 495.63324  (451.46085)\n",
            "     | > D_mse_gan_loss: 0.00028  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.0544208635110408e-06  (2.7803024822809098e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.8714729321800405e-06  (1.7882774722383035e-06)\n",
            "     | > loss_1: 0.00028  (0.00029)\n",
            "     | > grad_norm_1: 4.99594  (4.99473)\n",
            "     | > current_lr_0: 2.6179295266146488e-14 \n",
            "     | > current_lr_1: 2.6179295266146488e-14 \n",
            "     | > step_time: 2.43850  (2.44456)\n",
            "     | > loader_time: 0.00320  (0.00282)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 239/263 -- GLOBAL_STEP: 19775\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00791  (1.04635)\n",
            "     | > G_mse_fake_loss: 1.00198  (0.99920)\n",
            "     | > G_feat_match_loss: 0.16445  (0.16400)\n",
            "     | > G_gen_loss: 45.35574  (47.08569)\n",
            "     | > G_adv_loss: 2.64652  (2.63916)\n",
            "     | > loss_0: 48.00227  (49.72485)\n",
            "     | > grad_norm_0: 372.47998  (450.22491)\n",
            "     | > D_mse_gan_loss: 0.00043  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 4.8975011850416195e-06  (2.7871577061474475e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5783567732796655e-06  (1.7877302238139617e-06)\n",
            "     | > loss_1: 0.00043  (0.00030)\n",
            "     | > grad_norm_1: 4.99305  (4.99470)\n",
            "     | > current_lr_0: 2.553260679047537e-14 \n",
            "     | > current_lr_1: 2.553260679047537e-14 \n",
            "     | > step_time: 2.43890  (2.44417)\n",
            "     | > loader_time: 0.00270  (0.00280)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00150 \u001b[0m(-0.00003)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 1.09853 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss: 0.18010 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 49.43375 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss: 2.80218 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 52.23593 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 75/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 09:27:47) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/263 -- GLOBAL_STEP: 19800\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.20867  (1.20867)\n",
            "     | > G_mse_fake_loss: 0.98947  (0.98947)\n",
            "     | > G_feat_match_loss: 0.16317  (0.16317)\n",
            "     | > G_gen_loss: 54.39036  (54.39036)\n",
            "     | > G_adv_loss: 2.62113  (2.62113)\n",
            "     | > loss_0: 57.01150  (57.01150)\n",
            "     | > grad_norm_0: 1215.32581  (1215.32581)\n",
            "     | > D_mse_gan_loss: 0.00042  (0.00042)\n",
            "     | > D_mse_gan_real_loss: 4.377429377200315e-06  (4.377429377200315e-06)\n",
            "     | > D_mse_gan_fake_loss: 3.2698521863494534e-06  (3.2698521863494534e-06)\n",
            "     | > loss_1: 0.00042  (0.00042)\n",
            "     | > grad_norm_1: 4.99978  (4.99978)\n",
            "     | > current_lr_0: 2.4901892999390447e-14 \n",
            "     | > current_lr_1: 2.4901892999390447e-14 \n",
            "     | > step_time: 3.89280  (3.89283)\n",
            "     | > loader_time: 5.00500  (5.00502)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 25/263 -- GLOBAL_STEP: 19825\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04606  (1.08587)\n",
            "     | > G_mse_fake_loss: 0.99936  (0.99537)\n",
            "     | > G_feat_match_loss: 0.16392  (0.16365)\n",
            "     | > G_gen_loss: 47.07256  (48.86432)\n",
            "     | > G_adv_loss: 2.63860  (2.63184)\n",
            "     | > loss_0: 49.71116  (51.49616)\n",
            "     | > grad_norm_0: 461.65094  (654.99939)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 2.41516522692109e-06  (2.3039811753733373e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.489635337748041e-06  (2.151969481474225e-06)\n",
            "     | > loss_1: 0.00030  (0.00032)\n",
            "     | > grad_norm_1: 4.99551  (4.99704)\n",
            "     | > current_lr_0: 2.4286759281641913e-14 \n",
            "     | > current_lr_1: 2.4286759281641913e-14 \n",
            "     | > step_time: 2.43470  (2.47211)\n",
            "     | > loader_time: 0.00230  (0.00332)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/263 -- GLOBAL_STEP: 19850\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03677  (1.08350)\n",
            "     | > G_mse_fake_loss: 1.00166  (0.99553)\n",
            "     | > G_feat_match_loss: 0.16425  (0.16368)\n",
            "     | > G_gen_loss: 46.65485  (48.75736)\n",
            "     | > G_adv_loss: 2.64419  (2.63228)\n",
            "     | > loss_0: 49.29904  (51.38964)\n",
            "     | > grad_norm_0: 385.61218  (639.46326)\n",
            "     | > D_mse_gan_loss: 0.00018  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 7.818460971975583e-07  (2.770062801573658e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.0810279036377324e-06  (2.081688541011317e-06)\n",
            "     | > loss_1: 0.00018  (0.00031)\n",
            "     | > grad_norm_1: 4.99344  (4.99677)\n",
            "     | > current_lr_0: 2.3686820773780446e-14 \n",
            "     | > current_lr_1: 2.3686820773780446e-14 \n",
            "     | > step_time: 2.43630  (2.45791)\n",
            "     | > loader_time: 0.00220  (0.00295)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 75/263 -- GLOBAL_STEP: 19875\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.11894  (1.08548)\n",
            "     | > G_mse_fake_loss: 0.99192  (0.99563)\n",
            "     | > G_feat_match_loss: 0.16339  (0.16369)\n",
            "     | > G_gen_loss: 50.35250  (48.84646)\n",
            "     | > G_adv_loss: 2.62584  (2.63256)\n",
            "     | > loss_0: 52.97835  (51.47901)\n",
            "     | > grad_norm_0: 664.53717  (631.62769)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.528309439891018e-06  (2.794296423189734e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.592287955849315e-06  (2.133608335649721e-06)\n",
            "     | > loss_1: 0.00025  (0.00032)\n",
            "     | > grad_norm_1: 4.99824  (4.99679)\n",
            "     | > current_lr_0: 2.310170211936427e-14 \n",
            "     | > current_lr_1: 2.310170211936427e-14 \n",
            "     | > step_time: 2.44430  (2.45257)\n",
            "     | > loader_time: 0.00260  (0.00284)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/263 -- GLOBAL_STEP: 19900\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04801  (1.08521)\n",
            "     | > G_mse_fake_loss: 0.99515  (0.99566)\n",
            "     | > G_feat_match_loss: 0.16357  (0.16368)\n",
            "     | > G_gen_loss: 47.16025  (48.83427)\n",
            "     | > G_adv_loss: 2.63082  (2.63246)\n",
            "     | > loss_0: 49.79107  (51.46674)\n",
            "     | > grad_norm_0: 435.25635  (625.25891)\n",
            "     | > D_mse_gan_loss: 0.00017  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.1582320667002932e-06  (2.5676379814854043e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.194488277178607e-06  (2.1064823994265686e-06)\n",
            "     | > loss_1: 0.00017  (0.00031)\n",
            "     | > grad_norm_1: 4.99487  (4.99669)\n",
            "     | > current_lr_0: 2.2531037234114317e-14 \n",
            "     | > current_lr_1: 2.2531037234114317e-14 \n",
            "     | > step_time: 2.43670  (2.45002)\n",
            "     | > loader_time: 0.00260  (0.00278)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 125/263 -- GLOBAL_STEP: 19925\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03866  (1.08457)\n",
            "     | > G_mse_fake_loss: 0.99944  (0.99577)\n",
            "     | > G_feat_match_loss: 0.16391  (0.16369)\n",
            "     | > G_gen_loss: 46.73959  (48.80564)\n",
            "     | > G_adv_loss: 2.63855  (2.63263)\n",
            "     | > loss_0: 49.37814  (51.43827)\n",
            "     | > grad_norm_0: 545.82379  (619.63007)\n",
            "     | > D_mse_gan_loss: 0.00032  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.2322007023612969e-06  (2.6897186282894838e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.8106818515661871e-06  (2.105005145949691e-06)\n",
            "     | > loss_1: 0.00032  (0.00031)\n",
            "     | > grad_norm_1: 4.99673  (4.99668)\n",
            "     | > current_lr_0: 2.197446907687059e-14 \n",
            "     | > current_lr_1: 2.197446907687059e-14 \n",
            "     | > step_time: 2.44740  (2.44861)\n",
            "     | > loader_time: 0.00240  (0.00274)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 150/263 -- GLOBAL_STEP: 19950\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08337  (1.08440)\n",
            "     | > G_mse_fake_loss: 0.99524  (0.99575)\n",
            "     | > G_feat_match_loss: 0.16359  (0.16368)\n",
            "     | > G_gen_loss: 48.75145  (48.79808)\n",
            "     | > G_adv_loss: 2.63114  (2.63253)\n",
            "     | > loss_0: 51.38259  (51.43061)\n",
            "     | > grad_norm_0: 483.85031  (620.25415)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.0107311254614615e-06  (2.590885057429659e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3870695738660288e-06  (2.101503428472522e-06)\n",
            "     | > loss_1: 0.00021  (0.00031)\n",
            "     | > grad_norm_1: 4.99581  (4.99668)\n",
            "     | > current_lr_0: 2.143164942620642e-14 \n",
            "     | > current_lr_1: 2.143164942620642e-14 \n",
            "     | > step_time: 2.44100  (2.44735)\n",
            "     | > loader_time: 0.00250  (0.00271)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 175/263 -- GLOBAL_STEP: 19975\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06176  (1.08407)\n",
            "     | > G_mse_fake_loss: 0.99596  (0.99571)\n",
            "     | > G_feat_match_loss: 0.16371  (0.16368)\n",
            "     | > G_gen_loss: 47.77909  (48.78314)\n",
            "     | > G_adv_loss: 2.63307  (2.63250)\n",
            "     | > loss_0: 50.41216  (51.41564)\n",
            "     | > grad_norm_0: 632.52356  (623.84778)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 2.4738485535635846e-06  (2.5144774243861836e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5263719888025662e-06  (2.0857629647252905e-06)\n",
            "     | > loss_1: 0.00024  (0.00031)\n",
            "     | > grad_norm_1: 4.99776  (4.99675)\n",
            "     | > current_lr_0: 2.090223866256093e-14 \n",
            "     | > current_lr_1: 2.090223866256093e-14 \n",
            "     | > step_time: 2.45130  (2.44646)\n",
            "     | > loader_time: 0.00260  (0.00270)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 200/263 -- GLOBAL_STEP: 20000\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.23762  (1.08514)\n",
            "     | > G_mse_fake_loss: 0.98451  (0.99568)\n",
            "     | > G_feat_match_loss: 0.16280  (0.16367)\n",
            "     | > G_gen_loss: 55.69271  (48.83122)\n",
            "     | > G_adv_loss: 2.61256  (2.63242)\n",
            "     | > loss_0: 58.30527  (51.46363)\n",
            "     | > grad_norm_0: 1342.48291  (629.62268)\n",
            "     | > D_mse_gan_loss: 0.00034  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.0210235359409126e-06  (2.419490599407935e-06)\n",
            "     | > D_mse_gan_fake_loss: 3.4425554531480884e-06  (2.1027474772949982e-06)\n",
            "     | > loss_1: 0.00034  (0.00031)\n",
            "     | > grad_norm_1: 5.00201  (4.99678)\n",
            "     | > current_lr_0: 2.038590555575321e-14 \n",
            "     | > current_lr_1: 2.038590555575321e-14 \n",
            "     | > step_time: 2.44760  (2.44571)\n",
            "     | > loader_time: 0.00260  (0.00268)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2/checkpoint_20000.pth.tar\n",
            "\n",
            "\u001b[1m   --> STEP: 225/263 -- GLOBAL_STEP: 20025\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02364  (1.08544)\n",
            "     | > G_mse_fake_loss: 0.99885  (0.99561)\n",
            "     | > G_feat_match_loss: 0.16388  (0.16367)\n",
            "     | > G_gen_loss: 46.06392  (48.84488)\n",
            "     | > G_adv_loss: 2.63763  (2.63234)\n",
            "     | > loss_0: 48.70155  (51.47722)\n",
            "     | > grad_norm_0: 381.71902  (630.10187)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 6.072419864722178e-07  (2.3515888119618455e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.438709318790643e-06  (2.1133679302288847e-06)\n",
            "     | > loss_1: 0.00020  (0.00031)\n",
            "     | > grad_norm_1: 4.99338  (4.99682)\n",
            "     | > current_lr_0: 1.9882327057745518e-14 \n",
            "     | > current_lr_1: 1.9882327057745518e-14 \n",
            "     | > step_time: 2.44670  (2.44547)\n",
            "     | > loader_time: 0.00240  (0.00267)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 250/263 -- GLOBAL_STEP: 20050\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03686  (1.08520)\n",
            "     | > G_mse_fake_loss: 1.00764  (0.99576)\n",
            "     | > G_feat_match_loss: 0.16488  (0.16369)\n",
            "     | > G_gen_loss: 46.65871  (48.83420)\n",
            "     | > G_adv_loss: 2.65647  (2.63267)\n",
            "     | > loss_0: 49.31518  (51.46687)\n",
            "     | > grad_norm_0: 316.30328  (626.00543)\n",
            "     | > D_mse_gan_loss: 0.00035  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 2.7349872198101366e-06  (2.388822929447087e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.0595525711541995e-06  (2.111416855086646e-06)\n",
            "     | > loss_1: 0.00035  (0.00032)\n",
            "     | > grad_norm_1: 4.99104  (4.99677)\n",
            "     | > current_lr_0: 1.9391188100525565e-14 \n",
            "     | > current_lr_1: 1.9391188100525565e-14 \n",
            "     | > step_time: 2.44000  (2.44500)\n",
            "     | > loader_time: 0.00300  (0.00266)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00152 \u001b[0m(+0.00003)\n",
            "     | > avg_G_l1_spec_loss: 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss: 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0: 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 76/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 09:38:49) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 11/263 -- GLOBAL_STEP: 20075\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04167  (1.04554)\n",
            "     | > G_mse_fake_loss: 1.00229  (0.99813)\n",
            "     | > G_feat_match_loss: 0.16425  (0.16390)\n",
            "     | > G_gen_loss: 46.87528  (47.04947)\n",
            "     | > G_adv_loss: 2.64480  (2.63709)\n",
            "     | > loss_0: 49.52008  (49.68657)\n",
            "     | > grad_norm_0: 392.40182  (459.18454)\n",
            "     | > D_mse_gan_loss: 0.00032  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.502844838796591e-06  (2.8571248187538667e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.001596612899448e-06  (1.6914797656268505e-06)\n",
            "     | > loss_1: 0.00032  (0.00030)\n",
            "     | > grad_norm_1: 4.99383  (4.99509)\n",
            "     | > current_lr_0: 1.8912181398981644e-14 \n",
            "     | > current_lr_1: 1.8912181398981644e-14 \n",
            "     | > step_time: 2.44110  (2.50957)\n",
            "     | > loader_time: 0.00540  (0.00746)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 36/263 -- GLOBAL_STEP: 20100\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02715  (1.04535)\n",
            "     | > G_mse_fake_loss: 1.00021  (0.99895)\n",
            "     | > G_feat_match_loss: 0.16385  (0.16397)\n",
            "     | > G_gen_loss: 46.22165  (47.04059)\n",
            "     | > G_adv_loss: 2.63872  (2.63864)\n",
            "     | > loss_0: 48.86037  (49.67923)\n",
            "     | > grad_norm_0: 349.92877  (444.65765)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.445376142328314e-06  (2.6021115391788244e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.1977134590779315e-06  (1.6356390707035946e-06)\n",
            "     | > loss_1: 0.00020  (0.00029)\n",
            "     | > grad_norm_1: 4.99215  (4.99458)\n",
            "     | > current_lr_0: 1.84450072586472e-14 \n",
            "     | > current_lr_1: 1.84450072586472e-14 \n",
            "     | > step_time: 2.43160  (2.46231)\n",
            "     | > loader_time: 0.00240  (0.00397)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 61/263 -- GLOBAL_STEP: 20125\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.99733  (1.04964)\n",
            "     | > G_mse_fake_loss: 1.00274  (0.99916)\n",
            "     | > G_feat_match_loss: 0.16430  (0.16398)\n",
            "     | > G_gen_loss: 44.87991  (47.23390)\n",
            "     | > G_adv_loss: 2.64571  (2.63898)\n",
            "     | > loss_0: 47.52562  (49.87287)\n",
            "     | > grad_norm_0: 387.00757  (453.14200)\n",
            "     | > D_mse_gan_loss: 0.00022  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 2.101380459862412e-06  (2.6844833019099423e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5764125009809504e-06  (1.7375063875392026e-06)\n",
            "     | > loss_1: 0.00022  (0.00030)\n",
            "     | > grad_norm_1: 4.99360  (4.99469)\n",
            "     | > current_lr_0: 1.798937338819453e-14 \n",
            "     | > current_lr_1: 1.798937338819453e-14 \n",
            "     | > step_time: 2.43490  (2.45322)\n",
            "     | > loader_time: 0.00220  (0.00338)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 86/263 -- GLOBAL_STEP: 20150\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.10648  (1.05295)\n",
            "     | > G_mse_fake_loss: 0.99812  (0.99901)\n",
            "     | > G_feat_match_loss: 0.16382  (0.16397)\n",
            "     | > G_gen_loss: 49.79161  (47.38284)\n",
            "     | > G_adv_loss: 2.63628  (2.63871)\n",
            "     | > loss_0: 52.42789  (50.02155)\n",
            "     | > grad_norm_0: 568.42310  (460.80637)\n",
            "     | > D_mse_gan_loss: 0.00045  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.013541805150453e-06  (2.6112332248927947e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.2189487935975194e-06  (1.783814913413914e-06)\n",
            "     | > loss_1: 0.00045  (0.00030)\n",
            "     | > grad_norm_1: 4.99688  (4.99480)\n",
            "     | > current_lr_0: 1.7544994716560302e-14 \n",
            "     | > current_lr_1: 1.7544994716560302e-14 \n",
            "     | > step_time: 2.43930  (2.44991)\n",
            "     | > loader_time: 0.00280  (0.00315)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 111/263 -- GLOBAL_STEP: 20175\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04954  (1.05324)\n",
            "     | > G_mse_fake_loss: 1.00184  (0.99888)\n",
            "     | > G_feat_match_loss: 0.16411  (0.16396)\n",
            "     | > G_gen_loss: 47.22934  (47.39559)\n",
            "     | > G_adv_loss: 2.64291  (2.63846)\n",
            "     | > loss_0: 49.87225  (50.03405)\n",
            "     | > grad_norm_0: 400.44040  (463.11392)\n",
            "     | > D_mse_gan_loss: 0.00031  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 1.8511126427256386e-06  (1.7817091619493826e-06)\n",
            "     | > loss_1: 0.00031  (0.00030)\n",
            "     | > grad_norm_1: 4.99434  (4.99490)\n",
            "     | > current_lr_0: 1.711159321458853e-14 \n",
            "     | > current_lr_1: 1.711159321458853e-14 \n",
            "     | > step_time: 2.44970  (2.44797)\n",
            "     | > loader_time: 0.00240  (0.00303)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 136/263 -- GLOBAL_STEP: 20200\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09524  (1.05198)\n",
            "     | > G_mse_fake_loss: 0.99602  (0.99907)\n",
            "     | > G_feat_match_loss: 0.16360  (0.16397)\n",
            "     | > G_gen_loss: 49.28568  (47.33926)\n",
            "     | > G_adv_loss: 2.63199  (2.63881)\n",
            "     | > loss_0: 51.91768  (49.97807)\n",
            "     | > grad_norm_0: 561.46417  (458.33832)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.9621743376774248e-06  (2.5655305889127994e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9923636500607245e-06  (1.778140766863517e-06)\n",
            "     | > loss_1: 0.00023  (0.00030)\n",
            "     | > grad_norm_1: 4.99684  (4.99482)\n",
            "     | > current_lr_0: 1.668889772107933e-14 \n",
            "     | > current_lr_1: 1.668889772107933e-14 \n",
            "     | > step_time: 2.44520  (2.44696)\n",
            "     | > loader_time: 0.00270  (0.00294)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 161/263 -- GLOBAL_STEP: 20225\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.10244  (1.05055)\n",
            "     | > G_mse_fake_loss: 0.99396  (0.99898)\n",
            "     | > G_feat_match_loss: 0.16350  (0.16396)\n",
            "     | > G_gen_loss: 49.60998  (47.27488)\n",
            "     | > G_adv_loss: 2.62892  (2.63858)\n",
            "     | > loss_0: 52.23890  (49.91346)\n",
            "     | > grad_norm_0: 736.85260  (457.82916)\n",
            "     | > D_mse_gan_loss: 0.00039  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.6033485508160084e-06  (2.5023550869679205e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.3730090106255375e-06  (1.7631212726431028e-06)\n",
            "     | > loss_1: 0.00039  (0.00031)\n",
            "     | > grad_norm_1: 4.99833  (4.99481)\n",
            "     | > current_lr_0: 1.6276643773134732e-14 \n",
            "     | > current_lr_1: 1.6276643773134732e-14 \n",
            "     | > step_time: 2.43890  (2.44604)\n",
            "     | > loader_time: 0.00260  (0.00288)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 186/263 -- GLOBAL_STEP: 20250\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04295  (1.04975)\n",
            "     | > G_mse_fake_loss: 0.99623  (0.99896)\n",
            "     | > G_feat_match_loss: 0.16359  (0.16395)\n",
            "     | > G_gen_loss: 46.93269  (47.23853)\n",
            "     | > G_adv_loss: 2.63212  (2.63850)\n",
            "     | > loss_0: 49.56482  (49.87703)\n",
            "     | > grad_norm_0: 370.15070  (457.04688)\n",
            "     | > D_mse_gan_loss: 0.00017  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 4.644255966468336e-07  (2.5103212946823882e-06)\n",
            "     | > D_mse_gan_fake_loss: 9.93025537354697e-07  (1.7492000509836573e-06)\n",
            "     | > loss_1: 0.00017  (0.00030)\n",
            "     | > grad_norm_1: 4.99275  (4.99478)\n",
            "     | > current_lr_0: 1.587457344069526e-14 \n",
            "     | > current_lr_1: 1.587457344069526e-14 \n",
            "     | > step_time: 2.47000  (2.44558)\n",
            "     | > loader_time: 0.00260  (0.00283)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 211/263 -- GLOBAL_STEP: 20275\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.97595  (1.05075)\n",
            "     | > G_mse_fake_loss: 1.00099  (0.99897)\n",
            "     | > G_feat_match_loss: 0.16406  (0.16396)\n",
            "     | > G_gen_loss: 43.91782  (47.28375)\n",
            "     | > G_adv_loss: 2.64162  (2.63854)\n",
            "     | > loss_0: 46.55944  (49.92229)\n",
            "     | > grad_norm_0: 431.66806  (459.27081)\n",
            "     | > D_mse_gan_loss: 0.00017  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.0528665370657109e-06  (2.5000479291236112e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.1618463986451388e-06  (1.7662649188919225e-06)\n",
            "     | > loss_1: 0.00017  (0.00030)\n",
            "     | > grad_norm_1: 4.99485  (4.99481)\n",
            "     | > current_lr_0: 1.5482435165163922e-14 \n",
            "     | > current_lr_1: 1.5482435165163922e-14 \n",
            "     | > step_time: 2.45530  (2.44510)\n",
            "     | > loader_time: 0.00270  (0.00281)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 236/263 -- GLOBAL_STEP: 20300\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01001  (1.05127)\n",
            "     | > G_mse_fake_loss: 0.99801  (0.99894)\n",
            "     | > G_feat_match_loss: 0.16387  (0.16396)\n",
            "     | > G_gen_loss: 45.45028  (47.30718)\n",
            "     | > G_adv_loss: 2.63670  (2.63854)\n",
            "     | > loss_0: 48.08698  (49.94572)\n",
            "     | > grad_norm_0: 332.02710  (461.17059)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.3971026646686369e-06  (2.471999368020664e-06)\n",
            "     | > D_mse_gan_fake_loss: 9.570592283125734e-07  (1.7715319066606537e-06)\n",
            "     | > loss_1: 0.00021  (0.00030)\n",
            "     | > grad_norm_1: 4.99120  (4.99482)\n",
            "     | > current_lr_0: 1.5099983602016455e-14 \n",
            "     | > current_lr_1: 1.5099983602016455e-14 \n",
            "     | > step_time: 2.44970  (2.44484)\n",
            "     | > loader_time: 0.00230  (0.00278)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 261/263 -- GLOBAL_STEP: 20325\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01277  (1.05054)\n",
            "     | > G_mse_fake_loss: 1.00685  (0.99904)\n",
            "     | > G_feat_match_loss: 0.16497  (0.16397)\n",
            "     | > G_gen_loss: 45.57445  (47.27430)\n",
            "     | > G_adv_loss: 2.65657  (2.63875)\n",
            "     | > loss_0: 48.23102  (49.91306)\n",
            "     | > grad_norm_0: 312.74194  (457.37582)\n",
            "     | > D_mse_gan_loss: 0.00058  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.8940077097795438e-06  (2.5505095938754547e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.999071855607326e-06  (1.7666863240181597e-06)\n",
            "     | > loss_1: 0.00058  (0.00030)\n",
            "     | > grad_norm_1: 4.99055  (4.99472)\n",
            "     | > current_lr_0: 1.4726979467299556e-14 \n",
            "     | > current_lr_1: 1.4726979467299556e-14 \n",
            "     | > step_time: 2.43870  (2.44405)\n",
            "     | > loader_time: 0.00260  (0.00276)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00207 \u001b[0m(+0.00054)\n",
            "     | > avg_G_l1_spec_loss: 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.18010 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss: 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 2.80218 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0: 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 77/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 09:49:47) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/263 -- GLOBAL_STEP: 20350\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08320  (1.06031)\n",
            "     | > G_mse_fake_loss: 1.00554  (0.99718)\n",
            "     | > G_feat_match_loss: 0.16524  (0.16384)\n",
            "     | > G_gen_loss: 48.74420  (47.71386)\n",
            "     | > G_adv_loss: 2.65798  (2.63559)\n",
            "     | > loss_0: 51.40218  (50.34945)\n",
            "     | > grad_norm_0: 346.40536  (484.98709)\n",
            "     | > D_mse_gan_loss: 0.00089  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.847110181392054e-06  (1.9437959637839826e-06)\n",
            "     | > loss_1: 0.00089  (0.00031)\n",
            "     | > grad_norm_1: 4.99211  (4.99537)\n",
            "     | > current_lr_0: 1.436318938792093e-14 \n",
            "     | > current_lr_1: 1.436318938792093e-14 \n",
            "     | > step_time: 2.45550  (2.46579)\n",
            "     | > loader_time: 0.00250  (0.00367)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 47/263 -- GLOBAL_STEP: 20375\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.17464  (1.05796)\n",
            "     | > G_mse_fake_loss: 0.99006  (0.99794)\n",
            "     | > G_feat_match_loss: 0.16311  (0.16389)\n",
            "     | > G_gen_loss: 52.85877  (47.60836)\n",
            "     | > G_adv_loss: 2.62113  (2.63682)\n",
            "     | > loss_0: 55.47990  (50.24519)\n",
            "     | > grad_norm_0: 603.84711  (472.95166)\n",
            "     | > D_mse_gan_loss: 0.00033  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 7.040778200462228e-07  (1.8059729409336061e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.5165945771732368e-06  (1.826340194916253e-06)\n",
            "     | > loss_1: 0.00033  (0.00031)\n",
            "     | > grad_norm_1: 4.99778  (4.99521)\n",
            "     | > current_lr_0: 1.400838575563746e-14 \n",
            "     | > current_lr_1: 1.400838575563746e-14 \n",
            "     | > step_time: 2.43250  (2.45132)\n",
            "     | > loader_time: 0.00220  (0.00309)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 72/263 -- GLOBAL_STEP: 20400\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.99996  (1.06057)\n",
            "     | > G_mse_fake_loss: 1.00149  (0.99784)\n",
            "     | > G_feat_match_loss: 0.16422  (0.16386)\n",
            "     | > G_gen_loss: 44.99841  (47.72581)\n",
            "     | > G_adv_loss: 2.64373  (2.63648)\n",
            "     | > loss_0: 47.64213  (50.36229)\n",
            "     | > grad_norm_0: 432.36938  (485.32190)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.751313448039582e-06  (1.9848023203255452e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.639405127207283e-06  (1.8715782069812626e-06)\n",
            "     | > loss_1: 0.00025  (0.00031)\n",
            "     | > grad_norm_1: 4.99492  (4.99545)\n",
            "     | > current_lr_0: 1.3662346584650263e-14 \n",
            "     | > current_lr_1: 1.3662346584650263e-14 \n",
            "     | > step_time: 2.44250  (2.44731)\n",
            "     | > loader_time: 0.00240  (0.00292)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 97/263 -- GLOBAL_STEP: 20425\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05016  (1.06095)\n",
            "     | > G_mse_fake_loss: 1.00069  (0.99783)\n",
            "     | > G_feat_match_loss: 0.16442  (0.16386)\n",
            "     | > G_gen_loss: 47.25712  (47.74270)\n",
            "     | > G_adv_loss: 2.64488  (2.63644)\n",
            "     | > loss_0: 49.90200  (50.37914)\n",
            "     | > grad_norm_0: 376.26874  (479.98520)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 3.514797526804614e-06  (1.984353811385237e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.910973423946416e-06  (1.8850715779999799e-06)\n",
            "     | > loss_1: 0.00030  (0.00030)\n",
            "     | > grad_norm_1: 4.99324  (4.99535)\n",
            "     | > current_lr_0: 1.3324855372717478e-14 \n",
            "     | > current_lr_1: 1.3324855372717478e-14 \n",
            "     | > step_time: 2.43700  (2.44553)\n",
            "     | > loader_time: 0.00240  (0.00283)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 122/263 -- GLOBAL_STEP: 20450\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04821  (1.05944)\n",
            "     | > G_mse_fake_loss: 0.99956  (0.99796)\n",
            "     | > G_feat_match_loss: 0.16398  (0.16387)\n",
            "     | > G_gen_loss: 47.16943  (47.67476)\n",
            "     | > G_adv_loss: 2.63932  (2.63667)\n",
            "     | > loss_0: 49.80876  (50.31143)\n",
            "     | > grad_norm_0: 392.28549  (482.30936)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.9097408312518382e-06  (2.1178412984708025e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5789822782608098e-06  (1.8875364336598396e-06)\n",
            "     | > loss_1: 0.00029  (0.00030)\n",
            "     | > grad_norm_1: 4.99360  (4.99535)\n",
            "     | > current_lr_0: 1.2995700965697823e-14 \n",
            "     | > current_lr_1: 1.2995700965697823e-14 \n",
            "     | > step_time: 2.43870  (2.44460)\n",
            "     | > loader_time: 0.00250  (0.00277)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 147/263 -- GLOBAL_STEP: 20475\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02508  (1.05757)\n",
            "     | > G_mse_fake_loss: 0.99982  (0.99800)\n",
            "     | > G_feat_match_loss: 0.16391  (0.16387)\n",
            "     | > G_gen_loss: 46.12868  (47.59060)\n",
            "     | > G_adv_loss: 2.63895  (2.63674)\n",
            "     | > loss_0: 48.76764  (50.22733)\n",
            "     | > grad_norm_0: 482.45914  (482.00290)\n",
            "     | > D_mse_gan_loss: 0.00039  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 1.8683440430322662e-06  (1.8776107178852731e-06)\n",
            "     | > loss_1: 0.00039  (0.00030)\n",
            "     | > grad_norm_1: 4.99615  (4.99536)\n",
            "     | > current_lr_0: 1.2674677425440316e-14 \n",
            "     | > current_lr_1: 1.2674677425440316e-14 \n",
            "     | > step_time: 2.43860  (2.44433)\n",
            "     | > loader_time: 0.00200  (0.00272)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 172/263 -- GLOBAL_STEP: 20500\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.11137  (1.05691)\n",
            "     | > G_mse_fake_loss: 0.99160  (0.99793)\n",
            "     | > G_feat_match_loss: 0.16327  (0.16386)\n",
            "     | > G_gen_loss: 50.01178  (47.56107)\n",
            "     | > G_adv_loss: 2.62434  (2.63656)\n",
            "     | > loss_0: 52.63612  (50.19764)\n",
            "     | > grad_norm_0: 802.80078  (483.80042)\n",
            "     | > D_mse_gan_loss: 0.00032  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 4.587408000134019e-07  (2.1763661852330277e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.722208819250227e-06  (1.8843329410517896e-06)\n",
            "     | > loss_1: 0.00032  (0.00030)\n",
            "     | > grad_norm_1: 4.99895  (4.99535)\n",
            "     | > current_lr_0: 1.236158390093737e-14 \n",
            "     | > current_lr_1: 1.236158390093737e-14 \n",
            "     | > step_time: 2.43990  (2.44329)\n",
            "     | > loader_time: 0.00260  (0.00269)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 197/263 -- GLOBAL_STEP: 20525\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07068  (1.05627)\n",
            "     | > G_mse_fake_loss: 0.99937  (0.99794)\n",
            "     | > G_feat_match_loss: 0.16414  (0.16386)\n",
            "     | > G_gen_loss: 48.18056  (47.53224)\n",
            "     | > G_adv_loss: 2.64073  (2.63654)\n",
            "     | > loss_0: 50.82129  (50.16878)\n",
            "     | > grad_norm_0: 469.07520  (482.05804)\n",
            "     | > D_mse_gan_loss: 0.00037  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 2.4927423964982154e-06  (2.379783037388637e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.082695345961838e-06  (1.8634021529495502e-06)\n",
            "     | > loss_1: 0.00037  (0.00030)\n",
            "     | > grad_norm_1: 4.99556  (4.99535)\n",
            "     | > current_lr_0: 1.2056224502660695e-14 \n",
            "     | > current_lr_1: 1.2056224502660695e-14 \n",
            "     | > step_time: 2.44980  (2.44358)\n",
            "     | > loader_time: 0.00240  (0.00269)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 222/263 -- GLOBAL_STEP: 20550\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01797  (1.05566)\n",
            "     | > G_mse_fake_loss: 1.00463  (0.99801)\n",
            "     | > G_feat_match_loss: 0.16446  (0.16387)\n",
            "     | > G_gen_loss: 45.80863  (47.50482)\n",
            "     | > G_adv_loss: 2.64925  (2.63671)\n",
            "     | > loss_0: 48.45788  (50.14153)\n",
            "     | > grad_norm_0: 303.55078  (480.70581)\n",
            "     | > D_mse_gan_loss: 0.00032  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.9684116523421835e-06  (2.4119093345965716e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5917901237116894e-06  (1.8646889407853713e-06)\n",
            "     | > loss_1: 0.00032  (0.00030)\n",
            "     | > grad_norm_1: 4.99002  (4.99531)\n",
            "     | > current_lr_0: 1.175840818000144e-14 \n",
            "     | > current_lr_1: 1.175840818000144e-14 \n",
            "     | > step_time: 2.44240  (2.44350)\n",
            "     | > loader_time: 0.00250  (0.00268)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 247/263 -- GLOBAL_STEP: 20575\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07180  (1.05579)\n",
            "     | > G_mse_fake_loss: 0.99550  (0.99802)\n",
            "     | > G_feat_match_loss: 0.16365  (0.16387)\n",
            "     | > G_gen_loss: 48.23104  (47.51036)\n",
            "     | > G_adv_loss: 2.63203  (2.63677)\n",
            "     | > loss_0: 50.86307  (50.14712)\n",
            "     | > grad_norm_0: 469.77548  (481.08847)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.5821669876459055e-06  (2.3651077768997644e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.169251959254325e-06  (1.8560886937498603e-06)\n",
            "     | > loss_1: 0.00021  (0.00030)\n",
            "     | > grad_norm_1: 4.99555  (4.99534)\n",
            "     | > current_lr_0: 1.1467948601737805e-14 \n",
            "     | > current_lr_1: 1.1467948601737805e-14 \n",
            "     | > step_time: 2.44110  (2.44328)\n",
            "     | > loader_time: 0.00270  (0.00268)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00173 \u001b[0m(-0.00033)\n",
            "     | > avg_G_l1_spec_loss: 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss: 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0: 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 78/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 10:00:45) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 8/263 -- GLOBAL_STEP: 20600\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08099  (1.06773)\n",
            "     | > G_mse_fake_loss: 0.99735  (0.99600)\n",
            "     | > G_feat_match_loss: 0.16382  (0.16369)\n",
            "     | > G_gen_loss: 48.64447  (48.04777)\n",
            "     | > G_adv_loss: 2.63556  (2.63292)\n",
            "     | > loss_0: 51.28003  (50.68069)\n",
            "     | > grad_norm_0: 496.01584  (601.29663)\n",
            "     | > D_mse_gan_loss: 0.00055  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.16001672015409e-06  (2.1325823666984434e-06)\n",
            "     | > loss_1: 0.00055  (0.00033)\n",
            "     | > grad_norm_1: 4.99612  (4.99628)\n",
            "     | > current_lr_0: 1.1184664039455379e-14 \n",
            "     | > current_lr_1: 1.1184664039455379e-14 \n",
            "     | > step_time: 2.43310  (2.54337)\n",
            "     | > loader_time: 0.00360  (0.00344)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 33/263 -- GLOBAL_STEP: 20625\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05798  (1.06819)\n",
            "     | > G_mse_fake_loss: 0.99528  (0.99656)\n",
            "     | > G_feat_match_loss: 0.16345  (0.16375)\n",
            "     | > G_gen_loss: 47.60925  (48.06861)\n",
            "     | > G_adv_loss: 2.62982  (2.63406)\n",
            "     | > loss_0: 50.23907  (50.70267)\n",
            "     | > grad_norm_0: 457.66830  (554.24231)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 2.0342570223874645e-06  (2.1487873669222992e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.1080205695179757e-06  (1.983905259762788e-06)\n",
            "     | > loss_1: 0.00023  (0.00028)\n",
            "     | > grad_norm_1: 4.99527  (4.99591)\n",
            "     | > current_lr_0: 1.0908377253847271e-14 \n",
            "     | > current_lr_1: 1.0908377253847271e-14 \n",
            "     | > step_time: 2.44310  (2.46781)\n",
            "     | > loader_time: 0.00220  (0.00274)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 58/263 -- GLOBAL_STEP: 20650\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07286  (1.06864)\n",
            "     | > G_mse_fake_loss: 0.99618  (0.99678)\n",
            "     | > G_feat_match_loss: 0.16366  (0.16377)\n",
            "     | > G_gen_loss: 48.27873  (48.08891)\n",
            "     | > G_adv_loss: 2.63282  (2.63447)\n",
            "     | > loss_0: 50.91156  (50.72338)\n",
            "     | > grad_norm_0: 657.16705  (556.42767)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 3.2044404179032426e-06  (3.8465216331340755e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.5814006221480668e-06  (2.0230717115643117e-06)\n",
            "     | > loss_1: 0.00030  (0.00029)\n",
            "     | > grad_norm_1: 4.99772  (4.99601)\n",
            "     | > current_lr_0: 1.0638915383822896e-14 \n",
            "     | > current_lr_1: 1.0638915383822896e-14 \n",
            "     | > step_time: 2.43470  (2.45610)\n",
            "     | > loader_time: 0.00270  (0.00268)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 83/263 -- GLOBAL_STEP: 20675\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09797  (1.06932)\n",
            "     | > G_mse_fake_loss: 0.99613  (0.99676)\n",
            "     | > G_feat_match_loss: 0.16388  (0.16376)\n",
            "     | > G_gen_loss: 49.40882  (48.11955)\n",
            "     | > G_adv_loss: 2.63493  (2.63435)\n",
            "     | > loss_0: 52.04375  (50.75390)\n",
            "     | > grad_norm_0: 598.73395  (551.35089)\n",
            "     | > D_mse_gan_loss: 0.00039  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.407291733812599e-06  (3.3936459758766673e-06)\n",
            "     | > D_mse_gan_fake_loss: 3.2847069633135106e-06  (2.016076149334093e-06)\n",
            "     | > loss_1: 0.00039  (0.00029)\n",
            "     | > grad_norm_1: 4.99739  (4.99590)\n",
            "     | > current_lr_0: 1.0376109838356002e-14 \n",
            "     | > current_lr_1: 1.0376109838356002e-14 \n",
            "     | > step_time: 2.43900  (2.45141)\n",
            "     | > loader_time: 0.00250  (0.00264)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 108/263 -- GLOBAL_STEP: 20700\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04615  (1.07046)\n",
            "     | > G_mse_fake_loss: 0.99759  (0.99664)\n",
            "     | > G_feat_match_loss: 0.16396  (0.16374)\n",
            "     | > G_gen_loss: 47.07671  (48.17079)\n",
            "     | > G_adv_loss: 2.63718  (2.63405)\n",
            "     | > loss_0: 49.71388  (50.80484)\n",
            "     | > grad_norm_0: 443.50861  (558.11963)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 3.5342379760550102e-06  (3.0725838543753945e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6320790336976643e-06  (2.034783491922098e-06)\n",
            "     | > loss_1: 0.00025  (0.00029)\n",
            "     | > grad_norm_1: 4.99511  (4.99592)\n",
            "     | > current_lr_0: 1.0119796191004323e-14 \n",
            "     | > current_lr_1: 1.0119796191004323e-14 \n",
            "     | > step_time: 2.43920  (2.44902)\n",
            "     | > loader_time: 0.00260  (0.00264)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 133/263 -- GLOBAL_STEP: 20725\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00178  (1.07005)\n",
            "     | > G_mse_fake_loss: 1.00472  (0.99689)\n",
            "     | > G_feat_match_loss: 0.16443  (0.16377)\n",
            "     | > G_gen_loss: 45.08028  (48.15239)\n",
            "     | > G_adv_loss: 2.64898  (2.63455)\n",
            "     | > loss_0: 47.72927  (50.78694)\n",
            "     | > grad_norm_0: 340.06342  (555.51282)\n",
            "     | > D_mse_gan_loss: 0.00040  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.3314800071384525e-06  (2.9835624479673264e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3783462691208115e-06  (2.0517555307741075e-06)\n",
            "     | > loss_1: 0.00040  (0.00030)\n",
            "     | > grad_norm_1: 4.99209  (4.99586)\n",
            "     | > current_lr_0: 9.869814077034824e-15 \n",
            "     | > current_lr_1: 9.869814077034824e-15 \n",
            "     | > step_time: 2.45070  (2.44774)\n",
            "     | > loader_time: 0.00260  (0.00264)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 158/263 -- GLOBAL_STEP: 20750\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05689  (1.07005)\n",
            "     | > G_mse_fake_loss: 0.99934  (0.99688)\n",
            "     | > G_feat_match_loss: 0.16423  (0.16376)\n",
            "     | > G_gen_loss: 47.56002  (48.15230)\n",
            "     | > G_adv_loss: 2.64160  (2.63451)\n",
            "     | > loss_0: 50.20163  (50.78681)\n",
            "     | > grad_norm_0: 439.11057  (559.90112)\n",
            "     | > D_mse_gan_loss: 0.00052  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 2.1273585844028275e-06  (2.746273356414884e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.868646336333768e-06  (2.046210837562871e-06)\n",
            "     | > loss_1: 0.00052  (0.00030)\n",
            "     | > grad_norm_1: 4.99500  (4.99587)\n",
            "     | > current_lr_0: 9.626007093090194e-15 \n",
            "     | > current_lr_1: 9.626007093090194e-15 \n",
            "     | > step_time: 2.43570  (2.44666)\n",
            "     | > loader_time: 0.00240  (0.00264)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 183/263 -- GLOBAL_STEP: 20775\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.14781  (1.07027)\n",
            "     | > G_mse_fake_loss: 0.98855  (0.99681)\n",
            "     | > G_feat_match_loss: 0.16300  (0.16376)\n",
            "     | > G_gen_loss: 51.65167  (48.16220)\n",
            "     | > G_adv_loss: 2.61851  (2.63440)\n",
            "     | > loss_0: 54.27018  (50.79660)\n",
            "     | > grad_norm_0: 924.93579  (568.37683)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 1.1151995522595826e-06  (2.5729895667670402e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.463041028022417e-06  (2.071999377649213e-06)\n",
            "     | > loss_1: 0.00030  (0.00030)\n",
            "     | > grad_norm_1: 4.99990  (4.99595)\n",
            "     | > current_lr_0: 9.388222699333813e-15 \n",
            "     | > current_lr_1: 9.388222699333813e-15 \n",
            "     | > step_time: 2.45500  (2.44596)\n",
            "     | > loader_time: 0.00240  (0.00264)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 208/263 -- GLOBAL_STEP: 20800\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.11030  (1.07063)\n",
            "     | > G_mse_fake_loss: 0.99277  (0.99683)\n",
            "     | > G_feat_match_loss: 0.16339  (0.16376)\n",
            "     | > G_gen_loss: 49.96369  (48.17857)\n",
            "     | > G_adv_loss: 2.62664  (2.63447)\n",
            "     | > loss_0: 52.59033  (50.81304)\n",
            "     | > grad_norm_0: 518.51166  (570.36993)\n",
            "     | > D_mse_gan_loss: 0.00022  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 8.144331218318257e-07  (2.5456138342560244e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.156376694983919e-06  (2.08927361455835e-06)\n",
            "     | > loss_1: 0.00022  (0.00030)\n",
            "     | > grad_norm_1: 4.99662  (4.99595)\n",
            "     | > current_lr_0: 9.15631212401194e-15 \n",
            "     | > current_lr_1: 9.15631212401194e-15 \n",
            "     | > step_time: 2.44680  (2.44516)\n",
            "     | > loader_time: 0.00210  (0.00264)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 233/263 -- GLOBAL_STEP: 20825\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06052  (1.07082)\n",
            "     | > G_mse_fake_loss: 0.99795  (0.99682)\n",
            "     | > G_feat_match_loss: 0.16381  (0.16376)\n",
            "     | > G_gen_loss: 47.72350  (48.18680)\n",
            "     | > G_adv_loss: 2.63601  (2.63442)\n",
            "     | > loss_0: 50.35950  (50.82122)\n",
            "     | > grad_norm_0: 456.00732  (566.96454)\n",
            "     | > D_mse_gan_loss: 0.00032  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 3.2613340863463236e-06  (2.5531427584443476e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.848682586569339e-06  (2.0851939967550212e-06)\n",
            "     | > loss_1: 0.00032  (0.00031)\n",
            "     | > grad_norm_1: 4.99519  (4.99593)\n",
            "     | > current_lr_0: 8.930130270373451e-15 \n",
            "     | > current_lr_1: 8.930130270373451e-15 \n",
            "     | > step_time: 2.44880  (2.44499)\n",
            "     | > loader_time: 0.00260  (0.00264)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 258/263 -- GLOBAL_STEP: 20850\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00823  (1.07118)\n",
            "     | > G_mse_fake_loss: 0.99778  (0.99678)\n",
            "     | > G_feat_match_loss: 0.16393  (0.16376)\n",
            "     | > G_gen_loss: 45.37024  (48.20310)\n",
            "     | > G_adv_loss: 2.63711  (2.63436)\n",
            "     | > loss_0: 48.00735  (50.83746)\n",
            "     | > grad_norm_0: 496.87592  (572.76727)\n",
            "     | > D_mse_gan_loss: 0.00022  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.2477157724788412e-06  (2.567986957660562e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.462754426029278e-06  (2.1032417086023824e-06)\n",
            "     | > loss_1: 0.00022  (0.00031)\n",
            "     | > grad_norm_1: 4.99604  (4.99598)\n",
            "     | > current_lr_0: 8.709535625888873e-15 \n",
            "     | > current_lr_1: 8.709535625888873e-15 \n",
            "     | > step_time: 2.43640  (2.44430)\n",
            "     | > loader_time: 0.00230  (0.00264)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00154 \u001b[0m(-0.00020)\n",
            "     | > avg_G_l1_spec_loss: 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.18010 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss: 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 2.80218 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0: 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 79/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 10:11:44) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 19/263 -- GLOBAL_STEP: 20875\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.17428  (1.06819)\n",
            "     | > G_mse_fake_loss: 0.99481  (0.99819)\n",
            "     | > G_feat_match_loss: 0.16374  (0.16392)\n",
            "     | > G_gen_loss: 52.84268  (48.06850)\n",
            "     | > G_adv_loss: 2.63224  (2.63740)\n",
            "     | > loss_0: 55.47492  (50.70591)\n",
            "     | > grad_norm_0: 862.62640  (508.57660)\n",
            "     | > D_mse_gan_loss: 0.00051  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.376715431433695e-06  (2.984754548594394e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.092161594191566e-06  (1.75044692097895e-06)\n",
            "     | > loss_1: 0.00051  (0.00033)\n",
            "     | > grad_norm_1: 4.99916  (4.99563)\n",
            "     | > current_lr_0: 8.494390173711903e-15 \n",
            "     | > current_lr_1: 8.494390173711903e-15 \n",
            "     | > step_time: 2.43500  (2.47137)\n",
            "     | > loader_time: 0.00270  (0.00635)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 44/263 -- GLOBAL_STEP: 20900\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03391  (1.06249)\n",
            "     | > G_mse_fake_loss: 1.00021  (0.99867)\n",
            "     | > G_feat_match_loss: 0.16410  (0.16396)\n",
            "     | > G_gen_loss: 46.52603  (47.81188)\n",
            "     | > G_adv_loss: 2.64124  (2.63823)\n",
            "     | > loss_0: 49.16727  (50.45011)\n",
            "     | > grad_norm_0: 356.57477  (494.79977)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.2227966408318025e-06  (3.483299142782347e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.4467394748862716e-06  (1.7607847269118793e-06)\n",
            "     | > loss_1: 0.00023  (0.00031)\n",
            "     | > grad_norm_1: 4.99230  (4.99542)\n",
            "     | > current_lr_0: 8.28455930632805e-15 \n",
            "     | > current_lr_1: 8.28455930632805e-15 \n",
            "     | > step_time: 2.43550  (2.45488)\n",
            "     | > loader_time: 0.00250  (0.00415)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 69/263 -- GLOBAL_STEP: 20925\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08357  (1.06694)\n",
            "     | > G_mse_fake_loss: 0.99798  (0.99848)\n",
            "     | > G_feat_match_loss: 0.16392  (0.16394)\n",
            "     | > G_gen_loss: 48.76083  (48.01222)\n",
            "     | > G_adv_loss: 2.63714  (2.63786)\n",
            "     | > loss_0: 51.39797  (50.65008)\n",
            "     | > grad_norm_0: 560.62927  (497.61057)\n",
            "     | > D_mse_gan_loss: 0.00043  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.804685891533154e-06  (1.8461708703108488e-06)\n",
            "     | > loss_1: 0.00043  (0.00031)\n",
            "     | > grad_norm_1: 4.99692  (4.99548)\n",
            "     | > current_lr_0: 8.07991174133632e-15 \n",
            "     | > current_lr_1: 8.07991174133632e-15 \n",
            "     | > step_time: 2.43810  (2.45040)\n",
            "     | > loader_time: 0.00280  (0.00357)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 94/263 -- GLOBAL_STEP: 20950\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.16612  (1.06956)\n",
            "     | > G_mse_fake_loss: 0.98671  (0.99820)\n",
            "     | > G_feat_match_loss: 0.16282  (0.16391)\n",
            "     | > G_gen_loss: 52.47525  (48.13042)\n",
            "     | > G_adv_loss: 2.61486  (2.63731)\n",
            "     | > loss_0: 55.09011  (50.76773)\n",
            "     | > grad_norm_0: 702.39783  (503.30911)\n",
            "     | > D_mse_gan_loss: 0.00027  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 2.5444768425586517e-07  (3.051092845608048e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.9227460345282452e-06  (1.8740339785526498e-06)\n",
            "     | > loss_1: 0.00027  (0.00031)\n",
            "     | > grad_norm_1: 4.99936  (4.99558)\n",
            "     | > current_lr_0: 7.880319439311333e-15 \n",
            "     | > current_lr_1: 7.880319439311333e-15 \n",
            "     | > step_time: 2.45040  (2.44762)\n",
            "     | > loader_time: 0.00250  (0.00329)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 119/263 -- GLOBAL_STEP: 20975\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08889  (1.06909)\n",
            "     | > G_mse_fake_loss: 0.99564  (0.99823)\n",
            "     | > G_feat_match_loss: 0.16372  (0.16392)\n",
            "     | > G_gen_loss: 49.00017  (48.10922)\n",
            "     | > G_adv_loss: 2.63285  (2.63740)\n",
            "     | > loss_0: 51.63302  (50.74661)\n",
            "     | > grad_norm_0: 539.66919  (504.47760)\n",
            "     | > D_mse_gan_loss: 0.00026  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.2749233064823784e-06  (2.9623356639405313e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9431724922469584e-06  (1.8709272859926103e-06)\n",
            "     | > loss_1: 0.00026  (0.00031)\n",
            "     | > grad_norm_1: 4.99694  (4.99560)\n",
            "     | > current_lr_0: 7.685657523694385e-15 \n",
            "     | > current_lr_1: 7.685657523694385e-15 \n",
            "     | > step_time: 2.44730  (2.44648)\n",
            "     | > loader_time: 0.00280  (0.00314)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 144/263 -- GLOBAL_STEP: 21000\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02485  (1.06824)\n",
            "     | > G_mse_fake_loss: 1.00533  (0.99827)\n",
            "     | > G_feat_match_loss: 0.16460  (0.16392)\n",
            "     | > G_gen_loss: 46.11812  (48.07076)\n",
            "     | > G_adv_loss: 2.65130  (2.63744)\n",
            "     | > loss_0: 48.76942  (50.70820)\n",
            "     | > grad_norm_0: 315.10626  (502.90826)\n",
            "     | > D_mse_gan_loss: 0.00050  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 2.901942707467242e-06  (2.8452838013571918e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7863130779005587e-06  (1.8704132524286858e-06)\n",
            "     | > loss_1: 0.00050  (0.00031)\n",
            "     | > grad_norm_1: 4.99113  (4.99557)\n",
            "     | > current_lr_0: 7.495804202663416e-15 \n",
            "     | > current_lr_1: 7.495804202663416e-15 \n",
            "     | > step_time: 2.44190  (2.44545)\n",
            "     | > loader_time: 0.00240  (0.00304)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 169/263 -- GLOBAL_STEP: 21025\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.10543  (1.06787)\n",
            "     | > G_mse_fake_loss: 0.99476  (0.99819)\n",
            "     | > G_feat_match_loss: 0.16368  (0.16391)\n",
            "     | > G_gen_loss: 49.74422  (48.05400)\n",
            "     | > G_adv_loss: 2.63152  (2.63730)\n",
            "     | > loss_0: 52.37574  (50.69130)\n",
            "     | > grad_norm_0: 588.30054  (507.03879)\n",
            "     | > D_mse_gan_loss: 0.00036  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 6.638098852818075e-07  (2.8518491992276712e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.2166284452396212e-06  (1.8798886057240087e-06)\n",
            "     | > loss_1: 0.00036  (0.00031)\n",
            "     | > grad_norm_1: 4.99734  (4.99561)\n",
            "     | > current_lr_0: 7.310640692932956e-15 \n",
            "     | > current_lr_1: 7.310640692932956e-15 \n",
            "     | > step_time: 2.43440  (2.44476)\n",
            "     | > loader_time: 0.00250  (0.00298)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 194/263 -- GLOBAL_STEP: 21050\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01944  (1.06801)\n",
            "     | > G_mse_fake_loss: 1.00028  (0.99817)\n",
            "     | > G_feat_match_loss: 0.16373  (0.16390)\n",
            "     | > G_gen_loss: 45.87498  (48.06023)\n",
            "     | > G_adv_loss: 2.63758  (2.63722)\n",
            "     | > loss_0: 48.51256  (50.69745)\n",
            "     | > grad_norm_0: 383.59409  (507.54886)\n",
            "     | > D_mse_gan_loss: 0.00041  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 1.4189548664944596e-06  (1.8734245621854103e-06)\n",
            "     | > loss_1: 0.00041  (0.00031)\n",
            "     | > grad_norm_1: 4.99443  (4.99560)\n",
            "     | > current_lr_0: 7.130051145436402e-15 \n",
            "     | > current_lr_1: 7.130051145436402e-15 \n",
            "     | > step_time: 2.43540  (2.44466)\n",
            "     | > loader_time: 0.00250  (0.00293)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 219/263 -- GLOBAL_STEP: 21075\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.18460  (1.06788)\n",
            "     | > G_mse_fake_loss: 1.00088  (0.99828)\n",
            "     | > G_feat_match_loss: 0.16455  (0.16392)\n",
            "     | > G_gen_loss: 53.30721  (48.05451)\n",
            "     | > G_adv_loss: 2.64634  (2.63744)\n",
            "     | > loss_0: 55.95355  (50.69195)\n",
            "     | > grad_norm_0: 682.04614  (506.38303)\n",
            "     | > D_mse_gan_loss: 0.00062  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 3.157600531267235e-06  (2.76943323591151e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.595992327769636e-06  (1.879450109430614e-06)\n",
            "     | > loss_1: 0.00062  (0.00031)\n",
            "     | > grad_norm_1: 4.99785  (4.99558)\n",
            "     | > current_lr_0: 6.953922572844075e-15 \n",
            "     | > current_lr_1: 6.953922572844075e-15 \n",
            "     | > step_time: 2.43640  (2.44419)\n",
            "     | > loader_time: 0.00280  (0.00289)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 244/263 -- GLOBAL_STEP: 21100\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09998  (1.06821)\n",
            "     | > G_mse_fake_loss: 0.99722  (0.99825)\n",
            "     | > G_feat_match_loss: 0.16376  (0.16392)\n",
            "     | > G_gen_loss: 49.49908  (48.06967)\n",
            "     | > G_adv_loss: 2.63485  (2.63745)\n",
            "     | > loss_0: 52.13394  (50.70712)\n",
            "     | > grad_norm_0: 585.38947  (506.35916)\n",
            "     | > D_mse_gan_loss: 0.00028  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 8.879808888195839e-07  (2.810332311986393e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.3929801500344183e-06  (1.8816581017256898e-06)\n",
            "     | > loss_1: 0.00028  (0.00031)\n",
            "     | > grad_norm_1: 4.99704  (4.99559)\n",
            "     | > current_lr_0: 6.782144778871797e-15 \n",
            "     | > current_lr_1: 6.782144778871797e-15 \n",
            "     | > step_time: 2.43210  (2.44371)\n",
            "     | > loader_time: 0.00260  (0.00286)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00163 \u001b[0m(+0.00010)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 1.09853 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.18010 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 49.43375 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 2.80218 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 52.23593 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 80/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 10:22:42) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/263 -- GLOBAL_STEP: 21125\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07935  (1.07837)\n",
            "     | > G_mse_fake_loss: 0.99375  (0.99424)\n",
            "     | > G_feat_match_loss: 0.16334  (0.16347)\n",
            "     | > G_gen_loss: 48.57078  (48.52650)\n",
            "     | > G_adv_loss: 2.62720  (2.62892)\n",
            "     | > loss_0: 51.19798  (51.15542)\n",
            "     | > grad_norm_0: 524.17236  (588.82391)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00024)\n",
            "     | > D_mse_gan_real_loss: 1.459803002035187e-06  (1.432281612778752e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6544724985578796e-06  (1.9397208006921575e-06)\n",
            "     | > loss_1: 0.00020  (0.00024)\n",
            "     | > grad_norm_1: 4.99655  (4.99668)\n",
            "     | > current_lr_0: 6.614610289335665e-15 \n",
            "     | > current_lr_1: 6.614610289335665e-15 \n",
            "     | > step_time: 2.44400  (2.58153)\n",
            "     | > loader_time: 0.00100  (0.00862)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/263 -- GLOBAL_STEP: 21150\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09588  (1.09264)\n",
            "     | > G_mse_fake_loss: 0.99408  (0.99404)\n",
            "     | > G_feat_match_loss: 0.16358  (0.16351)\n",
            "     | > G_gen_loss: 49.31457  (49.16863)\n",
            "     | > G_adv_loss: 2.62990  (2.62912)\n",
            "     | > loss_0: 51.94447  (51.79775)\n",
            "     | > grad_norm_0: 675.07281  (755.50299)\n",
            "     | > D_mse_gan_loss: 0.00038  (0.00031)\n",
            "     | > D_mse_gan_real_loss: 1.7254662907362217e-06  (2.103329397584731e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.808233602991095e-06  (2.7220207243772165e-06)\n",
            "     | > loss_1: 0.00038  (0.00031)\n",
            "     | > grad_norm_1: 4.99801  (4.99729)\n",
            "     | > current_lr_0: 6.451214284909962e-15 \n",
            "     | > current_lr_1: 6.451214284909962e-15 \n",
            "     | > step_time: 2.43490  (2.46488)\n",
            "     | > loader_time: 0.00290  (0.00345)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/263 -- GLOBAL_STEP: 21175\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04234  (1.09642)\n",
            "     | > G_mse_fake_loss: 1.00288  (0.99429)\n",
            "     | > G_feat_match_loss: 0.16430  (0.16356)\n",
            "     | > G_gen_loss: 46.90525  (49.33871)\n",
            "     | > G_adv_loss: 2.64585  (2.62988)\n",
            "     | > loss_0: 49.55111  (51.96859)\n",
            "     | > grad_norm_0: 390.27762  (756.46075)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.063766148945433e-06  (2.032107767294375e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9474907730909763e-06  (2.7873420549440193e-06)\n",
            "     | > loss_1: 0.00030  (0.00032)\n",
            "     | > grad_norm_1: 4.99391  (4.99725)\n",
            "     | > current_lr_0: 6.2918545355460754e-15 \n",
            "     | > current_lr_1: 6.2918545355460754e-15 \n",
            "     | > step_time: 2.43460  (2.45487)\n",
            "     | > loader_time: 0.00260  (0.00308)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/263 -- GLOBAL_STEP: 21200\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.28836  (1.09903)\n",
            "     | > G_mse_fake_loss: 0.97530  (0.99389)\n",
            "     | > G_feat_match_loss: 0.16196  (0.16351)\n",
            "     | > G_gen_loss: 57.97632  (49.45614)\n",
            "     | > G_adv_loss: 2.59491  (2.62901)\n",
            "     | > loss_0: 60.57123  (52.08514)\n",
            "     | > grad_norm_0: 2246.20581  (775.08173)\n",
            "     | > D_mse_gan_loss: 0.00052  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 3.78815201429461e-07  (2.0337200526299177e-06)\n",
            "     | > D_mse_gan_fake_loss: 0.00001  (0.00000)\n",
            "     | > loss_1: 0.00052  (0.00033)\n",
            "     | > grad_norm_1: 5.00633  (4.99740)\n",
            "     | > current_lr_0: 6.13643133651144e-15 \n",
            "     | > current_lr_1: 6.13643133651144e-15 \n",
            "     | > step_time: 2.44650  (2.45126)\n",
            "     | > loader_time: 0.00250  (0.00291)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/263 -- GLOBAL_STEP: 21225\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08587  (1.09890)\n",
            "     | > G_mse_fake_loss: 0.99753  (0.99396)\n",
            "     | > G_feat_match_loss: 0.16384  (0.16352)\n",
            "     | > G_gen_loss: 48.86422  (49.45061)\n",
            "     | > G_adv_loss: 2.63593  (2.62914)\n",
            "     | > loss_0: 51.50015  (52.07975)\n",
            "     | > grad_norm_0: 512.28625  (764.85822)\n",
            "     | > D_mse_gan_loss: 0.00031  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.2397864566082717e-06  (2.0912860654626845e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7166886436825735e-06  (2.8248929744395787e-06)\n",
            "     | > loss_1: 0.00031  (0.00032)\n",
            "     | > grad_norm_1: 4.99634  (4.99734)\n",
            "     | > current_lr_0: 5.984847446008445e-15 \n",
            "     | > current_lr_1: 5.984847446008445e-15 \n",
            "     | > step_time: 2.44040  (2.44923)\n",
            "     | > loader_time: 0.00230  (0.00284)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/263 -- GLOBAL_STEP: 21250\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00191  (1.09610)\n",
            "     | > G_mse_fake_loss: 1.00220  (0.99425)\n",
            "     | > G_feat_match_loss: 0.16410  (0.16354)\n",
            "     | > G_gen_loss: 45.08587  (49.32431)\n",
            "     | > G_adv_loss: 2.64319  (2.62968)\n",
            "     | > loss_0: 47.72905  (51.95400)\n",
            "     | > grad_norm_0: 370.54648  (746.97943)\n",
            "     | > D_mse_gan_loss: 0.00026  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 2.635800228745211e-06  (2.054802307874035e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3340151099328068e-06  (2.7810254262196677e-06)\n",
            "     | > loss_1: 0.00026  (0.00032)\n",
            "     | > grad_norm_1: 4.99350  (4.99715)\n",
            "     | > current_lr_0: 5.837008024334313e-15 \n",
            "     | > current_lr_1: 5.837008024334313e-15 \n",
            "     | > step_time: 2.44910  (2.44833)\n",
            "     | > loader_time: 0.00210  (0.00279)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 155/263 -- GLOBAL_STEP: 21275\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.12057  (1.09579)\n",
            "     | > G_mse_fake_loss: 0.99081  (0.99425)\n",
            "     | > G_feat_match_loss: 0.16329  (0.16354)\n",
            "     | > G_gen_loss: 50.42549  (49.31041)\n",
            "     | > G_adv_loss: 2.62374  (2.62968)\n",
            "     | > loss_0: 53.04924  (51.94009)\n",
            "     | > grad_norm_0: 739.52643  (753.52319)\n",
            "     | > D_mse_gan_loss: 0.00027  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.3308073221196537e-06  (2.008007656730027e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9251403955422575e-06  (2.7798364785227142e-06)\n",
            "     | > loss_1: 0.00027  (0.00032)\n",
            "     | > grad_norm_1: 4.99907  (4.99715)\n",
            "     | > current_lr_0: 5.69282057454386e-15 \n",
            "     | > current_lr_1: 5.69282057454386e-15 \n",
            "     | > step_time: 2.44780  (2.44783)\n",
            "     | > loader_time: 0.00220  (0.00276)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 180/263 -- GLOBAL_STEP: 21300\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05310  (1.09515)\n",
            "     | > G_mse_fake_loss: 0.99892  (0.99433)\n",
            "     | > G_feat_match_loss: 0.16404  (0.16355)\n",
            "     | > G_gen_loss: 47.38936  (49.28159)\n",
            "     | > G_adv_loss: 2.63932  (2.62985)\n",
            "     | > loss_0: 50.02868  (51.91144)\n",
            "     | > grad_norm_0: 393.29535  (747.12683)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.2950143855050555e-06  (2.0709819741979476e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.4796966070207418e-06  (2.757393934239695e-06)\n",
            "     | > loss_1: 0.00021  (0.00032)\n",
            "     | > grad_norm_1: 4.99376  (4.99705)\n",
            "     | > current_lr_0: 5.552194884578031e-15 \n",
            "     | > current_lr_1: 5.552194884578031e-15 \n",
            "     | > step_time: 2.44800  (2.44719)\n",
            "     | > loader_time: 0.00290  (0.00274)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 205/263 -- GLOBAL_STEP: 21325\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04264  (1.09563)\n",
            "     | > G_mse_fake_loss: 0.99990  (0.99430)\n",
            "     | > G_feat_match_loss: 0.16408  (0.16355)\n",
            "     | > G_gen_loss: 46.91882  (49.30344)\n",
            "     | > G_adv_loss: 2.64068  (2.62975)\n",
            "     | > loss_0: 49.55950  (51.93319)\n",
            "     | > grad_norm_0: 448.21490  (746.94769)\n",
            "     | > D_mse_gan_loss: 0.00028  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 2.367340130149387e-06  (2.018059240487961e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.8265820926899323e-06  (2.774933410564256e-06)\n",
            "     | > loss_1: 0.00028  (0.00032)\n",
            "     | > grad_norm_1: 4.99507  (4.99707)\n",
            "     | > current_lr_0: 5.4150429708219765e-15 \n",
            "     | > current_lr_1: 5.4150429708219765e-15 \n",
            "     | > step_time: 2.43900  (2.44675)\n",
            "     | > loader_time: 0.00250  (0.00273)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/263 -- GLOBAL_STEP: 21350\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09512  (1.09571)\n",
            "     | > G_mse_fake_loss: 0.99428  (0.99427)\n",
            "     | > G_feat_match_loss: 0.16345  (0.16354)\n",
            "     | > G_gen_loss: 49.28038  (49.30688)\n",
            "     | > G_adv_loss: 2.62880  (2.62969)\n",
            "     | > loss_0: 51.90918  (51.93658)\n",
            "     | > grad_norm_0: 404.48514  (747.07001)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 5.456529947878153e-07  (2.0934087887247217e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.619314389245119e-06  (2.7779965154442595e-06)\n",
            "     | > loss_1: 0.00021  (0.00032)\n",
            "     | > grad_norm_1: 4.99391  (4.99710)\n",
            "     | > current_lr_0: 5.28127902305739e-15 \n",
            "     | > current_lr_1: 5.28127902305739e-15 \n",
            "     | > step_time: 2.44110  (2.44655)\n",
            "     | > loader_time: 0.00270  (0.00271)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 255/263 -- GLOBAL_STEP: 21375\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02109  (1.09651)\n",
            "     | > G_mse_fake_loss: 1.00599  (0.99436)\n",
            "     | > G_feat_match_loss: 0.16470  (0.16356)\n",
            "     | > G_gen_loss: 45.94925  (49.34297)\n",
            "     | > G_adv_loss: 2.65298  (2.62992)\n",
            "     | > loss_0: 48.60223  (51.97288)\n",
            "     | > grad_norm_0: 338.04330  (744.47528)\n",
            "     | > D_mse_gan_loss: 0.00050  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 3.1945719456416555e-06  (2.081583710155338e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6911651528062066e-06  (2.7840477465432223e-06)\n",
            "     | > loss_1: 0.00050  (0.00032)\n",
            "     | > grad_norm_1: 4.99245  (4.99709)\n",
            "     | > current_lr_0: 5.1508193507746374e-15 \n",
            "     | > current_lr_1: 5.1508193507746374e-15 \n",
            "     | > step_time: 2.44530  (2.44602)\n",
            "     | > loader_time: 0.00240  (0.00270)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00157 \u001b[0m(-0.00006)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[91m 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00003 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 81/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 10:33:40) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/263 -- GLOBAL_STEP: 21400\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09770  (1.06214)\n",
            "     | > G_mse_fake_loss: 0.99350  (0.99784)\n",
            "     | > G_feat_match_loss: 0.16341  (0.16388)\n",
            "     | > G_gen_loss: 49.39635  (47.79610)\n",
            "     | > G_adv_loss: 2.62765  (2.63662)\n",
            "     | > loss_0: 52.02400  (50.43272)\n",
            "     | > grad_norm_0: 459.42606  (497.74084)\n",
            "     | > D_mse_gan_loss: 0.00022  (0.00035)\n",
            "     | > D_mse_gan_real_loss: 6.006057446938939e-07  (2.4663361983812138e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5521877685387153e-06  (1.890055671083246e-06)\n",
            "     | > loss_1: 0.00022  (0.00035)\n",
            "     | > grad_norm_1: 4.99547  (4.99547)\n",
            "     | > current_lr_0: 5.023582330811109e-15 \n",
            "     | > current_lr_1: 5.023582330811109e-15 \n",
            "     | > step_time: 2.43290  (2.48295)\n",
            "     | > loader_time: 0.00350  (0.00363)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 41/263 -- GLOBAL_STEP: 21425\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07243  (1.06091)\n",
            "     | > G_mse_fake_loss: 0.99776  (0.99850)\n",
            "     | > G_feat_match_loss: 0.16389  (0.16394)\n",
            "     | > G_gen_loss: 48.25941  (47.74097)\n",
            "     | > G_adv_loss: 2.63670  (2.63789)\n",
            "     | > loss_0: 50.89611  (50.37887)\n",
            "     | > grad_norm_0: 732.18396  (485.03342)\n",
            "     | > D_mse_gan_loss: 0.00044  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 8.249920711023151e-07  (2.5714964674701082e-06)\n",
            "     | > D_mse_gan_fake_loss: 3.0440401133091655e-06  (1.8646262394279957e-06)\n",
            "     | > loss_1: 0.00044  (0.00034)\n",
            "     | > grad_norm_1: 4.99818  (4.99514)\n",
            "     | > current_lr_0: 4.899488356283014e-15 \n",
            "     | > current_lr_1: 4.899488356283014e-15 \n",
            "     | > step_time: 2.43590  (2.46036)\n",
            "     | > loader_time: 0.00290  (0.00301)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 66/263 -- GLOBAL_STEP: 21450\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02170  (1.06131)\n",
            "     | > G_mse_fake_loss: 1.00169  (0.99847)\n",
            "     | > G_feat_match_loss: 0.16432  (0.16392)\n",
            "     | > G_gen_loss: 45.97663  (47.75913)\n",
            "     | > G_adv_loss: 2.64489  (2.63771)\n",
            "     | > loss_0: 48.62152  (50.39684)\n",
            "     | > grad_norm_0: 368.93497  (485.37671)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 2.448713757985388e-06  (2.3292035578923316e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6080105069704587e-06  (1.8738083612692153e-06)\n",
            "     | > loss_1: 0.00030  (0.00033)\n",
            "     | > grad_norm_1: 4.99290  (4.99522)\n",
            "     | > current_lr_0: 4.778459786778688e-15 \n",
            "     | > current_lr_1: 4.778459786778688e-15 \n",
            "     | > step_time: 2.44390  (2.45292)\n",
            "     | > loader_time: 0.00270  (0.00284)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 91/263 -- GLOBAL_STEP: 21475\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01250  (1.06198)\n",
            "     | > G_mse_fake_loss: 1.00537  (0.99851)\n",
            "     | > G_feat_match_loss: 0.16467  (0.16395)\n",
            "     | > G_gen_loss: 45.56269  (47.78897)\n",
            "     | > G_adv_loss: 2.65212  (2.63799)\n",
            "     | > loss_0: 48.21481  (50.42696)\n",
            "     | > grad_norm_0: 315.37128  (486.34592)\n",
            "     | > D_mse_gan_loss: 0.00028  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 2.711079559958307e-06  (2.8755045353956197e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6277473378067953e-06  (1.910171285849324e-06)\n",
            "     | > loss_1: 0.00028  (0.00033)\n",
            "     | > grad_norm_1: 4.99086  (4.99526)\n",
            "     | > current_lr_0: 4.6604208997822254e-15 \n",
            "     | > current_lr_1: 4.6604208997822254e-15 \n",
            "     | > step_time: 2.44620  (2.44956)\n",
            "     | > loader_time: 0.00240  (0.00279)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 116/263 -- GLOBAL_STEP: 21500\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.13010  (1.06116)\n",
            "     | > G_mse_fake_loss: 0.99884  (0.99852)\n",
            "     | > G_feat_match_loss: 0.16389  (0.16394)\n",
            "     | > G_gen_loss: 50.85446  (47.75213)\n",
            "     | > G_adv_loss: 2.63778  (2.63788)\n",
            "     | > loss_0: 53.49224  (50.39001)\n",
            "     | > grad_norm_0: 443.08047  (484.15314)\n",
            "     | > D_mse_gan_loss: 0.00043  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.652456148804049e-06  (2.8403705936229604e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.46239619627886e-06  (1.8760054494993607e-06)\n",
            "     | > loss_1: 0.00043  (0.00032)\n",
            "     | > grad_norm_1: 4.99490  (4.99518)\n",
            "     | > current_lr_0: 4.545297843297073e-15 \n",
            "     | > current_lr_1: 4.545297843297073e-15 \n",
            "     | > step_time: 2.43300  (2.44764)\n",
            "     | > loader_time: 0.00270  (0.00273)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 141/263 -- GLOBAL_STEP: 21525\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00656  (1.06098)\n",
            "     | > G_mse_fake_loss: 1.00110  (0.99865)\n",
            "     | > G_feat_match_loss: 0.16411  (0.16395)\n",
            "     | > G_gen_loss: 45.29514  (47.74413)\n",
            "     | > G_adv_loss: 2.64222  (2.63818)\n",
            "     | > loss_0: 47.93736  (50.38231)\n",
            "     | > grad_norm_0: 471.99289  (482.19363)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 1.7261378388866433e-06  (1.888608368701286e-06)\n",
            "     | > loss_1: 0.00023  (0.00032)\n",
            "     | > grad_norm_1: 4.99568  (4.99522)\n",
            "     | > current_lr_0: 4.4330185896399235e-15 \n",
            "     | > current_lr_1: 4.4330185896399235e-15 \n",
            "     | > step_time: 2.47970  (2.44688)\n",
            "     | > loader_time: 0.00270  (0.00272)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 166/263 -- GLOBAL_STEP: 21550\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03304  (1.05951)\n",
            "     | > G_mse_fake_loss: 1.00065  (0.99871)\n",
            "     | > G_feat_match_loss: 0.16414  (0.16396)\n",
            "     | > G_gen_loss: 46.48696  (47.67811)\n",
            "     | > G_adv_loss: 2.64208  (2.63827)\n",
            "     | > loss_0: 49.12904  (50.31638)\n",
            "     | > grad_norm_0: 415.80167  (478.80603)\n",
            "     | > D_mse_gan_loss: 0.00027  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 2.0134850728936726e-06  (2.7007496323087616e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.626668904464168e-06  (1.8719495676714902e-06)\n",
            "     | > loss_1: 0.00027  (0.00032)\n",
            "     | > grad_norm_1: 4.99437  (4.99513)\n",
            "     | > current_lr_0: 4.323512890376003e-15 \n",
            "     | > current_lr_1: 4.323512890376003e-15 \n",
            "     | > step_time: 2.43710  (2.44633)\n",
            "     | > loader_time: 0.00250  (0.00269)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 191/263 -- GLOBAL_STEP: 21575\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05000  (1.05835)\n",
            "     | > G_mse_fake_loss: 0.99770  (0.99868)\n",
            "     | > G_feat_match_loss: 0.16381  (0.16395)\n",
            "     | > G_gen_loss: 47.25004  (47.62572)\n",
            "     | > G_adv_loss: 2.63577  (2.63819)\n",
            "     | > loss_0: 49.88581  (50.26391)\n",
            "     | > grad_norm_0: 427.09415  (477.12173)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 7.822079624020262e-07  (2.617217108901815e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7470251805207226e-06  (1.8599080487347326e-06)\n",
            "     | > loss_1: 0.00024  (0.00032)\n",
            "     | > grad_norm_1: 4.99459  (4.99514)\n",
            "     | > current_lr_0: 4.21671223236756e-15 \n",
            "     | > current_lr_1: 4.21671223236756e-15 \n",
            "     | > step_time: 2.44620  (2.44554)\n",
            "     | > loader_time: 0.00240  (0.00268)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 216/263 -- GLOBAL_STEP: 21600\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02030  (1.05831)\n",
            "     | > G_mse_fake_loss: 0.99708  (0.99868)\n",
            "     | > G_feat_match_loss: 0.16368  (0.16395)\n",
            "     | > G_gen_loss: 45.91370  (47.62386)\n",
            "     | > G_adv_loss: 2.63388  (2.63818)\n",
            "     | > loss_0: 48.54759  (50.26204)\n",
            "     | > grad_norm_0: 629.33246  (476.87161)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.3168089481041534e-06  (2.5729715119645646e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.8115584907718585e-06  (1.8694757949754564e-06)\n",
            "     | > loss_1: 0.00024  (0.00032)\n",
            "     | > grad_norm_1: 4.99755  (4.99513)\n",
            "     | > current_lr_0: 4.112549794908069e-15 \n",
            "     | > current_lr_1: 4.112549794908069e-15 \n",
            "     | > step_time: 2.43620  (2.44476)\n",
            "     | > loader_time: 0.00220  (0.00267)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 241/263 -- GLOBAL_STEP: 21625\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06522  (1.05918)\n",
            "     | > G_mse_fake_loss: 0.99748  (0.99870)\n",
            "     | > G_feat_match_loss: 0.16378  (0.16396)\n",
            "     | > G_gen_loss: 47.93476  (47.66304)\n",
            "     | > G_adv_loss: 2.63524  (2.63831)\n",
            "     | > loss_0: 50.57001  (50.30135)\n",
            "     | > grad_norm_0: 456.27771  (476.36554)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.0950237765428028e-06  (2.745716547322338e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.567824028825271e-06  (1.8848615937785174e-06)\n",
            "     | > loss_1: 0.00023  (0.00033)\n",
            "     | > grad_norm_1: 4.99529  (4.99516)\n",
            "     | > current_lr_0: 4.0109604079152914e-15 \n",
            "     | > current_lr_1: 4.0109604079152914e-15 \n",
            "     | > step_time: 2.43410  (2.44412)\n",
            "     | > loader_time: 0.00280  (0.00267)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00174 \u001b[0m(+0.00017)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 1.09853 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 49.43375 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 52.23593 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss: 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 82/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 10:44:38) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/263 -- GLOBAL_STEP: 21650\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04366  (1.01616)\n",
            "     | > G_mse_fake_loss: 0.99457  (0.99615)\n",
            "     | > G_feat_match_loss: 0.16351  (0.16359)\n",
            "     | > G_gen_loss: 46.96455  (45.72703)\n",
            "     | > G_adv_loss: 2.62967  (2.63207)\n",
            "     | > loss_0: 49.59422  (48.35910)\n",
            "     | > grad_norm_0: 413.28772  (449.73706)\n",
            "     | > D_mse_gan_loss: 0.00016  (0.00015)\n",
            "     | > D_mse_gan_real_loss: 2.6259331775690953e-07  (7.959241230537373e-07)\n",
            "     | > D_mse_gan_fake_loss: 8.328651688316313e-07  (8.654685075271118e-07)\n",
            "     | > loss_1: 0.00016  (0.00015)\n",
            "     | > grad_norm_1: 4.99442  (4.99517)\n",
            "     | > current_lr_0: 3.9118805111570995e-15 \n",
            "     | > current_lr_1: 3.9118805111570995e-15 \n",
            "     | > step_time: 2.43790  (2.58829)\n",
            "     | > loader_time: 0.00090  (0.00096)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 27/263 -- GLOBAL_STEP: 21675\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08694  (1.06927)\n",
            "     | > G_mse_fake_loss: 0.99425  (0.99567)\n",
            "     | > G_feat_match_loss: 0.16355  (0.16366)\n",
            "     | > G_gen_loss: 48.91210  (48.11711)\n",
            "     | > G_adv_loss: 2.62973  (2.63227)\n",
            "     | > loss_0: 51.54183  (50.74939)\n",
            "     | > grad_norm_0: 650.02795  (542.10278)\n",
            "     | > D_mse_gan_loss: 0.00034  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 4.758098839374725e-06  (2.0526608371559027e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.2761103082302725e-06  (1.9347785698502976e-06)\n",
            "     | > loss_1: 0.00034  (0.00028)\n",
            "     | > grad_norm_1: 4.99779  (4.99625)\n",
            "     | > current_lr_0: 3.815248114484485e-15 \n",
            "     | > current_lr_1: 3.815248114484485e-15 \n",
            "     | > step_time: 2.43550  (2.44895)\n",
            "     | > loader_time: 0.00230  (0.00240)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 52/263 -- GLOBAL_STEP: 21700\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07640  (1.07123)\n",
            "     | > G_mse_fake_loss: 0.99429  (0.99571)\n",
            "     | > G_feat_match_loss: 0.16348  (0.16364)\n",
            "     | > G_gen_loss: 48.43781  (48.20551)\n",
            "     | > G_adv_loss: 2.62911  (2.63215)\n",
            "     | > loss_0: 51.06692  (50.83766)\n",
            "     | > grad_norm_0: 468.17557  (533.73938)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00027)\n",
            "     | > D_mse_gan_real_loss: 5.297512188917608e-07  (2.2301859548161488e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.4201334579411196e-06  (1.8632504584012654e-06)\n",
            "     | > loss_1: 0.00023  (0.00027)\n",
            "     | > grad_norm_1: 4.99568  (4.99617)\n",
            "     | > current_lr_0: 3.721002759046912e-15 \n",
            "     | > current_lr_1: 3.721002759046912e-15 \n",
            "     | > step_time: 2.44450  (2.44482)\n",
            "     | > loader_time: 0.00250  (0.00246)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 77/263 -- GLOBAL_STEP: 21725\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.14514  (1.07426)\n",
            "     | > G_mse_fake_loss: 0.99185  (0.99571)\n",
            "     | > G_feat_match_loss: 0.16340  (0.16365)\n",
            "     | > G_gen_loss: 51.53124  (48.34157)\n",
            "     | > G_adv_loss: 2.62586  (2.63219)\n",
            "     | > loss_0: 54.15710  (50.97376)\n",
            "     | > grad_norm_0: 673.03394  (543.10596)\n",
            "     | > D_mse_gan_loss: 0.00034  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 4.96965367347002e-06  (2.1799462033029994e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.3752179458824685e-06  (1.9096698028161794e-06)\n",
            "     | > loss_1: 0.00034  (0.00028)\n",
            "     | > grad_norm_1: 4.99826  (4.99630)\n",
            "     | > current_lr_0: 3.6290854794657534e-15 \n",
            "     | > current_lr_1: 3.6290854794657534e-15 \n",
            "     | > step_time: 2.43820  (2.44327)\n",
            "     | > loader_time: 0.00240  (0.00252)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 102/263 -- GLOBAL_STEP: 21750\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07575  (1.07434)\n",
            "     | > G_mse_fake_loss: 0.99678  (0.99562)\n",
            "     | > G_feat_match_loss: 0.16376  (0.16364)\n",
            "     | > G_gen_loss: 48.40886  (48.34548)\n",
            "     | > G_adv_loss: 2.63437  (2.63204)\n",
            "     | > loss_0: 51.04323  (50.97751)\n",
            "     | > grad_norm_0: 459.36819  (547.43823)\n",
            "     | > D_mse_gan_loss: 0.00027  (0.00027)\n",
            "     | > D_mse_gan_real_loss: 1.7316347111773212e-06  (1.942173804369465e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.1314958758011926e-06  (1.923836315003713e-06)\n",
            "     | > loss_1: 0.00027  (0.00027)\n",
            "     | > grad_norm_1: 4.99529  (4.99639)\n",
            "     | > current_lr_0: 3.539438766942106e-15 \n",
            "     | > current_lr_1: 3.539438766942106e-15 \n",
            "     | > step_time: 2.43430  (2.44304)\n",
            "     | > loader_time: 0.00210  (0.00253)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 127/263 -- GLOBAL_STEP: 21775\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08631  (1.07259)\n",
            "     | > G_mse_fake_loss: 0.99236  (0.99573)\n",
            "     | > G_feat_match_loss: 0.16335  (0.16365)\n",
            "     | > G_gen_loss: 48.88414  (48.26634)\n",
            "     | > G_adv_loss: 2.62591  (2.63225)\n",
            "     | > loss_0: 51.51006  (50.89859)\n",
            "     | > grad_norm_0: 656.39331  (543.05005)\n",
            "     | > D_mse_gan_loss: 0.00021  (0.00027)\n",
            "     | > D_mse_gan_real_loss: 9.218765626428649e-07  (1.9722745719711507e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.3055636120261624e-06  (1.909052998133292e-06)\n",
            "     | > loss_1: 0.00021  (0.00027)\n",
            "     | > grad_norm_1: 4.99812  (4.99634)\n",
            "     | > current_lr_0: 3.452006533275947e-15 \n",
            "     | > current_lr_1: 3.452006533275947e-15 \n",
            "     | > step_time: 2.43690  (2.44248)\n",
            "     | > loader_time: 0.00240  (0.00253)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 152/263 -- GLOBAL_STEP: 21800\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09824  (1.07094)\n",
            "     | > G_mse_fake_loss: 0.99221  (0.99568)\n",
            "     | > G_feat_match_loss: 0.16343  (0.16365)\n",
            "     | > G_gen_loss: 49.42063  (48.19244)\n",
            "     | > G_adv_loss: 2.62655  (2.63216)\n",
            "     | > loss_0: 52.04718  (50.82460)\n",
            "     | > grad_norm_0: 612.68109  (542.88989)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00027)\n",
            "     | > D_mse_gan_real_loss: 9.526228836875816e-07  (1.924639311268768e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.122959358530352e-06  (1.8904274120838015e-06)\n",
            "     | > loss_1: 0.00023  (0.00027)\n",
            "     | > grad_norm_1: 4.99800  (4.99632)\n",
            "     | > current_lr_0: 3.3667340757740917e-15 \n",
            "     | > current_lr_1: 3.3667340757740917e-15 \n",
            "     | > step_time: 2.44840  (2.44248)\n",
            "     | > loader_time: 0.00280  (0.00254)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 177/263 -- GLOBAL_STEP: 21825\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09392  (1.07212)\n",
            "     | > G_mse_fake_loss: 0.99165  (0.99558)\n",
            "     | > G_feat_match_loss: 0.16333  (0.16364)\n",
            "     | > G_gen_loss: 49.22629  (48.24527)\n",
            "     | > G_adv_loss: 2.62496  (2.63200)\n",
            "     | > loss_0: 51.85125  (50.87727)\n",
            "     | > grad_norm_0: 655.51215  (546.77051)\n",
            "     | > D_mse_gan_loss: 0.00018  (0.00027)\n",
            "     | > D_mse_gan_real_loss: 3.4749828614621947e-07  (1.8721406003656882e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6979192878352478e-06  (1.908078651323929e-06)\n",
            "     | > loss_1: 0.00018  (0.00027)\n",
            "     | > grad_norm_1: 4.99845  (4.99638)\n",
            "     | > current_lr_0: 3.283568043025004e-15 \n",
            "     | > current_lr_1: 3.283568043025004e-15 \n",
            "     | > step_time: 2.45460  (2.44239)\n",
            "     | > loader_time: 0.00250  (0.00256)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 202/263 -- GLOBAL_STEP: 21850\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05776  (1.07127)\n",
            "     | > G_mse_fake_loss: 0.99990  (0.99560)\n",
            "     | > G_feat_match_loss: 0.16390  (0.16364)\n",
            "     | > G_gen_loss: 47.59910  (48.20723)\n",
            "     | > G_adv_loss: 2.63888  (2.63196)\n",
            "     | > loss_0: 50.23798  (50.83919)\n",
            "     | > grad_norm_0: 355.29645  (545.82855)\n",
            "     | > D_mse_gan_loss: 0.00034  (0.00027)\n",
            "     | > D_mse_gan_real_loss: 2.496573642929434e-06  (2.055837664488085e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.8357827684667427e-06  (1.9141294932533045e-06)\n",
            "     | > loss_1: 0.00034  (0.00027)\n",
            "     | > grad_norm_1: 4.99258  (4.99636)\n",
            "     | > current_lr_0: 3.2024564015190478e-15 \n",
            "     | > current_lr_1: 3.2024564015190478e-15 \n",
            "     | > step_time: 2.43680  (2.44226)\n",
            "     | > loader_time: 0.00230  (0.00255)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 227/263 -- GLOBAL_STEP: 21875\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.09290  (1.07166)\n",
            "     | > G_mse_fake_loss: 0.99838  (0.99555)\n",
            "     | > G_feat_match_loss: 0.16373  (0.16363)\n",
            "     | > G_gen_loss: 49.18040  (48.22479)\n",
            "     | > G_adv_loss: 2.63572  (2.63190)\n",
            "     | > loss_0: 51.81612  (50.85668)\n",
            "     | > grad_norm_0: 505.44482  (551.28650)\n",
            "     | > D_mse_gan_loss: 0.00034  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 1.7762907873475342e-06  (2.065095952880435e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.392576334386831e-06  (1.9263242992192615e-06)\n",
            "     | > loss_1: 0.00034  (0.00028)\n",
            "     | > grad_norm_1: 4.99626  (4.99641)\n",
            "     | > current_lr_0: 3.123348403093296e-15 \n",
            "     | > current_lr_1: 3.123348403093296e-15 \n",
            "     | > step_time: 2.45210  (2.44192)\n",
            "     | > loader_time: 0.00230  (0.00255)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 252/263 -- GLOBAL_STEP: 21900\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08813  (1.07218)\n",
            "     | > G_mse_fake_loss: 0.99403  (0.99554)\n",
            "     | > G_feat_match_loss: 0.16354  (0.16363)\n",
            "     | > G_gen_loss: 48.96583  (48.24809)\n",
            "     | > G_adv_loss: 2.62939  (2.63188)\n",
            "     | > loss_0: 51.59521  (50.87997)\n",
            "     | > grad_norm_0: 544.63062  (550.50818)\n",
            "     | > D_mse_gan_loss: 0.00028  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 1.551145260236808e-06  (2.0284700621713228e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.247919155706768e-06  (1.933380579047814e-06)\n",
            "     | > loss_1: 0.00028  (0.00028)\n",
            "     | > grad_norm_1: 4.99691  (4.99640)\n",
            "     | > current_lr_0: 3.04619455318053e-15 \n",
            "     | > current_lr_1: 3.04619455318053e-15 \n",
            "     | > step_time: 2.45040  (2.44192)\n",
            "     | > loader_time: 0.00230  (0.00255)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00143 \u001b[0m(-0.00032)\n",
            "     | > avg_G_l1_spec_loss: 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss: 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0: 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss: 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 83/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 10:55:37) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 13/263 -- GLOBAL_STEP: 21925\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05106  (1.06892)\n",
            "     | > G_mse_fake_loss: 1.00412  (0.99832)\n",
            "     | > G_feat_match_loss: 0.16454  (0.16397)\n",
            "     | > G_gen_loss: 47.29773  (48.10123)\n",
            "     | > G_adv_loss: 2.64950  (2.63798)\n",
            "     | > loss_0: 49.94723  (50.73921)\n",
            "     | > grad_norm_0: 360.59473  (538.76855)\n",
            "     | > D_mse_gan_loss: 0.00049  (0.00035)\n",
            "     | > D_mse_gan_real_loss: 3.499827926134458e-06  (4.268022048738483e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.783250127118663e-06  (2.038471953150852e-06)\n",
            "     | > loss_1: 0.00049  (0.00035)\n",
            "     | > grad_norm_1: 4.99274  (4.99493)\n",
            "     | > current_lr_0: 2.9709465798425533e-15 \n",
            "     | > current_lr_1: 2.9709465798425533e-15 \n",
            "     | > step_time: 2.44380  (2.52062)\n",
            "     | > loader_time: 0.00100  (0.00539)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 38/263 -- GLOBAL_STEP: 21950\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02671  (1.06676)\n",
            "     | > G_mse_fake_loss: 0.99788  (0.99796)\n",
            "     | > G_feat_match_loss: 0.16378  (0.16392)\n",
            "     | > G_gen_loss: 46.20216  (48.00426)\n",
            "     | > G_adv_loss: 2.63567  (2.63713)\n",
            "     | > loss_0: 48.83783  (50.64139)\n",
            "     | > grad_norm_0: 415.39099  (559.40765)\n",
            "     | > D_mse_gan_loss: 0.00023  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 2.005168653340661e-06  (2.909948055898176e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.503824933024589e-06  (2.0379248948493596e-06)\n",
            "     | > loss_1: 0.00023  (0.00032)\n",
            "     | > grad_norm_1: 4.99425  (4.99520)\n",
            "     | > current_lr_0: 2.89755740356846e-15 \n",
            "     | > current_lr_1: 2.89755740356846e-15 \n",
            "     | > step_time: 2.44740  (2.46889)\n",
            "     | > loader_time: 0.00260  (0.00348)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 63/263 -- GLOBAL_STEP: 21975\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.22997  (1.07054)\n",
            "     | > G_mse_fake_loss: 0.98507  (0.99782)\n",
            "     | > G_feat_match_loss: 0.16276  (0.16389)\n",
            "     | > G_gen_loss: 55.34843  (48.17436)\n",
            "     | > G_adv_loss: 2.61266  (2.63677)\n",
            "     | > loss_0: 57.96109  (50.81112)\n",
            "     | > grad_norm_0: 1578.59094  (583.38397)\n",
            "     | > D_mse_gan_loss: 0.00068  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 6.50810477509367e-07  (3.715584902750827e-06)\n",
            "     | > D_mse_gan_fake_loss: 3.853537691611564e-06  (2.100485195865437e-06)\n",
            "     | > loss_1: 0.00068  (0.00034)\n",
            "     | > grad_norm_1: 5.00184  (4.99538)\n",
            "     | > current_lr_0: 2.8259811078189554e-15 \n",
            "     | > current_lr_1: 2.8259811078189554e-15 \n",
            "     | > step_time: 2.45100  (2.45752)\n",
            "     | > loader_time: 0.00260  (0.00316)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/263 -- GLOBAL_STEP: 22000\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.99007  (1.06813)\n",
            "     | > G_mse_fake_loss: 1.00180  (0.99809)\n",
            "     | > G_feat_match_loss: 0.16421  (0.16392)\n",
            "     | > G_gen_loss: 44.55298  (48.06578)\n",
            "     | > G_adv_loss: 2.64388  (2.63726)\n",
            "     | > loss_0: 47.19686  (50.70304)\n",
            "     | > grad_norm_0: 377.83035  (574.83380)\n",
            "     | > D_mse_gan_loss: 0.00016  (0.00035)\n",
            "     | > D_mse_gan_real_loss: 1.488953671469062e-06  (3.4510337430566036e-06)\n",
            "     | > D_mse_gan_fake_loss: 8.970305884759e-07  (2.0998487667870074e-06)\n",
            "     | > loss_1: 0.00016  (0.00035)\n",
            "     | > grad_norm_1: 4.99345  (4.99515)\n",
            "     | > current_lr_0: 2.756172910298295e-15 \n",
            "     | > current_lr_1: 2.756172910298295e-15 \n",
            "     | > step_time: 2.44830  (2.45220)\n",
            "     | > loader_time: 0.00290  (0.00300)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 113/263 -- GLOBAL_STEP: 22025\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03210  (1.06629)\n",
            "     | > G_mse_fake_loss: 0.99988  (0.99815)\n",
            "     | > G_feat_match_loss: 0.16394  (0.16392)\n",
            "     | > G_gen_loss: 46.44467  (47.98285)\n",
            "     | > G_adv_loss: 2.63931  (2.63736)\n",
            "     | > loss_0: 49.08398  (50.62021)\n",
            "     | > grad_norm_0: 393.28568  (566.71240)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 2.6896548206423176e-06  (3.1887874650185475e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.662238901189994e-06  (2.0487908010439417e-06)\n",
            "     | > loss_1: 0.00025  (0.00034)\n",
            "     | > grad_norm_1: 4.99381  (4.99509)\n",
            "     | > current_lr_0: 2.688089134935872e-15 \n",
            "     | > current_lr_1: 2.688089134935872e-15 \n",
            "     | > step_time: 2.43280  (2.44887)\n",
            "     | > loader_time: 0.00250  (0.00293)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 138/263 -- GLOBAL_STEP: 22050\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02453  (1.06732)\n",
            "     | > G_mse_fake_loss: 1.00856  (0.99836)\n",
            "     | > G_feat_match_loss: 0.16488  (0.16394)\n",
            "     | > G_gen_loss: 46.10407  (48.02927)\n",
            "     | > G_adv_loss: 2.65737  (2.63772)\n",
            "     | > loss_0: 48.76145  (50.66699)\n",
            "     | > grad_norm_0: 307.45560  (569.26373)\n",
            "     | > D_mse_gan_loss: 0.00041  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 2.180617002522922e-06  (3.3066623250289817e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.5865165298455395e-06  (2.102038882949801e-06)\n",
            "     | > loss_1: 0.00041  (0.00034)\n",
            "     | > grad_norm_1: 4.99057  (4.99499)\n",
            "     | > current_lr_0: 2.62168718455993e-15 \n",
            "     | > current_lr_1: 2.62168718455993e-15 \n",
            "     | > step_time: 2.44650  (2.44691)\n",
            "     | > loader_time: 0.00280  (0.00287)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 163/263 -- GLOBAL_STEP: 22075\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03800  (1.06666)\n",
            "     | > G_mse_fake_loss: 0.99777  (0.99830)\n",
            "     | > G_feat_match_loss: 0.16403  (0.16393)\n",
            "     | > G_gen_loss: 46.70990  (47.99972)\n",
            "     | > G_adv_loss: 2.63805  (2.63760)\n",
            "     | > loss_0: 49.34795  (50.63733)\n",
            "     | > grad_norm_0: 541.49133  (569.89178)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 2.014282017626101e-06  (3.1094560063905698e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.0127729385421844e-06  (2.089665137947384e-06)\n",
            "     | > loss_1: 0.00025  (0.00034)\n",
            "     | > grad_norm_1: 4.99685  (4.99499)\n",
            "     | > current_lr_0: 2.5569255142462907e-15 \n",
            "     | > current_lr_1: 2.5569255142462907e-15 \n",
            "     | > step_time: 2.43340  (2.44564)\n",
            "     | > loader_time: 0.00240  (0.00282)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 188/263 -- GLOBAL_STEP: 22100\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01508  (1.06553)\n",
            "     | > G_mse_fake_loss: 1.00173  (0.99832)\n",
            "     | > G_feat_match_loss: 0.16421  (0.16393)\n",
            "     | > G_gen_loss: 45.67875  (47.94891)\n",
            "     | > G_adv_loss: 2.64380  (2.63758)\n",
            "     | > loss_0: 48.32254  (50.58649)\n",
            "     | > grad_norm_0: 366.48215  (571.05945)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 1.5474985275432118e-06  (2.0841334756906157e-06)\n",
            "     | > loss_1: 0.00024  (0.00034)\n",
            "     | > grad_norm_1: 4.99280  (4.99503)\n",
            "     | > current_lr_0: 2.4937636053254338e-15 \n",
            "     | > current_lr_1: 2.4937636053254338e-15 \n",
            "     | > step_time: 2.43070  (2.44463)\n",
            "     | > loader_time: 0.00250  (0.00278)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 213/263 -- GLOBAL_STEP: 22125\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.99889  (1.06599)\n",
            "     | > G_mse_fake_loss: 1.00642  (0.99832)\n",
            "     | > G_feat_match_loss: 0.16455  (0.16393)\n",
            "     | > G_gen_loss: 44.95019  (47.96946)\n",
            "     | > G_adv_loss: 2.65192  (2.63761)\n",
            "     | > loss_0: 47.60211  (50.60707)\n",
            "     | > grad_norm_0: 282.22186  (571.75427)\n",
            "     | > D_mse_gan_loss: 0.00035  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.627461983844114e-06  (2.8169959141973884e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7536621044200729e-06  (2.0982767376713135e-06)\n",
            "     | > loss_1: 0.00035  (0.00033)\n",
            "     | > grad_norm_1: 4.98887  (4.99503)\n",
            "     | > current_lr_0: 2.4321619400316595e-15 \n",
            "     | > current_lr_1: 2.4321619400316595e-15 \n",
            "     | > step_time: 2.42940  (2.44392)\n",
            "     | > loader_time: 0.00250  (0.00275)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 238/263 -- GLOBAL_STEP: 22150\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03528  (1.06646)\n",
            "     | > G_mse_fake_loss: 1.00315  (0.99834)\n",
            "     | > G_feat_match_loss: 0.16436  (0.16394)\n",
            "     | > G_gen_loss: 46.58777  (47.99058)\n",
            "     | > G_adv_loss: 2.64679  (2.63773)\n",
            "     | > loss_0: 49.23457  (50.62831)\n",
            "     | > grad_norm_0: 390.51123  (572.16333)\n",
            "     | > D_mse_gan_loss: 0.00032  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 2.832172867783811e-06  (2.922373698312231e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.946043994394131e-06  (2.10804006285286e-06)\n",
            "     | > loss_1: 0.00032  (0.00034)\n",
            "     | > grad_norm_1: 4.99378  (4.99510)\n",
            "     | > current_lr_0: 2.372081976778472e-15 \n",
            "     | > current_lr_1: 2.372081976778472e-15 \n",
            "     | > step_time: 2.44950  (2.44342)\n",
            "     | > loader_time: 0.00260  (0.00273)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 263/263 -- GLOBAL_STEP: 22175\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.30811  (1.06663)\n",
            "     | > G_mse_fake_loss: 0.98040  (0.99837)\n",
            "     | > G_feat_match_loss: 0.16226  (0.16394)\n",
            "     | > G_gen_loss: 58.86494  (47.99845)\n",
            "     | > G_adv_loss: 2.60300  (2.63779)\n",
            "     | > loss_0: 61.46795  (50.63624)\n",
            "     | > grad_norm_0: 5900.94873  (592.33038)\n",
            "     | > D_mse_gan_loss: 0.00042  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 6.150078490918531e-08  (2.9248956402232114e-06)\n",
            "     | > D_mse_gan_fake_loss: 3.5500777357810875e-06  (2.1192550341596584e-06)\n",
            "     | > loss_1: 0.00042  (0.00034)\n",
            "     | > grad_norm_1: 5.00358  (4.99509)\n",
            "     | > current_lr_0: 2.3134861260447243e-15 \n",
            "     | > current_lr_1: 2.3134861260447243e-15 \n",
            "     | > step_time: 0.48600  (2.43543)\n",
            "     | > loader_time: 0.00210  (0.00270)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00171 \u001b[0m(+0.00029)\n",
            "     | > avg_G_l1_spec_loss: 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.18010 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss: 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 2.80218 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0: 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.08590 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss: 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.08590 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 84/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 11:06:34) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 24/263 -- GLOBAL_STEP: 22200\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06557  (1.05614)\n",
            "     | > G_mse_fake_loss: 0.99621  (0.99932)\n",
            "     | > G_feat_match_loss: 0.16361  (0.16404)\n",
            "     | > G_gen_loss: 47.95083  (47.52616)\n",
            "     | > G_adv_loss: 2.63226  (2.63974)\n",
            "     | > loss_0: 50.58310  (50.16590)\n",
            "     | > grad_norm_0: 494.28925  (455.21478)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00030)\n",
            "     | > D_mse_gan_real_loss: 2.057047822745517e-06  (2.130323856874838e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6524860484423698e-06  (1.9324134825637884e-06)\n",
            "     | > loss_1: 0.00025  (0.00030)\n",
            "     | > grad_norm_1: 4.99599  (4.99480)\n",
            "     | > current_lr_0: 2.256337726856421e-15 \n",
            "     | > current_lr_1: 2.256337726856421e-15 \n",
            "     | > step_time: 2.43340  (2.45343)\n",
            "     | > loader_time: 0.00270  (0.03184)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 49/263 -- GLOBAL_STEP: 22225\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.01011  (1.05628)\n",
            "     | > G_mse_fake_loss: 1.00349  (0.99936)\n",
            "     | > G_feat_match_loss: 0.16450  (0.16404)\n",
            "     | > G_gen_loss: 45.45515  (47.53261)\n",
            "     | > G_adv_loss: 2.64852  (2.63974)\n",
            "     | > loss_0: 48.10367  (50.17234)\n",
            "     | > grad_norm_0: 328.06613  (452.36945)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 9.704284593681223e-07  (1.9108217958667095e-06)\n",
            "     | > loss_1: 0.00029  (0.00032)\n",
            "     | > grad_norm_1: 4.99134  (4.99461)\n",
            "     | > current_lr_0: 2.2006010238494875e-15 \n",
            "     | > current_lr_1: 2.2006010238494875e-15 \n",
            "     | > step_time: 2.44710  (2.44633)\n",
            "     | > loader_time: 0.00250  (0.01691)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 74/263 -- GLOBAL_STEP: 22250\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07948  (1.05993)\n",
            "     | > G_mse_fake_loss: 0.99626  (0.99931)\n",
            "     | > G_feat_match_loss: 0.16373  (0.16403)\n",
            "     | > G_gen_loss: 48.57675  (47.69682)\n",
            "     | > G_adv_loss: 2.63352  (2.63965)\n",
            "     | > loss_0: 51.21027  (50.33647)\n",
            "     | > grad_norm_0: 558.20605  (454.01700)\n",
            "     | > D_mse_gan_loss: 0.00032  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 3.385473291928065e-06  (2.166376384132712e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9869935385941062e-06  (1.944480752749891e-06)\n",
            "     | > loss_1: 0.00032  (0.00033)\n",
            "     | > grad_norm_1: 4.99699  (4.99467)\n",
            "     | > current_lr_0: 2.1462411448991238e-15 \n",
            "     | > current_lr_1: 2.1462411448991238e-15 \n",
            "     | > step_time: 2.42750  (2.44359)\n",
            "     | > loader_time: 0.00240  (0.01206)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 99/263 -- GLOBAL_STEP: 22275\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04733  (1.05985)\n",
            "     | > G_mse_fake_loss: 0.99921  (0.99919)\n",
            "     | > G_feat_match_loss: 0.16389  (0.16402)\n",
            "     | > G_gen_loss: 47.12974  (47.69344)\n",
            "     | > G_adv_loss: 2.63810  (2.63938)\n",
            "     | > loss_0: 49.76784  (50.33282)\n",
            "     | > grad_norm_0: 530.40637  (456.23413)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 3.834215931419749e-06  (2.1575355244427977e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6916040976866498e-06  (1.9194857724290726e-06)\n",
            "     | > loss_1: 0.00024  (0.00032)\n",
            "     | > grad_norm_1: 4.99665  (4.99478)\n",
            "     | > current_lr_0: 2.093224079301782e-15 \n",
            "     | > current_lr_1: 2.093224079301782e-15 \n",
            "     | > step_time: 2.42740  (2.44195)\n",
            "     | > loader_time: 0.00210  (0.00965)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 124/263 -- GLOBAL_STEP: 22300\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.10130  (1.05957)\n",
            "     | > G_mse_fake_loss: 0.99471  (0.99927)\n",
            "     | > G_feat_match_loss: 0.16347  (0.16403)\n",
            "     | > G_gen_loss: 49.55831  (47.68085)\n",
            "     | > G_adv_loss: 2.62937  (2.63953)\n",
            "     | > loss_0: 52.18767  (50.32038)\n",
            "     | > grad_norm_0: 538.04932  (458.77756)\n",
            "     | > D_mse_gan_loss: 0.00028  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 2.098716549880919e-06  (2.2288709805978576e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.594640591269126e-06  (1.9492385207293456e-06)\n",
            "     | > loss_1: 0.00028  (0.00033)\n",
            "     | > grad_norm_1: 4.99675  (4.99482)\n",
            "     | > current_lr_0: 2.041516656496087e-15 \n",
            "     | > current_lr_1: 2.041516656496087e-15 \n",
            "     | > step_time: 2.43440  (2.44116)\n",
            "     | > loader_time: 0.00270  (0.00824)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 149/263 -- GLOBAL_STEP: 22325\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04386  (1.05852)\n",
            "     | > G_mse_fake_loss: 1.00123  (0.99935)\n",
            "     | > G_feat_match_loss: 0.16409  (0.16403)\n",
            "     | > G_gen_loss: 46.97370  (47.63361)\n",
            "     | > G_adv_loss: 2.64215  (2.63966)\n",
            "     | > loss_0: 49.61585  (50.27327)\n",
            "     | > grad_norm_0: 541.13123  (459.07358)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.4129235523796524e-06  (2.2990948597630126e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7347259699818096e-06  (1.94754761383934e-06)\n",
            "     | > loss_1: 0.00024  (0.00033)\n",
            "     | > grad_norm_1: 4.99687  (4.99489)\n",
            "     | > current_lr_0: 1.991086525309404e-15 \n",
            "     | > current_lr_1: 1.991086525309404e-15 \n",
            "     | > step_time: 2.45560  (2.44086)\n",
            "     | > loader_time: 0.00230  (0.00730)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 174/263 -- GLOBAL_STEP: 22350\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03182  (1.05791)\n",
            "     | > G_mse_fake_loss: 1.00002  (0.99924)\n",
            "     | > G_feat_match_loss: 0.16392  (0.16402)\n",
            "     | > G_gen_loss: 46.43206  (47.60613)\n",
            "     | > G_adv_loss: 2.63926  (2.63942)\n",
            "     | > loss_0: 49.07132  (50.24554)\n",
            "     | > grad_norm_0: 424.20465  (462.79050)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.197974597744178e-06  (2.3777904875819275e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3309626183399814e-06  (1.9532219091981718e-06)\n",
            "     | > loss_1: 0.00024  (0.00033)\n",
            "     | > grad_norm_1: 4.99469  (4.99496)\n",
            "     | > current_lr_0: 1.941902133717063e-15 \n",
            "     | > current_lr_1: 1.941902133717063e-15 \n",
            "     | > step_time: 2.43510  (2.44110)\n",
            "     | > loader_time: 0.00280  (0.00663)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 199/263 -- GLOBAL_STEP: 22375\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02133  (1.05679)\n",
            "     | > G_mse_fake_loss: 1.00239  (0.99930)\n",
            "     | > G_feat_match_loss: 0.16426  (0.16402)\n",
            "     | > G_gen_loss: 45.95979  (47.55536)\n",
            "     | > G_adv_loss: 2.64500  (2.63955)\n",
            "     | > loss_0: 48.60479  (50.19491)\n",
            "     | > grad_norm_0: 509.63733  (462.45480)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 1.216632767864212e-06  (2.3520943511583704e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.6993079725580174e-06  (1.941015228229062e-06)\n",
            "     | > loss_1: 0.00029  (0.00032)\n",
            "     | > grad_norm_1: 4.99635  (4.99496)\n",
            "     | > current_lr_0: 1.8939327091015757e-15 \n",
            "     | > current_lr_1: 1.8939327091015757e-15 \n",
            "     | > step_time: 2.43660  (2.44138)\n",
            "     | > loader_time: 0.00240  (0.00612)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 224/263 -- GLOBAL_STEP: 22400\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05401  (1.05747)\n",
            "     | > G_mse_fake_loss: 0.99238  (0.99919)\n",
            "     | > G_feat_match_loss: 0.16336  (0.16401)\n",
            "     | > G_gen_loss: 47.43059  (47.58609)\n",
            "     | > G_adv_loss: 2.62595  (2.63933)\n",
            "     | > loss_0: 50.05655  (50.22542)\n",
            "     | > grad_norm_0: 678.50714  (467.82553)\n",
            "     | > D_mse_gan_loss: 0.00018  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 4.1493717617413495e-07  (2.2977744611145975e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.714660356810782e-06  (1.949068485781628e-06)\n",
            "     | > loss_1: 0.00018  (0.00032)\n",
            "     | > grad_norm_1: 4.99823  (4.99500)\n",
            "     | > current_lr_0: 1.8471482389994944e-15 \n",
            "     | > current_lr_1: 1.8471482389994944e-15 \n",
            "     | > step_time: 2.42910  (2.44121)\n",
            "     | > loader_time: 0.00270  (0.00573)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 249/263 -- GLOBAL_STEP: 22425\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.10097  (1.05787)\n",
            "     | > G_mse_fake_loss: 1.00795  (0.99917)\n",
            "     | > G_feat_match_loss: 0.16503  (0.16401)\n",
            "     | > G_gen_loss: 49.54385  (47.60409)\n",
            "     | > G_adv_loss: 2.65825  (2.63928)\n",
            "     | > loss_0: 52.20210  (50.24337)\n",
            "     | > grad_norm_0: 384.11407  (470.01050)\n",
            "     | > D_mse_gan_loss: 0.00056  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00000)\n",
            "     | > D_mse_gan_fake_loss: 2.986990239151055e-06  (1.9522150371659633e-06)\n",
            "     | > loss_1: 0.00056  (0.00032)\n",
            "     | > grad_norm_1: 4.99464  (4.99505)\n",
            "     | > current_lr_0: 1.8015194523238685e-15 \n",
            "     | > current_lr_1: 1.8015194523238685e-15 \n",
            "     | > step_time: 2.43680  (2.44109)\n",
            "     | > loader_time: 0.00250  (0.00543)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00193 \u001b[0m(+0.00021)\n",
            "     | > avg_G_l1_spec_loss: 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.18010 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss: 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[92m 2.80218 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_0: 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss: 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 85/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 11:17:33) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/263 -- GLOBAL_STEP: 22450\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06678  (1.07701)\n",
            "     | > G_mse_fake_loss: 0.99417  (0.99805)\n",
            "     | > G_feat_match_loss: 0.16356  (0.16402)\n",
            "     | > G_gen_loss: 48.00516  (48.46538)\n",
            "     | > G_adv_loss: 2.62972  (2.63827)\n",
            "     | > loss_0: 50.63488  (51.10365)\n",
            "     | > grad_norm_0: 595.61108  (498.65097)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 8.858588671500911e-07  (3.4690046021523814e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.4470559765177313e-06  (2.0409663648024434e-06)\n",
            "     | > loss_1: 0.00025  (0.00033)\n",
            "     | > grad_norm_1: 4.99736  (4.99584)\n",
            "     | > current_lr_0: 1.7570178010505526e-15 \n",
            "     | > current_lr_1: 1.7570178010505526e-15 \n",
            "     | > step_time: 2.42720  (2.50193)\n",
            "     | > loader_time: 0.00090  (0.00858)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 35/263 -- GLOBAL_STEP: 22475\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08503  (1.06527)\n",
            "     | > G_mse_fake_loss: 0.99844  (0.99848)\n",
            "     | > G_feat_match_loss: 0.16425  (0.16400)\n",
            "     | > G_gen_loss: 48.82644  (47.93732)\n",
            "     | > G_adv_loss: 2.64092  (2.63850)\n",
            "     | > loss_0: 51.46736  (50.57583)\n",
            "     | > grad_norm_0: 434.63959  (519.22760)\n",
            "     | > D_mse_gan_loss: 0.00054  (0.00035)\n",
            "     | > D_mse_gan_real_loss: 3.5993207347928546e-06  (3.0073920689639114e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.130156190105481e-06  (2.1164350398196673e-06)\n",
            "     | > loss_1: 0.00054  (0.00035)\n",
            "     | > grad_norm_1: 4.99493  (4.99581)\n",
            "     | > current_lr_0: 1.713615442356897e-15 \n",
            "     | > current_lr_1: 1.713615442356897e-15 \n",
            "     | > step_time: 2.43270  (2.45744)\n",
            "     | > loader_time: 0.00290  (0.00420)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/263 -- GLOBAL_STEP: 22500\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.08344  (1.06114)\n",
            "     | > G_mse_fake_loss: 1.00036  (0.99880)\n",
            "     | > G_feat_match_loss: 0.16405  (0.16403)\n",
            "     | > G_gen_loss: 48.75471  (47.75110)\n",
            "     | > G_adv_loss: 2.64082  (2.63906)\n",
            "     | > loss_0: 51.39553  (50.39016)\n",
            "     | > grad_norm_0: 517.00415  (496.68826)\n",
            "     | > D_mse_gan_loss: 0.00033  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 2.9991299470566446e-06  (2.939012912861472e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.2725084818375763e-06  (1.979065389908405e-06)\n",
            "     | > loss_1: 0.00033  (0.00034)\n",
            "     | > grad_norm_1: 4.99662  (4.99554)\n",
            "     | > current_lr_0: 1.671285221201658e-15 \n",
            "     | > current_lr_1: 1.671285221201658e-15 \n",
            "     | > step_time: 2.43740  (2.44983)\n",
            "     | > loader_time: 0.00290  (0.00353)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 85/263 -- GLOBAL_STEP: 22525\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05997  (1.06134)\n",
            "     | > G_mse_fake_loss: 0.99549  (0.99872)\n",
            "     | > G_feat_match_loss: 0.16381  (0.16402)\n",
            "     | > G_gen_loss: 47.69869  (47.76041)\n",
            "     | > G_adv_loss: 2.63361  (2.63891)\n",
            "     | > loss_0: 50.33230  (50.39932)\n",
            "     | > grad_norm_0: 492.49069  (492.94681)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00034)\n",
            "     | > D_mse_gan_real_loss: 1.4808651940256823e-06  (2.9135493638718533e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.0778134057763964e-06  (1.9855357670488e-06)\n",
            "     | > loss_1: 0.00030  (0.00034)\n",
            "     | > grad_norm_1: 4.99614  (4.99542)\n",
            "     | > current_lr_0: 1.6300006533352267e-15 \n",
            "     | > current_lr_1: 1.6300006533352267e-15 \n",
            "     | > step_time: 2.46940  (2.44763)\n",
            "     | > loader_time: 0.00280  (0.00329)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 110/263 -- GLOBAL_STEP: 22550\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05354  (1.06102)\n",
            "     | > G_mse_fake_loss: 0.99621  (0.99854)\n",
            "     | > G_feat_match_loss: 0.16368  (0.16398)\n",
            "     | > G_gen_loss: 47.40934  (47.74604)\n",
            "     | > G_adv_loss: 2.63300  (2.63835)\n",
            "     | > loss_0: 50.04234  (50.38440)\n",
            "     | > grad_norm_0: 630.96771  (498.52652)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 9.447147704122472e-07  (2.679366513999329e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.025839648922556e-06  (1.979111951041803e-06)\n",
            "     | > loss_1: 0.00024  (0.00033)\n",
            "     | > grad_norm_1: 4.99754  (4.99547)\n",
            "     | > current_lr_0: 1.5897359087295378e-15 \n",
            "     | > current_lr_1: 1.5897359087295378e-15 \n",
            "     | > step_time: 2.43670  (2.44569)\n",
            "     | > loader_time: 0.00230  (0.00311)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 135/263 -- GLOBAL_STEP: 22575\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.05502  (1.06068)\n",
            "     | > G_mse_fake_loss: 0.99822  (0.99865)\n",
            "     | > G_feat_match_loss: 0.16390  (0.16398)\n",
            "     | > G_gen_loss: 47.47575  (47.73073)\n",
            "     | > G_adv_loss: 2.63723  (2.63843)\n",
            "     | > loss_0: 50.11298  (50.36916)\n",
            "     | > grad_norm_0: 487.19098  (496.94681)\n",
            "     | > D_mse_gan_loss: 0.00024  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.654021048125287e-06  (2.529305331748858e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.020819920289796e-06  (2.0010830469261597e-06)\n",
            "     | > loss_1: 0.00024  (0.00033)\n",
            "     | > grad_norm_1: 4.99586  (4.99547)\n",
            "     | > current_lr_0: 1.5504657954173053e-15 \n",
            "     | > current_lr_1: 1.5504657954173053e-15 \n",
            "     | > step_time: 2.44700  (2.44504)\n",
            "     | > loader_time: 0.00290  (0.00301)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 160/263 -- GLOBAL_STEP: 22600\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03853  (1.05977)\n",
            "     | > G_mse_fake_loss: 1.00744  (0.99863)\n",
            "     | > G_feat_match_loss: 0.16478  (0.16397)\n",
            "     | > G_gen_loss: 46.73388  (47.68975)\n",
            "     | > G_adv_loss: 2.65526  (2.63835)\n",
            "     | > loss_0: 49.38913  (50.32810)\n",
            "     | > grad_norm_0: 320.00610  (495.08496)\n",
            "     | > D_mse_gan_loss: 0.00033  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 3.111124669885612e-06  (2.4781526875017326e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.9038857317354996e-06  (1.9838996156096297e-06)\n",
            "     | > loss_1: 0.00033  (0.00033)\n",
            "     | > grad_norm_1: 4.99161  (4.99540)\n",
            "     | > current_lr_0: 1.5121657437304586e-15 \n",
            "     | > current_lr_1: 1.5121657437304586e-15 \n",
            "     | > step_time: 2.43720  (2.44374)\n",
            "     | > loader_time: 0.00240  (0.00293)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 185/263 -- GLOBAL_STEP: 22625\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02804  (1.05786)\n",
            "     | > G_mse_fake_loss: 1.00843  (0.99866)\n",
            "     | > G_feat_match_loss: 0.16500  (0.16397)\n",
            "     | > G_gen_loss: 46.26175  (47.60360)\n",
            "     | > G_adv_loss: 2.65847  (2.63835)\n",
            "     | > loss_0: 48.92022  (50.24195)\n",
            "     | > grad_norm_0: 336.13184  (492.53323)\n",
            "     | > D_mse_gan_loss: 0.00049  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 3.7818117561982945e-06  (2.390727643067883e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.126558001691592e-06  (1.9692458903517876e-06)\n",
            "     | > loss_1: 0.00049  (0.00033)\n",
            "     | > grad_norm_1: 4.99249  (4.99540)\n",
            "     | > current_lr_0: 1.4748117909279277e-15 \n",
            "     | > current_lr_1: 1.4748117909279277e-15 \n",
            "     | > step_time: 2.44840  (2.44299)\n",
            "     | > loader_time: 0.00330  (0.00289)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 210/263 -- GLOBAL_STEP: 22650\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04435  (1.05742)\n",
            "     | > G_mse_fake_loss: 0.99859  (0.99871)\n",
            "     | > G_feat_match_loss: 0.16389  (0.16397)\n",
            "     | > G_gen_loss: 46.99570  (47.58375)\n",
            "     | > G_adv_loss: 2.63748  (2.63840)\n",
            "     | > loss_0: 49.63317  (50.22214)\n",
            "     | > grad_norm_0: 495.85114  (488.70068)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00032)\n",
            "     | > D_mse_gan_real_loss: 2.7006697109754896e-06  (2.4046748612655772e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.4679090984136565e-06  (1.967158404102607e-06)\n",
            "     | > loss_1: 0.00030  (0.00032)\n",
            "     | > grad_norm_1: 4.99590  (4.99532)\n",
            "     | > current_lr_0: 1.4383805662031614e-15 \n",
            "     | > current_lr_1: 1.4383805662031614e-15 \n",
            "     | > step_time: 2.44100  (2.44243)\n",
            "     | > loader_time: 0.00260  (0.00285)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 235/263 -- GLOBAL_STEP: 22675\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07794  (1.05711)\n",
            "     | > G_mse_fake_loss: 0.99791  (0.99875)\n",
            "     | > G_feat_match_loss: 0.16401  (0.16397)\n",
            "     | > G_gen_loss: 48.50713  (47.56974)\n",
            "     | > G_adv_loss: 2.63803  (2.63849)\n",
            "     | > loss_0: 51.14516  (50.20823)\n",
            "     | > grad_norm_0: 444.23672  (486.71521)\n",
            "     | > D_mse_gan_loss: 0.00034  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 1.6591529856668785e-06  (2.4841732217156908e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.842162760112842e-06  (1.9619847010173857e-06)\n",
            "     | > loss_1: 0.00034  (0.00033)\n",
            "     | > grad_norm_1: 4.99514  (4.99532)\n",
            "     | > current_lr_0: 1.4028492760619885e-15 \n",
            "     | > current_lr_1: 1.4028492760619885e-15 \n",
            "     | > step_time: 2.45590  (2.44215)\n",
            "     | > loader_time: 0.00240  (0.00281)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 260/263 -- GLOBAL_STEP: 22700\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07953  (1.05718)\n",
            "     | > G_mse_fake_loss: 1.00234  (0.99874)\n",
            "     | > G_feat_match_loss: 0.16423  (0.16397)\n",
            "     | > G_gen_loss: 48.57867  (47.57323)\n",
            "     | > G_adv_loss: 2.64461  (2.63846)\n",
            "     | > loss_0: 51.22329  (50.21170)\n",
            "     | > grad_norm_0: 492.84741  (489.66116)\n",
            "     | > D_mse_gan_loss: 0.00039  (0.00033)\n",
            "     | > D_mse_gan_real_loss: 4.340874966146657e-06  (2.4667922610013604e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.3834652438381454e-06  (1.9772063014810412e-06)\n",
            "     | > loss_1: 0.00039  (0.00033)\n",
            "     | > grad_norm_1: 4.99628  (4.99537)\n",
            "     | > current_lr_0: 1.3681956900616807e-15 \n",
            "     | > current_lr_1: 1.3681956900616807e-15 \n",
            "     | > step_time: 2.43280  (2.44181)\n",
            "     | > loader_time: 0.00290  (0.00278)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00173 \u001b[0m(-0.00020)\n",
            "     | > avg_G_l1_spec_loss: 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss: 0.18010 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss: 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss: 2.80218 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0: 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss: 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 86/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 11:28:31) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 21/263 -- GLOBAL_STEP: 22725\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.99983  (1.05131)\n",
            "     | > G_mse_fake_loss: 0.99902  (0.99739)\n",
            "     | > G_feat_match_loss: 0.16392  (0.16383)\n",
            "     | > G_gen_loss: 44.99243  (47.30902)\n",
            "     | > G_adv_loss: 2.63826  (2.63572)\n",
            "     | > loss_0: 47.63069  (49.94474)\n",
            "     | > grad_norm_0: 523.31842  (496.91788)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00025)\n",
            "     | > D_mse_gan_real_loss: 1.3946038279755157e-06  (2.2500164288344206e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.4460709962804685e-06  (1.7469611450559613e-06)\n",
            "     | > loss_1: 0.00020  (0.00025)\n",
            "     | > grad_norm_1: 4.99639  (4.99509)\n",
            "     | > current_lr_0: 1.334398126902295e-15 \n",
            "     | > current_lr_1: 1.334398126902295e-15 \n",
            "     | > step_time: 2.43470  (2.47533)\n",
            "     | > loader_time: 0.00230  (0.00518)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 46/263 -- GLOBAL_STEP: 22750\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.07204  (1.05157)\n",
            "     | > G_mse_fake_loss: 0.99408  (0.99800)\n",
            "     | > G_feat_match_loss: 0.16340  (0.16390)\n",
            "     | > G_gen_loss: 48.24195  (47.32063)\n",
            "     | > G_adv_loss: 2.62806  (2.63696)\n",
            "     | > loss_0: 50.87002  (49.95759)\n",
            "     | > grad_norm_0: 580.73975  (478.38333)\n",
            "     | > D_mse_gan_loss: 0.00020  (0.00026)\n",
            "     | > D_mse_gan_real_loss: 8.316235948768735e-07  (2.9231456468521535e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.177639089495642e-06  (1.7860662205254376e-06)\n",
            "     | > loss_1: 0.00020  (0.00026)\n",
            "     | > grad_norm_1: 4.99708  (4.99478)\n",
            "     | > current_lr_0: 1.301435440861592e-15 \n",
            "     | > current_lr_1: 1.301435440861592e-15 \n",
            "     | > step_time: 2.43980  (2.45645)\n",
            "     | > loader_time: 0.00250  (0.00379)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 71/263 -- GLOBAL_STEP: 22775\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03124  (1.05609)\n",
            "     | > G_mse_fake_loss: 0.99795  (0.99799)\n",
            "     | > G_feat_match_loss: 0.16414  (0.16390)\n",
            "     | > G_gen_loss: 46.40586  (47.52423)\n",
            "     | > G_adv_loss: 2.63937  (2.63700)\n",
            "     | > loss_0: 49.04522  (50.16123)\n",
            "     | > grad_norm_0: 420.91296  (478.07504)\n",
            "     | > D_mse_gan_loss: 0.00022  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 9.02636031696602e-07  (2.5630150270297265e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.2528106481113355e-06  (1.8450328748002275e-06)\n",
            "     | > loss_1: 0.00022  (0.00029)\n",
            "     | > grad_norm_1: 4.99482  (4.99498)\n",
            "     | > current_lr_0: 1.26928700856504e-15 \n",
            "     | > current_lr_1: 1.26928700856504e-15 \n",
            "     | > step_time: 2.43470  (2.45038)\n",
            "     | > loader_time: 0.00230  (0.00335)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 96/263 -- GLOBAL_STEP: 22800\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06274  (1.05528)\n",
            "     | > G_mse_fake_loss: 1.00104  (0.99803)\n",
            "     | > G_feat_match_loss: 0.16451  (0.16389)\n",
            "     | > G_gen_loss: 47.82350  (47.48774)\n",
            "     | > G_adv_loss: 2.64614  (2.63692)\n",
            "     | > loss_0: 50.46964  (50.12466)\n",
            "     | > grad_norm_0: 407.79822  (477.56000)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 2.0243971903255442e-06  (2.5465954438530503e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.7821724895838997e-06  (1.849625495727499e-06)\n",
            "     | > loss_1: 0.00030  (0.00029)\n",
            "     | > grad_norm_1: 4.99433  (4.99504)\n",
            "     | > current_lr_0: 1.2379327160826318e-15 \n",
            "     | > current_lr_1: 1.2379327160826318e-15 \n",
            "     | > step_time: 2.43390  (2.44748)\n",
            "     | > loader_time: 0.00230  (0.00312)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 121/263 -- GLOBAL_STEP: 22825\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.12656  (1.05394)\n",
            "     | > G_mse_fake_loss: 0.98948  (0.99808)\n",
            "     | > G_feat_match_loss: 0.16299  (0.16388)\n",
            "     | > G_gen_loss: 50.69510  (47.42717)\n",
            "     | > G_adv_loss: 2.61940  (2.63693)\n",
            "     | > loss_0: 53.31450  (50.06410)\n",
            "     | > grad_norm_0: 840.57739  (476.64987)\n",
            "     | > D_mse_gan_loss: 0.00036  (0.00028)\n",
            "     | > D_mse_gan_real_loss: 1.2334422763160546e-06  (2.435267051463943e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.9590662506961962e-06  (1.8357993376186702e-06)\n",
            "     | > loss_1: 0.00036  (0.00028)\n",
            "     | > grad_norm_1: 4.99922  (4.99501)\n",
            "     | > current_lr_0: 1.2073529463444397e-15 \n",
            "     | > current_lr_1: 1.2073529463444397e-15 \n",
            "     | > step_time: 2.43830  (2.44528)\n",
            "     | > loader_time: 0.00260  (0.00302)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 146/263 -- GLOBAL_STEP: 22850\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.03376  (1.05232)\n",
            "     | > G_mse_fake_loss: 1.00040  (0.99820)\n",
            "     | > G_feat_match_loss: 0.16413  (0.16389)\n",
            "     | > G_gen_loss: 46.51910  (47.35454)\n",
            "     | > G_adv_loss: 2.64169  (2.63712)\n",
            "     | > loss_0: 49.16079  (49.99167)\n",
            "     | > grad_norm_0: 369.65967  (475.63986)\n",
            "     | > D_mse_gan_loss: 0.00068  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 3.5096452393190702e-06  (2.4359467959955084e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3988129694553209e-06  (1.8257036726500289e-06)\n",
            "     | > loss_1: 0.00068  (0.00029)\n",
            "     | > grad_norm_1: 4.99293  (4.99496)\n",
            "     | > current_lr_0: 1.1775285668670366e-15 \n",
            "     | > current_lr_1: 1.1775285668670366e-15 \n",
            "     | > step_time: 2.43070  (2.44389)\n",
            "     | > loader_time: 0.00240  (0.00295)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 171/263 -- GLOBAL_STEP: 22875\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04805  (1.05182)\n",
            "     | > G_mse_fake_loss: 0.99589  (0.99821)\n",
            "     | > G_feat_match_loss: 0.16372  (0.16389)\n",
            "     | > G_gen_loss: 47.16230  (47.33195)\n",
            "     | > G_adv_loss: 2.63313  (2.63715)\n",
            "     | > loss_0: 49.79543  (49.96910)\n",
            "     | > grad_norm_0: 719.14056  (473.90057)\n",
            "     | > D_mse_gan_loss: 0.00029  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.121963009609317e-06  (2.37489907973261e-06)\n",
            "     | > D_mse_gan_fake_loss: 2.384195568083669e-06  (1.8146593484685283e-06)\n",
            "     | > loss_1: 0.00029  (0.00029)\n",
            "     | > grad_norm_1: 4.99817  (4.99490)\n",
            "     | > current_lr_0: 1.1484409177830997e-15 \n",
            "     | > current_lr_1: 1.1484409177830997e-15 \n",
            "     | > step_time: 2.44540  (2.44296)\n",
            "     | > loader_time: 0.00270  (0.00289)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 196/263 -- GLOBAL_STEP: 22900\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.00302  (1.05082)\n",
            "     | > G_mse_fake_loss: 1.00315  (0.99824)\n",
            "     | > G_feat_match_loss: 0.16439  (0.16389)\n",
            "     | > G_gen_loss: 45.13611  (47.28671)\n",
            "     | > G_adv_loss: 2.64707  (2.63718)\n",
            "     | > loss_0: 47.78319  (49.92389)\n",
            "     | > grad_norm_0: 377.90958  (470.97073)\n",
            "     | > D_mse_gan_loss: 0.00030  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 9.68985773397435e-07  (2.317668115149352e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.3935745073467842e-06  (1.8024755858584043e-06)\n",
            "     | > loss_1: 0.00030  (0.00029)\n",
            "     | > grad_norm_1: 4.99333  (4.99486)\n",
            "     | > current_lr_0: 1.1200718001667101e-15 \n",
            "     | > current_lr_1: 1.1200718001667101e-15 \n",
            "     | > step_time: 2.45020  (2.44224)\n",
            "     | > loader_time: 0.00230  (0.00286)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 221/263 -- GLOBAL_STEP: 22925\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.02301  (1.05123)\n",
            "     | > G_mse_fake_loss: 0.99805  (0.99828)\n",
            "     | > G_feat_match_loss: 0.16384  (0.16390)\n",
            "     | > G_gen_loss: 46.03524  (47.30531)\n",
            "     | > G_adv_loss: 2.63641  (2.63725)\n",
            "     | > loss_0: 48.67165  (49.94256)\n",
            "     | > grad_norm_0: 503.33456  (472.04599)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.2874039612142951e-06  (2.2652853830729166e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.587854285389767e-06  (1.813878399300588e-06)\n",
            "     | > loss_1: 0.00025  (0.00029)\n",
            "     | > grad_norm_1: 4.99611  (4.99488)\n",
            "     | > current_lr_0: 1.0924034646470486e-15 \n",
            "     | > current_lr_1: 1.0924034646470486e-15 \n",
            "     | > step_time: 2.43440  (2.44174)\n",
            "     | > loader_time: 0.00230  (0.00284)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 246/263 -- GLOBAL_STEP: 22950\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.06198  (1.05162)\n",
            "     | > G_mse_fake_loss: 0.99443  (0.99825)\n",
            "     | > G_feat_match_loss: 0.16343  (0.16390)\n",
            "     | > G_gen_loss: 47.78922  (47.32270)\n",
            "     | > G_adv_loss: 2.62871  (2.63721)\n",
            "     | > loss_0: 50.41793  (49.95991)\n",
            "     | > grad_norm_0: 573.71558  (470.06546)\n",
            "     | > D_mse_gan_loss: 0.00025  (0.00029)\n",
            "     | > D_mse_gan_real_loss: 1.9579795207391726e-06  (2.4397837831625508e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.4800721146457363e-06  (1.8096087883091466e-06)\n",
            "     | > loss_1: 0.00025  (0.00029)\n",
            "     | > grad_norm_1: 4.99709  (4.99485)\n",
            "     | > current_lr_0: 1.0654186003033552e-15 \n",
            "     | > current_lr_1: 1.0654186003033552e-15 \n",
            "     | > step_time: 2.43600  (2.44130)\n",
            "     | > loader_time: 0.00270  (0.00282)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00175 \u001b[0m(+0.00002)\n",
            "     | > avg_G_l1_spec_loss: 1.09853 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 1.00113 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.18010 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss: 49.43375 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 2.80218 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0: 52.23593 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.08590 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.17106 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss: 0.00003 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1: 0.08590 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 87/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-03 11:39:28) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 7/263 -- GLOBAL_STEP: 22975\u001b[0m\n",
            "     | > G_l1_spec_loss: 1.04167  (1.06468)\n",
            "     | > G_mse_fake_loss: 0.99564  (0.99638)\n",
            "     | > G_feat_match_loss: 0.16359  (0.16374)\n",
            "     | > G_gen_loss: 46.87510  (47.91082)\n",
            "     | > G_adv_loss: 2.63150  (2.63376)\n",
            "     | > loss_0: 49.50660  (50.54459)\n",
            "     | > grad_norm_0: 648.08728  (523.41150)\n",
            "     | > D_mse_gan_loss: 0.00019  (0.00027)\n",
            "     | > D_mse_gan_real_loss: 6.191345391926006e-07  (1.7101195745843661e-06)\n",
            "     | > D_mse_gan_fake_loss: 1.5484803270737757e-06  (1.687999640255709e-06)\n",
            "     | > loss_1: 0.00019  (0.00027)\n",
            "     | > grad_norm_1: 4.99779  (4.99628)\n",
            "     | > current_lr_0: 1.0391003238342097e-15 \n",
            "     | > current_lr_1: 1.0391003238342097e-15 \n",
            "     | > step_time: 2.43650  (2.54165)\n",
            "     | > loader_time: 0.00320  (0.01342)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Emergent/train/hifigan/train_hifigan.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vknT6_fPWGLW",
        "outputId": "9605af4e-4eeb-4000-b83d-3c289d725e96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:1024\n",
            " | > power:1.5\n",
            " | > preemphasis:0.98\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:True\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000\n",
            " | > spec_gain:1.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60.0\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > stats_path:None\n",
            " | > base:2.718281828459045\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Generator Model: hifigan_generator\n",
            " > Discriminator Model: hifigan_discriminator\n",
            " > Using CUDA:  True\n",
            " > Number of GPUs:  1\n",
            " > Restoring from checkpoint_80000.pth.tar ...\n",
            " > Restoring Model...\n",
            " > Restoring Optimizer...\n",
            " > Model restored from step 80000\n",
            "\n",
            " > Model has 84660721 parameters\n",
            " > Restoring best loss from best_model_78712.pth.tar ...\n",
            " > Starting with loaded last best loss 31.50347328186035.\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 0/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-15 18:06:48) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/263 -- GLOBAL_STEP: 80000\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.79143  (0.79143)\n",
            "     | > G_mse_fake_loss: 0.99152  (0.99152)\n",
            "     | > G_feat_match_loss: 0.09847  (0.09847)\n",
            "     | > G_gen_loss: 35.61425  (35.61425)\n",
            "     | > G_adv_loss: 1.97620  (1.97620)\n",
            "     | > loss_0: 37.59045  (37.59045)\n",
            "     | > grad_norm_0: 2929.37158  (2929.37158)\n",
            "     | > D_mse_gan_loss: 0.29334  (0.29334)\n",
            "     | > D_mse_gan_real_loss: 0.00324  (0.00324)\n",
            "     | > D_mse_gan_fake_loss: 0.02726  (0.02726)\n",
            "     | > loss_1: 0.29334  (0.29334)\n",
            "     | > grad_norm_1: 290.85489  (290.85489)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 8.24180  (8.24182)\n",
            "     | > loader_time: 171.71810  (171.71814)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2/checkpoint_80000.pth.tar\n",
            "\n",
            "\u001b[1m   --> STEP: 25/263 -- GLOBAL_STEP: 80025\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.72180  (0.73610)\n",
            "     | > G_mse_fake_loss: 0.69329  (0.67378)\n",
            "     | > G_feat_match_loss: 0.08426  (0.07786)\n",
            "     | > G_gen_loss: 32.48089  (33.12435)\n",
            "     | > G_adv_loss: 1.53591  (1.45240)\n",
            "     | > loss_0: 34.01680  (34.57675)\n",
            "     | > grad_norm_0: 3397.44531  (3189.94409)\n",
            "     | > D_mse_gan_loss: 0.23811  (0.29674)\n",
            "     | > D_mse_gan_real_loss: 0.00040  (0.00179)\n",
            "     | > D_mse_gan_fake_loss: 0.00056  (0.00565)\n",
            "     | > loss_1: 0.23811  (0.29674)\n",
            "     | > grad_norm_1: 17.69624  (162.88316)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.23140  (5.18789)\n",
            "     | > loader_time: 0.00210  (0.00207)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/263 -- GLOBAL_STEP: 80050\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.67918  (0.72267)\n",
            "     | > G_mse_fake_loss: 0.65072  (0.65172)\n",
            "     | > G_feat_match_loss: 0.07889  (0.07930)\n",
            "     | > G_gen_loss: 30.56293  (32.52036)\n",
            "     | > G_adv_loss: 1.43958  (1.44470)\n",
            "     | > loss_0: 32.00251  (33.96506)\n",
            "     | > grad_norm_0: 4002.53760  (3278.53833)\n",
            "     | > D_mse_gan_loss: 0.24987  (0.27383)\n",
            "     | > D_mse_gan_real_loss: 0.00203  (0.00152)\n",
            "     | > D_mse_gan_fake_loss: 0.00196  (0.00587)\n",
            "     | > loss_1: 0.24987  (0.27383)\n",
            "     | > grad_norm_1: 97.12202  (145.96849)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24690  (5.21963)\n",
            "     | > loader_time: 0.00230  (0.00211)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 75/263 -- GLOBAL_STEP: 80075\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.70072  (0.71423)\n",
            "     | > G_mse_fake_loss: 0.51841  (0.63679)\n",
            "     | > G_feat_match_loss: 0.08777  (0.07988)\n",
            "     | > G_gen_loss: 31.53219  (32.14039)\n",
            "     | > G_adv_loss: 1.39607  (1.43558)\n",
            "     | > loss_0: 32.92826  (33.57597)\n",
            "     | > grad_norm_0: 1792.75916  (3299.39185)\n",
            "     | > D_mse_gan_loss: 0.21602  (0.26381)\n",
            "     | > D_mse_gan_real_loss: 0.00027  (0.00137)\n",
            "     | > D_mse_gan_fake_loss: 0.00106  (0.00493)\n",
            "     | > loss_1: 0.21602  (0.26381)\n",
            "     | > grad_norm_1: 134.33403  (145.01338)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24800  (5.23106)\n",
            "     | > loader_time: 0.00250  (0.00213)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/263 -- GLOBAL_STEP: 80100\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62871  (0.70675)\n",
            "     | > G_mse_fake_loss: 0.60256  (0.63061)\n",
            "     | > G_feat_match_loss: 0.07843  (0.08034)\n",
            "     | > G_gen_loss: 28.29190  (31.80367)\n",
            "     | > G_adv_loss: 1.38682  (1.43400)\n",
            "     | > loss_0: 29.67872  (33.23768)\n",
            "     | > grad_norm_0: 3037.27368  (3318.51172)\n",
            "     | > D_mse_gan_loss: 0.23710  (0.25676)\n",
            "     | > D_mse_gan_real_loss: 0.00108  (0.00137)\n",
            "     | > D_mse_gan_fake_loss: 0.00039  (0.00389)\n",
            "     | > loss_1: 0.23710  (0.25676)\n",
            "     | > grad_norm_1: 109.24909  (137.40471)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.23730  (5.23402)\n",
            "     | > loader_time: 0.00210  (0.00214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 125/263 -- GLOBAL_STEP: 80125\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.67672  (0.69897)\n",
            "     | > G_mse_fake_loss: 0.67087  (0.62876)\n",
            "     | > G_feat_match_loss: 0.08713  (0.08092)\n",
            "     | > G_gen_loss: 30.45229  (31.45387)\n",
            "     | > G_adv_loss: 1.54214  (1.43798)\n",
            "     | > loss_0: 31.99442  (32.89185)\n",
            "     | > grad_norm_0: 3435.78687  (3259.08643)\n",
            "     | > D_mse_gan_loss: 0.21457  (0.25149)\n",
            "     | > D_mse_gan_real_loss: 0.00053  (0.00135)\n",
            "     | > D_mse_gan_fake_loss: 0.00204  (0.00342)\n",
            "     | > loss_1: 0.21457  (0.25149)\n",
            "     | > grad_norm_1: 79.22194  (129.55238)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.25440  (5.23568)\n",
            "     | > loader_time: 0.00260  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 150/263 -- GLOBAL_STEP: 80150\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.67292  (0.69448)\n",
            "     | > G_mse_fake_loss: 0.69001  (0.62809)\n",
            "     | > G_feat_match_loss: 0.07813  (0.08122)\n",
            "     | > G_gen_loss: 30.28139  (31.25173)\n",
            "     | > G_adv_loss: 1.47131  (1.44028)\n",
            "     | > loss_0: 31.75270  (32.69201)\n",
            "     | > grad_norm_0: 3791.91992  (3244.31470)\n",
            "     | > D_mse_gan_loss: 0.25836  (0.24858)\n",
            "     | > D_mse_gan_real_loss: 0.00142  (0.00136)\n",
            "     | > D_mse_gan_fake_loss: 0.00111  (0.00303)\n",
            "     | > loss_1: 0.25836  (0.24858)\n",
            "     | > grad_norm_1: 86.79520  (124.52822)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.25180  (5.23844)\n",
            "     | > loader_time: 0.00220  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 175/263 -- GLOBAL_STEP: 80175\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63860  (0.68910)\n",
            "     | > G_mse_fake_loss: 0.53742  (0.62688)\n",
            "     | > G_feat_match_loss: 0.08497  (0.08146)\n",
            "     | > G_gen_loss: 28.73703  (31.00950)\n",
            "     | > G_adv_loss: 1.38716  (1.44150)\n",
            "     | > loss_0: 30.12420  (32.45101)\n",
            "     | > grad_norm_0: 2352.86499  (3216.10425)\n",
            "     | > D_mse_gan_loss: 0.22648  (0.24622)\n",
            "     | > D_mse_gan_real_loss: 0.00323  (0.00138)\n",
            "     | > D_mse_gan_fake_loss: 0.00373  (0.00276)\n",
            "     | > loss_1: 0.22648  (0.24622)\n",
            "     | > grad_norm_1: 92.53251  (120.89816)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.26190  (5.24055)\n",
            "     | > loader_time: 0.00230  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 200/263 -- GLOBAL_STEP: 80200\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.64544  (0.68575)\n",
            "     | > G_mse_fake_loss: 0.55062  (0.62579)\n",
            "     | > G_feat_match_loss: 0.07702  (0.08174)\n",
            "     | > G_gen_loss: 29.04476  (30.85884)\n",
            "     | > G_adv_loss: 1.32077  (1.44315)\n",
            "     | > loss_0: 30.36553  (32.30199)\n",
            "     | > grad_norm_0: 3195.18164  (3189.12280)\n",
            "     | > D_mse_gan_loss: 0.24183  (0.24412)\n",
            "     | > D_mse_gan_real_loss: 0.00090  (0.00135)\n",
            "     | > D_mse_gan_fake_loss: 0.00073  (0.00254)\n",
            "     | > loss_1: 0.24183  (0.24412)\n",
            "     | > grad_norm_1: 89.91611  (116.99625)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24380  (5.24223)\n",
            "     | > loader_time: 0.00210  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 225/263 -- GLOBAL_STEP: 80225\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63917  (0.68194)\n",
            "     | > G_mse_fake_loss: 0.65367  (0.62565)\n",
            "     | > G_feat_match_loss: 0.08203  (0.08186)\n",
            "     | > G_gen_loss: 28.76249  (30.68748)\n",
            "     | > G_adv_loss: 1.47397  (1.44424)\n",
            "     | > loss_0: 30.23646  (32.13172)\n",
            "     | > grad_norm_0: 3759.70435  (3168.73120)\n",
            "     | > D_mse_gan_loss: 0.23517  (0.24264)\n",
            "     | > D_mse_gan_real_loss: 0.00030  (0.00130)\n",
            "     | > D_mse_gan_fake_loss: 0.00057  (0.00237)\n",
            "     | > loss_1: 0.23517  (0.24264)\n",
            "     | > grad_norm_1: 113.25755  (114.17410)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.25190  (5.24287)\n",
            "     | > loader_time: 0.00200  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 250/263 -- GLOBAL_STEP: 80250\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63348  (0.67979)\n",
            "     | > G_mse_fake_loss: 0.65618  (0.62624)\n",
            "     | > G_feat_match_loss: 0.08196  (0.08206)\n",
            "     | > G_gen_loss: 28.50650  (30.59063)\n",
            "     | > G_adv_loss: 1.47576  (1.44686)\n",
            "     | > loss_0: 29.98226  (32.03749)\n",
            "     | > grad_norm_0: 2629.39551  (3140.77466)\n",
            "     | > D_mse_gan_loss: 0.23179  (0.24141)\n",
            "     | > D_mse_gan_real_loss: 0.00291  (0.00131)\n",
            "     | > D_mse_gan_fake_loss: 0.00245  (0.00236)\n",
            "     | > loss_1: 0.23179  (0.24141)\n",
            "     | > grad_norm_1: 83.75507  (111.75735)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.25150  (5.24374)\n",
            "     | > loader_time: 0.00180  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.00107 \u001b[0m(+0.00000)\n",
            "     | > avg_G_l1_spec_loss: 0.76422 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss: 0.58083 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss: 0.09746 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss: 34.39004 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss: 1.55546 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0: 35.94549 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss: 0.37437 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.12923 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss: 0.00372 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1: 0.37437 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 1/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-15 18:32:54) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 11/263 -- GLOBAL_STEP: 80275\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.66933  (0.67754)\n",
            "     | > G_mse_fake_loss: 0.59885  (0.61529)\n",
            "     | > G_feat_match_loss: 0.09320  (0.08343)\n",
            "     | > G_gen_loss: 30.11966  (30.48923)\n",
            "     | > G_adv_loss: 1.53087  (1.44957)\n",
            "     | > loss_0: 31.65052  (31.93881)\n",
            "     | > grad_norm_0: 1851.54749  (3098.60303)\n",
            "     | > D_mse_gan_loss: 0.18903  (0.22921)\n",
            "     | > D_mse_gan_real_loss: 0.00006  (0.00137)\n",
            "     | > D_mse_gan_fake_loss: 0.00033  (0.00264)\n",
            "     | > loss_1: 0.18903  (0.22921)\n",
            "     | > grad_norm_1: 107.91147  (79.81332)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.23280  (5.26565)\n",
            "     | > loader_time: 0.00090  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 36/263 -- GLOBAL_STEP: 80300\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62666  (0.67197)\n",
            "     | > G_mse_fake_loss: 0.60069  (0.62368)\n",
            "     | > G_feat_match_loss: 0.08838  (0.08488)\n",
            "     | > G_gen_loss: 28.19950  (30.23887)\n",
            "     | > G_adv_loss: 1.48451  (1.47247)\n",
            "     | > loss_0: 29.68401  (31.71134)\n",
            "     | > grad_norm_0: 2468.34424  (2874.40015)\n",
            "     | > D_mse_gan_loss: 0.20445  (0.22676)\n",
            "     | > D_mse_gan_real_loss: 0.00087  (0.00099)\n",
            "     | > D_mse_gan_fake_loss: 0.00023  (0.00288)\n",
            "     | > loss_1: 0.20445  (0.22676)\n",
            "     | > grad_norm_1: 93.91522  (79.88016)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24450  (5.25217)\n",
            "     | > loader_time: 0.00200  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 61/263 -- GLOBAL_STEP: 80325\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62934  (0.67282)\n",
            "     | > G_mse_fake_loss: 0.63933  (0.62910)\n",
            "     | > G_feat_match_loss: 0.07487  (0.08520)\n",
            "     | > G_gen_loss: 28.32020  (30.27695)\n",
            "     | > G_adv_loss: 1.38799  (1.48113)\n",
            "     | > loss_0: 29.70819  (31.75808)\n",
            "     | > grad_norm_0: 3176.67822  (2871.75317)\n",
            "     | > D_mse_gan_loss: 0.26302  (0.22734)\n",
            "     | > D_mse_gan_real_loss: 0.00670  (0.00098)\n",
            "     | > D_mse_gan_fake_loss: 0.00083  (0.00266)\n",
            "     | > loss_1: 0.26302  (0.22734)\n",
            "     | > grad_norm_1: 132.22176  (78.37560)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24710  (5.25068)\n",
            "     | > loader_time: 0.00260  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 86/263 -- GLOBAL_STEP: 80350\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.75454  (0.67237)\n",
            "     | > G_mse_fake_loss: 0.67451  (0.63023)\n",
            "     | > G_feat_match_loss: 0.09038  (0.08540)\n",
            "     | > G_gen_loss: 33.95433  (30.25675)\n",
            "     | > G_adv_loss: 1.57834  (1.48421)\n",
            "     | > loss_0: 35.53267  (31.74096)\n",
            "     | > grad_norm_0: 3437.57129  (2821.73120)\n",
            "     | > D_mse_gan_loss: 0.20821  (0.22723)\n",
            "     | > D_mse_gan_real_loss: 0.00073  (0.00089)\n",
            "     | > D_mse_gan_fake_loss: 0.00206  (0.00233)\n",
            "     | > loss_1: 0.20821  (0.22723)\n",
            "     | > grad_norm_1: 42.10099  (78.32442)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24730  (5.25113)\n",
            "     | > loader_time: 0.00250  (0.00220)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 111/263 -- GLOBAL_STEP: 80375\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.65751  (0.66959)\n",
            "     | > G_mse_fake_loss: 0.65130  (0.63075)\n",
            "     | > G_feat_match_loss: 0.08681  (0.08545)\n",
            "     | > G_gen_loss: 29.58804  (30.13159)\n",
            "     | > G_adv_loss: 1.51939  (1.48525)\n",
            "     | > loss_0: 31.10743  (31.61684)\n",
            "     | > grad_norm_0: 2671.98975  (2811.94067)\n",
            "     | > D_mse_gan_loss: 0.22906  (0.22592)\n",
            "     | > D_mse_gan_real_loss: 0.00220  (0.00095)\n",
            "     | > D_mse_gan_fake_loss: 0.00077  (0.00207)\n",
            "     | > loss_1: 0.22906  (0.22592)\n",
            "     | > grad_norm_1: 68.10536  (77.59604)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24840  (5.25041)\n",
            "     | > loader_time: 0.00210  (0.00220)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 136/263 -- GLOBAL_STEP: 80400\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.64130  (0.66881)\n",
            "     | > G_mse_fake_loss: 0.53084  (0.63248)\n",
            "     | > G_feat_match_loss: 0.08297  (0.08564)\n",
            "     | > G_gen_loss: 28.85853  (30.09660)\n",
            "     | > G_adv_loss: 1.36053  (1.48888)\n",
            "     | > loss_0: 30.21906  (31.58548)\n",
            "     | > grad_norm_0: 2543.84131  (2794.00854)\n",
            "     | > D_mse_gan_loss: 0.22378  (0.22518)\n",
            "     | > D_mse_gan_real_loss: 0.00014  (0.00096)\n",
            "     | > D_mse_gan_fake_loss: 0.00538  (0.00191)\n",
            "     | > loss_1: 0.22378  (0.22518)\n",
            "     | > grad_norm_1: 63.37854  (76.60238)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24250  (5.24965)\n",
            "     | > loader_time: 0.00250  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 161/263 -- GLOBAL_STEP: 80425\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62519  (0.66646)\n",
            "     | > G_mse_fake_loss: 0.69925  (0.63352)\n",
            "     | > G_feat_match_loss: 0.07980  (0.08567)\n",
            "     | > G_gen_loss: 28.13368  (29.99064)\n",
            "     | > G_adv_loss: 1.49725  (1.49024)\n",
            "     | > loss_0: 29.63092  (31.48088)\n",
            "     | > grad_norm_0: 3206.65381  (2785.85938)\n",
            "     | > D_mse_gan_loss: 0.25251  (0.22469)\n",
            "     | > D_mse_gan_real_loss: 0.00046  (0.00092)\n",
            "     | > D_mse_gan_fake_loss: 0.00037  (0.00179)\n",
            "     | > loss_1: 0.25251  (0.22469)\n",
            "     | > grad_norm_1: 154.30534  (76.09392)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24420  (5.24965)\n",
            "     | > loader_time: 0.00230  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 186/263 -- GLOBAL_STEP: 80450\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60558  (0.66521)\n",
            "     | > G_mse_fake_loss: 0.68604  (0.63598)\n",
            "     | > G_feat_match_loss: 0.08699  (0.08584)\n",
            "     | > G_gen_loss: 27.25104  (29.93466)\n",
            "     | > G_adv_loss: 1.55592  (1.49442)\n",
            "     | > loss_0: 28.80696  (31.42908)\n",
            "     | > grad_norm_0: 2489.77002  (2758.52173)\n",
            "     | > D_mse_gan_loss: 0.22203  (0.22428)\n",
            "     | > D_mse_gan_real_loss: 0.00295  (0.00090)\n",
            "     | > D_mse_gan_fake_loss: 0.00111  (0.00194)\n",
            "     | > loss_1: 0.22203  (0.22428)\n",
            "     | > grad_norm_1: 90.88899  (76.14845)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24340  (5.24995)\n",
            "     | > loader_time: 0.00220  (0.00223)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 211/263 -- GLOBAL_STEP: 80475\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61704  (0.66467)\n",
            "     | > G_mse_fake_loss: 0.57465  (0.63591)\n",
            "     | > G_feat_match_loss: 0.08876  (0.08595)\n",
            "     | > G_gen_loss: 27.76665  (29.91030)\n",
            "     | > G_adv_loss: 1.46222  (1.49546)\n",
            "     | > loss_0: 29.22886  (31.40575)\n",
            "     | > grad_norm_0: 1906.85950  (2750.49438)\n",
            "     | > D_mse_gan_loss: 0.20472  (0.22422)\n",
            "     | > D_mse_gan_real_loss: 0.00007  (0.00090)\n",
            "     | > D_mse_gan_fake_loss: 0.00057  (0.00186)\n",
            "     | > loss_1: 0.20472  (0.22422)\n",
            "     | > grad_norm_1: 70.81383  (74.94921)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24900  (5.24954)\n",
            "     | > loader_time: 0.00220  (0.00224)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 236/263 -- GLOBAL_STEP: 80500\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62654  (0.66394)\n",
            "     | > G_mse_fake_loss: 0.66904  (0.63677)\n",
            "     | > G_feat_match_loss: 0.08468  (0.08602)\n",
            "     | > G_gen_loss: 28.19445  (29.87747)\n",
            "     | > G_adv_loss: 1.51581  (1.49693)\n",
            "     | > loss_0: 29.71026  (31.37440)\n",
            "     | > grad_norm_0: 2316.70435  (2755.48486)\n",
            "     | > D_mse_gan_loss: 0.22236  (0.22423)\n",
            "     | > D_mse_gan_real_loss: 0.00048  (0.00088)\n",
            "     | > D_mse_gan_fake_loss: 0.00108  (0.00183)\n",
            "     | > loss_1: 0.22236  (0.22423)\n",
            "     | > grad_norm_1: 70.58582  (73.88094)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.25480  (5.24998)\n",
            "     | > loader_time: 0.00200  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 261/263 -- GLOBAL_STEP: 80525\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.66098  (0.66272)\n",
            "     | > G_mse_fake_loss: 0.66638  (0.63557)\n",
            "     | > G_feat_match_loss: 0.08977  (0.08610)\n",
            "     | > G_gen_loss: 29.74406  (29.82218)\n",
            "     | > G_adv_loss: 1.56404  (1.49661)\n",
            "     | > loss_0: 31.30809  (31.31878)\n",
            "     | > grad_norm_0: 2314.91455  (2731.23242)\n",
            "     | > D_mse_gan_loss: 0.21146  (0.22362)\n",
            "     | > D_mse_gan_real_loss: 0.00094  (0.00086)\n",
            "     | > D_mse_gan_fake_loss: 0.00119  (0.00176)\n",
            "     | > loss_1: 0.21146  (0.22362)\n",
            "     | > grad_norm_1: 58.99458  (72.99684)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.25630  (5.25025)\n",
            "     | > loader_time: 0.00230  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00134 \u001b[0m(+0.00027)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.71754 \u001b[0m(-0.04668)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.74524 \u001b[0m(+0.16441)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.10605 \u001b[0m(+0.00859)\n",
            "     | > avg_G_gen_loss:\u001b[92m 32.28944 \u001b[0m(-2.10060)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.80573 \u001b[0m(+0.25027)\n",
            "     | > avg_loss_0:\u001b[92m 34.09517 \u001b[0m(-1.85033)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.36159 \u001b[0m(-0.01277)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.14336 \u001b[0m(+0.01413)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.01063 \u001b[0m(+0.00691)\n",
            "     | > avg_loss_1:\u001b[92m 0.36159 \u001b[0m(-0.01277)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 2/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-15 18:56:08) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/263 -- GLOBAL_STEP: 80550\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.70728  (0.64472)\n",
            "     | > G_mse_fake_loss: 0.51840  (0.63189)\n",
            "     | > G_feat_match_loss: 0.09550  (0.08695)\n",
            "     | > G_gen_loss: 31.82741  (29.01255)\n",
            "     | > G_adv_loss: 1.47340  (1.50140)\n",
            "     | > loss_0: 33.30081  (30.51395)\n",
            "     | > grad_norm_0: 1100.98901  (2241.36938)\n",
            "     | > D_mse_gan_loss: 0.20475  (0.21899)\n",
            "     | > D_mse_gan_real_loss: 0.00009  (0.00124)\n",
            "     | > D_mse_gan_fake_loss: 0.00283  (0.00123)\n",
            "     | > loss_1: 0.20475  (0.21899)\n",
            "     | > grad_norm_1: 111.03291  (54.06543)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.26530  (5.25868)\n",
            "     | > loader_time: 0.00230  (0.00220)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 47/263 -- GLOBAL_STEP: 80575\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.65009  (0.64623)\n",
            "     | > G_mse_fake_loss: 0.69399  (0.63399)\n",
            "     | > G_feat_match_loss: 0.07696  (0.08676)\n",
            "     | > G_gen_loss: 29.25392  (29.08047)\n",
            "     | > G_adv_loss: 1.46358  (1.50159)\n",
            "     | > loss_0: 30.71750  (30.58206)\n",
            "     | > grad_norm_0: 3685.10181  (2239.02075)\n",
            "     | > D_mse_gan_loss: 0.26612  (0.22263)\n",
            "     | > D_mse_gan_real_loss: 0.00129  (0.00120)\n",
            "     | > D_mse_gan_fake_loss: 0.00102  (0.00145)\n",
            "     | > loss_1: 0.26612  (0.22263)\n",
            "     | > grad_norm_1: 90.92403  (54.98042)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.25060  (5.25435)\n",
            "     | > loader_time: 0.00210  (0.00220)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 72/263 -- GLOBAL_STEP: 80600\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.64943  (0.64598)\n",
            "     | > G_mse_fake_loss: 0.56963  (0.63626)\n",
            "     | > G_feat_match_loss: 0.08652  (0.08694)\n",
            "     | > G_gen_loss: 29.22420  (29.06927)\n",
            "     | > G_adv_loss: 1.43480  (1.50565)\n",
            "     | > loss_0: 30.65900  (30.57492)\n",
            "     | > grad_norm_0: 1940.32434  (2259.52466)\n",
            "     | > D_mse_gan_loss: 0.22129  (0.22171)\n",
            "     | > D_mse_gan_real_loss: 0.00016  (0.00114)\n",
            "     | > D_mse_gan_fake_loss: 0.00309  (0.00136)\n",
            "     | > loss_1: 0.22129  (0.22171)\n",
            "     | > grad_norm_1: 72.78308  (60.89001)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24960  (5.25464)\n",
            "     | > loader_time: 0.00210  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 97/263 -- GLOBAL_STEP: 80625\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.66445  (0.64475)\n",
            "     | > G_mse_fake_loss: 0.65819  (0.63721)\n",
            "     | > G_feat_match_loss: 0.09606  (0.08720)\n",
            "     | > G_gen_loss: 29.90016  (29.01387)\n",
            "     | > G_adv_loss: 1.61879  (1.50917)\n",
            "     | > loss_0: 31.51894  (30.52304)\n",
            "     | > grad_norm_0: 1538.10693  (2233.89307)\n",
            "     | > D_mse_gan_loss: 0.18877  (0.21992)\n",
            "     | > D_mse_gan_real_loss: 0.00010  (0.00104)\n",
            "     | > D_mse_gan_fake_loss: 0.00304  (0.00157)\n",
            "     | > loss_1: 0.18877  (0.21992)\n",
            "     | > grad_norm_1: 36.63645  (65.34931)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.24670  (5.25282)\n",
            "     | > loader_time: 0.00210  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 122/263 -- GLOBAL_STEP: 80650\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63377  (0.64380)\n",
            "     | > G_mse_fake_loss: 0.67730  (0.64108)\n",
            "     | > G_feat_match_loss: 0.08921  (0.08736)\n",
            "     | > G_gen_loss: 28.51953  (28.97114)\n",
            "     | > G_adv_loss: 1.56943  (1.51467)\n",
            "     | > loss_0: 30.08896  (30.48581)\n",
            "     | > grad_norm_0: 2188.98535  (2222.98486)\n",
            "     | > D_mse_gan_loss: 0.21981  (0.22054)\n",
            "     | > D_mse_gan_real_loss: 0.00007  (0.00104)\n",
            "     | > D_mse_gan_fake_loss: 0.00084  (0.00223)\n",
            "     | > loss_1: 0.21981  (0.22054)\n",
            "     | > grad_norm_1: 120.60216  (67.66400)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.25780  (5.25260)\n",
            "     | > loader_time: 0.00210  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 147/263 -- GLOBAL_STEP: 80675\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61353  (0.64259)\n",
            "     | > G_mse_fake_loss: 0.67443  (0.64252)\n",
            "     | > G_feat_match_loss: 0.09140  (0.08750)\n",
            "     | > G_gen_loss: 27.60877  (28.91663)\n",
            "     | > G_adv_loss: 1.58846  (1.51748)\n",
            "     | > loss_0: 29.19723  (30.43412)\n",
            "     | > grad_norm_0: 1671.55737  (2211.59253)\n",
            "     | > D_mse_gan_loss: 0.20677  (0.21975)\n",
            "     | > D_mse_gan_real_loss: 0.00006  (0.00104)\n",
            "     | > D_mse_gan_fake_loss: 0.00078  (0.00206)\n",
            "     | > loss_1: 0.20677  (0.21975)\n",
            "     | > grad_norm_1: 105.47572  (72.48515)\n",
            "     | > current_lr_0: 0.00001 \n",
            "     | > current_lr_1: 0.00001 \n",
            "     | > step_time: 5.25410  (5.25276)\n",
            "     | > loader_time: 0.00220  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 172/263 -- GLOBAL_STEP: 80700\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60660  (0.64130)\n",
            "     | > G_mse_fake_loss: 0.69483  (0.64373)\n",
            "     | > G_feat_match_loss: 0.08844  (0.08761)\n",
            "     | > G_gen_loss: 27.29700  (28.85846)\n",
            "     | > G_adv_loss: 1.57924  (1.51985)\n",
            "     | > loss_0: 28.87624  (30.37831)\n",
            "     | > grad_norm_0: 1903.62085  (2188.14185)\n",
            "     | > D_mse_gan_loss: 0.21557  (0.21898)\n",
            "     | > D_mse_gan_real_loss: 0.00006  (0.00099)\n",
            "     | > D_mse_gan_fake_loss: 0.00154  (0.00191)\n",
            "     | > loss_1: 0.21557  (0.21898)\n",
            "     | > grad_norm_1: 79.89671  (74.50631)\n",
            "     | > current_lr_0: 4.959150020176672e-06 \n",
            "     | > current_lr_1: 4.959150020176672e-06 \n",
            "     | > step_time: 5.25570  (5.25312)\n",
            "     | > loader_time: 0.00220  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 197/263 -- GLOBAL_STEP: 80725\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61705  (0.64045)\n",
            "     | > G_mse_fake_loss: 0.58839  (0.64357)\n",
            "     | > G_feat_match_loss: 0.09006  (0.08758)\n",
            "     | > G_gen_loss: 27.76724  (28.82018)\n",
            "     | > G_adv_loss: 1.48895  (1.51936)\n",
            "     | > loss_0: 29.25620  (30.33955)\n",
            "     | > grad_norm_0: 2192.50537  (2201.96460)\n",
            "     | > D_mse_gan_loss: 0.20898  (0.21879)\n",
            "     | > D_mse_gan_real_loss: 0.00016  (0.00098)\n",
            "     | > D_mse_gan_fake_loss: 0.00097  (0.00178)\n",
            "     | > loss_1: 0.20898  (0.21879)\n",
            "     | > grad_norm_1: 86.98038  (75.67860)\n",
            "     | > current_lr_0: 4.836647671103905e-06 \n",
            "     | > current_lr_1: 4.836647671103905e-06 \n",
            "     | > step_time: 5.25630  (5.25342)\n",
            "     | > loader_time: 0.00240  (0.00224)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 222/263 -- GLOBAL_STEP: 80750\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63837  (0.64024)\n",
            "     | > G_mse_fake_loss: 0.54899  (0.64407)\n",
            "     | > G_feat_match_loss: 0.09234  (0.08760)\n",
            "     | > G_gen_loss: 28.72674  (28.81070)\n",
            "     | > G_adv_loss: 1.47242  (1.52004)\n",
            "     | > loss_0: 30.19916  (30.33074)\n",
            "     | > grad_norm_0: 1525.49792  (2172.94897)\n",
            "     | > D_mse_gan_loss: 0.19552  (0.21893)\n",
            "     | > D_mse_gan_real_loss: 0.00011  (0.00103)\n",
            "     | > D_mse_gan_fake_loss: 0.00132  (0.00172)\n",
            "     | > loss_1: 0.19552  (0.21893)\n",
            "     | > grad_norm_1: 70.21725  (74.33770)\n",
            "     | > current_lr_0: 4.717171410265469e-06 \n",
            "     | > current_lr_1: 4.717171410265469e-06 \n",
            "     | > step_time: 5.25280  (5.25322)\n",
            "     | > loader_time: 0.00220  (0.00223)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 247/263 -- GLOBAL_STEP: 80775\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.64650  (0.64028)\n",
            "     | > G_mse_fake_loss: 0.70697  (0.64390)\n",
            "     | > G_feat_match_loss: 0.08250  (0.08770)\n",
            "     | > G_gen_loss: 29.09234  (28.81277)\n",
            "     | > G_adv_loss: 1.53199  (1.52086)\n",
            "     | > loss_0: 30.62432  (30.33364)\n",
            "     | > grad_norm_0: 2097.52271  (2180.14404)\n",
            "     | > D_mse_gan_loss: 0.25038  (0.21836)\n",
            "     | > D_mse_gan_real_loss: 0.00367  (0.00112)\n",
            "     | > D_mse_gan_fake_loss: 0.00138  (0.00166)\n",
            "     | > loss_1: 0.25038  (0.21836)\n",
            "     | > grad_norm_1: 129.08305  (72.07220)\n",
            "     | > current_lr_0: 4.6006464863600965e-06 \n",
            "     | > current_lr_1: 4.6006464863600965e-06 \n",
            "     | > step_time: 5.24730  (5.25261)\n",
            "     | > loader_time: 0.00220  (0.00224)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00164 \u001b[0m(+0.00030)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.71384 \u001b[0m(-0.00371)\n",
            "     | > avg_G_mse_fake_loss:\u001b[92m 0.63594 \u001b[0m(-0.10930)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.10234 \u001b[0m(-0.00371)\n",
            "     | > avg_G_gen_loss:\u001b[92m 32.12264 \u001b[0m(-0.16680)\n",
            "     | > avg_G_adv_loss:\u001b[92m 1.65930 \u001b[0m(-0.14643)\n",
            "     | > avg_loss_0:\u001b[92m 33.78194 \u001b[0m(-0.31323)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.36079 \u001b[0m(-0.00080)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.14899 \u001b[0m(+0.00563)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00331 \u001b[0m(-0.00732)\n",
            "     | > avg_loss_1:\u001b[92m 0.36079 \u001b[0m(-0.00080)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 3/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-15 19:19:24) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 8/263 -- GLOBAL_STEP: 80800\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.71887  (0.64599)\n",
            "     | > G_mse_fake_loss: 0.65766  (0.62740)\n",
            "     | > G_feat_match_loss: 0.09372  (0.08515)\n",
            "     | > G_gen_loss: 32.34893  (29.06965)\n",
            "     | > G_adv_loss: 1.59483  (1.47888)\n",
            "     | > loss_0: 33.94376  (30.54854)\n",
            "     | > grad_norm_0: 2590.01294  (2609.71045)\n",
            "     | > D_mse_gan_loss: 0.22002  (0.22969)\n",
            "     | > D_mse_gan_real_loss: 0.00057  (0.00061)\n",
            "     | > D_mse_gan_fake_loss: 0.00039  (0.00154)\n",
            "     | > loss_1: 0.22002  (0.22969)\n",
            "     | > grad_norm_1: 66.72774  (53.91890)\n",
            "     | > current_lr_0: 4.486999994614641e-06 \n",
            "     | > current_lr_1: 4.486999994614641e-06 \n",
            "     | > step_time: 5.25190  (5.27820)\n",
            "     | > loader_time: 0.00220  (0.00223)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 33/263 -- GLOBAL_STEP: 80825\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61995  (0.63454)\n",
            "     | > G_mse_fake_loss: 0.67525  (0.63752)\n",
            "     | > G_feat_match_loss: 0.08185  (0.08639)\n",
            "     | > G_gen_loss: 27.89781  (28.55427)\n",
            "     | > G_adv_loss: 1.49371  (1.50139)\n",
            "     | > loss_0: 29.39151  (30.05566)\n",
            "     | > grad_norm_0: 2521.59473  (2326.61621)\n",
            "     | > D_mse_gan_loss: 0.23485  (0.22062)\n",
            "     | > D_mse_gan_real_loss: 0.00042  (0.00072)\n",
            "     | > D_mse_gan_fake_loss: 0.00021  (0.00090)\n",
            "     | > loss_1: 0.23485  (0.22062)\n",
            "     | > grad_norm_1: 66.70971  (58.05925)\n",
            "     | > current_lr_0: 4.376160831170622e-06 \n",
            "     | > current_lr_1: 4.376160831170622e-06 \n",
            "     | > step_time: 5.24360  (5.25331)\n",
            "     | > loader_time: 0.00240  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 58/263 -- GLOBAL_STEP: 80850\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.64763  (0.63605)\n",
            "     | > G_mse_fake_loss: 0.59432  (0.63870)\n",
            "     | > G_feat_match_loss: 0.08932  (0.08683)\n",
            "     | > G_gen_loss: 29.14332  (28.62222)\n",
            "     | > G_adv_loss: 1.48754  (1.50696)\n",
            "     | > loss_0: 30.63086  (30.12918)\n",
            "     | > grad_norm_0: 1916.04236  (2284.65649)\n",
            "     | > D_mse_gan_loss: 0.21265  (0.21889)\n",
            "     | > D_mse_gan_real_loss: 0.00040  (0.00064)\n",
            "     | > D_mse_gan_fake_loss: 0.00046  (0.00106)\n",
            "     | > loss_1: 0.21265  (0.21889)\n",
            "     | > grad_norm_1: 112.94420  (62.49637)\n",
            "     | > current_lr_0: 4.268059648597496e-06 \n",
            "     | > current_lr_1: 4.268059648597496e-06 \n",
            "     | > step_time: 5.25450  (5.25203)\n",
            "     | > loader_time: 0.00210  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 83/263 -- GLOBAL_STEP: 80875\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.66164  (0.63517)\n",
            "     | > G_mse_fake_loss: 0.60011  (0.63838)\n",
            "     | > G_feat_match_loss: 0.08592  (0.08658)\n",
            "     | > G_gen_loss: 29.77395  (28.58256)\n",
            "     | > G_adv_loss: 1.45934  (1.50416)\n",
            "     | > loss_0: 31.23329  (30.08672)\n",
            "     | > grad_norm_0: 2876.76440  (2258.08643)\n",
            "     | > D_mse_gan_loss: 0.22165  (0.22027)\n",
            "     | > D_mse_gan_real_loss: 0.00061  (0.00100)\n",
            "     | > D_mse_gan_fake_loss: 0.00286  (0.00105)\n",
            "     | > loss_1: 0.22165  (0.22027)\n",
            "     | > grad_norm_1: 33.40238  (68.02412)\n",
            "     | > current_lr_0: 4.162628812504895e-06 \n",
            "     | > current_lr_1: 4.162628812504895e-06 \n",
            "     | > step_time: 5.24680  (5.25149)\n",
            "     | > loader_time: 0.00200  (0.00224)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 108/263 -- GLOBAL_STEP: 80900\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61519  (0.63354)\n",
            "     | > G_mse_fake_loss: 0.61856  (0.63878)\n",
            "     | > G_feat_match_loss: 0.08482  (0.08649)\n",
            "     | > G_gen_loss: 27.68339  (28.50945)\n",
            "     | > G_adv_loss: 1.46679  (1.50363)\n",
            "     | > loss_0: 29.15018  (30.01308)\n",
            "     | > grad_norm_0: 2046.53381  (2224.27905)\n",
            "     | > D_mse_gan_loss: 0.22006  (0.22089)\n",
            "     | > D_mse_gan_real_loss: 0.00026  (0.00105)\n",
            "     | > D_mse_gan_fake_loss: 0.00398  (0.00150)\n",
            "     | > loss_1: 0.22006  (0.22089)\n",
            "     | > grad_norm_1: 17.73306  (67.92145)\n",
            "     | > current_lr_0: 4.059802359226586e-06 \n",
            "     | > current_lr_1: 4.059802359226586e-06 \n",
            "     | > step_time: 5.25270  (5.25045)\n",
            "     | > loader_time: 0.00250  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 133/263 -- GLOBAL_STEP: 80925\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62825  (0.63318)\n",
            "     | > G_mse_fake_loss: 0.58325  (0.64174)\n",
            "     | > G_feat_match_loss: 0.08395  (0.08682)\n",
            "     | > G_gen_loss: 28.27121  (28.49308)\n",
            "     | > G_adv_loss: 1.42275  (1.50995)\n",
            "     | > loss_0: 29.69396  (30.00303)\n",
            "     | > grad_norm_0: 1493.33765  (2171.23169)\n",
            "     | > D_mse_gan_loss: 0.21870  (0.22076)\n",
            "     | > D_mse_gan_real_loss: 0.00016  (0.00112)\n",
            "     | > D_mse_gan_fake_loss: 0.00287  (0.00192)\n",
            "     | > loss_1: 0.21870  (0.22076)\n",
            "     | > grad_norm_1: 33.18782  (65.14790)\n",
            "     | > current_lr_0: 3.9595159545497845e-06 \n",
            "     | > current_lr_1: 3.9595159545497845e-06 \n",
            "     | > step_time: 5.24730  (5.25119)\n",
            "     | > loader_time: 0.00210  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 158/263 -- GLOBAL_STEP: 80950\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61971  (0.63229)\n",
            "     | > G_mse_fake_loss: 0.67992  (0.64139)\n",
            "     | > G_feat_match_loss: 0.08672  (0.08659)\n",
            "     | > G_gen_loss: 27.88680  (28.45322)\n",
            "     | > G_adv_loss: 1.54716  (1.50728)\n",
            "     | > loss_0: 29.43396  (29.96050)\n",
            "     | > grad_norm_0: 2508.02075  (2171.60278)\n",
            "     | > D_mse_gan_loss: 0.22824  (0.22126)\n",
            "     | > D_mse_gan_real_loss: 0.00032  (0.00112)\n",
            "     | > D_mse_gan_fake_loss: 0.00069  (0.00181)\n",
            "     | > loss_1: 0.22824  (0.22126)\n",
            "     | > grad_norm_1: 111.48225  (62.05465)\n",
            "     | > current_lr_0: 3.861706853463917e-06 \n",
            "     | > current_lr_1: 3.861706853463917e-06 \n",
            "     | > step_time: 5.25680  (5.25094)\n",
            "     | > loader_time: 0.00210  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 183/263 -- GLOBAL_STEP: 80975\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58950  (0.63124)\n",
            "     | > G_mse_fake_loss: 0.65587  (0.64128)\n",
            "     | > G_feat_match_loss: 0.08460  (0.08674)\n",
            "     | > G_gen_loss: 26.52770  (28.40584)\n",
            "     | > G_adv_loss: 1.50191  (1.50873)\n",
            "     | > loss_0: 28.02961  (29.91457)\n",
            "     | > grad_norm_0: 2364.17798  (2166.10815)\n",
            "     | > D_mse_gan_loss: 0.21998  (0.22019)\n",
            "     | > D_mse_gan_real_loss: 0.00284  (0.00109)\n",
            "     | > D_mse_gan_fake_loss: 0.00157  (0.00174)\n",
            "     | > loss_1: 0.21998  (0.22019)\n",
            "     | > grad_norm_1: 25.10792  (59.20155)\n",
            "     | > current_lr_0: 3.766313860903696e-06 \n",
            "     | > current_lr_1: 3.766313860903696e-06 \n",
            "     | > step_time: 5.24920  (5.25048)\n",
            "     | > loader_time: 0.00240  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 208/263 -- GLOBAL_STEP: 81000\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.65072  (0.63088)\n",
            "     | > G_mse_fake_loss: 0.69362  (0.64096)\n",
            "     | > G_feat_match_loss: 0.09018  (0.08685)\n",
            "     | > G_gen_loss: 29.28261  (28.38961)\n",
            "     | > G_adv_loss: 1.59545  (1.50950)\n",
            "     | > loss_0: 30.87807  (29.89911)\n",
            "     | > grad_norm_0: 2054.09155  (2154.32300)\n",
            "     | > D_mse_gan_loss: 0.20329  (0.21963)\n",
            "     | > D_mse_gan_real_loss: 0.00011  (0.00108)\n",
            "     | > D_mse_gan_fake_loss: 0.00062  (0.00168)\n",
            "     | > loss_1: 0.20329  (0.21963)\n",
            "     | > grad_norm_1: 34.87032  (57.26741)\n",
            "     | > current_lr_0: 3.6732772934619258e-06 \n",
            "     | > current_lr_1: 3.6732772934619258e-06 \n",
            "     | > step_time: 5.25490  (5.25075)\n",
            "     | > loader_time: 0.00220  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 233/263 -- GLOBAL_STEP: 81025\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61728  (0.63034)\n",
            "     | > G_mse_fake_loss: 0.59466  (0.63997)\n",
            "     | > G_feat_match_loss: 0.08282  (0.08687)\n",
            "     | > G_gen_loss: 27.77766  (28.36537)\n",
            "     | > G_adv_loss: 1.42286  (1.50866)\n",
            "     | > loss_0: 29.20052  (29.87403)\n",
            "     | > grad_norm_0: 2004.50989  (2147.09546)\n",
            "     | > D_mse_gan_loss: 0.22587  (0.21932)\n",
            "     | > D_mse_gan_real_loss: 0.00007  (0.00104)\n",
            "     | > D_mse_gan_fake_loss: 0.00081  (0.00160)\n",
            "     | > loss_1: 0.22587  (0.21932)\n",
            "     | > grad_norm_1: 13.38241  (56.04001)\n",
            "     | > current_lr_0: 3.5825389420480865e-06 \n",
            "     | > current_lr_1: 3.5825389420480865e-06 \n",
            "     | > step_time: 5.24030  (5.25121)\n",
            "     | > loader_time: 0.00210  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 258/263 -- GLOBAL_STEP: 81050\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62502  (0.62996)\n",
            "     | > G_mse_fake_loss: 0.69032  (0.64078)\n",
            "     | > G_feat_match_loss: 0.09244  (0.08688)\n",
            "     | > G_gen_loss: 28.12604  (28.34799)\n",
            "     | > G_adv_loss: 1.61475  (1.50960)\n",
            "     | > loss_0: 29.74080  (29.85759)\n",
            "     | > grad_norm_0: 1787.80579  (2131.21362)\n",
            "     | > D_mse_gan_loss: 0.19932  (0.21946)\n",
            "     | > D_mse_gan_real_loss: 0.00265  (0.00104)\n",
            "     | > D_mse_gan_fake_loss: 0.00018  (0.00154)\n",
            "     | > loss_1: 0.19932  (0.21946)\n",
            "     | > grad_norm_1: 86.35102  (58.39465)\n",
            "     | > current_lr_0: 3.4940420354693423e-06 \n",
            "     | > current_lr_1: 3.4940420354693423e-06 \n",
            "     | > step_time: 5.23720  (5.25059)\n",
            "     | > loader_time: 0.00210  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00138 \u001b[0m(-0.00026)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.68168 \u001b[0m(-0.03215)\n",
            "     | > avg_G_mse_fake_loss:\u001b[92m 0.60457 \u001b[0m(-0.03137)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.10220 \u001b[0m(-0.00014)\n",
            "     | > avg_G_gen_loss:\u001b[92m 30.67577 \u001b[0m(-1.44686)\n",
            "     | > avg_G_adv_loss:\u001b[92m 1.62652 \u001b[0m(-0.03278)\n",
            "     | > avg_loss_0:\u001b[92m 32.30230 \u001b[0m(-1.47964)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.37050 \u001b[0m(+0.00971)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.15148 \u001b[0m(+0.00250)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00242 \u001b[0m(-0.00090)\n",
            "     | > avg_loss_1:\u001b[91m 0.37050 \u001b[0m(+0.00971)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 4/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-15 19:42:38) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 19/263 -- GLOBAL_STEP: 81075\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60504  (0.63337)\n",
            "     | > G_mse_fake_loss: 0.61753  (0.65292)\n",
            "     | > G_feat_match_loss: 0.07679  (0.08612)\n",
            "     | > G_gen_loss: 27.22683  (28.50161)\n",
            "     | > G_adv_loss: 1.38544  (1.51416)\n",
            "     | > loss_0: 28.61227  (30.01578)\n",
            "     | > grad_norm_0: 2327.69507  (2341.90259)\n",
            "     | > D_mse_gan_loss: 0.24385  (0.22082)\n",
            "     | > D_mse_gan_real_loss: 0.00471  (0.00115)\n",
            "     | > D_mse_gan_fake_loss: 0.00030  (0.00121)\n",
            "     | > loss_1: 0.24385  (0.22082)\n",
            "     | > grad_norm_1: 88.18626  (50.31378)\n",
            "     | > current_lr_0: 3.407731204911179e-06 \n",
            "     | > current_lr_1: 3.407731204911179e-06 \n",
            "     | > step_time: 5.24760  (5.25725)\n",
            "     | > loader_time: 0.00210  (0.00192)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 44/263 -- GLOBAL_STEP: 81100\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.67090  (0.63740)\n",
            "     | > G_mse_fake_loss: 0.63658  (0.64738)\n",
            "     | > G_feat_match_loss: 0.08758  (0.08712)\n",
            "     | > G_gen_loss: 30.19032  (28.68287)\n",
            "     | > G_adv_loss: 1.51243  (1.51860)\n",
            "     | > loss_0: 31.70275  (30.20147)\n",
            "     | > grad_norm_0: 2799.22070  (2147.21631)\n",
            "     | > D_mse_gan_loss: 0.20826  (0.22050)\n",
            "     | > D_mse_gan_real_loss: 0.00394  (0.00147)\n",
            "     | > D_mse_gan_fake_loss: 0.00075  (0.00103)\n",
            "     | > loss_1: 0.20826  (0.22050)\n",
            "     | > grad_norm_1: 24.83256  (49.40341)\n",
            "     | > current_lr_0: 3.3235524492954506e-06 \n",
            "     | > current_lr_1: 3.3235524492954506e-06 \n",
            "     | > step_time: 5.23630  (5.25372)\n",
            "     | > loader_time: 0.00220  (0.00210)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 69/263 -- GLOBAL_STEP: 81125\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.66108  (0.63826)\n",
            "     | > G_mse_fake_loss: 0.63707  (0.64772)\n",
            "     | > G_feat_match_loss: 0.09443  (0.08756)\n",
            "     | > G_gen_loss: 29.74867  (28.72189)\n",
            "     | > G_adv_loss: 1.58133  (1.52336)\n",
            "     | > loss_0: 31.33000  (30.24525)\n",
            "     | > grad_norm_0: 1376.61951  (2109.90723)\n",
            "     | > D_mse_gan_loss: 0.20152  (0.21827)\n",
            "     | > D_mse_gan_real_loss: 0.00018  (0.00119)\n",
            "     | > D_mse_gan_fake_loss: 0.00180  (0.00096)\n",
            "     | > loss_1: 0.20152  (0.21827)\n",
            "     | > grad_norm_1: 26.18724  (49.21967)\n",
            "     | > current_lr_0: 3.2414531014941643e-06 \n",
            "     | > current_lr_1: 3.2414531014941643e-06 \n",
            "     | > step_time: 5.25210  (5.25243)\n",
            "     | > loader_time: 0.00230  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 94/263 -- GLOBAL_STEP: 81150\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61307  (0.63827)\n",
            "     | > G_mse_fake_loss: 0.68144  (0.64565)\n",
            "     | > G_feat_match_loss: 0.08583  (0.08752)\n",
            "     | > G_gen_loss: 27.58798  (28.72217)\n",
            "     | > G_adv_loss: 1.53979  (1.52085)\n",
            "     | > loss_0: 29.12777  (30.24302)\n",
            "     | > grad_norm_0: 1420.88733  (2119.47803)\n",
            "     | > D_mse_gan_loss: 0.22276  (0.21766)\n",
            "     | > D_mse_gan_real_loss: 0.00035  (0.00121)\n",
            "     | > D_mse_gan_fake_loss: 0.00033  (0.00109)\n",
            "     | > loss_1: 0.22276  (0.21766)\n",
            "     | > grad_norm_1: 50.83287  (48.78405)\n",
            "     | > current_lr_0: 3.161381795377861e-06 \n",
            "     | > current_lr_1: 3.161381795377861e-06 \n",
            "     | > step_time: 5.25930  (5.25377)\n",
            "     | > loader_time: 0.00230  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 119/263 -- GLOBAL_STEP: 81175\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.64401  (0.63801)\n",
            "     | > G_mse_fake_loss: 0.63857  (0.64448)\n",
            "     | > G_feat_match_loss: 0.08836  (0.08762)\n",
            "     | > G_gen_loss: 28.98058  (28.71049)\n",
            "     | > G_adv_loss: 1.52218  (1.52072)\n",
            "     | > loss_0: 30.50277  (30.23121)\n",
            "     | > grad_norm_0: 1724.47644  (2105.98364)\n",
            "     | > D_mse_gan_loss: 0.20910  (0.21671)\n",
            "     | > D_mse_gan_real_loss: 0.00108  (0.00112)\n",
            "     | > D_mse_gan_fake_loss: 0.00018  (0.00106)\n",
            "     | > loss_1: 0.20910  (0.21671)\n",
            "     | > grad_norm_1: 15.37772  (48.63670)\n",
            "     | > current_lr_0: 3.083288433677972e-06 \n",
            "     | > current_lr_1: 3.083288433677972e-06 \n",
            "     | > step_time: 5.24160  (5.25286)\n",
            "     | > loader_time: 0.00230  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 144/263 -- GLOBAL_STEP: 81200\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.68197  (0.63871)\n",
            "     | > G_mse_fake_loss: 0.61969  (0.64509)\n",
            "     | > G_feat_match_loss: 0.09686  (0.08800)\n",
            "     | > G_gen_loss: 30.68863  (28.74203)\n",
            "     | > G_adv_loss: 1.58827  (1.52513)\n",
            "     | > loss_0: 32.27689  (30.26716)\n",
            "     | > grad_norm_0: 1134.95325  (2083.93555)\n",
            "     | > D_mse_gan_loss: 0.18010  (0.21548)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00109)\n",
            "     | > D_mse_gan_fake_loss: 0.00057  (0.00111)\n",
            "     | > loss_1: 0.18010  (0.21548)\n",
            "     | > grad_norm_1: 63.34906  (47.99248)\n",
            "     | > current_lr_0: 3.0071241566430568e-06 \n",
            "     | > current_lr_1: 3.0071241566430568e-06 \n",
            "     | > step_time: 5.26250  (5.25261)\n",
            "     | > loader_time: 0.00220  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 169/263 -- GLOBAL_STEP: 81225\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62548  (0.63771)\n",
            "     | > G_mse_fake_loss: 0.69435  (0.64579)\n",
            "     | > G_feat_match_loss: 0.08493  (0.08809)\n",
            "     | > G_gen_loss: 28.14651  (28.69717)\n",
            "     | > G_adv_loss: 1.54361  (1.52672)\n",
            "     | > loss_0: 29.69012  (30.22388)\n",
            "     | > grad_norm_0: 2100.33838  (2066.74902)\n",
            "     | > D_mse_gan_loss: 0.23453  (0.21488)\n",
            "     | > D_mse_gan_real_loss: 0.00025  (0.00101)\n",
            "     | > D_mse_gan_fake_loss: 0.00070  (0.00104)\n",
            "     | > loss_1: 0.23453  (0.21488)\n",
            "     | > grad_norm_1: 64.52843  (47.74246)\n",
            "     | > current_lr_0: 2.9328413114692958e-06 \n",
            "     | > current_lr_1: 2.9328413114692958e-06 \n",
            "     | > step_time: 5.24860  (5.25214)\n",
            "     | > loader_time: 0.00220  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 194/263 -- GLOBAL_STEP: 81250\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61130  (0.63676)\n",
            "     | > G_mse_fake_loss: 0.74469  (0.64600)\n",
            "     | > G_feat_match_loss: 0.08808  (0.08808)\n",
            "     | > G_gen_loss: 27.50833  (28.65437)\n",
            "     | > G_adv_loss: 1.62549  (1.52683)\n",
            "     | > loss_0: 29.13381  (30.18120)\n",
            "     | > grad_norm_0: 2155.00806  (2058.18774)\n",
            "     | > D_mse_gan_loss: 0.24059  (0.21479)\n",
            "     | > D_mse_gan_real_loss: 0.00113  (0.00102)\n",
            "     | > D_mse_gan_fake_loss: 0.00213  (0.00106)\n",
            "     | > loss_1: 0.24059  (0.21479)\n",
            "     | > grad_norm_1: 118.56618  (50.29399)\n",
            "     | > current_lr_0: 2.860393422486126e-06 \n",
            "     | > current_lr_1: 2.860393422486126e-06 \n",
            "     | > step_time: 5.25300  (5.25215)\n",
            "     | > loader_time: 0.00200  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 219/263 -- GLOBAL_STEP: 81275\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63468  (0.63701)\n",
            "     | > G_mse_fake_loss: 0.71941  (0.64713)\n",
            "     | > G_feat_match_loss: 0.08650  (0.08814)\n",
            "     | > G_gen_loss: 28.56054  (28.66554)\n",
            "     | > G_adv_loss: 1.58438  (1.52857)\n",
            "     | > loss_0: 30.14491  (30.19411)\n",
            "     | > grad_norm_0: 2195.15112  (2041.45935)\n",
            "     | > D_mse_gan_loss: 0.23961  (0.21474)\n",
            "     | > D_mse_gan_real_loss: 0.00038  (0.00099)\n",
            "     | > D_mse_gan_fake_loss: 0.00039  (0.00108)\n",
            "     | > loss_1: 0.23961  (0.21474)\n",
            "     | > grad_norm_1: 51.59241  (51.75118)\n",
            "     | > current_lr_0: 2.7897351620783573e-06 \n",
            "     | > current_lr_1: 2.7897351620783573e-06 \n",
            "     | > step_time: 5.25480  (5.25237)\n",
            "     | > loader_time: 0.00230  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 244/263 -- GLOBAL_STEP: 81300\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.67300  (0.63665)\n",
            "     | > G_mse_fake_loss: 0.65903  (0.64718)\n",
            "     | > G_feat_match_loss: 0.08883  (0.08799)\n",
            "     | > G_gen_loss: 30.28494  (28.64934)\n",
            "     | > G_adv_loss: 1.54733  (1.52704)\n",
            "     | > loss_0: 31.83228  (30.17637)\n",
            "     | > grad_norm_0: 2776.48828  (2057.73682)\n",
            "     | > D_mse_gan_loss: 0.20618  (0.21524)\n",
            "     | > D_mse_gan_real_loss: 0.00010  (0.00102)\n",
            "     | > D_mse_gan_fake_loss: 0.00438  (0.00110)\n",
            "     | > loss_1: 0.20618  (0.21524)\n",
            "     | > grad_norm_1: 49.29056  (52.93502)\n",
            "     | > current_lr_0: 2.7208223223265737e-06 \n",
            "     | > current_lr_1: 2.7208223223265737e-06 \n",
            "     | > step_time: 5.24110  (5.25241)\n",
            "     | > loader_time: 0.00240  (0.00223)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00158 \u001b[0m(+0.00020)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 0.68835 \u001b[0m(+0.00667)\n",
            "     | > avg_G_mse_fake_loss:\u001b[92m 0.59364 \u001b[0m(-0.01093)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.10150 \u001b[0m(-0.00070)\n",
            "     | > avg_G_gen_loss:\u001b[91m 30.97581 \u001b[0m(+0.30003)\n",
            "     | > avg_G_adv_loss:\u001b[92m 1.60859 \u001b[0m(-0.01793)\n",
            "     | > avg_loss_0:\u001b[91m 32.58440 \u001b[0m(+0.28210)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.35929 \u001b[0m(-0.01121)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.15530 \u001b[0m(+0.00382)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00097 \u001b[0m(-0.00145)\n",
            "     | > avg_loss_1:\u001b[92m 0.35929 \u001b[0m(-0.01121)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 5/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-15 20:05:52) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/263 -- GLOBAL_STEP: 81325\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59882  (0.60782)\n",
            "     | > G_mse_fake_loss: 0.66682  (0.62814)\n",
            "     | > G_feat_match_loss: 0.08190  (0.08011)\n",
            "     | > G_gen_loss: 26.94679  (27.35173)\n",
            "     | > G_adv_loss: 1.48586  (1.42923)\n",
            "     | > loss_0: 28.43265  (28.78096)\n",
            "     | > grad_norm_0: 2144.19629  (2429.84937)\n",
            "     | > D_mse_gan_loss: 0.23251  (0.23610)\n",
            "     | > D_mse_gan_real_loss: 0.00305  (0.00199)\n",
            "     | > D_mse_gan_fake_loss: 0.00201  (0.00163)\n",
            "     | > loss_1: 0.23251  (0.23610)\n",
            "     | > grad_norm_1: 61.27013  (32.85234)\n",
            "     | > current_lr_0: 2.653611787348091e-06 \n",
            "     | > current_lr_1: 2.653611787348091e-06 \n",
            "     | > step_time: 5.31560  (5.28488)\n",
            "     | > loader_time: 0.00090  (0.00197)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/263 -- GLOBAL_STEP: 81350\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63120  (0.60989)\n",
            "     | > G_mse_fake_loss: 0.60219  (0.62873)\n",
            "     | > G_feat_match_loss: 0.08372  (0.08396)\n",
            "     | > G_gen_loss: 28.40417  (27.44517)\n",
            "     | > G_adv_loss: 1.43936  (1.46835)\n",
            "     | > loss_0: 29.84352  (28.91352)\n",
            "     | > grad_norm_0: 2485.23584  (2026.81836)\n",
            "     | > D_mse_gan_loss: 0.22718  (0.22522)\n",
            "     | > D_mse_gan_real_loss: 0.00054  (0.00143)\n",
            "     | > D_mse_gan_fake_loss: 0.00025  (0.00135)\n",
            "     | > loss_1: 0.22718  (0.22522)\n",
            "     | > grad_norm_1: 59.51613  (31.11282)\n",
            "     | > current_lr_0: 2.5880615063211527e-06 \n",
            "     | > current_lr_1: 2.5880615063211527e-06 \n",
            "     | > step_time: 5.24380  (5.25810)\n",
            "     | > loader_time: 0.00210  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/263 -- GLOBAL_STEP: 81375\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63169  (0.61373)\n",
            "     | > G_mse_fake_loss: 0.66120  (0.63154)\n",
            "     | > G_feat_match_loss: 0.09221  (0.08492)\n",
            "     | > G_gen_loss: 28.42585  (27.61795)\n",
            "     | > G_adv_loss: 1.58335  (1.48076)\n",
            "     | > loss_0: 30.00920  (29.09871)\n",
            "     | > grad_norm_0: 1543.46655  (1937.99402)\n",
            "     | > D_mse_gan_loss: 0.21071  (0.22323)\n",
            "     | > D_mse_gan_real_loss: 0.00023  (0.00098)\n",
            "     | > D_mse_gan_fake_loss: 0.00469  (0.00113)\n",
            "     | > loss_1: 0.21071  (0.22323)\n",
            "     | > grad_norm_1: 32.90248  (28.24027)\n",
            "     | > current_lr_0: 2.5241304671754868e-06 \n",
            "     | > current_lr_1: 2.5241304671754868e-06 \n",
            "     | > step_time: 5.26310  (5.25472)\n",
            "     | > loader_time: 0.00200  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/263 -- GLOBAL_STEP: 81400\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58635  (0.61353)\n",
            "     | > G_mse_fake_loss: 0.59371  (0.63428)\n",
            "     | > G_feat_match_loss: 0.08417  (0.08486)\n",
            "     | > G_gen_loss: 26.38584  (27.60894)\n",
            "     | > G_adv_loss: 1.43537  (1.48293)\n",
            "     | > loss_0: 27.82121  (29.09187)\n",
            "     | > grad_norm_0: 1636.84668  (1943.13281)\n",
            "     | > D_mse_gan_loss: 0.21601  (0.22376)\n",
            "     | > D_mse_gan_real_loss: 0.00035  (0.00094)\n",
            "     | > D_mse_gan_fake_loss: 0.00019  (0.00101)\n",
            "     | > loss_1: 0.21601  (0.22376)\n",
            "     | > grad_norm_1: 67.01815  (37.80290)\n",
            "     | > current_lr_0: 2.461778670932767e-06 \n",
            "     | > current_lr_1: 2.461778670932767e-06 \n",
            "     | > step_time: 5.26570  (5.25603)\n",
            "     | > loader_time: 0.00240  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/263 -- GLOBAL_STEP: 81425\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57957  (0.61243)\n",
            "     | > G_mse_fake_loss: 0.62820  (0.63540)\n",
            "     | > G_feat_match_loss: 0.08175  (0.08491)\n",
            "     | > G_gen_loss: 26.08073  (27.55929)\n",
            "     | > G_adv_loss: 1.44569  (1.48446)\n",
            "     | > loss_0: 27.52642  (29.04375)\n",
            "     | > grad_norm_0: 1835.05859  (1913.77100)\n",
            "     | > D_mse_gan_loss: 0.22292  (0.22354)\n",
            "     | > D_mse_gan_real_loss: 0.00012  (0.00102)\n",
            "     | > D_mse_gan_fake_loss: 0.00045  (0.00093)\n",
            "     | > loss_1: 0.22292  (0.22354)\n",
            "     | > grad_norm_1: 32.78844  (42.59929)\n",
            "     | > current_lr_0: 2.400967106680925e-06 \n",
            "     | > current_lr_1: 2.400967106680925e-06 \n",
            "     | > step_time: 5.25280  (5.25529)\n",
            "     | > loader_time: 0.00200  (0.00220)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/263 -- GLOBAL_STEP: 81450\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59800  (0.61181)\n",
            "     | > G_mse_fake_loss: 0.66012  (0.63635)\n",
            "     | > G_feat_match_loss: 0.09085  (0.08521)\n",
            "     | > G_gen_loss: 26.90978  (27.53127)\n",
            "     | > G_adv_loss: 1.56861  (1.48849)\n",
            "     | > loss_0: 28.47839  (29.01976)\n",
            "     | > grad_norm_0: 1122.24487  (1879.81470)\n",
            "     | > D_mse_gan_loss: 0.20117  (0.22254)\n",
            "     | > D_mse_gan_real_loss: 0.00241  (0.00097)\n",
            "     | > D_mse_gan_fake_loss: 0.00047  (0.00091)\n",
            "     | > loss_1: 0.20117  (0.22254)\n",
            "     | > grad_norm_1: 54.02392  (45.64714)\n",
            "     | > current_lr_0: 2.341657727166656e-06 \n",
            "     | > current_lr_1: 2.341657727166656e-06 \n",
            "     | > step_time: 5.25220  (5.25356)\n",
            "     | > loader_time: 0.00240  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 155/263 -- GLOBAL_STEP: 81475\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61560  (0.61100)\n",
            "     | > G_mse_fake_loss: 0.62170  (0.63525)\n",
            "     | > G_feat_match_loss: 0.07927  (0.08505)\n",
            "     | > G_gen_loss: 27.70188  (27.49515)\n",
            "     | > G_adv_loss: 1.41444  (1.48576)\n",
            "     | > loss_0: 29.11633  (28.98091)\n",
            "     | > grad_norm_0: 2030.44189  (1894.49878)\n",
            "     | > D_mse_gan_loss: 0.24182  (0.22307)\n",
            "     | > D_mse_gan_real_loss: 0.00013  (0.00106)\n",
            "     | > D_mse_gan_fake_loss: 0.00350  (0.00105)\n",
            "     | > loss_1: 0.24182  (0.22307)\n",
            "     | > grad_norm_1: 58.88052  (46.43197)\n",
            "     | > current_lr_0: 2.2838134249908384e-06 \n",
            "     | > current_lr_1: 2.2838134249908384e-06 \n",
            "     | > step_time: 5.24720  (5.25243)\n",
            "     | > loader_time: 0.00250  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 180/263 -- GLOBAL_STEP: 81500\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63172  (0.61094)\n",
            "     | > G_mse_fake_loss: 0.66640  (0.63519)\n",
            "     | > G_feat_match_loss: 0.08617  (0.08511)\n",
            "     | > G_gen_loss: 28.42748  (27.49210)\n",
            "     | > G_adv_loss: 1.52812  (1.48624)\n",
            "     | > loss_0: 29.95560  (28.97834)\n",
            "     | > grad_norm_0: 2186.07056  (1879.86414)\n",
            "     | > D_mse_gan_loss: 0.22129  (0.22278)\n",
            "     | > D_mse_gan_real_loss: 0.00014  (0.00102)\n",
            "     | > D_mse_gan_fake_loss: 0.00076  (0.00113)\n",
            "     | > loss_1: 0.22129  (0.22278)\n",
            "     | > grad_norm_1: 16.87628  (44.11756)\n",
            "     | > current_lr_0: 2.227398009391991e-06 \n",
            "     | > current_lr_1: 2.227398009391991e-06 \n",
            "     | > step_time: 5.25010  (5.25224)\n",
            "     | > loader_time: 0.00210  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 205/263 -- GLOBAL_STEP: 81525\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62988  (0.61077)\n",
            "     | > G_mse_fake_loss: 0.60672  (0.63587)\n",
            "     | > G_feat_match_loss: 0.08894  (0.08514)\n",
            "     | > G_gen_loss: 28.34464  (27.48463)\n",
            "     | > G_adv_loss: 1.49611  (1.48722)\n",
            "     | > loss_0: 29.84075  (28.97185)\n",
            "     | > grad_norm_0: 1359.46484  (1873.65308)\n",
            "     | > D_mse_gan_loss: 0.21552  (0.22287)\n",
            "     | > D_mse_gan_real_loss: 0.00021  (0.00102)\n",
            "     | > D_mse_gan_fake_loss: 0.00150  (0.00116)\n",
            "     | > loss_1: 0.21552  (0.22287)\n",
            "     | > grad_norm_1: 57.83472  (44.03362)\n",
            "     | > current_lr_0: 2.1723761836032224e-06 \n",
            "     | > current_lr_1: 2.1723761836032224e-06 \n",
            "     | > step_time: 5.24340  (5.25181)\n",
            "     | > loader_time: 0.00260  (0.00223)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/263 -- GLOBAL_STEP: 81550\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61369  (0.61044)\n",
            "     | > G_mse_fake_loss: 0.66198  (0.63533)\n",
            "     | > G_feat_match_loss: 0.08418  (0.08513)\n",
            "     | > G_gen_loss: 27.61618  (27.46984)\n",
            "     | > G_adv_loss: 1.50378  (1.48664)\n",
            "     | > loss_0: 29.11996  (28.95649)\n",
            "     | > grad_norm_0: 1949.54126  (1870.25354)\n",
            "     | > D_mse_gan_loss: 0.22873  (0.22284)\n",
            "     | > D_mse_gan_real_loss: 0.00099  (0.00103)\n",
            "     | > D_mse_gan_fake_loss: 0.00031  (0.00111)\n",
            "     | > loss_1: 0.22873  (0.22284)\n",
            "     | > grad_norm_1: 13.87878  (43.06942)\n",
            "     | > current_lr_0: 2.1187135227685247e-06 \n",
            "     | > current_lr_1: 2.1187135227685247e-06 \n",
            "     | > step_time: 5.26220  (5.25171)\n",
            "     | > loader_time: 0.00240  (0.00224)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 255/263 -- GLOBAL_STEP: 81575\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.65237  (0.61098)\n",
            "     | > G_mse_fake_loss: 0.59821  (0.63501)\n",
            "     | > G_feat_match_loss: 0.09748  (0.08519)\n",
            "     | > G_gen_loss: 29.35645  (27.49394)\n",
            "     | > G_adv_loss: 1.57301  (1.48694)\n",
            "     | > loss_0: 30.92946  (28.98087)\n",
            "     | > grad_norm_0: 1013.56812  (1854.70728)\n",
            "     | > D_mse_gan_loss: 0.19745  (0.22265)\n",
            "     | > D_mse_gan_real_loss: 0.00027  (0.00099)\n",
            "     | > D_mse_gan_fake_loss: 0.00034  (0.00110)\n",
            "     | > loss_1: 0.19745  (0.22265)\n",
            "     | > grad_norm_1: 36.19930  (41.46160)\n",
            "     | > current_lr_0: 2.0663764524045725e-06 \n",
            "     | > current_lr_1: 2.0663764524045725e-06 \n",
            "     | > step_time: 5.24470  (5.25205)\n",
            "     | > loader_time: 0.00210  (0.00224)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00159 \u001b[0m(+0.00000)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.66996 \u001b[0m(-0.01839)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.59664 \u001b[0m(+0.00300)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.10288 \u001b[0m(+0.00139)\n",
            "     | > avg_G_gen_loss:\u001b[92m 30.14823 \u001b[0m(-0.82758)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.62546 \u001b[0m(+0.01687)\n",
            "     | > avg_loss_0:\u001b[92m 31.77369 \u001b[0m(-0.81071)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.36240 \u001b[0m(+0.00311)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.14692 \u001b[0m(-0.00838)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00520 \u001b[0m(+0.00424)\n",
            "     | > avg_loss_1:\u001b[91m 0.36240 \u001b[0m(+0.00311)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 6/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-15 20:29:07) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/263 -- GLOBAL_STEP: 81600\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61682  (0.62291)\n",
            "     | > G_mse_fake_loss: 0.71555  (0.65137)\n",
            "     | > G_feat_match_loss: 0.08891  (0.08768)\n",
            "     | > G_gen_loss: 27.75679  (28.03112)\n",
            "     | > G_adv_loss: 1.60470  (1.52816)\n",
            "     | > loss_0: 29.36148  (29.55928)\n",
            "     | > grad_norm_0: 2307.77222  (1835.68994)\n",
            "     | > D_mse_gan_loss: 0.22057  (0.21331)\n",
            "     | > D_mse_gan_real_loss: 0.00104  (0.00096)\n",
            "     | > D_mse_gan_fake_loss: 0.00078  (0.00105)\n",
            "     | > loss_1: 0.22057  (0.21331)\n",
            "     | > grad_norm_1: 66.23135  (44.41913)\n",
            "     | > current_lr_0: 2.0153322273945777e-06 \n",
            "     | > current_lr_1: 2.0153322273945777e-06 \n",
            "     | > step_time: 5.24280  (5.25966)\n",
            "     | > loader_time: 0.00210  (0.00210)\n",
            "\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Emergent/train/hifigan \\\n",
        "    --continue_path /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xA2Y-0_t48Q",
        "outputId": "7d6ab23b-ec1a-4471-8b70-886179abf0ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting TTS\n",
            "  Downloading TTS-0.4.2.tar.gz (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from TTS) (3.6.4)\n",
            "Requirement already satisfied: pyworld in /usr/local/lib/python3.7/dist-packages (from TTS) (0.3.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (from TTS) (2.4.1)\n",
            "Requirement already satisfied: fsspec>=2021.04.0 in /usr/local/lib/python3.7/dist-packages (from TTS) (2021.11.1)\n",
            "Requirement already satisfied: coqpit in /usr/local/lib/python3.7/dist-packages (from TTS) (0.0.14)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from TTS) (0.42.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from TTS) (1.4.1)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from TTS) (2.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from TTS) (3.13)\n",
            "Requirement already satisfied: mecab-python3==1.0.3 in /usr/local/lib/python3.7/dist-packages (from TTS) (1.0.3)\n",
            "Requirement already satisfied: gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from TTS) (2.0.4)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from TTS) (0.10.3.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from TTS) (3.2.2)\n",
            "Requirement already satisfied: librosa==0.8.0 in /usr/local/lib/python3.7/dist-packages (from TTS) (0.8.0)\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from TTS) (1.19.5)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.7/dist-packages (from TTS) (0.3.4)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from TTS) (1.10.0+cu111)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from TTS) (0.3.0)\n",
            "Requirement already satisfied: umap-learn==0.5.1 in /usr/local/lib/python3.7/dist-packages (from TTS) (0.5.1)\n",
            "Requirement already satisfied: unidic-lite==1.0.8 in /usr/local/lib/python3.7/dist-packages (from TTS) (1.0.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from TTS) (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from TTS) (4.62.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from TTS) (1.1.4)\n",
            "Requirement already satisfied: pypinyin in /usr/local/lib/python3.7/dist-packages (from TTS) (0.44.0)\n",
            "Requirement already satisfied: numba==0.53 in /usr/local/lib/python3.7/dist-packages (from TTS) (0.53.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from TTS) (0.29.24)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (1.1.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (2.1.9)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS) (1.5.2)\n",
            "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba==0.53->TTS) (0.36.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.53->TTS) (57.4.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.5.1->TTS) (0.5.5)\n",
            "Requirement already satisfied: dateparser~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (1.0.0)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.9.1)\n",
            "Requirement already satisfied: gruut-ipa~=0.10.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (0.10.1)\n",
            "Requirement already satisfied: networkx<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.6.3)\n",
            "Requirement already satisfied: jsonlines~=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (1.2.0)\n",
            "Requirement already satisfied: python-crfsuite~=0.9.7 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (0.9.7)\n",
            "Requirement already satisfied: num2words<1.0.0,>=0.5.10 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (0.5.10)\n",
            "Requirement already satisfied: gruut-lang-ru~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-sv~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-cs~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-fr~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-pt~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-it~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-de~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-nl~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-es~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.0.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from Babel<3.0.0,>=2.8.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2018.9)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.7/dist-packages (from dateparser~=1.0.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2019.12.20)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser~=1.0.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser~=1.0.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from jsonlines~=1.2.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (1.15.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words<1.0.0,>=0.5.10->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=2.0.0->TTS) (0.6.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS) (2.23.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.0->TTS) (3.0.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->TTS) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->TTS) (2.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->TTS) (3.10.0.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->TTS) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->TTS) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->TTS) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->TTS) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->TTS) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS) (0.11.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS) (3.0.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->TTS) (3.17.3)\n",
            "Building wheels for collected packages: TTS\n",
            "  Building wheel for TTS (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for TTS: filename=TTS-0.4.2-cp37-cp37m-linux_x86_64.whl size=572310 sha256=cdf0cd24d4a9befd487386939076f27dc97d4790a6cbfa872a989aa70035daa0\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/a5/d8/ad1b09ee4031f7369d6174a975a8ce30c528f3f8949f4fb70b\n",
            "Successfully built TTS\n",
            "Installing collected packages: TTS\n",
            "Successfully installed TTS-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixx0lUS3zSmA",
        "outputId": "d44b2cd1-cba8-46e0-b099-b2eaf5f0e6a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "     | > G_mse_fake_loss: 0.74517  (0.76021)\n",
            "     | > G_feat_match_loss: 0.12094  (0.11585)\n",
            "     | > G_gen_loss: 26.86922  (24.93018)\n",
            "     | > G_adv_loss: 1.95461  (1.91871)\n",
            "     | > loss_0: 28.82382  (26.84889)\n",
            "     | > grad_norm_0: 796.71436  (780.65729)\n",
            "     | > D_mse_gan_loss: 0.10547  (0.11737)\n",
            "     | > D_mse_gan_real_loss: 0.00010  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00018)\n",
            "     | > loss_1: 0.10547  (0.11737)\n",
            "     | > grad_norm_1: 31.43296  (19.35070)\n",
            "     | > current_lr_0: 4.864476739897787e-10 \n",
            "     | > current_lr_1: 4.864476739897787e-10 \n",
            "     | > step_time: 2.42680  (2.42746)\n",
            "     | > loader_time: 0.00230  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 95/364 -- GLOBAL_STEP: 189950\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54956  (0.55310)\n",
            "     | > G_mse_fake_loss: 0.75910  (0.75976)\n",
            "     | > G_feat_match_loss: 0.10695  (0.11561)\n",
            "     | > G_gen_loss: 24.73024  (24.88942)\n",
            "     | > G_adv_loss: 1.82857  (1.91591)\n",
            "     | > loss_0: 26.55880  (26.80533)\n",
            "     | > grad_norm_0: 994.30225  (787.26312)\n",
            "     | > D_mse_gan_loss: 0.14002  (0.11777)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00017)\n",
            "     | > loss_1: 0.14002  (0.11777)\n",
            "     | > grad_norm_1: 22.41882  (18.50105)\n",
            "     | > current_lr_0: 4.744313037403853e-10 \n",
            "     | > current_lr_1: 4.744313037403853e-10 \n",
            "     | > step_time: 2.42320  (2.42650)\n",
            "     | > loader_time: 0.00240  (0.00220)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/364 -- GLOBAL_STEP: 189975\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54769  (0.55267)\n",
            "     | > G_mse_fake_loss: 0.75995  (0.75933)\n",
            "     | > G_feat_match_loss: 0.10627  (0.11540)\n",
            "     | > G_gen_loss: 24.64586  (24.87018)\n",
            "     | > G_adv_loss: 1.82268  (1.91334)\n",
            "     | > loss_0: 26.46854  (26.78353)\n",
            "     | > grad_norm_0: 887.15302  (791.78247)\n",
            "     | > D_mse_gan_loss: 0.14811  (0.11845)\n",
            "     | > D_mse_gan_real_loss: 0.00034  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00016)\n",
            "     | > loss_1: 0.14811  (0.11845)\n",
            "     | > grad_norm_1: 25.05372  (18.41314)\n",
            "     | > current_lr_0: 4.6271176532243247e-10 \n",
            "     | > current_lr_1: 4.6271176532243247e-10 \n",
            "     | > step_time: 2.42850  (2.42609)\n",
            "     | > loader_time: 0.00250  (0.00223)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 145/364 -- GLOBAL_STEP: 190000\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54032  (0.55268)\n",
            "     | > G_mse_fake_loss: 0.76042  (0.75926)\n",
            "     | > G_feat_match_loss: 0.11813  (0.11541)\n",
            "     | > G_gen_loss: 24.31459  (24.87054)\n",
            "     | > G_adv_loss: 1.94172  (1.91340)\n",
            "     | > loss_0: 26.25631  (26.78394)\n",
            "     | > grad_norm_0: 702.87256  (787.49207)\n",
            "     | > D_mse_gan_loss: 0.11216  (0.11821)\n",
            "     | > D_mse_gan_real_loss: 0.00007  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00015)\n",
            "     | > loss_1: 0.11216  (0.11821)\n",
            "     | > grad_norm_1: 13.42245  (18.77736)\n",
            "     | > current_lr_0: 4.512817263107098e-10 \n",
            "     | > current_lr_1: 4.512817263107098e-10 \n",
            "     | > step_time: 2.41950  (2.42606)\n",
            "     | > loader_time: 0.00230  (0.00223)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2/checkpoint_190000.pth.tar\n",
            "\n",
            "\u001b[1m   --> STEP: 170/364 -- GLOBAL_STEP: 190025\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53782  (0.55294)\n",
            "     | > G_mse_fake_loss: 0.77575  (0.75977)\n",
            "     | > G_feat_match_loss: 0.11282  (0.11558)\n",
            "     | > G_gen_loss: 24.20169  (24.88221)\n",
            "     | > G_adv_loss: 1.90391  (1.91554)\n",
            "     | > loss_0: 26.10560  (26.79775)\n",
            "     | > grad_norm_0: 739.82422  (787.42114)\n",
            "     | > D_mse_gan_loss: 0.12162  (0.11776)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00014)\n",
            "     | > loss_1: 0.12162  (0.11776)\n",
            "     | > grad_norm_1: 17.62773  (18.52685)\n",
            "     | > current_lr_0: 4.4013403540768164e-10 \n",
            "     | > current_lr_1: 4.4013403540768164e-10 \n",
            "     | > step_time: 2.41680  (2.42611)\n",
            "     | > loader_time: 0.00220  (0.00224)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 195/364 -- GLOBAL_STEP: 190050\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58340  (0.55322)\n",
            "     | > G_mse_fake_loss: 0.74130  (0.75945)\n",
            "     | > G_feat_match_loss: 0.12381  (0.11574)\n",
            "     | > G_gen_loss: 26.25287  (24.89495)\n",
            "     | > G_adv_loss: 1.97937  (1.91688)\n",
            "     | > loss_0: 28.23224  (26.81182)\n",
            "     | > grad_norm_0: 667.48877  (786.48041)\n",
            "     | > D_mse_gan_loss: 0.09222  (0.11731)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00018  (0.00014)\n",
            "     | > loss_1: 0.09222  (0.11731)\n",
            "     | > grad_norm_1: 45.48721  (18.58870)\n",
            "     | > current_lr_0: 4.292617179692194e-10 \n",
            "     | > current_lr_1: 4.292617179692194e-10 \n",
            "     | > step_time: 2.42110  (2.42606)\n",
            "     | > loader_time: 0.00210  (0.00225)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 220/364 -- GLOBAL_STEP: 190075\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54684  (0.55320)\n",
            "     | > G_mse_fake_loss: 0.76213  (0.75971)\n",
            "     | > G_feat_match_loss: 0.11768  (0.11575)\n",
            "     | > G_gen_loss: 24.60773  (24.89410)\n",
            "     | > G_adv_loss: 1.93898  (1.91723)\n",
            "     | > loss_0: 26.54671  (26.81133)\n",
            "     | > grad_norm_0: 752.45374  (788.83710)\n",
            "     | > D_mse_gan_loss: 0.11648  (0.11746)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00013)\n",
            "     | > loss_1: 0.11648  (0.11746)\n",
            "     | > grad_norm_1: 9.89283  (18.27473)\n",
            "     | > current_lr_0: 4.186579716408582e-10 \n",
            "     | > current_lr_1: 4.186579716408582e-10 \n",
            "     | > step_time: 2.43130  (2.42609)\n",
            "     | > loader_time: 0.00230  (0.00226)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 245/364 -- GLOBAL_STEP: 190100\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55157  (0.55331)\n",
            "     | > G_mse_fake_loss: 0.76807  (0.75986)\n",
            "     | > G_feat_match_loss: 0.11989  (0.11585)\n",
            "     | > G_gen_loss: 24.82082  (24.89894)\n",
            "     | > G_adv_loss: 1.96695  (1.91841)\n",
            "     | > loss_0: 26.78777  (26.81734)\n",
            "     | > grad_norm_0: 689.64240  (788.58710)\n",
            "     | > D_mse_gan_loss: 0.10696  (0.11713)\n",
            "     | > D_mse_gan_real_loss: 0.00022  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00014)\n",
            "     | > loss_1: 0.10696  (0.11713)\n",
            "     | > grad_norm_1: 18.02003  (18.23406)\n",
            "     | > current_lr_0: 4.083161621018482e-10 \n",
            "     | > current_lr_1: 4.083161621018482e-10 \n",
            "     | > step_time: 2.42260  (2.42613)\n",
            "     | > loader_time: 0.00220  (0.00227)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 270/364 -- GLOBAL_STEP: 190125\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57300  (0.55326)\n",
            "     | > G_mse_fake_loss: 0.76657  (0.76011)\n",
            "     | > G_feat_match_loss: 0.11820  (0.11582)\n",
            "     | > G_gen_loss: 25.78505  (24.89655)\n",
            "     | > G_adv_loss: 1.94853  (1.91833)\n",
            "     | > loss_0: 27.73358  (26.81488)\n",
            "     | > grad_norm_0: 789.41492  (789.54126)\n",
            "     | > D_mse_gan_loss: 0.10617  (0.11721)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00013)\n",
            "     | > loss_1: 0.10617  (0.11721)\n",
            "     | > grad_norm_1: 14.65371  (18.20611)\n",
            "     | > current_lr_0: 3.9822981891433755e-10 \n",
            "     | > current_lr_1: 3.9822981891433755e-10 \n",
            "     | > step_time: 2.42640  (2.42595)\n",
            "     | > loader_time: 0.00250  (0.00229)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 295/364 -- GLOBAL_STEP: 190150\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55892  (0.55359)\n",
            "     | > G_mse_fake_loss: 0.76699  (0.75986)\n",
            "     | > G_feat_match_loss: 0.11762  (0.11586)\n",
            "     | > G_gen_loss: 25.15146  (24.91176)\n",
            "     | > G_adv_loss: 1.94320  (1.91849)\n",
            "     | > loss_0: 27.09466  (26.83025)\n",
            "     | > grad_norm_0: 852.84875  (788.67700)\n",
            "     | > D_mse_gan_loss: 0.10856  (0.11715)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00014)\n",
            "     | > loss_1: 0.10856  (0.11715)\n",
            "     | > grad_norm_1: 10.32644  (18.09102)\n",
            "     | > current_lr_0: 3.8839263147508944e-10 \n",
            "     | > current_lr_1: 3.8839263147508944e-10 \n",
            "     | > step_time: 2.42500  (2.42584)\n",
            "     | > loader_time: 0.00230  (0.00229)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 320/364 -- GLOBAL_STEP: 190175\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53021  (0.55425)\n",
            "     | > G_mse_fake_loss: 0.76605  (0.75982)\n",
            "     | > G_feat_match_loss: 0.11152  (0.11598)\n",
            "     | > G_gen_loss: 23.85950  (24.94121)\n",
            "     | > G_adv_loss: 1.88125  (1.91958)\n",
            "     | > loss_0: 25.74075  (26.86079)\n",
            "     | > grad_norm_0: 728.24414  (787.73151)\n",
            "     | > D_mse_gan_loss: 0.12578  (0.11700)\n",
            "     | > D_mse_gan_real_loss: 0.00120  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00013)\n",
            "     | > loss_1: 0.12578  (0.11700)\n",
            "     | > grad_norm_1: 14.33056  (18.26855)\n",
            "     | > current_lr_0: 3.787984450672025e-10 \n",
            "     | > current_lr_1: 3.787984450672025e-10 \n",
            "     | > step_time: 2.43380  (2.42577)\n",
            "     | > loader_time: 0.00220  (0.00230)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 345/364 -- GLOBAL_STEP: 190200\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55705  (0.55406)\n",
            "     | > G_mse_fake_loss: 0.76713  (0.75992)\n",
            "     | > G_feat_match_loss: 0.11919  (0.11596)\n",
            "     | > G_gen_loss: 25.06724  (24.93291)\n",
            "     | > G_adv_loss: 1.95903  (1.91951)\n",
            "     | > loss_0: 27.02627  (26.85242)\n",
            "     | > grad_norm_0: 823.69861  (788.50110)\n",
            "     | > D_mse_gan_loss: 0.10702  (0.11701)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00092  (0.00013)\n",
            "     | > loss_1: 0.10702  (0.11701)\n",
            "     | > grad_norm_1: 12.49172  (18.41542)\n",
            "     | > current_lr_0: 3.6944125700936065e-10 \n",
            "     | > current_lr_1: 3.6944125700936065e-10 \n",
            "     | > step_time: 2.42480  (2.42567)\n",
            "     | > loader_time: 0.00260  (0.00230)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00160 \u001b[0m(+0.00016)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.58177 \u001b[0m(-0.00004)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70154 \u001b[0m(+0.00023)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11151 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 26.17945 \u001b[0m(-0.00181)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81660 \u001b[0m(+0.00024)\n",
            "     | > avg_loss_0:\u001b[92m 27.99605 \u001b[0m(-0.00157)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.27952 \u001b[0m(+0.00003)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.13728 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00056 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.27952 \u001b[0m(+0.00003)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 28/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 14:51:12) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/364 -- GLOBAL_STEP: 190225\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58895  (0.57068)\n",
            "     | > G_mse_fake_loss: 0.72565  (0.74313)\n",
            "     | > G_feat_match_loss: 0.11611  (0.11454)\n",
            "     | > G_gen_loss: 26.50292  (25.68038)\n",
            "     | > G_adv_loss: 1.88680  (1.88849)\n",
            "     | > loss_0: 28.38972  (27.56887)\n",
            "     | > grad_norm_0: 728.81335  (877.07031)\n",
            "     | > D_mse_gan_loss: 0.11698  (0.11767)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00024  (0.00020)\n",
            "     | > loss_1: 0.11698  (0.11767)\n",
            "     | > grad_norm_1: 27.45756  (20.74761)\n",
            "     | > current_lr_0: 3.6031521290020707e-10 \n",
            "     | > current_lr_1: 3.6031521290020707e-10 \n",
            "     | > step_time: 2.43150  (2.48605)\n",
            "     | > loader_time: 0.00090  (0.00804)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/364 -- GLOBAL_STEP: 190250\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52472  (0.56447)\n",
            "     | > G_mse_fake_loss: 0.76125  (0.75135)\n",
            "     | > G_feat_match_loss: 0.10639  (0.11363)\n",
            "     | > G_gen_loss: 23.61246  (25.40108)\n",
            "     | > G_adv_loss: 1.82512  (1.88764)\n",
            "     | > loss_0: 25.43758  (27.28872)\n",
            "     | > grad_norm_0: 925.07617  (931.63898)\n",
            "     | > D_mse_gan_loss: 0.13743  (0.12113)\n",
            "     | > D_mse_gan_real_loss: 0.00149  (0.00023)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00010)\n",
            "     | > loss_1: 0.13743  (0.12113)\n",
            "     | > grad_norm_1: 19.98257  (23.85397)\n",
            "     | > current_lr_0: 3.514146029554899e-10 \n",
            "     | > current_lr_1: 3.514146029554899e-10 \n",
            "     | > step_time: 2.42530  (2.43526)\n",
            "     | > loader_time: 0.00240  (0.00314)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/364 -- GLOBAL_STEP: 190275\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55004  (0.56592)\n",
            "     | > G_mse_fake_loss: 0.73746  (0.74955)\n",
            "     | > G_feat_match_loss: 0.09912  (0.11324)\n",
            "     | > G_gen_loss: 24.75171  (25.46633)\n",
            "     | > G_adv_loss: 1.72869  (1.88191)\n",
            "     | > loss_0: 26.48039  (27.34824)\n",
            "     | > grad_norm_0: 1726.92529  (947.98792)\n",
            "     | > D_mse_gan_loss: 0.16168  (0.12334)\n",
            "     | > D_mse_gan_real_loss: 0.00009  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00511  (0.00025)\n",
            "     | > loss_1: 0.16168  (0.12334)\n",
            "     | > grad_norm_1: 33.89730  (24.50679)\n",
            "     | > current_lr_0: 3.427338584356887e-10 \n",
            "     | > current_lr_1: 3.427338584356887e-10 \n",
            "     | > step_time: 2.42070  (2.43064)\n",
            "     | > loader_time: 0.00200  (0.00273)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/364 -- GLOBAL_STEP: 190300\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60403  (0.56692)\n",
            "     | > G_mse_fake_loss: 0.76026  (0.75128)\n",
            "     | > G_feat_match_loss: 0.12219  (0.11382)\n",
            "     | > G_gen_loss: 27.18148  (25.51142)\n",
            "     | > G_adv_loss: 1.98212  (1.88951)\n",
            "     | > loss_0: 29.16360  (27.40093)\n",
            "     | > grad_norm_0: 678.42389  (941.76855)\n",
            "     | > D_mse_gan_loss: 0.11058  (0.12176)\n",
            "     | > D_mse_gan_real_loss: 0.00006  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00020)\n",
            "     | > loss_1: 0.11058  (0.12176)\n",
            "     | > grad_norm_1: 17.69122  (24.55746)\n",
            "     | > current_lr_0: 3.342675481618872e-10 \n",
            "     | > current_lr_1: 3.342675481618872e-10 \n",
            "     | > step_time: 2.42720  (2.42861)\n",
            "     | > loader_time: 0.00220  (0.00258)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/364 -- GLOBAL_STEP: 190325\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51493  (0.56573)\n",
            "     | > G_mse_fake_loss: 0.75908  (0.75233)\n",
            "     | > G_feat_match_loss: 0.10750  (0.11390)\n",
            "     | > G_gen_loss: 23.17192  (25.45788)\n",
            "     | > G_adv_loss: 1.83413  (1.89130)\n",
            "     | > loss_0: 25.00604  (27.34918)\n",
            "     | > grad_norm_0: 860.01379  (941.55609)\n",
            "     | > D_mse_gan_loss: 0.14028  (0.12151)\n",
            "     | > D_mse_gan_real_loss: 0.00007  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00019)\n",
            "     | > loss_1: 0.14028  (0.12151)\n",
            "     | > grad_norm_1: 16.50618  (24.84674)\n",
            "     | > current_lr_0: 3.260103751177117e-10 \n",
            "     | > current_lr_1: 3.260103751177117e-10 \n",
            "     | > step_time: 2.42550  (2.42750)\n",
            "     | > loader_time: 0.00210  (0.00252)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/364 -- GLOBAL_STEP: 190350\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59486  (0.56621)\n",
            "     | > G_mse_fake_loss: 0.72828  (0.75221)\n",
            "     | > G_feat_match_loss: 0.10329  (0.11385)\n",
            "     | > G_gen_loss: 26.76882  (25.47926)\n",
            "     | > G_adv_loss: 1.76116  (1.89071)\n",
            "     | > loss_0: 28.52998  (27.36997)\n",
            "     | > grad_norm_0: 1561.63269  (950.65417)\n",
            "     | > D_mse_gan_loss: 0.14616  (0.12129)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00076  (0.00021)\n",
            "     | > loss_1: 0.14616  (0.12129)\n",
            "     | > grad_norm_1: 23.77779  (24.55964)\n",
            "     | > current_lr_0: 3.179571731352093e-10 \n",
            "     | > current_lr_1: 3.179571731352093e-10 \n",
            "     | > step_time: 2.42600  (2.42680)\n",
            "     | > loader_time: 0.00230  (0.00248)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 155/364 -- GLOBAL_STEP: 190375\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57359  (0.56654)\n",
            "     | > G_mse_fake_loss: 0.74256  (0.75199)\n",
            "     | > G_feat_match_loss: 0.11740  (0.11393)\n",
            "     | > G_gen_loss: 25.81139  (25.49436)\n",
            "     | > G_adv_loss: 1.91659  (1.89130)\n",
            "     | > loss_0: 27.72797  (27.38566)\n",
            "     | > grad_norm_0: 709.74774  (943.38202)\n",
            "     | > D_mse_gan_loss: 0.11311  (0.12105)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00036  (0.00020)\n",
            "     | > loss_1: 0.11311  (0.12105)\n",
            "     | > grad_norm_1: 22.50118  (24.53141)\n",
            "     | > current_lr_0: 3.101029036625928e-10 \n",
            "     | > current_lr_1: 3.101029036625928e-10 \n",
            "     | > step_time: 2.41580  (2.42626)\n",
            "     | > loader_time: 0.00220  (0.00246)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 180/364 -- GLOBAL_STEP: 190400\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56975  (0.56730)\n",
            "     | > G_mse_fake_loss: 0.76645  (0.75203)\n",
            "     | > G_feat_match_loss: 0.11829  (0.11404)\n",
            "     | > G_gen_loss: 25.63870  (25.52865)\n",
            "     | > G_adv_loss: 1.94934  (1.89240)\n",
            "     | > loss_0: 27.58804  (27.42105)\n",
            "     | > grad_norm_0: 712.42572  (941.94647)\n",
            "     | > D_mse_gan_loss: 0.10704  (0.12077)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00019)\n",
            "     | > loss_1: 0.10704  (0.12077)\n",
            "     | > grad_norm_1: 9.67859  (24.24711)\n",
            "     | > current_lr_0: 3.024426526118291e-10 \n",
            "     | > current_lr_1: 3.024426526118291e-10 \n",
            "     | > step_time: 2.41990  (2.42606)\n",
            "     | > loader_time: 0.00230  (0.00244)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 205/364 -- GLOBAL_STEP: 190425\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62532  (0.56752)\n",
            "     | > G_mse_fake_loss: 0.74645  (0.75196)\n",
            "     | > G_feat_match_loss: 0.12577  (0.11415)\n",
            "     | > G_gen_loss: 28.13960  (25.53851)\n",
            "     | > G_adv_loss: 2.00412  (1.89344)\n",
            "     | > loss_0: 30.14372  (27.43196)\n",
            "     | > grad_norm_0: 765.47058  (937.30597)\n",
            "     | > D_mse_gan_loss: 0.10339  (0.12058)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00019)\n",
            "     | > loss_1: 0.10339  (0.12058)\n",
            "     | > grad_norm_1: 36.75616  (24.39913)\n",
            "     | > current_lr_0: 2.949716272841001e-10 \n",
            "     | > current_lr_1: 2.949716272841001e-10 \n",
            "     | > step_time: 2.43090  (2.42606)\n",
            "     | > loader_time: 0.00230  (0.00243)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/364 -- GLOBAL_STEP: 190450\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54994  (0.56789)\n",
            "     | > G_mse_fake_loss: 0.75350  (0.75215)\n",
            "     | > G_feat_match_loss: 0.11029  (0.11426)\n",
            "     | > G_gen_loss: 24.74728  (25.55517)\n",
            "     | > G_adv_loss: 1.85637  (1.89473)\n",
            "     | > loss_0: 26.60365  (27.44990)\n",
            "     | > grad_norm_0: 846.81989  (938.48376)\n",
            "     | > D_mse_gan_loss: 0.13544  (0.12041)\n",
            "     | > D_mse_gan_real_loss: 0.00007  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00019)\n",
            "     | > loss_1: 0.13544  (0.12041)\n",
            "     | > grad_norm_1: 10.25655  (24.33513)\n",
            "     | > current_lr_0: 2.8768515337121147e-10 \n",
            "     | > current_lr_1: 2.8768515337121147e-10 \n",
            "     | > step_time: 2.42650  (2.42602)\n",
            "     | > loader_time: 0.00220  (0.00241)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 255/364 -- GLOBAL_STEP: 190475\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54842  (0.56807)\n",
            "     | > G_mse_fake_loss: 0.76408  (0.75167)\n",
            "     | > G_feat_match_loss: 0.10420  (0.11417)\n",
            "     | > G_gen_loss: 24.67901  (25.56295)\n",
            "     | > G_adv_loss: 1.80613  (1.89339)\n",
            "     | > loss_0: 26.48514  (27.45634)\n",
            "     | > grad_norm_0: 1492.61414  (942.98395)\n",
            "     | > D_mse_gan_loss: 0.14222  (0.12072)\n",
            "     | > D_mse_gan_real_loss: 0.00006  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00019)\n",
            "     | > loss_1: 0.14222  (0.12072)\n",
            "     | > grad_norm_1: 36.76163  (24.51454)\n",
            "     | > current_lr_0: 2.80578672031073e-10 \n",
            "     | > current_lr_1: 2.80578672031073e-10 \n",
            "     | > step_time: 2.42310  (2.42579)\n",
            "     | > loader_time: 0.00220  (0.00240)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 280/364 -- GLOBAL_STEP: 190500\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59798  (0.56819)\n",
            "     | > G_mse_fake_loss: 0.74441  (0.75192)\n",
            "     | > G_feat_match_loss: 0.11971  (0.11425)\n",
            "     | > G_gen_loss: 26.90911  (25.56857)\n",
            "     | > G_adv_loss: 1.94147  (1.89441)\n",
            "     | > loss_0: 28.85058  (27.46298)\n",
            "     | > grad_norm_0: 674.75299  (940.21039)\n",
            "     | > D_mse_gan_loss: 0.11604  (0.12051)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00019)\n",
            "     | > loss_1: 0.11604  (0.12051)\n",
            "     | > grad_norm_1: 23.29560  (24.92922)\n",
            "     | > current_lr_0: 2.7364773703542237e-10 \n",
            "     | > current_lr_1: 2.7364773703542237e-10 \n",
            "     | > step_time: 2.42040  (2.42579)\n",
            "     | > loader_time: 0.00230  (0.00239)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 305/364 -- GLOBAL_STEP: 190525\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53732  (0.56854)\n",
            "     | > G_mse_fake_loss: 0.77083  (0.75144)\n",
            "     | > G_feat_match_loss: 0.11105  (0.11427)\n",
            "     | > G_gen_loss: 24.17937  (25.58413)\n",
            "     | > G_adv_loss: 1.88133  (1.89411)\n",
            "     | > loss_0: 26.06070  (27.47824)\n",
            "     | > grad_norm_0: 930.15039  (938.52234)\n",
            "     | > D_mse_gan_loss: 0.13082  (0.12065)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00019)\n",
            "     | > loss_1: 0.13082  (0.12065)\n",
            "     | > grad_norm_1: 19.43382  (25.19580)\n",
            "     | > current_lr_0: 2.6688801198800563e-10 \n",
            "     | > current_lr_1: 2.6688801198800563e-10 \n",
            "     | > step_time: 2.42450  (2.42575)\n",
            "     | > loader_time: 0.00240  (0.00238)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 330/364 -- GLOBAL_STEP: 190550\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60827  (0.56867)\n",
            "     | > G_mse_fake_loss: 0.74710  (0.75105)\n",
            "     | > G_feat_match_loss: 0.11014  (0.11418)\n",
            "     | > G_gen_loss: 27.37212  (25.59025)\n",
            "     | > G_adv_loss: 1.84850  (1.89284)\n",
            "     | > loss_0: 29.22062  (27.48309)\n",
            "     | > grad_norm_0: 1289.65576  (939.60687)\n",
            "     | > D_mse_gan_loss: 0.12410  (0.12087)\n",
            "     | > D_mse_gan_real_loss: 0.00096  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00019)\n",
            "     | > loss_1: 0.12410  (0.12087)\n",
            "     | > grad_norm_1: 24.04266  (25.23165)\n",
            "     | > current_lr_0: 2.60295267611475e-10 \n",
            "     | > current_lr_1: 2.60295267611475e-10 \n",
            "     | > step_time: 2.42430  (2.42575)\n",
            "     | > loader_time: 0.00230  (0.00238)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 355/364 -- GLOBAL_STEP: 190575\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54019  (0.56843)\n",
            "     | > G_mse_fake_loss: 0.77126  (0.75130)\n",
            "     | > G_feat_match_loss: 0.11559  (0.11416)\n",
            "     | > G_gen_loss: 24.30862  (25.57921)\n",
            "     | > G_adv_loss: 1.92719  (1.89289)\n",
            "     | > loss_0: 26.23581  (27.47210)\n",
            "     | > grad_norm_0: 677.89899  (936.87830)\n",
            "     | > D_mse_gan_loss: 0.11362  (0.12083)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00018)\n",
            "     | > loss_1: 0.11362  (0.12083)\n",
            "     | > grad_norm_1: 7.27141  (25.22313)\n",
            "     | > current_lr_0: 2.5386537910130763e-10 \n",
            "     | > current_lr_1: 2.5386537910130763e-10 \n",
            "     | > step_time: 2.42050  (2.42566)\n",
            "     | > loader_time: 0.00200  (0.00237)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00119 \u001b[0m(-0.00041)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 0.58177 \u001b[0m(+0.00001)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70187 \u001b[0m(+0.00033)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00001)\n",
            "     | > avg_G_gen_loss:\u001b[91m 26.17973 \u001b[0m(+0.00028)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81702 \u001b[0m(+0.00042)\n",
            "     | > avg_loss_0:\u001b[91m 27.99676 \u001b[0m(+0.00071)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.27954 \u001b[0m(+0.00002)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.13729 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00057 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.27954 \u001b[0m(+0.00002)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 29/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 15:06:10) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/364 -- GLOBAL_STEP: 190600\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.65062  (0.56965)\n",
            "     | > G_mse_fake_loss: 0.72891  (0.74831)\n",
            "     | > G_feat_match_loss: 0.12571  (0.11375)\n",
            "     | > G_gen_loss: 29.27797  (25.63434)\n",
            "     | > G_adv_loss: 1.98602  (1.88585)\n",
            "     | > loss_0: 31.26399  (27.52019)\n",
            "     | > grad_norm_0: 724.96246  (907.74371)\n",
            "     | > D_mse_gan_loss: 0.10134  (0.12254)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00039)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00011)\n",
            "     | > loss_1: 0.10134  (0.12254)\n",
            "     | > grad_norm_1: 54.92154  (25.87934)\n",
            "     | > current_lr_0: 2.475943235450874e-10 \n",
            "     | > current_lr_1: 2.475943235450874e-10 \n",
            "     | > step_time: 2.42400  (2.43934)\n",
            "     | > loader_time: 0.00220  (0.00241)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 40/364 -- GLOBAL_STEP: 190625\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54514  (0.56666)\n",
            "     | > G_mse_fake_loss: 0.77416  (0.75105)\n",
            "     | > G_feat_match_loss: 0.12224  (0.11383)\n",
            "     | > G_gen_loss: 24.53120  (25.49948)\n",
            "     | > G_adv_loss: 1.99659  (1.88930)\n",
            "     | > loss_0: 26.52779  (27.38878)\n",
            "     | > grad_norm_0: 678.11627  (945.62256)\n",
            "     | > D_mse_gan_loss: 0.09686  (0.12183)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00025)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00012)\n",
            "     | > loss_1: 0.09686  (0.12183)\n",
            "     | > grad_norm_1: 17.86138  (25.33862)\n",
            "     | > current_lr_0: 2.414781774055367e-10 \n",
            "     | > current_lr_1: 2.414781774055367e-10 \n",
            "     | > step_time: 2.43190  (2.42998)\n",
            "     | > loader_time: 0.00260  (0.00234)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 65/364 -- GLOBAL_STEP: 190650\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55688  (0.56430)\n",
            "     | > G_mse_fake_loss: 0.76921  (0.75229)\n",
            "     | > G_feat_match_loss: 0.11488  (0.11401)\n",
            "     | > G_gen_loss: 25.05956  (25.39360)\n",
            "     | > G_adv_loss: 1.91804  (1.89241)\n",
            "     | > loss_0: 26.97760  (27.28601)\n",
            "     | > grad_norm_0: 728.44873  (939.77924)\n",
            "     | > D_mse_gan_loss: 0.11413  (0.12112)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00013)\n",
            "     | > loss_1: 0.11413  (0.12112)\n",
            "     | > grad_norm_1: 8.86778  (23.63260)\n",
            "     | > current_lr_0: 2.355131140657237e-10 \n",
            "     | > current_lr_1: 2.355131140657237e-10 \n",
            "     | > step_time: 2.43030  (2.42825)\n",
            "     | > loader_time: 0.00240  (0.00234)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/364 -- GLOBAL_STEP: 190675\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52339  (0.56352)\n",
            "     | > G_mse_fake_loss: 0.74537  (0.75174)\n",
            "     | > G_feat_match_loss: 0.10810  (0.11380)\n",
            "     | > G_gen_loss: 23.55251  (25.35837)\n",
            "     | > G_adv_loss: 1.82633  (1.88975)\n",
            "     | > loss_0: 25.37884  (27.24812)\n",
            "     | > grad_norm_0: 795.46942  (942.01624)\n",
            "     | > D_mse_gan_loss: 0.14042  (0.12161)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00009  (0.00013)\n",
            "     | > loss_1: 0.14042  (0.12161)\n",
            "     | > grad_norm_1: 19.52842  (24.36935)\n",
            "     | > current_lr_0: 2.296954014349077e-10 \n",
            "     | > current_lr_1: 2.296954014349077e-10 \n",
            "     | > step_time: 2.42130  (2.42747)\n",
            "     | > loader_time: 0.00230  (0.00232)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 115/364 -- GLOBAL_STEP: 190700\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53482  (0.56287)\n",
            "     | > G_mse_fake_loss: 0.73808  (0.75193)\n",
            "     | > G_feat_match_loss: 0.10631  (0.11373)\n",
            "     | > G_gen_loss: 24.06674  (25.32932)\n",
            "     | > G_adv_loss: 1.80113  (1.88928)\n",
            "     | > loss_0: 25.86787  (27.21859)\n",
            "     | > grad_norm_0: 1002.56378  (935.79419)\n",
            "     | > D_mse_gan_loss: 0.14233  (0.12150)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00047  (0.00014)\n",
            "     | > loss_1: 0.14233  (0.12150)\n",
            "     | > grad_norm_1: 15.16931  (24.53978)\n",
            "     | > current_lr_0: 2.2402139961352594e-10 \n",
            "     | > current_lr_1: 2.2402139961352594e-10 \n",
            "     | > step_time: 2.42010  (2.42688)\n",
            "     | > loader_time: 0.00230  (0.00231)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 140/364 -- GLOBAL_STEP: 190725\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54812  (0.56321)\n",
            "     | > G_mse_fake_loss: 0.76508  (0.75175)\n",
            "     | > G_feat_match_loss: 0.11874  (0.11376)\n",
            "     | > G_gen_loss: 24.66530  (25.34459)\n",
            "     | > G_adv_loss: 1.95243  (1.88940)\n",
            "     | > loss_0: 26.61773  (27.23399)\n",
            "     | > grad_norm_0: 677.43335  (929.68036)\n",
            "     | > D_mse_gan_loss: 0.10823  (0.12137)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00016)\n",
            "     | > loss_1: 0.10823  (0.12137)\n",
            "     | > grad_norm_1: 11.75741  (24.14394)\n",
            "     | > current_lr_0: 2.1848755861586077e-10 \n",
            "     | > current_lr_1: 2.1848755861586077e-10 \n",
            "     | > step_time: 2.42360  (2.42675)\n",
            "     | > loader_time: 0.00220  (0.00231)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 165/364 -- GLOBAL_STEP: 190750\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60735  (0.56423)\n",
            "     | > G_mse_fake_loss: 0.73031  (0.75093)\n",
            "     | > G_feat_match_loss: 0.10193  (0.11372)\n",
            "     | > G_gen_loss: 27.33097  (25.39021)\n",
            "     | > G_adv_loss: 1.74963  (1.88812)\n",
            "     | > loss_0: 29.08060  (27.27832)\n",
            "     | > grad_norm_0: 1706.18384  (934.50348)\n",
            "     | > D_mse_gan_loss: 0.14427  (0.12152)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00009  (0.00017)\n",
            "     | > loss_1: 0.14427  (0.12152)\n",
            "     | > grad_norm_1: 34.38443  (23.95422)\n",
            "     | > current_lr_0: 2.130904161489621e-10 \n",
            "     | > current_lr_1: 2.130904161489621e-10 \n",
            "     | > step_time: 2.42590  (2.42645)\n",
            "     | > loader_time: 0.00210  (0.00231)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 190/364 -- GLOBAL_STEP: 190775\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54615  (0.56409)\n",
            "     | > G_mse_fake_loss: 0.74964  (0.75055)\n",
            "     | > G_feat_match_loss: 0.10180  (0.11360)\n",
            "     | > G_gen_loss: 24.57673  (25.38423)\n",
            "     | > G_adv_loss: 1.76762  (1.88656)\n",
            "     | > loss_0: 26.34435  (27.27080)\n",
            "     | > grad_norm_0: 1260.26941  (941.03088)\n",
            "     | > D_mse_gan_loss: 0.14944  (0.12179)\n",
            "     | > D_mse_gan_real_loss: 0.00008  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00016)\n",
            "     | > loss_1: 0.14944  (0.12179)\n",
            "     | > grad_norm_1: 34.07039  (24.54098)\n",
            "     | > current_lr_0: 2.0782659544643543e-10 \n",
            "     | > current_lr_1: 2.0782659544643543e-10 \n",
            "     | > step_time: 2.42490  (2.42625)\n",
            "     | > loader_time: 0.00220  (0.00231)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 215/364 -- GLOBAL_STEP: 190800\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61460  (0.56475)\n",
            "     | > G_mse_fake_loss: 0.74207  (0.75084)\n",
            "     | > G_feat_match_loss: 0.12154  (0.11377)\n",
            "     | > G_gen_loss: 27.65678  (25.41385)\n",
            "     | > G_adv_loss: 1.95749  (1.88853)\n",
            "     | > loss_0: 29.61427  (27.30238)\n",
            "     | > grad_norm_0: 819.75305  (941.96356)\n",
            "     | > D_mse_gan_loss: 0.10276  (0.12143)\n",
            "     | > D_mse_gan_real_loss: 4.80247535961098e-06  (0.0001407326439897758)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00017)\n",
            "     | > loss_1: 0.10276  (0.12143)\n",
            "     | > grad_norm_1: 35.94370  (24.41819)\n",
            "     | > current_lr_0: 2.0269280315574022e-10 \n",
            "     | > current_lr_1: 2.0269280315574022e-10 \n",
            "     | > step_time: 2.42230  (2.42624)\n",
            "     | > loader_time: 0.00240  (0.00232)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 240/364 -- GLOBAL_STEP: 190825\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56271  (0.56458)\n",
            "     | > G_mse_fake_loss: 0.76826  (0.75144)\n",
            "     | > G_feat_match_loss: 0.11723  (0.11391)\n",
            "     | > G_gen_loss: 25.32214  (25.40620)\n",
            "     | > G_adv_loss: 1.94059  (1.89053)\n",
            "     | > loss_0: 27.26273  (27.29673)\n",
            "     | > grad_norm_0: 778.25690  (940.81628)\n",
            "     | > D_mse_gan_loss: 0.11398  (0.12120)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00016)\n",
            "     | > loss_1: 0.11398  (0.12120)\n",
            "     | > grad_norm_1: 6.39273  (24.15229)\n",
            "     | > current_lr_0: 1.9768582727767698e-10 \n",
            "     | > current_lr_1: 1.9768582727767698e-10 \n",
            "     | > step_time: 2.41490  (2.42617)\n",
            "     | > loader_time: 0.00250  (0.00232)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 265/364 -- GLOBAL_STEP: 190850\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53576  (0.56451)\n",
            "     | > G_mse_fake_loss: 0.75938  (0.75165)\n",
            "     | > G_feat_match_loss: 0.11445  (0.11393)\n",
            "     | > G_gen_loss: 24.10932  (25.40291)\n",
            "     | > G_adv_loss: 1.90390  (1.89091)\n",
            "     | > loss_0: 26.01322  (27.29382)\n",
            "     | > grad_norm_0: 810.77893  (943.19208)\n",
            "     | > D_mse_gan_loss: 0.11595  (0.12104)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00009  (0.00015)\n",
            "     | > loss_1: 0.11595  (0.12104)\n",
            "     | > grad_norm_1: 7.26762  (23.88719)\n",
            "     | > current_lr_0: 1.9280253515677316e-10 \n",
            "     | > current_lr_1: 1.9280253515677316e-10 \n",
            "     | > step_time: 2.42410  (2.42602)\n",
            "     | > loader_time: 0.00220  (0.00231)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 290/364 -- GLOBAL_STEP: 190875\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53050  (0.56452)\n",
            "     | > G_mse_fake_loss: 0.76589  (0.75161)\n",
            "     | > G_feat_match_loss: 0.11342  (0.11395)\n",
            "     | > G_gen_loss: 23.87259  (25.40344)\n",
            "     | > G_adv_loss: 1.90011  (1.89113)\n",
            "     | > loss_0: 25.77270  (27.29457)\n",
            "     | > grad_norm_0: 697.58673  (939.50842)\n",
            "     | > D_mse_gan_loss: 0.12318  (0.12094)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00016)\n",
            "     | > loss_1: 0.12318  (0.12094)\n",
            "     | > grad_norm_1: 9.81135  (23.89655)\n",
            "     | > current_lr_0: 1.8803987152131253e-10 \n",
            "     | > current_lr_1: 1.8803987152131253e-10 \n",
            "     | > step_time: 2.42500  (2.42590)\n",
            "     | > loader_time: 0.00210  (0.00230)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 315/364 -- GLOBAL_STEP: 190900\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52163  (0.56487)\n",
            "     | > G_mse_fake_loss: 0.76304  (0.75178)\n",
            "     | > G_feat_match_loss: 0.11230  (0.11405)\n",
            "     | > G_gen_loss: 23.47326  (25.41901)\n",
            "     | > G_adv_loss: 1.88608  (1.89232)\n",
            "     | > loss_0: 25.35934  (27.31133)\n",
            "     | > grad_norm_0: 770.45605  (939.59229)\n",
            "     | > D_mse_gan_loss: 0.12031  (0.12060)\n",
            "     | > D_mse_gan_real_loss: 0.00018  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00016)\n",
            "     | > loss_1: 0.12031  (0.12060)\n",
            "     | > grad_norm_1: 4.47695  (23.80174)\n",
            "     | > current_lr_0: 1.8339485657177858e-10 \n",
            "     | > current_lr_1: 1.8339485657177858e-10 \n",
            "     | > step_time: 2.42290  (2.42575)\n",
            "     | > loader_time: 0.00230  (0.00230)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 340/364 -- GLOBAL_STEP: 190925\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55963  (0.56497)\n",
            "     | > G_mse_fake_loss: 0.75145  (0.75193)\n",
            "     | > G_feat_match_loss: 0.11919  (0.11400)\n",
            "     | > G_gen_loss: 25.18342  (25.42345)\n",
            "     | > G_adv_loss: 1.94332  (1.89195)\n",
            "     | > loss_0: 27.12673  (27.31540)\n",
            "     | > grad_norm_0: 683.44922  (938.99255)\n",
            "     | > D_mse_gan_loss: 0.10253  (0.12078)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00046  (0.00015)\n",
            "     | > loss_1: 0.10253  (0.12078)\n",
            "     | > grad_norm_1: 23.95288  (23.89432)\n",
            "     | > current_lr_0: 1.7886458411651912e-10 \n",
            "     | > current_lr_1: 1.7886458411651912e-10 \n",
            "     | > step_time: 2.42400  (2.42572)\n",
            "     | > loader_time: 0.00220  (0.00229)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00121 \u001b[0m(+0.00002)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 0.58178 \u001b[0m(+0.00001)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70198 \u001b[0m(+0.00011)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 26.18009 \u001b[0m(+0.00035)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81717 \u001b[0m(+0.00014)\n",
            "     | > avg_loss_0:\u001b[91m 27.99725 \u001b[0m(+0.00049)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.27955 \u001b[0m(+0.00001)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.13729 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00057 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.27955 \u001b[0m(+0.00001)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 30/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 15:21:09) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/364 -- GLOBAL_STEP: 190950\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53498  (0.53498)\n",
            "     | > G_mse_fake_loss: 0.77552  (0.77552)\n",
            "     | > G_feat_match_loss: 0.11829  (0.11829)\n",
            "     | > G_gen_loss: 24.07397  (24.07397)\n",
            "     | > G_adv_loss: 1.95843  (1.95843)\n",
            "     | > loss_0: 26.03240  (26.03240)\n",
            "     | > grad_norm_0: 705.07483  (705.07483)\n",
            "     | > D_mse_gan_loss: 0.10777  (0.10777)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00001)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00004)\n",
            "     | > loss_1: 0.10777  (0.10777)\n",
            "     | > grad_norm_1: 7.67073  (7.67073)\n",
            "     | > current_lr_0: 1.7444621975346315e-10 \n",
            "     | > current_lr_1: 1.7444621975346315e-10 \n",
            "     | > step_time: 3.18120  (3.18115)\n",
            "     | > loader_time: 5.27940  (5.27935)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 25/364 -- GLOBAL_STEP: 190975\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52177  (0.56430)\n",
            "     | > G_mse_fake_loss: 0.75967  (0.75679)\n",
            "     | > G_feat_match_loss: 0.11124  (0.11507)\n",
            "     | > G_gen_loss: 23.47943  (25.39365)\n",
            "     | > G_adv_loss: 1.87211  (1.90745)\n",
            "     | > loss_0: 25.35154  (27.30110)\n",
            "     | > grad_norm_0: 762.12665  (858.43250)\n",
            "     | > D_mse_gan_loss: 0.12970  (0.11904)\n",
            "     | > D_mse_gan_real_loss: 0.00214  (0.00024)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00011)\n",
            "     | > loss_1: 0.12970  (0.11904)\n",
            "     | > grad_norm_1: 8.01698  (21.73799)\n",
            "     | > current_lr_0: 1.701369990967543e-10 \n",
            "     | > current_lr_1: 1.701369990967543e-10 \n",
            "     | > step_time: 2.42430  (2.43244)\n",
            "     | > loader_time: 0.00200  (0.00172)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/364 -- GLOBAL_STEP: 191000\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53923  (0.56385)\n",
            "     | > G_mse_fake_loss: 0.78890  (0.75809)\n",
            "     | > G_feat_match_loss: 0.11921  (0.11493)\n",
            "     | > G_gen_loss: 24.26550  (25.37317)\n",
            "     | > G_adv_loss: 1.98100  (1.90740)\n",
            "     | > loss_0: 26.24650  (27.28057)\n",
            "     | > grad_norm_0: 808.49866  (876.66583)\n",
            "     | > D_mse_gan_loss: 0.11166  (0.12029)\n",
            "     | > D_mse_gan_real_loss: 0.00025  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00019)\n",
            "     | > loss_1: 0.11166  (0.12029)\n",
            "     | > grad_norm_1: 8.38096  (23.20478)\n",
            "     | > current_lr_0: 1.659342260471903e-10 \n",
            "     | > current_lr_1: 1.659342260471903e-10 \n",
            "     | > step_time: 2.42150  (2.42831)\n",
            "     | > loader_time: 0.00220  (0.00196)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 75/364 -- GLOBAL_STEP: 191025\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54075  (0.56321)\n",
            "     | > G_mse_fake_loss: 0.77282  (0.75796)\n",
            "     | > G_feat_match_loss: 0.11583  (0.11476)\n",
            "     | > G_gen_loss: 24.33373  (25.34457)\n",
            "     | > G_adv_loss: 1.93113  (1.90553)\n",
            "     | > loss_0: 26.26486  (27.25010)\n",
            "     | > grad_norm_0: 802.07690  (878.45343)\n",
            "     | > D_mse_gan_loss: 0.12415  (0.12074)\n",
            "     | > D_mse_gan_real_loss: 0.00447  (0.00023)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00016)\n",
            "     | > loss_1: 0.12415  (0.12074)\n",
            "     | > grad_norm_1: 8.36333  (23.03051)\n",
            "     | > current_lr_0: 1.6183527110538596e-10 \n",
            "     | > current_lr_1: 1.6183527110538596e-10 \n",
            "     | > step_time: 2.42560  (2.42682)\n",
            "     | > loader_time: 0.00210  (0.00206)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/364 -- GLOBAL_STEP: 191050\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53383  (0.56418)\n",
            "     | > G_mse_fake_loss: 0.75907  (0.75780)\n",
            "     | > G_feat_match_loss: 0.11240  (0.11482)\n",
            "     | > G_gen_loss: 24.02257  (25.38795)\n",
            "     | > G_adv_loss: 1.88304  (1.90604)\n",
            "     | > loss_0: 25.90561  (27.29399)\n",
            "     | > grad_norm_0: 1075.22522  (871.32758)\n",
            "     | > D_mse_gan_loss: 0.12494  (0.12049)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00024)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00016)\n",
            "     | > loss_1: 0.12494  (0.12049)\n",
            "     | > grad_norm_1: 5.92625  (22.86566)\n",
            "     | > current_lr_0: 1.578375697266058e-10 \n",
            "     | > current_lr_1: 1.578375697266058e-10 \n",
            "     | > step_time: 2.42740  (2.42599)\n",
            "     | > loader_time: 0.00240  (0.00210)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 125/364 -- GLOBAL_STEP: 191075\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59739  (0.56357)\n",
            "     | > G_mse_fake_loss: 0.72563  (0.75684)\n",
            "     | > G_feat_match_loss: 0.12081  (0.11470)\n",
            "     | > G_gen_loss: 26.88275  (25.36065)\n",
            "     | > G_adv_loss: 1.93373  (1.90380)\n",
            "     | > loss_0: 28.81648  (27.26444)\n",
            "     | > grad_norm_0: 723.92609  (874.11224)\n",
            "     | > D_mse_gan_loss: 0.10690  (0.12057)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00023)\n",
            "     | > D_mse_gan_fake_loss: 0.00170  (0.00019)\n",
            "     | > loss_1: 0.10690  (0.12057)\n",
            "     | > grad_norm_1: 41.92421  (22.96865)\n",
            "     | > current_lr_0: 1.5393862071623547e-10 \n",
            "     | > current_lr_1: 1.5393862071623547e-10 \n",
            "     | > step_time: 2.42210  (2.42580)\n",
            "     | > loader_time: 0.00220  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 150/364 -- GLOBAL_STEP: 191100\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59666  (0.56402)\n",
            "     | > G_mse_fake_loss: 0.73732  (0.75532)\n",
            "     | > G_feat_match_loss: 0.11140  (0.11468)\n",
            "     | > G_gen_loss: 26.84979  (25.38090)\n",
            "     | > G_adv_loss: 1.85137  (1.90215)\n",
            "     | > loss_0: 28.70115  (27.28305)\n",
            "     | > grad_norm_0: 1378.68347  (879.85437)\n",
            "     | > D_mse_gan_loss: 0.12611  (0.12058)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00023)\n",
            "     | > D_mse_gan_fake_loss: 0.00034  (0.00022)\n",
            "     | > loss_1: 0.12611  (0.12058)\n",
            "     | > grad_norm_1: 15.08858  (22.98197)\n",
            "     | > current_lr_0: 1.5013598466488873e-10 \n",
            "     | > current_lr_1: 1.5013598466488873e-10 \n",
            "     | > step_time: 2.42530  (2.42540)\n",
            "     | > loader_time: 0.00200  (0.00214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 175/364 -- GLOBAL_STEP: 191125\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63311  (0.56445)\n",
            "     | > G_mse_fake_loss: 0.75105  (0.75549)\n",
            "     | > G_feat_match_loss: 0.12142  (0.11473)\n",
            "     | > G_gen_loss: 28.49015  (25.40033)\n",
            "     | > G_adv_loss: 1.96526  (1.90279)\n",
            "     | > loss_0: 30.45541  (27.30312)\n",
            "     | > grad_norm_0: 937.61658  (881.57758)\n",
            "     | > D_mse_gan_loss: 0.10539  (0.12045)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00022)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00020)\n",
            "     | > loss_1: 0.10539  (0.12045)\n",
            "     | > grad_norm_1: 29.30171  (23.08667)\n",
            "     | > current_lr_0: 1.464272824221713e-10 \n",
            "     | > current_lr_1: 1.464272824221713e-10 \n",
            "     | > step_time: 2.42220  (2.42513)\n",
            "     | > loader_time: 0.00220  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 200/364 -- GLOBAL_STEP: 191150\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56639  (0.56473)\n",
            "     | > G_mse_fake_loss: 0.71122  (0.75552)\n",
            "     | > G_feat_match_loss: 0.11360  (0.11480)\n",
            "     | > G_gen_loss: 25.48746  (25.41285)\n",
            "     | > G_adv_loss: 1.84725  (1.90354)\n",
            "     | > loss_0: 27.33471  (27.31640)\n",
            "     | > grad_norm_0: 670.87286  (884.41125)\n",
            "     | > D_mse_gan_loss: 0.12349  (0.12013)\n",
            "     | > D_mse_gan_real_loss: 0.00111  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00013  (0.00019)\n",
            "     | > loss_1: 0.12349  (0.12013)\n",
            "     | > grad_norm_1: 36.79607  (23.33278)\n",
            "     | > current_lr_0: 1.4281019360814536e-10 \n",
            "     | > current_lr_1: 1.4281019360814536e-10 \n",
            "     | > step_time: 2.42420  (2.42513)\n",
            "     | > loader_time: 0.00220  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 225/364 -- GLOBAL_STEP: 191175\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53926  (0.56467)\n",
            "     | > G_mse_fake_loss: 0.76480  (0.75617)\n",
            "     | > G_feat_match_loss: 0.11651  (0.11495)\n",
            "     | > G_gen_loss: 24.26678  (25.41002)\n",
            "     | > G_adv_loss: 1.92991  (1.90562)\n",
            "     | > loss_0: 26.19669  (27.31564)\n",
            "     | > grad_norm_0: 850.59863  (884.05890)\n",
            "     | > D_mse_gan_loss: 0.11357  (0.11979)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00016  (0.00019)\n",
            "     | > loss_1: 0.11357  (0.11979)\n",
            "     | > grad_norm_1: 4.99842  (23.21369)\n",
            "     | > current_lr_0: 1.392824551615655e-10 \n",
            "     | > current_lr_1: 1.392824551615655e-10 \n",
            "     | > step_time: 2.42860  (2.42518)\n",
            "     | > loader_time: 0.00210  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 250/364 -- GLOBAL_STEP: 191200\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51794  (0.56454)\n",
            "     | > G_mse_fake_loss: 0.76622  (0.75632)\n",
            "     | > G_feat_match_loss: 0.10767  (0.11489)\n",
            "     | > G_gen_loss: 23.30730  (25.40415)\n",
            "     | > G_adv_loss: 1.84292  (1.90523)\n",
            "     | > loss_0: 25.15021  (27.30938)\n",
            "     | > grad_norm_0: 873.88776  (886.14465)\n",
            "     | > D_mse_gan_loss: 0.13605  (0.11987)\n",
            "     | > D_mse_gan_real_loss: 0.00051  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00018)\n",
            "     | > loss_1: 0.13605  (0.11987)\n",
            "     | > grad_norm_1: 20.14772  (23.22730)\n",
            "     | > current_lr_0: 1.3584185992397557e-10 \n",
            "     | > current_lr_1: 1.3584185992397557e-10 \n",
            "     | > step_time: 2.42270  (2.42516)\n",
            "     | > loader_time: 0.00240  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 275/364 -- GLOBAL_STEP: 191225\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54458  (0.56447)\n",
            "     | > G_mse_fake_loss: 0.76624  (0.75650)\n",
            "     | > G_feat_match_loss: 0.11706  (0.11491)\n",
            "     | > G_gen_loss: 24.50591  (25.40108)\n",
            "     | > G_adv_loss: 1.93684  (1.90556)\n",
            "     | > loss_0: 26.44275  (27.30664)\n",
            "     | > grad_norm_0: 808.48279  (886.19238)\n",
            "     | > D_mse_gan_loss: 0.11302  (0.11973)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00019)\n",
            "     | > loss_1: 0.11302  (0.11973)\n",
            "     | > grad_norm_1: 7.23224  (23.21656)\n",
            "     | > current_lr_0: 1.324862552587818e-10 \n",
            "     | > current_lr_1: 1.324862552587818e-10 \n",
            "     | > step_time: 2.42220  (2.42506)\n",
            "     | > loader_time: 0.00210  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 300/364 -- GLOBAL_STEP: 191250\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52154  (0.56466)\n",
            "     | > G_mse_fake_loss: 0.76660  (0.75654)\n",
            "     | > G_feat_match_loss: 0.10705  (0.11489)\n",
            "     | > G_gen_loss: 23.46925  (25.40989)\n",
            "     | > G_adv_loss: 1.83710  (1.90547)\n",
            "     | > loss_0: 25.30635  (27.31536)\n",
            "     | > grad_norm_0: 866.25873  (885.63629)\n",
            "     | > D_mse_gan_loss: 0.13595  (0.11973)\n",
            "     | > D_mse_gan_real_loss: 0.00095  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00019)\n",
            "     | > loss_1: 0.13595  (0.11973)\n",
            "     | > grad_norm_1: 24.66326  (23.33562)\n",
            "     | > current_lr_0: 1.292135417044383e-10 \n",
            "     | > current_lr_1: 1.292135417044383e-10 \n",
            "     | > step_time: 2.41960  (2.42500)\n",
            "     | > loader_time: 0.00210  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 325/364 -- GLOBAL_STEP: 191275\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59903  (0.56530)\n",
            "     | > G_mse_fake_loss: 0.74998  (0.75609)\n",
            "     | > G_feat_match_loss: 0.12008  (0.11493)\n",
            "     | > G_gen_loss: 26.95626  (25.43860)\n",
            "     | > G_adv_loss: 1.95080  (1.90535)\n",
            "     | > loss_0: 28.90706  (27.34396)\n",
            "     | > grad_norm_0: 754.56006  (883.37982)\n",
            "     | > D_mse_gan_loss: 0.10134  (0.11968)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00020)\n",
            "     | > loss_1: 0.10134  (0.11968)\n",
            "     | > grad_norm_1: 30.29450  (23.46329)\n",
            "     | > current_lr_0: 1.2602167166090178e-10 \n",
            "     | > current_lr_1: 1.2602167166090178e-10 \n",
            "     | > step_time: 2.42100  (2.42511)\n",
            "     | > loader_time: 0.00220  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 350/364 -- GLOBAL_STEP: 191300\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60951  (0.56543)\n",
            "     | > G_mse_fake_loss: 0.75114  (0.75639)\n",
            "     | > G_feat_match_loss: 0.11405  (0.11494)\n",
            "     | > G_gen_loss: 27.42815  (25.44436)\n",
            "     | > G_adv_loss: 1.89163  (1.90575)\n",
            "     | > loss_0: 29.31978  (27.35012)\n",
            "     | > grad_norm_0: 1717.11829  (884.15546)\n",
            "     | > D_mse_gan_loss: 0.11582  (0.11968)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00019)\n",
            "     | > loss_1: 0.11582  (0.11968)\n",
            "     | > grad_norm_1: 15.71240  (23.68432)\n",
            "     | > current_lr_0: 1.229086481085336e-10 \n",
            "     | > current_lr_1: 1.229086481085336e-10 \n",
            "     | > step_time: 2.42800  (2.42508)\n",
            "     | > loader_time: 0.00210  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00120 \u001b[0m(-0.00001)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.58177 \u001b[0m(-0.00001)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70199 \u001b[0m(+0.00002)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 26.17978 \u001b[0m(-0.00031)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81719 \u001b[0m(+0.00003)\n",
            "     | > avg_loss_0:\u001b[92m 27.99697 \u001b[0m(-0.00029)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.27955 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.13729 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00057 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.27955 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 31/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 15:36:07) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/364 -- GLOBAL_STEP: 191325\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53782  (0.54459)\n",
            "     | > G_mse_fake_loss: 0.76080  (0.76100)\n",
            "     | > G_feat_match_loss: 0.11094  (0.11418)\n",
            "     | > G_gen_loss: 24.20168  (24.50654)\n",
            "     | > G_adv_loss: 1.87022  (1.90284)\n",
            "     | > loss_0: 26.07190  (26.40939)\n",
            "     | > grad_norm_0: 807.18103  (780.45831)\n",
            "     | > D_mse_gan_loss: 0.13214  (0.12143)\n",
            "     | > D_mse_gan_real_loss: 0.00013  (0.00004)\n",
            "     | > D_mse_gan_fake_loss: 0.00017  (0.00009)\n",
            "     | > loss_1: 0.13214  (0.12143)\n",
            "     | > grad_norm_1: 14.30279  (24.29808)\n",
            "     | > current_lr_0: 1.198725233586482e-10 \n",
            "     | > current_lr_1: 1.198725233586482e-10 \n",
            "     | > step_time: 2.42530  (2.45128)\n",
            "     | > loader_time: 0.00100  (0.00186)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 35/364 -- GLOBAL_STEP: 191350\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52796  (0.55065)\n",
            "     | > G_mse_fake_loss: 0.77221  (0.76022)\n",
            "     | > G_feat_match_loss: 0.11183  (0.11552)\n",
            "     | > G_gen_loss: 23.75835  (24.77930)\n",
            "     | > G_adv_loss: 1.89056  (1.91542)\n",
            "     | > loss_0: 25.64891  (26.69473)\n",
            "     | > grad_norm_0: 1070.37341  (808.20197)\n",
            "     | > D_mse_gan_loss: 0.12496  (0.11902)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00009)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00009)\n",
            "     | > loss_1: 0.12496  (0.11902)\n",
            "     | > grad_norm_1: 14.79338  (25.14504)\n",
            "     | > current_lr_0: 1.1691139783492562e-10 \n",
            "     | > current_lr_1: 1.1691139783492562e-10 \n",
            "     | > step_time: 2.43220  (2.43292)\n",
            "     | > loader_time: 0.00220  (0.00193)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/364 -- GLOBAL_STEP: 191375\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59968  (0.55277)\n",
            "     | > G_mse_fake_loss: 0.72969  (0.75957)\n",
            "     | > G_feat_match_loss: 0.12003  (0.11555)\n",
            "     | > G_gen_loss: 26.98548  (24.87461)\n",
            "     | > G_adv_loss: 1.93004  (1.91511)\n",
            "     | > loss_0: 28.91552  (26.78972)\n",
            "     | > grad_norm_0: 683.29675  (803.41766)\n",
            "     | > D_mse_gan_loss: 0.10712  (0.11878)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00021)\n",
            "     | > D_mse_gan_fake_loss: 0.00018  (0.00010)\n",
            "     | > loss_1: 0.10712  (0.11878)\n",
            "     | > grad_norm_1: 35.41465  (24.79304)\n",
            "     | > current_lr_0: 1.1402341888492624e-10 \n",
            "     | > current_lr_1: 1.1402341888492624e-10 \n",
            "     | > step_time: 2.42080  (2.42985)\n",
            "     | > loader_time: 0.00210  (0.00204)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 85/364 -- GLOBAL_STEP: 191400\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53129  (0.55396)\n",
            "     | > G_mse_fake_loss: 0.76368  (0.75922)\n",
            "     | > G_feat_match_loss: 0.11080  (0.11558)\n",
            "     | > G_gen_loss: 23.90817  (24.92821)\n",
            "     | > G_adv_loss: 1.87169  (1.91502)\n",
            "     | > loss_0: 25.77986  (26.84322)\n",
            "     | > grad_norm_0: 964.36572  (799.86353)\n",
            "     | > D_mse_gan_loss: 0.12080  (0.11865)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00010  (0.00010)\n",
            "     | > loss_1: 0.12080  (0.11865)\n",
            "     | > grad_norm_1: 10.65753  (24.95284)\n",
            "     | > current_lr_0: 1.1120677962096347e-10 \n",
            "     | > current_lr_1: 1.1120677962096347e-10 \n",
            "     | > step_time: 2.41830  (2.42806)\n",
            "     | > loader_time: 0.00220  (0.00210)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 110/364 -- GLOBAL_STEP: 191425\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54269  (0.55358)\n",
            "     | > G_mse_fake_loss: 0.75729  (0.75938)\n",
            "     | > G_feat_match_loss: 0.11467  (0.11561)\n",
            "     | > G_gen_loss: 24.42101  (24.91091)\n",
            "     | > G_adv_loss: 1.90399  (1.91544)\n",
            "     | > loss_0: 26.32500  (26.82635)\n",
            "     | > grad_norm_0: 743.43341  (799.04175)\n",
            "     | > D_mse_gan_loss: 0.11990  (0.11840)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00054  (0.00010)\n",
            "     | > loss_1: 0.11990  (0.11840)\n",
            "     | > grad_norm_1: 10.34840  (24.96428)\n",
            "     | > current_lr_0: 1.0845971778960954e-10 \n",
            "     | > current_lr_1: 1.0845971778960954e-10 \n",
            "     | > step_time: 2.42910  (2.42695)\n",
            "     | > loader_time: 0.00200  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 135/364 -- GLOBAL_STEP: 191450\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55692  (0.55326)\n",
            "     | > G_mse_fake_loss: 0.73830  (0.76008)\n",
            "     | > G_feat_match_loss: 0.11422  (0.11576)\n",
            "     | > G_gen_loss: 25.06123  (24.89665)\n",
            "     | > G_adv_loss: 1.88052  (1.91764)\n",
            "     | > loss_0: 26.94175  (26.81429)\n",
            "     | > grad_norm_0: 696.21271  (792.34235)\n",
            "     | > D_mse_gan_loss: 0.11543  (0.11780)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00033  (0.00010)\n",
            "     | > loss_1: 0.11543  (0.11780)\n",
            "     | > grad_norm_1: 17.93169  (24.47039)\n",
            "     | > current_lr_0: 1.0578051466912742e-10 \n",
            "     | > current_lr_1: 1.0578051466912742e-10 \n",
            "     | > step_time: 2.42460  (2.42609)\n",
            "     | > loader_time: 0.00230  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 160/364 -- GLOBAL_STEP: 191475\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58345  (0.55366)\n",
            "     | > G_mse_fake_loss: 0.76058  (0.75966)\n",
            "     | > G_feat_match_loss: 0.12345  (0.11582)\n",
            "     | > G_gen_loss: 26.25525  (24.91490)\n",
            "     | > G_adv_loss: 1.99507  (1.91783)\n",
            "     | > loss_0: 28.25031  (26.83273)\n",
            "     | > grad_norm_0: 663.75964  (791.98468)\n",
            "     | > D_mse_gan_loss: 0.09423  (0.11758)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00010)\n",
            "     | > loss_1: 0.09423  (0.11758)\n",
            "     | > grad_norm_1: 26.41139  (24.26405)\n",
            "     | > current_lr_0: 1.0316749399413832e-10 \n",
            "     | > current_lr_1: 1.0316749399413832e-10 \n",
            "     | > step_time: 2.42570  (2.42576)\n",
            "     | > loader_time: 0.00210  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 185/364 -- GLOBAL_STEP: 191500\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53745  (0.55394)\n",
            "     | > G_mse_fake_loss: 0.77352  (0.75954)\n",
            "     | > G_feat_match_loss: 0.11528  (0.11583)\n",
            "     | > G_gen_loss: 24.18540  (24.92715)\n",
            "     | > G_adv_loss: 1.92628  (1.91788)\n",
            "     | > loss_0: 26.11168  (26.84503)\n",
            "     | > grad_norm_0: 834.93652  (789.91150)\n",
            "     | > D_mse_gan_loss: 0.11438  (0.11751)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00010)\n",
            "     | > loss_1: 0.11438  (0.11751)\n",
            "     | > grad_norm_1: 8.11419  (24.43307)\n",
            "     | > current_lr_0: 1.0061902090685266e-10 \n",
            "     | > current_lr_1: 1.0061902090685266e-10 \n",
            "     | > step_time: 2.42200  (2.42574)\n",
            "     | > loader_time: 0.00210  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 210/364 -- GLOBAL_STEP: 191525\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53384  (0.55395)\n",
            "     | > G_mse_fake_loss: 0.78330  (0.75978)\n",
            "     | > G_feat_match_loss: 0.11291  (0.11586)\n",
            "     | > G_gen_loss: 24.02293  (24.92755)\n",
            "     | > G_adv_loss: 1.91244  (1.91837)\n",
            "     | > loss_0: 25.93538  (26.84592)\n",
            "     | > grad_norm_0: 915.51190  (789.53882)\n",
            "     | > D_mse_gan_loss: 0.12234  (0.11752)\n",
            "     | > D_mse_gan_real_loss: 0.00223  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00010)\n",
            "     | > loss_1: 0.12234  (0.11752)\n",
            "     | > grad_norm_1: 19.95945  (24.16680)\n",
            "     | > current_lr_0: 9.813350093420779e-11 \n",
            "     | > current_lr_1: 9.813350093420779e-11 \n",
            "     | > step_time: 2.41800  (2.42563)\n",
            "     | > loader_time: 0.00220  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 235/364 -- GLOBAL_STEP: 191550\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53162  (0.55429)\n",
            "     | > G_mse_fake_loss: 0.78597  (0.75987)\n",
            "     | > G_feat_match_loss: 0.11459  (0.11595)\n",
            "     | > G_gen_loss: 23.92280  (24.94305)\n",
            "     | > G_adv_loss: 1.93185  (1.91938)\n",
            "     | > loss_0: 25.85465  (26.86243)\n",
            "     | > grad_norm_0: 825.85742  (791.31152)\n",
            "     | > D_mse_gan_loss: 0.12558  (0.11743)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00011)\n",
            "     | > loss_1: 0.12558  (0.11743)\n",
            "     | > grad_norm_1: 20.94242  (24.11052)\n",
            "     | > current_lr_0: 9.5709378990273e-11 \n",
            "     | > current_lr_1: 9.5709378990273e-11 \n",
            "     | > step_time: 2.42220  (2.42561)\n",
            "     | > loader_time: 0.00200  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 260/364 -- GLOBAL_STEP: 191575\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57957  (0.55432)\n",
            "     | > G_mse_fake_loss: 0.76009  (0.75988)\n",
            "     | > G_feat_match_loss: 0.12662  (0.11603)\n",
            "     | > G_gen_loss: 26.08084  (24.94441)\n",
            "     | > G_adv_loss: 2.02632  (1.92020)\n",
            "     | > loss_0: 28.10716  (26.86461)\n",
            "     | > grad_norm_0: 664.45465  (788.19714)\n",
            "     | > D_mse_gan_loss: 0.08570  (0.11712)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00011)\n",
            "     | > loss_1: 0.08570  (0.11712)\n",
            "     | > grad_norm_1: 37.83165  (23.75043)\n",
            "     | > current_lr_0: 9.334513840329711e-11 \n",
            "     | > current_lr_1: 9.334513840329711e-11 \n",
            "     | > step_time: 2.42970  (2.42551)\n",
            "     | > loader_time: 0.00220  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 285/364 -- GLOBAL_STEP: 191600\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54983  (0.55434)\n",
            "     | > G_mse_fake_loss: 0.75793  (0.75965)\n",
            "     | > G_feat_match_loss: 0.10742  (0.11597)\n",
            "     | > G_gen_loss: 24.74239  (24.94520)\n",
            "     | > G_adv_loss: 1.83208  (1.91936)\n",
            "     | > loss_0: 26.57447  (26.86456)\n",
            "     | > grad_norm_0: 869.73438  (788.65070)\n",
            "     | > D_mse_gan_loss: 0.14676  (0.11727)\n",
            "     | > D_mse_gan_real_loss: 0.00012  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00011  (0.00011)\n",
            "     | > loss_1: 0.14676  (0.11727)\n",
            "     | > grad_norm_1: 20.50074  (24.00030)\n",
            "     | > current_lr_0: 9.10392999667904e-11 \n",
            "     | > current_lr_1: 9.10392999667904e-11 \n",
            "     | > step_time: 2.42120  (2.42547)\n",
            "     | > loader_time: 0.00210  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 310/364 -- GLOBAL_STEP: 191625\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53647  (0.55464)\n",
            "     | > G_mse_fake_loss: 0.76807  (0.75964)\n",
            "     | > G_feat_match_loss: 0.11633  (0.11603)\n",
            "     | > G_gen_loss: 24.14137  (24.95862)\n",
            "     | > G_adv_loss: 1.93142  (1.91997)\n",
            "     | > loss_0: 26.07279  (26.87859)\n",
            "     | > grad_norm_0: 735.35388  (786.55536)\n",
            "     | > D_mse_gan_loss: 0.11367  (0.11714)\n",
            "     | > D_mse_gan_real_loss: 0.00029  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00011)\n",
            "     | > loss_1: 0.11367  (0.11714)\n",
            "     | > grad_norm_1: 6.96260  (23.82171)\n",
            "     | > current_lr_0: 8.879042101404707e-11 \n",
            "     | > current_lr_1: 8.879042101404707e-11 \n",
            "     | > step_time: 2.41990  (2.42533)\n",
            "     | > loader_time: 0.00240  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 335/364 -- GLOBAL_STEP: 191650\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56350  (0.55478)\n",
            "     | > G_mse_fake_loss: 0.74554  (0.75946)\n",
            "     | > G_feat_match_loss: 0.12173  (0.11605)\n",
            "     | > G_gen_loss: 25.35768  (24.96498)\n",
            "     | > G_adv_loss: 1.96289  (1.91992)\n",
            "     | > loss_0: 27.32057  (26.88490)\n",
            "     | > grad_norm_0: 669.77228  (784.85345)\n",
            "     | > D_mse_gan_loss: 0.09482  (0.11721)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00011)\n",
            "     | > loss_1: 0.09482  (0.11721)\n",
            "     | > grad_norm_1: 35.17097  (23.88533)\n",
            "     | > current_lr_0: 8.659709451552886e-11 \n",
            "     | > current_lr_1: 8.659709451552886e-11 \n",
            "     | > step_time: 2.42500  (2.42513)\n",
            "     | > loader_time: 0.00230  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 360/364 -- GLOBAL_STEP: 191675\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58544  (0.55466)\n",
            "     | > G_mse_fake_loss: 0.74525  (0.75932)\n",
            "     | > G_feat_match_loss: 0.12590  (0.11603)\n",
            "     | > G_gen_loss: 26.34468  (24.95990)\n",
            "     | > G_adv_loss: 2.00426  (1.91965)\n",
            "     | > loss_0: 28.34894  (26.87955)\n",
            "     | > grad_norm_0: 670.38324  (787.42328)\n",
            "     | > D_mse_gan_loss: 0.09290  (0.11719)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00011)\n",
            "     | > loss_1: 0.09290  (0.11719)\n",
            "     | > grad_norm_1: 44.01472  (23.96398)\n",
            "     | > current_lr_0: 8.445794819854554e-11 \n",
            "     | > current_lr_1: 8.445794819854554e-11 \n",
            "     | > step_time: 2.42060  (2.42490)\n",
            "     | > loader_time: 0.00200  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00142 \u001b[0m(+0.00022)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.58177 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70200 \u001b[0m(+0.00001)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.11152 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 26.17968 \u001b[0m(-0.00010)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81720 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 27.99688 \u001b[0m(-0.00009)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.27955 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.13729 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00057 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.27955 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 32/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 15:51:05) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/364 -- GLOBAL_STEP: 191700\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51586  (0.54001)\n",
            "     | > G_mse_fake_loss: 0.75230  (0.76400)\n",
            "     | > G_feat_match_loss: 0.10706  (0.11349)\n",
            "     | > G_gen_loss: 23.21378  (24.30037)\n",
            "     | > G_adv_loss: 1.82292  (1.89891)\n",
            "     | > loss_0: 25.03670  (26.19928)\n",
            "     | > grad_norm_0: 755.44312  (846.31622)\n",
            "     | > D_mse_gan_loss: 0.13802  (0.12483)\n",
            "     | > D_mse_gan_real_loss: 0.00006  (0.00008)\n",
            "     | > D_mse_gan_fake_loss: 0.00022  (0.00018)\n",
            "     | > loss_1: 0.13802  (0.12483)\n",
            "     | > grad_norm_1: 18.91896  (21.90270)\n",
            "     | > current_lr_0: 8.237164368868132e-11 \n",
            "     | > current_lr_1: 8.237164368868132e-11 \n",
            "     | > step_time: 2.41720  (2.43050)\n",
            "     | > loader_time: 0.00200  (0.00209)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 45/364 -- GLOBAL_STEP: 191725\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53512  (0.54173)\n",
            "     | > G_mse_fake_loss: 0.74735  (0.76203)\n",
            "     | > G_feat_match_loss: 0.10934  (0.11273)\n",
            "     | > G_gen_loss: 24.08025  (24.37799)\n",
            "     | > G_adv_loss: 1.84071  (1.88933)\n",
            "     | > loss_0: 25.92096  (26.26732)\n",
            "     | > grad_norm_0: 781.93927  (842.60992)\n",
            "     | > D_mse_gan_loss: 0.13710  (0.12667)\n",
            "     | > D_mse_gan_real_loss: 0.00006  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00016  (0.00013)\n",
            "     | > loss_1: 0.13710  (0.12667)\n",
            "     | > grad_norm_1: 12.99514  (20.70232)\n",
            "     | > current_lr_0: 8.033687567242986e-11 \n",
            "     | > current_lr_1: 8.033687567242986e-11 \n",
            "     | > step_time: 2.42830  (2.42742)\n",
            "     | > loader_time: 0.00220  (0.00214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/364 -- GLOBAL_STEP: 191750\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53166  (0.54182)\n",
            "     | > G_mse_fake_loss: 0.76759  (0.76295)\n",
            "     | > G_feat_match_loss: 0.11389  (0.11248)\n",
            "     | > G_gen_loss: 23.92474  (24.38188)\n",
            "     | > G_adv_loss: 1.90645  (1.88772)\n",
            "     | > loss_0: 25.83119  (26.26960)\n",
            "     | > grad_norm_0: 862.95203  (869.99701)\n",
            "     | > D_mse_gan_loss: 0.11842  (0.12748)\n",
            "     | > D_mse_gan_real_loss: 0.00007  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00013)\n",
            "     | > loss_1: 0.11842  (0.12748)\n",
            "     | > grad_norm_1: 6.83738  (21.22380)\n",
            "     | > current_lr_0: 7.835237108051419e-11 \n",
            "     | > current_lr_1: 7.835237108051419e-11 \n",
            "     | > step_time: 2.42220  (2.42631)\n",
            "     | > loader_time: 0.00210  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 95/364 -- GLOBAL_STEP: 191775\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52469  (0.54216)\n",
            "     | > G_mse_fake_loss: 0.76461  (0.76296)\n",
            "     | > G_feat_match_loss: 0.10743  (0.11239)\n",
            "     | > G_gen_loss: 23.61104  (24.39707)\n",
            "     | > G_adv_loss: 1.83887  (1.88691)\n",
            "     | > loss_0: 25.44991  (26.28397)\n",
            "     | > grad_norm_0: 966.04596  (861.53241)\n",
            "     | > D_mse_gan_loss: 0.13843  (0.12709)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00014)\n",
            "     | > loss_1: 0.13843  (0.12709)\n",
            "     | > grad_norm_1: 25.96418  (20.89074)\n",
            "     | > current_lr_0: 7.64168882913805e-11 \n",
            "     | > current_lr_1: 7.64168882913805e-11 \n",
            "     | > step_time: 2.42400  (2.42557)\n",
            "     | > loader_time: 0.00240  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/364 -- GLOBAL_STEP: 191800\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54910  (0.54159)\n",
            "     | > G_mse_fake_loss: 0.75733  (0.76362)\n",
            "     | > G_feat_match_loss: 0.11217  (0.11238)\n",
            "     | > G_gen_loss: 24.70941  (24.37167)\n",
            "     | > G_adv_loss: 1.87908  (1.88746)\n",
            "     | > loss_0: 26.58850  (26.25912)\n",
            "     | > grad_norm_0: 767.11993  (866.73071)\n",
            "     | > D_mse_gan_loss: 0.13187  (0.12701)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00013  (0.00013)\n",
            "     | > loss_1: 0.13187  (0.12701)\n",
            "     | > grad_norm_1: 9.57838  (21.08455)\n",
            "     | > current_lr_0: 7.45292163543674e-11 \n",
            "     | > current_lr_1: 7.45292163543674e-11 \n",
            "     | > step_time: 2.42090  (2.42534)\n",
            "     | > loader_time: 0.00200  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 145/364 -- GLOBAL_STEP: 191825\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53305  (0.54117)\n",
            "     | > G_mse_fake_loss: 0.75661  (0.76385)\n",
            "     | > G_feat_match_loss: 0.11416  (0.11249)\n",
            "     | > G_gen_loss: 23.98709  (24.35282)\n",
            "     | > G_adv_loss: 1.89817  (1.88877)\n",
            "     | > loss_0: 25.88525  (26.24159)\n",
            "     | > grad_norm_0: 815.29114  (861.78052)\n",
            "     | > D_mse_gan_loss: 0.12032  (0.12659)\n",
            "     | > D_mse_gan_real_loss: 0.00009  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00012)\n",
            "     | > loss_1: 0.12032  (0.12659)\n",
            "     | > grad_norm_1: 5.35644  (20.78176)\n",
            "     | > current_lr_0: 7.268817423206497e-11 \n",
            "     | > current_lr_1: 7.268817423206497e-11 \n",
            "     | > step_time: 2.43200  (2.42536)\n",
            "     | > loader_time: 0.00210  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 170/364 -- GLOBAL_STEP: 191850\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61445  (0.54215)\n",
            "     | > G_mse_fake_loss: 0.75093  (0.76379)\n",
            "     | > G_feat_match_loss: 0.12224  (0.11259)\n",
            "     | > G_gen_loss: 27.65038  (24.39683)\n",
            "     | > G_adv_loss: 1.97331  (1.88972)\n",
            "     | > loss_0: 29.62368  (26.28655)\n",
            "     | > grad_norm_0: 820.29657  (861.93817)\n",
            "     | > D_mse_gan_loss: 0.10416  (0.12620)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00010  (0.00012)\n",
            "     | > loss_1: 0.10416  (0.12620)\n",
            "     | > grad_norm_1: 26.63106  (20.50076)\n",
            "     | > current_lr_0: 7.089261006138853e-11 \n",
            "     | > current_lr_1: 7.089261006138853e-11 \n",
            "     | > step_time: 2.42300  (2.42530)\n",
            "     | > loader_time: 0.00230  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 195/364 -- GLOBAL_STEP: 191875\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51613  (0.54183)\n",
            "     | > G_mse_fake_loss: 0.77692  (0.76404)\n",
            "     | > G_feat_match_loss: 0.10887  (0.11265)\n",
            "     | > G_gen_loss: 23.22600  (24.38229)\n",
            "     | > G_adv_loss: 1.86563  (1.89054)\n",
            "     | > loss_0: 25.09163  (26.27283)\n",
            "     | > grad_norm_0: 967.15674  (862.74780)\n",
            "     | > D_mse_gan_loss: 0.13198  (0.12596)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00012)\n",
            "     | > loss_1: 0.13198  (0.12596)\n",
            "     | > grad_norm_1: 28.63328  (20.60716)\n",
            "     | > current_lr_0: 6.914140043290663e-11 \n",
            "     | > current_lr_1: 6.914140043290663e-11 \n",
            "     | > step_time: 2.42250  (2.42507)\n",
            "     | > loader_time: 0.00220  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 220/364 -- GLOBAL_STEP: 191900\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52144  (0.54188)\n",
            "     | > G_mse_fake_loss: 0.77691  (0.76433)\n",
            "     | > G_feat_match_loss: 0.11107  (0.11260)\n",
            "     | > G_gen_loss: 23.46484  (24.38465)\n",
            "     | > G_adv_loss: 1.88756  (1.89030)\n",
            "     | > loss_0: 25.35241  (26.27495)\n",
            "     | > grad_norm_0: 843.60138  (862.29138)\n",
            "     | > D_mse_gan_loss: 0.12708  (0.12617)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00012)\n",
            "     | > loss_1: 0.12708  (0.12617)\n",
            "     | > grad_norm_1: 18.55436  (20.87982)\n",
            "     | > current_lr_0: 6.743344968797026e-11 \n",
            "     | > current_lr_1: 6.743344968797026e-11 \n",
            "     | > step_time: 2.41850  (2.42481)\n",
            "     | > loader_time: 0.00200  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 245/364 -- GLOBAL_STEP: 191925\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52487  (0.54168)\n",
            "     | > G_mse_fake_loss: 0.77455  (0.76458)\n",
            "     | > G_feat_match_loss: 0.11541  (0.11265)\n",
            "     | > G_gen_loss: 23.61934  (24.37581)\n",
            "     | > G_adv_loss: 1.92865  (1.89106)\n",
            "     | > loss_0: 25.54800  (26.26687)\n",
            "     | > grad_norm_0: 903.23596  (865.86212)\n",
            "     | > D_mse_gan_loss: 0.11419  (0.12603)\n",
            "     | > D_mse_gan_real_loss: 0.00012  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00013)\n",
            "     | > loss_1: 0.11419  (0.12603)\n",
            "     | > grad_norm_1: 11.98073  (20.73768)\n",
            "     | > current_lr_0: 6.57676892332054e-11 \n",
            "     | > current_lr_1: 6.57676892332054e-11 \n",
            "     | > step_time: 2.42530  (2.42485)\n",
            "     | > loader_time: 0.00210  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 270/364 -- GLOBAL_STEP: 191950\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.50512  (0.54153)\n",
            "     | > G_mse_fake_loss: 0.78166  (0.76458)\n",
            "     | > G_feat_match_loss: 0.10647  (0.11260)\n",
            "     | > G_gen_loss: 22.73024  (24.36867)\n",
            "     | > G_adv_loss: 1.84632  (1.89055)\n",
            "     | > loss_0: 24.57656  (26.25922)\n",
            "     | > grad_norm_0: 1055.96326  (865.13733)\n",
            "     | > D_mse_gan_loss: 0.14120  (0.12635)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00023)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00013)\n",
            "     | > loss_1: 0.14120  (0.12635)\n",
            "     | > grad_norm_1: 38.57513  (20.97805)\n",
            "     | > current_lr_0: 6.414307687193866e-11 \n",
            "     | > current_lr_1: 6.414307687193866e-11 \n",
            "     | > step_time: 2.42210  (2.42473)\n",
            "     | > loader_time: 0.00210  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 295/364 -- GLOBAL_STEP: 191975\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53144  (0.54167)\n",
            "     | > G_mse_fake_loss: 0.77270  (0.76437)\n",
            "     | > G_feat_match_loss: 0.10808  (0.11254)\n",
            "     | > G_gen_loss: 23.91477  (24.37525)\n",
            "     | > G_adv_loss: 1.85345  (1.88975)\n",
            "     | > loss_0: 25.76822  (26.26500)\n",
            "     | > grad_norm_0: 952.46826  (864.61578)\n",
            "     | > D_mse_gan_loss: 0.13711  (0.12649)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00024)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00013)\n",
            "     | > loss_1: 0.13711  (0.12649)\n",
            "     | > grad_norm_1: 24.58769  (21.20270)\n",
            "     | > current_lr_0: 6.255859615213827e-11 \n",
            "     | > current_lr_1: 6.255859615213827e-11 \n",
            "     | > step_time: 2.42260  (2.42474)\n",
            "     | > loader_time: 0.00210  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 320/364 -- GLOBAL_STEP: 192000\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53506  (0.54208)\n",
            "     | > G_mse_fake_loss: 0.77200  (0.76417)\n",
            "     | > G_feat_match_loss: 0.11274  (0.11261)\n",
            "     | > G_gen_loss: 24.07761  (24.39348)\n",
            "     | > G_adv_loss: 1.89938  (1.89028)\n",
            "     | > loss_0: 25.97698  (26.28376)\n",
            "     | > grad_norm_0: 874.98126  (866.17706)\n",
            "     | > D_mse_gan_loss: 0.12207  (0.12632)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00023)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00013)\n",
            "     | > loss_1: 0.12207  (0.12632)\n",
            "     | > grad_norm_1: 12.66192  (21.14111)\n",
            "     | > current_lr_0: 6.101325573046286e-11 \n",
            "     | > current_lr_1: 6.101325573046286e-11 \n",
            "     | > step_time: 2.42910  (2.42474)\n",
            "     | > loader_time: 0.00230  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 345/364 -- GLOBAL_STEP: 192025\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54065  (0.54181)\n",
            "     | > G_mse_fake_loss: 0.76270  (0.76431)\n",
            "     | > G_feat_match_loss: 0.11633  (0.11257)\n",
            "     | > G_gen_loss: 24.32926  (24.38160)\n",
            "     | > G_adv_loss: 1.92600  (1.89001)\n",
            "     | > loss_0: 26.25526  (26.27161)\n",
            "     | > grad_norm_0: 763.42249  (867.02448)\n",
            "     | > D_mse_gan_loss: 0.11816  (0.12644)\n",
            "     | > D_mse_gan_real_loss: 0.00012  (0.00023)\n",
            "     | > D_mse_gan_fake_loss: 0.00131  (0.00013)\n",
            "     | > loss_1: 0.11816  (0.12644)\n",
            "     | > grad_norm_1: 9.15188  (21.24962)\n",
            "     | > current_lr_0: 5.950608875201907e-11 \n",
            "     | > current_lr_1: 5.950608875201907e-11 \n",
            "     | > step_time: 2.42910  (2.42482)\n",
            "     | > loader_time: 0.00220  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00128 \u001b[0m(-0.00014)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 0.58177 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[92m 0.70195 \u001b[0m(-0.00004)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.11152 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 26.17970 \u001b[0m(+0.00002)\n",
            "     | > avg_G_adv_loss:\u001b[92m 1.81714 \u001b[0m(-0.00005)\n",
            "     | > avg_loss_0:\u001b[92m 27.99684 \u001b[0m(-0.00004)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.13729 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00057 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 33/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 16:06:03) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/364 -- GLOBAL_STEP: 192050\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52639  (0.54322)\n",
            "     | > G_mse_fake_loss: 0.78133  (0.77278)\n",
            "     | > G_feat_match_loss: 0.11046  (0.11452)\n",
            "     | > G_gen_loss: 23.68746  (24.44511)\n",
            "     | > G_adv_loss: 1.88591  (1.91801)\n",
            "     | > loss_0: 25.57337  (26.36312)\n",
            "     | > grad_norm_0: 749.26044  (761.17078)\n",
            "     | > D_mse_gan_loss: 0.13318  (0.12080)\n",
            "     | > D_mse_gan_real_loss: 0.00010  (0.00004)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00005)\n",
            "     | > loss_1: 0.13318  (0.12080)\n",
            "     | > grad_norm_1: 29.57149  (14.96243)\n",
            "     | > current_lr_0: 5.803615224544102e-11 \n",
            "     | > current_lr_1: 5.803615224544102e-11 \n",
            "     | > step_time: 2.43110  (2.45844)\n",
            "     | > loader_time: 0.00090  (0.00132)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/364 -- GLOBAL_STEP: 192075\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51903  (0.53627)\n",
            "     | > G_mse_fake_loss: 0.77232  (0.76864)\n",
            "     | > G_feat_match_loss: 0.10728  (0.11322)\n",
            "     | > G_gen_loss: 23.35613  (24.13194)\n",
            "     | > G_adv_loss: 1.84512  (1.90081)\n",
            "     | > loss_0: 25.20125  (26.03275)\n",
            "     | > grad_norm_0: 781.32129  (819.14624)\n",
            "     | > D_mse_gan_loss: 0.13615  (0.12362)\n",
            "     | > D_mse_gan_real_loss: 0.00299  (0.00022)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00009)\n",
            "     | > loss_1: 0.13615  (0.12362)\n",
            "     | > grad_norm_1: 22.60236  (16.38754)\n",
            "     | > current_lr_0: 5.6602526532912565e-11 \n",
            "     | > current_lr_1: 5.6602526532912565e-11 \n",
            "     | > step_time: 2.42250  (2.42990)\n",
            "     | > loader_time: 0.00230  (0.00187)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/364 -- GLOBAL_STEP: 192100\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53088  (0.53548)\n",
            "     | > G_mse_fake_loss: 0.77685  (0.76856)\n",
            "     | > G_feat_match_loss: 0.10973  (0.11319)\n",
            "     | > G_gen_loss: 23.88974  (24.09650)\n",
            "     | > G_adv_loss: 1.87414  (1.90041)\n",
            "     | > loss_0: 25.76388  (25.99691)\n",
            "     | > grad_norm_0: 878.96344  (823.15173)\n",
            "     | > D_mse_gan_loss: 0.13120  (0.12418)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00009)\n",
            "     | > loss_1: 0.13120  (0.12418)\n",
            "     | > grad_norm_1: 23.51966  (16.03381)\n",
            "     | > current_lr_0: 5.520431465476326e-11 \n",
            "     | > current_lr_1: 5.520431465476326e-11 \n",
            "     | > step_time: 2.41760  (2.42785)\n",
            "     | > loader_time: 0.00200  (0.00199)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/364 -- GLOBAL_STEP: 192125\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56136  (0.53613)\n",
            "     | > G_mse_fake_loss: 0.75191  (0.76801)\n",
            "     | > G_feat_match_loss: 0.11147  (0.11322)\n",
            "     | > G_gen_loss: 25.26126  (24.12580)\n",
            "     | > G_adv_loss: 1.86658  (1.90025)\n",
            "     | > loss_0: 27.12784  (26.02604)\n",
            "     | > grad_norm_0: 769.18066  (820.92578)\n",
            "     | > D_mse_gan_loss: 0.14020  (0.12450)\n",
            "     | > D_mse_gan_real_loss: 0.00008  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00010)\n",
            "     | > loss_1: 0.14020  (0.12450)\n",
            "     | > grad_norm_1: 7.47924  (15.63698)\n",
            "     | > current_lr_0: 5.384064180827821e-11 \n",
            "     | > current_lr_1: 5.384064180827821e-11 \n",
            "     | > step_time: 2.42860  (2.42670)\n",
            "     | > loader_time: 0.00200  (0.00205)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/364 -- GLOBAL_STEP: 192150\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51218  (0.53564)\n",
            "     | > G_mse_fake_loss: 0.77374  (0.76747)\n",
            "     | > G_feat_match_loss: 0.10604  (0.11308)\n",
            "     | > G_gen_loss: 23.04813  (24.10371)\n",
            "     | > G_adv_loss: 1.83416  (1.89828)\n",
            "     | > loss_0: 24.88230  (26.00199)\n",
            "     | > grad_norm_0: 872.41492  (818.86786)\n",
            "     | > D_mse_gan_loss: 0.14409  (0.12445)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00010  (0.00009)\n",
            "     | > loss_1: 0.14409  (0.12445)\n",
            "     | > grad_norm_1: 34.38784  (15.63343)\n",
            "     | > current_lr_0: 5.251065480037067e-11 \n",
            "     | > current_lr_1: 5.251065480037067e-11 \n",
            "     | > step_time: 2.42490  (2.42645)\n",
            "     | > loader_time: 0.00220  (0.00207)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/364 -- GLOBAL_STEP: 192175\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54566  (0.53569)\n",
            "     | > G_mse_fake_loss: 0.76443  (0.76732)\n",
            "     | > G_feat_match_loss: 0.11445  (0.11308)\n",
            "     | > G_gen_loss: 24.55449  (24.10605)\n",
            "     | > G_adv_loss: 1.90888  (1.89808)\n",
            "     | > loss_0: 26.46337  (26.00413)\n",
            "     | > grad_norm_0: 756.60278  (817.06653)\n",
            "     | > D_mse_gan_loss: 0.11741  (0.12447)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00012)\n",
            "     | > loss_1: 0.11741  (0.12447)\n",
            "     | > grad_norm_1: 6.31218  (15.34995)\n",
            "     | > current_lr_0: 5.1213521513774655e-11 \n",
            "     | > current_lr_1: 5.1213521513774655e-11 \n",
            "     | > step_time: 2.42070  (2.42610)\n",
            "     | > loader_time: 0.00200  (0.00209)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 155/364 -- GLOBAL_STEP: 192200\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58898  (0.53608)\n",
            "     | > G_mse_fake_loss: 0.75674  (0.76726)\n",
            "     | > G_feat_match_loss: 0.12060  (0.11309)\n",
            "     | > G_gen_loss: 26.50419  (24.12355)\n",
            "     | > G_adv_loss: 1.96277  (1.89811)\n",
            "     | > loss_0: 28.46696  (26.02166)\n",
            "     | > grad_norm_0: 878.40210  (817.93658)\n",
            "     | > D_mse_gan_loss: 0.11098  (0.12443)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00012  (0.00012)\n",
            "     | > loss_1: 0.11098  (0.12443)\n",
            "     | > grad_norm_1: 22.07076  (15.26234)\n",
            "     | > current_lr_0: 4.9948430386424045e-11 \n",
            "     | > current_lr_1: 4.9948430386424045e-11 \n",
            "     | > step_time: 2.42960  (2.42581)\n",
            "     | > loader_time: 0.00220  (0.00210)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 180/364 -- GLOBAL_STEP: 192225\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55038  (0.53718)\n",
            "     | > G_mse_fake_loss: 0.76392  (0.76699)\n",
            "     | > G_feat_match_loss: 0.11483  (0.11318)\n",
            "     | > G_gen_loss: 24.76694  (24.17328)\n",
            "     | > G_adv_loss: 1.91225  (1.89878)\n",
            "     | > loss_0: 26.67919  (26.07206)\n",
            "     | > grad_norm_0: 791.01843  (817.92804)\n",
            "     | > D_mse_gan_loss: 0.12520  (0.12432)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00012)\n",
            "     | > loss_1: 0.12520  (0.12432)\n",
            "     | > grad_norm_1: 9.07790  (14.90374)\n",
            "     | > current_lr_0: 4.871458990369219e-11 \n",
            "     | > current_lr_1: 4.871458990369219e-11 \n",
            "     | > step_time: 2.41930  (2.42560)\n",
            "     | > loader_time: 0.00220  (0.00211)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 205/364 -- GLOBAL_STEP: 192250\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55015  (0.53730)\n",
            "     | > G_mse_fake_loss: 0.76169  (0.76633)\n",
            "     | > G_feat_match_loss: 0.11507  (0.11308)\n",
            "     | > G_gen_loss: 24.75654  (24.17861)\n",
            "     | > G_adv_loss: 1.91240  (1.89717)\n",
            "     | > loss_0: 26.66894  (26.07578)\n",
            "     | > grad_norm_0: 837.52759  (821.67499)\n",
            "     | > D_mse_gan_loss: 0.12581  (0.12455)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00009  (0.00012)\n",
            "     | > loss_1: 0.12581  (0.12455)\n",
            "     | > grad_norm_1: 9.64839  (14.91589)\n",
            "     | > current_lr_0: 4.751122810317418e-11 \n",
            "     | > current_lr_1: 4.751122810317418e-11 \n",
            "     | > step_time: 2.42210  (2.42542)\n",
            "     | > loader_time: 0.00220  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/364 -- GLOBAL_STEP: 192275\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54551  (0.53755)\n",
            "     | > G_mse_fake_loss: 0.76016  (0.76675)\n",
            "     | > G_feat_match_loss: 0.11216  (0.11324)\n",
            "     | > G_gen_loss: 24.54814  (24.18971)\n",
            "     | > G_adv_loss: 1.88175  (1.89919)\n",
            "     | > loss_0: 26.42989  (26.08889)\n",
            "     | > grad_norm_0: 863.18597  (821.88391)\n",
            "     | > D_mse_gan_loss: 0.13127  (0.12421)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00016  (0.00013)\n",
            "     | > loss_1: 0.13127  (0.12421)\n",
            "     | > grad_norm_1: 9.83586  (14.80690)\n",
            "     | > current_lr_0: 4.633759209170231e-11 \n",
            "     | > current_lr_1: 4.633759209170231e-11 \n",
            "     | > step_time: 2.42070  (2.42542)\n",
            "     | > loader_time: 0.00220  (0.00214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 255/364 -- GLOBAL_STEP: 192300\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53053  (0.53768)\n",
            "     | > G_mse_fake_loss: 0.75886  (0.76632)\n",
            "     | > G_feat_match_loss: 0.10927  (0.11320)\n",
            "     | > G_gen_loss: 23.87368  (24.19573)\n",
            "     | > G_adv_loss: 1.85152  (1.89830)\n",
            "     | > loss_0: 25.72521  (26.09403)\n",
            "     | > grad_norm_0: 804.71399  (818.35474)\n",
            "     | > D_mse_gan_loss: 0.13536  (0.12434)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00013)\n",
            "     | > loss_1: 0.13536  (0.12434)\n",
            "     | > grad_norm_1: 16.11422  (14.82331)\n",
            "     | > current_lr_0: 4.519294757429225e-11 \n",
            "     | > current_lr_1: 4.519294757429225e-11 \n",
            "     | > step_time: 2.42380  (2.42522)\n",
            "     | > loader_time: 0.00210  (0.00214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 280/364 -- GLOBAL_STEP: 192325\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55064  (0.53772)\n",
            "     | > G_mse_fake_loss: 0.74324  (0.76673)\n",
            "     | > G_feat_match_loss: 0.11142  (0.11322)\n",
            "     | > G_gen_loss: 24.77872  (24.19718)\n",
            "     | > G_adv_loss: 1.85748  (1.89891)\n",
            "     | > loss_0: 26.63620  (26.09610)\n",
            "     | > grad_norm_0: 705.52747  (819.49420)\n",
            "     | > D_mse_gan_loss: 0.13393  (0.12423)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00019  (0.00012)\n",
            "     | > loss_1: 0.13393  (0.12423)\n",
            "     | > grad_norm_1: 11.51002  (15.00662)\n",
            "     | > current_lr_0: 4.407657839472547e-11 \n",
            "     | > current_lr_1: 4.407657839472547e-11 \n",
            "     | > step_time: 2.42680  (2.42525)\n",
            "     | > loader_time: 0.00200  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 305/364 -- GLOBAL_STEP: 192350\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53415  (0.53806)\n",
            "     | > G_mse_fake_loss: 0.76484  (0.76645)\n",
            "     | > G_feat_match_loss: 0.10743  (0.11321)\n",
            "     | > G_gen_loss: 24.03691  (24.21253)\n",
            "     | > G_adv_loss: 1.83917  (1.89851)\n",
            "     | > loss_0: 25.87608  (26.11105)\n",
            "     | > grad_norm_0: 923.48865  (819.45795)\n",
            "     | > D_mse_gan_loss: 0.13786  (0.12430)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00012)\n",
            "     | > loss_1: 0.13786  (0.12430)\n",
            "     | > grad_norm_1: 24.46895  (15.05032)\n",
            "     | > current_lr_0: 4.298778608748016e-11 \n",
            "     | > current_lr_1: 4.298778608748016e-11 \n",
            "     | > step_time: 2.42720  (2.42516)\n",
            "     | > loader_time: 0.00220  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 330/364 -- GLOBAL_STEP: 192375\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55706  (0.53790)\n",
            "     | > G_mse_fake_loss: 0.77031  (0.76626)\n",
            "     | > G_feat_match_loss: 0.11852  (0.11322)\n",
            "     | > G_gen_loss: 25.06776  (24.20537)\n",
            "     | > G_adv_loss: 1.95554  (1.89848)\n",
            "     | > loss_0: 27.02330  (26.10386)\n",
            "     | > grad_norm_0: 831.81152  (818.81635)\n",
            "     | > D_mse_gan_loss: 0.10707  (0.12426)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00010  (0.00012)\n",
            "     | > loss_1: 0.10707  (0.12426)\n",
            "     | > grad_norm_1: 10.06129  (15.01391)\n",
            "     | > current_lr_0: 4.192588944073052e-11 \n",
            "     | > current_lr_1: 4.192588944073052e-11 \n",
            "     | > step_time: 2.42290  (2.42514)\n",
            "     | > loader_time: 0.00210  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 355/364 -- GLOBAL_STEP: 192400\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53797  (0.53758)\n",
            "     | > G_mse_fake_loss: 0.76881  (0.76624)\n",
            "     | > G_feat_match_loss: 0.11253  (0.11318)\n",
            "     | > G_gen_loss: 24.20852  (24.19123)\n",
            "     | > G_adv_loss: 1.89412  (1.89801)\n",
            "     | > loss_0: 26.10264  (26.08924)\n",
            "     | > grad_norm_0: 820.56897  (818.12164)\n",
            "     | > D_mse_gan_loss: 0.12520  (0.12429)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00012)\n",
            "     | > loss_1: 0.12520  (0.12429)\n",
            "     | > grad_norm_1: 12.53547  (14.94442)\n",
            "     | > current_lr_0: 4.0890224070141155e-11 \n",
            "     | > current_lr_1: 4.0890224070141155e-11 \n",
            "     | > step_time: 2.42050  (2.42512)\n",
            "     | > loader_time: 0.00180  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00119 \u001b[0m(-0.00009)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 0.58177 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[92m 0.70194 \u001b[0m(-0.00002)\n",
            "     | > avg_G_feat_match_loss:\u001b[92m 0.11152 \u001b[0m(-0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 26.17971 \u001b[0m(+0.00002)\n",
            "     | > avg_G_adv_loss:\u001b[92m 1.81712 \u001b[0m(-0.00002)\n",
            "     | > avg_loss_0:\u001b[92m 27.99684 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.13729 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00057 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 34/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 16:21:01) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/364 -- GLOBAL_STEP: 192425\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56302  (0.55573)\n",
            "     | > G_mse_fake_loss: 0.78819  (0.76387)\n",
            "     | > G_feat_match_loss: 0.12273  (0.11738)\n",
            "     | > G_gen_loss: 25.33582  (25.00806)\n",
            "     | > G_adv_loss: 2.01547  (1.93763)\n",
            "     | > loss_0: 27.35129  (26.94569)\n",
            "     | > grad_norm_0: 713.52563  (750.59631)\n",
            "     | > D_mse_gan_loss: 0.09683  (0.11338)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00002)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00010)\n",
            "     | > loss_1: 0.09683  (0.11338)\n",
            "     | > grad_norm_1: 9.27920  (17.64668)\n",
            "     | > current_lr_0: 3.988014200318935e-11 \n",
            "     | > current_lr_1: 3.988014200318935e-11 \n",
            "     | > step_time: 2.42190  (2.44046)\n",
            "     | > loader_time: 0.00240  (0.00226)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 40/364 -- GLOBAL_STEP: 192450\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58690  (0.55371)\n",
            "     | > G_mse_fake_loss: 0.74140  (0.76082)\n",
            "     | > G_feat_match_loss: 0.11762  (0.11652)\n",
            "     | > G_gen_loss: 26.41043  (24.91673)\n",
            "     | > G_adv_loss: 1.91756  (1.92599)\n",
            "     | > loss_0: 28.32799  (26.84272)\n",
            "     | > grad_norm_0: 950.86548  (777.42566)\n",
            "     | > D_mse_gan_loss: 0.12713  (0.11603)\n",
            "     | > D_mse_gan_real_loss: 0.00031  (0.00005)\n",
            "     | > D_mse_gan_fake_loss: 0.00017  (0.00011)\n",
            "     | > loss_1: 0.12713  (0.11603)\n",
            "     | > grad_norm_1: 19.53214  (18.16863)\n",
            "     | > current_lr_0: 3.889501127375598e-11 \n",
            "     | > current_lr_1: 3.889501127375598e-11 \n",
            "     | > step_time: 2.42280  (2.43083)\n",
            "     | > loader_time: 0.00250  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 65/364 -- GLOBAL_STEP: 192475\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55488  (0.55385)\n",
            "     | > G_mse_fake_loss: 0.76422  (0.76123)\n",
            "     | > G_feat_match_loss: 0.11582  (0.11657)\n",
            "     | > G_gen_loss: 24.96946  (24.92335)\n",
            "     | > G_adv_loss: 1.92238  (1.92694)\n",
            "     | > loss_0: 26.89184  (26.85029)\n",
            "     | > grad_norm_0: 701.39832  (774.44501)\n",
            "     | > D_mse_gan_loss: 0.11064  (0.11563)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00004)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00011)\n",
            "     | > loss_1: 0.11064  (0.11563)\n",
            "     | > grad_norm_1: 9.67172  (17.68618)\n",
            "     | > current_lr_0: 3.793421552673054e-11 \n",
            "     | > current_lr_1: 3.793421552673054e-11 \n",
            "     | > step_time: 2.42630  (2.42875)\n",
            "     | > loader_time: 0.00230  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/364 -- GLOBAL_STEP: 192500\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51808  (0.55428)\n",
            "     | > G_mse_fake_loss: 0.79159  (0.76041)\n",
            "     | > G_feat_match_loss: 0.10830  (0.11639)\n",
            "     | > G_gen_loss: 23.31368  (24.94239)\n",
            "     | > G_adv_loss: 1.87461  (1.92433)\n",
            "     | > loss_0: 25.18828  (26.86672)\n",
            "     | > grad_norm_0: 788.90979  (779.32013)\n",
            "     | > D_mse_gan_loss: 0.14285  (0.11603)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00011)\n",
            "     | > loss_1: 0.14285  (0.11603)\n",
            "     | > grad_norm_1: 42.35897  (18.21246)\n",
            "     | > current_lr_0: 3.6997153632383655e-11 \n",
            "     | > current_lr_1: 3.6997153632383655e-11 \n",
            "     | > step_time: 2.42730  (2.42779)\n",
            "     | > loader_time: 0.00230  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 115/364 -- GLOBAL_STEP: 192525\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53013  (0.55314)\n",
            "     | > G_mse_fake_loss: 0.76705  (0.76071)\n",
            "     | > G_feat_match_loss: 0.11015  (0.11633)\n",
            "     | > G_gen_loss: 23.85594  (24.89111)\n",
            "     | > G_adv_loss: 1.86856  (1.92402)\n",
            "     | > loss_0: 25.72449  (26.81513)\n",
            "     | > grad_norm_0: 911.61493  (783.29187)\n",
            "     | > D_mse_gan_loss: 0.13091  (0.11557)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00012)\n",
            "     | > loss_1: 0.13091  (0.11557)\n",
            "     | > grad_norm_1: 18.45086  (18.22009)\n",
            "     | > current_lr_0: 3.6083239310265304e-11 \n",
            "     | > current_lr_1: 3.6083239310265304e-11 \n",
            "     | > step_time: 2.42270  (2.42739)\n",
            "     | > loader_time: 0.00200  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 140/364 -- GLOBAL_STEP: 192550\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52815  (0.55303)\n",
            "     | > G_mse_fake_loss: 0.75630  (0.76054)\n",
            "     | > G_feat_match_loss: 0.11102  (0.11614)\n",
            "     | > G_gen_loss: 23.76692  (24.88641)\n",
            "     | > G_adv_loss: 1.86654  (1.92198)\n",
            "     | > loss_0: 25.63346  (26.80839)\n",
            "     | > grad_norm_0: 806.44971  (785.10443)\n",
            "     | > D_mse_gan_loss: 0.12606  (0.11613)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00009  (0.00012)\n",
            "     | > loss_1: 0.12606  (0.11613)\n",
            "     | > grad_norm_1: 10.93961  (18.18483)\n",
            "     | > current_lr_0: 3.519190076239359e-11 \n",
            "     | > current_lr_1: 3.519190076239359e-11 \n",
            "     | > step_time: 2.42330  (2.42675)\n",
            "     | > loader_time: 0.00200  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 165/364 -- GLOBAL_STEP: 192575\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60436  (0.55418)\n",
            "     | > G_mse_fake_loss: 0.74027  (0.76033)\n",
            "     | > G_feat_match_loss: 0.12274  (0.11630)\n",
            "     | > G_gen_loss: 27.19634  (24.93822)\n",
            "     | > G_adv_loss: 1.96769  (1.92337)\n",
            "     | > loss_0: 29.16403  (26.86160)\n",
            "     | > grad_norm_0: 669.47235  (781.82629)\n",
            "     | > D_mse_gan_loss: 0.09946  (0.11567)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00010  (0.00012)\n",
            "     | > loss_1: 0.09946  (0.11567)\n",
            "     | > grad_norm_1: 47.76797  (18.26378)\n",
            "     | > current_lr_0: 3.432258031550474e-11 \n",
            "     | > current_lr_1: 3.432258031550474e-11 \n",
            "     | > step_time: 2.42990  (2.42625)\n",
            "     | > loader_time: 0.00200  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 190/364 -- GLOBAL_STEP: 192600\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56676  (0.55416)\n",
            "     | > G_mse_fake_loss: 0.76858  (0.76065)\n",
            "     | > G_feat_match_loss: 0.11725  (0.11633)\n",
            "     | > G_gen_loss: 25.50425  (24.93708)\n",
            "     | > G_adv_loss: 1.94106  (1.92398)\n",
            "     | > loss_0: 27.44531  (26.86106)\n",
            "     | > grad_norm_0: 750.21960  (785.71527)\n",
            "     | > D_mse_gan_loss: 0.10939  (0.11553)\n",
            "     | > D_mse_gan_real_loss: 0.00124  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00012)\n",
            "     | > loss_1: 0.10939  (0.11553)\n",
            "     | > grad_norm_1: 11.37533  (18.62636)\n",
            "     | > current_lr_0: 3.34747340721402e-11 \n",
            "     | > current_lr_1: 3.34747340721402e-11 \n",
            "     | > step_time: 2.42360  (2.42612)\n",
            "     | > loader_time: 0.00210  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 215/364 -- GLOBAL_STEP: 192625\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53320  (0.55430)\n",
            "     | > G_mse_fake_loss: 0.74541  (0.76063)\n",
            "     | > G_feat_match_loss: 0.11003  (0.11635)\n",
            "     | > G_gen_loss: 23.99386  (24.94336)\n",
            "     | > G_adv_loss: 1.84572  (1.92409)\n",
            "     | > loss_0: 25.83958  (26.86745)\n",
            "     | > grad_norm_0: 794.58636  (789.42426)\n",
            "     | > D_mse_gan_loss: 0.13190  (0.11556)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00044  (0.00012)\n",
            "     | > loss_1: 0.13190  (0.11556)\n",
            "     | > grad_norm_1: 5.56911  (18.59243)\n",
            "     | > current_lr_0: 3.2647831570352755e-11 \n",
            "     | > current_lr_1: 3.2647831570352755e-11 \n",
            "     | > step_time: 2.42390  (2.42600)\n",
            "     | > loader_time: 0.00210  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 240/364 -- GLOBAL_STEP: 192650\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57066  (0.55495)\n",
            "     | > G_mse_fake_loss: 0.75579  (0.76057)\n",
            "     | > G_feat_match_loss: 0.11529  (0.11647)\n",
            "     | > G_gen_loss: 25.67958  (24.97265)\n",
            "     | > G_adv_loss: 1.90870  (1.92530)\n",
            "     | > loss_0: 27.58828  (26.89796)\n",
            "     | > grad_norm_0: 871.25342  (786.20721)\n",
            "     | > D_mse_gan_loss: 0.12525  (0.11537)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00012)\n",
            "     | > loss_1: 0.12525  (0.11537)\n",
            "     | > grad_norm_1: 6.84037  (19.00733)\n",
            "     | > current_lr_0: 3.1841355451818675e-11 \n",
            "     | > current_lr_1: 3.1841355451818675e-11 \n",
            "     | > step_time: 2.42740  (2.42582)\n",
            "     | > loader_time: 0.00210  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 265/364 -- GLOBAL_STEP: 192675\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54446  (0.55493)\n",
            "     | > G_mse_fake_loss: 0.75750  (0.76045)\n",
            "     | > G_feat_match_loss: 0.11497  (0.11644)\n",
            "     | > G_gen_loss: 24.50076  (24.97190)\n",
            "     | > G_adv_loss: 1.90720  (1.92489)\n",
            "     | > loss_0: 26.40795  (26.89679)\n",
            "     | > grad_norm_0: 721.58282  (783.92975)\n",
            "     | > D_mse_gan_loss: 0.11461  (0.11552)\n",
            "     | > D_mse_gan_real_loss: 0.00019  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00010  (0.00012)\n",
            "     | > loss_1: 0.11461  (0.11552)\n",
            "     | > grad_norm_1: 8.70012  (18.78554)\n",
            "     | > current_lr_0: 3.1054801138148246e-11 \n",
            "     | > current_lr_1: 3.1054801138148246e-11 \n",
            "     | > step_time: 2.42520  (2.42569)\n",
            "     | > loader_time: 0.00220  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 290/364 -- GLOBAL_STEP: 192700\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54496  (0.55507)\n",
            "     | > G_mse_fake_loss: 0.77235  (0.76046)\n",
            "     | > G_feat_match_loss: 0.11200  (0.11643)\n",
            "     | > G_gen_loss: 24.52333  (24.97809)\n",
            "     | > G_adv_loss: 1.89231  (1.92476)\n",
            "     | > loss_0: 26.41565  (26.90285)\n",
            "     | > grad_norm_0: 784.49603  (782.50287)\n",
            "     | > D_mse_gan_loss: 0.12624  (0.11555)\n",
            "     | > D_mse_gan_real_loss: 0.00013  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00009  (0.00011)\n",
            "     | > loss_1: 0.12624  (0.11555)\n",
            "     | > grad_norm_1: 19.36597  (18.69032)\n",
            "     | > current_lr_0: 3.028767651519215e-11 \n",
            "     | > current_lr_1: 3.028767651519215e-11 \n",
            "     | > step_time: 2.42180  (2.42548)\n",
            "     | > loader_time: 0.00200  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 315/364 -- GLOBAL_STEP: 192725\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51889  (0.55528)\n",
            "     | > G_mse_fake_loss: 0.76884  (0.76018)\n",
            "     | > G_feat_match_loss: 0.11075  (0.11648)\n",
            "     | > G_gen_loss: 23.35008  (24.98738)\n",
            "     | > G_adv_loss: 1.87637  (1.92496)\n",
            "     | > loss_0: 25.22645  (26.91235)\n",
            "     | > grad_norm_0: 787.88544  (780.56128)\n",
            "     | > D_mse_gan_loss: 0.12855  (0.11536)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00011)\n",
            "     | > loss_1: 0.12855  (0.11536)\n",
            "     | > grad_norm_1: 15.70416  (18.52925)\n",
            "     | > current_lr_0: 2.953950162514619e-11 \n",
            "     | > current_lr_1: 2.953950162514619e-11 \n",
            "     | > step_time: 2.42720  (2.42536)\n",
            "     | > loader_time: 0.00240  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 340/364 -- GLOBAL_STEP: 192750\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51800  (0.55520)\n",
            "     | > G_mse_fake_loss: 0.77246  (0.75999)\n",
            "     | > G_feat_match_loss: 0.11487  (0.11642)\n",
            "     | > G_gen_loss: 23.31018  (24.98379)\n",
            "     | > G_adv_loss: 1.92115  (1.92415)\n",
            "     | > loss_0: 25.23133  (26.90794)\n",
            "     | > grad_norm_0: 721.12512  (781.19653)\n",
            "     | > D_mse_gan_loss: 0.11709  (0.11553)\n",
            "     | > D_mse_gan_real_loss: 0.00013  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00012  (0.00012)\n",
            "     | > loss_1: 0.11709  (0.11553)\n",
            "     | > grad_norm_1: 9.02906  (18.41565)\n",
            "     | > current_lr_0: 2.8809808366261815e-11 \n",
            "     | > current_lr_1: 2.8809808366261815e-11 \n",
            "     | > step_time: 2.42100  (2.42523)\n",
            "     | > loader_time: 0.00220  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00115 \u001b[0m(-0.00004)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 0.58177 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70194 \u001b[0m(+0.00001)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 26.17972 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81713 \u001b[0m(+0.00001)\n",
            "     | > avg_loss_0:\u001b[91m 27.99685 \u001b[0m(+0.00001)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.27955 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.13729 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00057 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.27955 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 35/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 16:35:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/364 -- GLOBAL_STEP: 192775\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53798  (0.53798)\n",
            "     | > G_mse_fake_loss: 0.76677  (0.76677)\n",
            "     | > G_feat_match_loss: 0.11019  (0.11019)\n",
            "     | > G_gen_loss: 24.20916  (24.20916)\n",
            "     | > G_adv_loss: 1.86867  (1.86867)\n",
            "     | > loss_0: 26.07783  (26.07783)\n",
            "     | > grad_norm_0: 852.98755  (852.98755)\n",
            "     | > D_mse_gan_loss: 0.12863  (0.12863)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00004)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00003)\n",
            "     | > loss_1: 0.12863  (0.12863)\n",
            "     | > grad_norm_1: 14.41949  (14.41949)\n",
            "     | > current_lr_0: 2.809814019997441e-11 \n",
            "     | > current_lr_1: 2.809814019997441e-11 \n",
            "     | > step_time: 4.82740  (4.82741)\n",
            "     | > loader_time: 5.13460  (5.13462)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 25/364 -- GLOBAL_STEP: 192800\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55266  (0.55933)\n",
            "     | > G_mse_fake_loss: 0.75545  (0.75548)\n",
            "     | > G_feat_match_loss: 0.11654  (0.11353)\n",
            "     | > G_gen_loss: 24.86992  (25.16988)\n",
            "     | > G_adv_loss: 1.92082  (1.89079)\n",
            "     | > loss_0: 26.79074  (27.06066)\n",
            "     | > grad_norm_0: 835.44965  (909.80310)\n",
            "     | > D_mse_gan_loss: 0.11378  (0.12316)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00014  (0.00023)\n",
            "     | > loss_1: 0.11378  (0.12316)\n",
            "     | > grad_norm_1: 11.29873  (18.89705)\n",
            "     | > current_lr_0: 2.7404051865266163e-11 \n",
            "     | > current_lr_1: 2.7404051865266163e-11 \n",
            "     | > step_time: 2.42870  (2.42615)\n",
            "     | > loader_time: 0.00200  (0.01695)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/364 -- GLOBAL_STEP: 192825\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52673  (0.55713)\n",
            "     | > G_mse_fake_loss: 0.78116  (0.75748)\n",
            "     | > G_feat_match_loss: 0.11230  (0.11318)\n",
            "     | > G_gen_loss: 23.70277  (25.07063)\n",
            "     | > G_adv_loss: 1.90419  (1.88927)\n",
            "     | > loss_0: 25.60696  (26.95990)\n",
            "     | > grad_norm_0: 875.88354  (898.05975)\n",
            "     | > D_mse_gan_loss: 0.12637  (0.12450)\n",
            "     | > D_mse_gan_real_loss: 0.00013  (0.00032)\n",
            "     | > D_mse_gan_fake_loss: 0.00029  (0.00021)\n",
            "     | > loss_1: 0.12637  (0.12450)\n",
            "     | > grad_norm_1: 16.62297  (20.67275)\n",
            "     | > current_lr_0: 2.6727109100084924e-11 \n",
            "     | > current_lr_1: 2.6727109100084924e-11 \n",
            "     | > step_time: 2.42500  (2.42502)\n",
            "     | > loader_time: 0.00230  (0.00954)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 75/364 -- GLOBAL_STEP: 192850\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61280  (0.55816)\n",
            "     | > G_mse_fake_loss: 0.73070  (0.75756)\n",
            "     | > G_feat_match_loss: 0.10916  (0.11332)\n",
            "     | > G_gen_loss: 27.57609  (25.11716)\n",
            "     | > G_adv_loss: 1.82232  (1.89079)\n",
            "     | > loss_0: 29.39841  (27.00795)\n",
            "     | > grad_norm_0: 1749.94263  (916.14899)\n",
            "     | > D_mse_gan_loss: 0.13020  (0.12398)\n",
            "     | > D_mse_gan_real_loss: 0.00010  (0.00028)\n",
            "     | > D_mse_gan_fake_loss: 0.00084  (0.00020)\n",
            "     | > loss_1: 0.13020  (0.12398)\n",
            "     | > grad_norm_1: 21.80769  (20.60032)\n",
            "     | > current_lr_0: 2.6066888369644537e-11 \n",
            "     | > current_lr_1: 2.6066888369644537e-11 \n",
            "     | > step_time: 2.42050  (2.42519)\n",
            "     | > loader_time: 0.00230  (0.00706)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/364 -- GLOBAL_STEP: 192875\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55458  (0.55828)\n",
            "     | > G_mse_fake_loss: 0.75163  (0.75724)\n",
            "     | > G_feat_match_loss: 0.11662  (0.11321)\n",
            "     | > G_gen_loss: 24.95620  (25.12268)\n",
            "     | > G_adv_loss: 1.91779  (1.88931)\n",
            "     | > loss_0: 26.87399  (27.01199)\n",
            "     | > grad_norm_0: 746.31793  (918.25714)\n",
            "     | > D_mse_gan_loss: 0.11861  (0.12451)\n",
            "     | > D_mse_gan_real_loss: 0.00019  (0.00024)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00020)\n",
            "     | > loss_1: 0.11861  (0.12451)\n",
            "     | > grad_norm_1: 13.98431  (20.92383)\n",
            "     | > current_lr_0: 2.5422976601436878e-11 \n",
            "     | > current_lr_1: 2.5422976601436878e-11 \n",
            "     | > step_time: 2.41970  (2.42502)\n",
            "     | > loader_time: 0.00220  (0.00582)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 125/364 -- GLOBAL_STEP: 192900\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61197  (0.55810)\n",
            "     | > G_mse_fake_loss: 0.72612  (0.75660)\n",
            "     | > G_feat_match_loss: 0.11936  (0.11315)\n",
            "     | > G_gen_loss: 27.53851  (25.11438)\n",
            "     | > G_adv_loss: 1.91970  (1.88811)\n",
            "     | > loss_0: 29.45821  (27.00249)\n",
            "     | > grad_norm_0: 706.60663  (918.50006)\n",
            "     | > D_mse_gan_loss: 0.11280  (0.12438)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00022)\n",
            "     | > D_mse_gan_fake_loss: 0.00228  (0.00025)\n",
            "     | > loss_1: 0.11280  (0.12438)\n",
            "     | > grad_norm_1: 36.88013  (20.46434)\n",
            "     | > current_lr_0: 2.479497092678963e-11 \n",
            "     | > current_lr_1: 2.479497092678963e-11 \n",
            "     | > step_time: 2.43060  (2.42498)\n",
            "     | > loader_time: 0.00210  (0.00508)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 150/364 -- GLOBAL_STEP: 192925\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53717  (0.55854)\n",
            "     | > G_mse_fake_loss: 0.77357  (0.75666)\n",
            "     | > G_feat_match_loss: 0.11014  (0.11320)\n",
            "     | > G_gen_loss: 24.17284  (25.13441)\n",
            "     | > G_adv_loss: 1.87500  (1.88863)\n",
            "     | > loss_0: 26.04784  (27.02305)\n",
            "     | > grad_norm_0: 774.68890  (921.24261)\n",
            "     | > D_mse_gan_loss: 0.13979  (0.12425)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00026)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00024)\n",
            "     | > loss_1: 0.13979  (0.12425)\n",
            "     | > grad_norm_1: 27.20480  (21.23259)\n",
            "     | > current_lr_0: 2.4182478428808197e-11 \n",
            "     | > current_lr_1: 2.4182478428808197e-11 \n",
            "     | > step_time: 2.42500  (2.42504)\n",
            "     | > loader_time: 0.00190  (0.00458)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 175/364 -- GLOBAL_STEP: 192950\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53594  (0.55870)\n",
            "     | > G_mse_fake_loss: 0.77405  (0.75634)\n",
            "     | > G_feat_match_loss: 0.11362  (0.11328)\n",
            "     | > G_gen_loss: 24.11730  (25.14145)\n",
            "     | > G_adv_loss: 1.91028  (1.88915)\n",
            "     | > loss_0: 26.02759  (27.03061)\n",
            "     | > grad_norm_0: 741.16339  (918.11340)\n",
            "     | > D_mse_gan_loss: 0.12714  (0.12418)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00026)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00024)\n",
            "     | > loss_1: 0.12714  (0.12418)\n",
            "     | > grad_norm_1: 12.90136  (21.57641)\n",
            "     | > current_lr_0: 2.358511589654405e-11 \n",
            "     | > current_lr_1: 2.358511589654405e-11 \n",
            "     | > step_time: 2.42380  (2.42488)\n",
            "     | > loader_time: 0.00200  (0.00422)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 200/364 -- GLOBAL_STEP: 192975\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55161  (0.55878)\n",
            "     | > G_mse_fake_loss: 0.77016  (0.75616)\n",
            "     | > G_feat_match_loss: 0.10981  (0.11332)\n",
            "     | > G_gen_loss: 24.82251  (25.14515)\n",
            "     | > G_adv_loss: 1.86825  (1.88934)\n",
            "     | > loss_0: 26.69077  (27.03449)\n",
            "     | > grad_norm_0: 857.91199  (922.59985)\n",
            "     | > D_mse_gan_loss: 0.13452  (0.12390)\n",
            "     | > D_mse_gan_real_loss: 0.00007  (0.00023)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00024)\n",
            "     | > loss_1: 0.13452  (0.12390)\n",
            "     | > grad_norm_1: 18.35103  (21.42567)\n",
            "     | > current_lr_0: 2.3002509585235653e-11 \n",
            "     | > current_lr_1: 2.3002509585235653e-11 \n",
            "     | > step_time: 2.42450  (2.42473)\n",
            "     | > loader_time: 0.00210  (0.00396)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 225/364 -- GLOBAL_STEP: 193000\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57270  (0.55908)\n",
            "     | > G_mse_fake_loss: 0.75489  (0.75671)\n",
            "     | > G_feat_match_loss: 0.11984  (0.11353)\n",
            "     | > G_gen_loss: 25.77128  (25.15841)\n",
            "     | > G_adv_loss: 1.95332  (1.89198)\n",
            "     | > loss_0: 27.72460  (27.05038)\n",
            "     | > grad_norm_0: 814.17633  (917.16882)\n",
            "     | > D_mse_gan_loss: 0.11118  (0.12345)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00027)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00023)\n",
            "     | > loss_1: 0.11118  (0.12345)\n",
            "     | > grad_norm_1: 17.96711  (21.11764)\n",
            "     | > current_lr_0: 2.2434294982471983e-11 \n",
            "     | > current_lr_1: 2.2434294982471983e-11 \n",
            "     | > step_time: 2.42250  (2.42466)\n",
            "     | > loader_time: 0.00230  (0.00376)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 250/364 -- GLOBAL_STEP: 193025\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51255  (0.55875)\n",
            "     | > G_mse_fake_loss: 0.77762  (0.75700)\n",
            "     | > G_feat_match_loss: 0.10756  (0.11350)\n",
            "     | > G_gen_loss: 23.06487  (25.14395)\n",
            "     | > G_adv_loss: 1.85324  (1.89197)\n",
            "     | > loss_0: 24.91811  (27.03591)\n",
            "     | > grad_norm_0: 966.56702  (917.60669)\n",
            "     | > D_mse_gan_loss: 0.13572  (0.12360)\n",
            "     | > D_mse_gan_real_loss: 0.00095  (0.00026)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00022)\n",
            "     | > loss_1: 0.13572  (0.12360)\n",
            "     | > grad_norm_1: 27.89493  (21.26243)\n",
            "     | > current_lr_0: 2.1880116580132323e-11 \n",
            "     | > current_lr_1: 2.1880116580132323e-11 \n",
            "     | > step_time: 2.41940  (2.42463)\n",
            "     | > loader_time: 0.00230  (0.00360)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 275/364 -- GLOBAL_STEP: 193050\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62583  (0.55895)\n",
            "     | > G_mse_fake_loss: 0.73369  (0.75708)\n",
            "     | > G_feat_match_loss: 0.11273  (0.11351)\n",
            "     | > G_gen_loss: 28.16254  (25.15282)\n",
            "     | > G_adv_loss: 1.86102  (1.89218)\n",
            "     | > loss_0: 30.02356  (27.04500)\n",
            "     | > grad_norm_0: 1468.11609  (919.10889)\n",
            "     | > D_mse_gan_loss: 0.11648  (0.12355)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00026)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00021)\n",
            "     | > loss_1: 0.11648  (0.12355)\n",
            "     | > grad_norm_1: 25.43689  (21.35876)\n",
            "     | > current_lr_0: 2.1339627651959772e-11 \n",
            "     | > current_lr_1: 2.1339627651959772e-11 \n",
            "     | > step_time: 2.41960  (2.42459)\n",
            "     | > loader_time: 0.00190  (0.00346)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 300/364 -- GLOBAL_STEP: 193075\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55105  (0.55917)\n",
            "     | > G_mse_fake_loss: 0.76739  (0.75687)\n",
            "     | > G_feat_match_loss: 0.11865  (0.11350)\n",
            "     | > G_gen_loss: 24.79747  (25.16262)\n",
            "     | > G_adv_loss: 1.95393  (1.89191)\n",
            "     | > loss_0: 26.75140  (27.05453)\n",
            "     | > grad_norm_0: 784.95245  (917.38409)\n",
            "     | > D_mse_gan_loss: 0.10447  (0.12360)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00026)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00021)\n",
            "     | > loss_1: 0.10447  (0.12360)\n",
            "     | > grad_norm_1: 12.99545  (20.87847)\n",
            "     | > current_lr_0: 2.0812490036629057e-11 \n",
            "     | > current_lr_1: 2.0812490036629057e-11 \n",
            "     | > step_time: 2.43310  (2.42465)\n",
            "     | > loader_time: 0.00200  (0.00335)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 325/364 -- GLOBAL_STEP: 193100\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58548  (0.55938)\n",
            "     | > G_mse_fake_loss: 0.76677  (0.75669)\n",
            "     | > G_feat_match_loss: 0.12253  (0.11354)\n",
            "     | > G_gen_loss: 26.34661  (25.17207)\n",
            "     | > G_adv_loss: 1.99209  (1.89207)\n",
            "     | > loss_0: 28.33870  (27.06414)\n",
            "     | > grad_norm_0: 805.13867  (916.14001)\n",
            "     | > D_mse_gan_loss: 0.09220  (0.12342)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00026)\n",
            "     | > D_mse_gan_fake_loss: 0.00012  (0.00020)\n",
            "     | > loss_1: 0.09220  (0.12342)\n",
            "     | > grad_norm_1: 29.48561  (20.50211)\n",
            "     | > current_lr_0: 2.0298373926173144e-11 \n",
            "     | > current_lr_1: 2.0298373926173144e-11 \n",
            "     | > step_time: 2.42330  (2.42471)\n",
            "     | > loader_time: 0.00230  (0.00326)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 350/364 -- GLOBAL_STEP: 193125\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53097  (0.55944)\n",
            "     | > G_mse_fake_loss: 0.76985  (0.75680)\n",
            "     | > G_feat_match_loss: 0.11295  (0.11357)\n",
            "     | > G_gen_loss: 23.89378  (25.17491)\n",
            "     | > G_adv_loss: 1.89937  (1.89253)\n",
            "     | > loss_0: 25.79314  (27.06745)\n",
            "     | > grad_norm_0: 889.55103  (915.90240)\n",
            "     | > D_mse_gan_loss: 0.12353  (0.12328)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00028)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00020)\n",
            "     | > loss_1: 0.12353  (0.12328)\n",
            "     | > grad_norm_1: 9.80411  (20.39864)\n",
            "     | > current_lr_0: 1.9796957659636194e-11 \n",
            "     | > current_lr_1: 1.9796957659636194e-11 \n",
            "     | > step_time: 2.42560  (2.42469)\n",
            "     | > loader_time: 0.00200  (0.00317)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00141 \u001b[0m(+0.00026)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.58177 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70195 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 26.17971 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81714 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 27.99685 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.13729 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00057 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 36/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 16:50:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/364 -- GLOBAL_STEP: 193150\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54524  (0.54142)\n",
            "     | > G_mse_fake_loss: 0.75041  (0.76471)\n",
            "     | > G_feat_match_loss: 0.11079  (0.11282)\n",
            "     | > G_gen_loss: 24.53587  (24.36393)\n",
            "     | > G_adv_loss: 1.85836  (1.89288)\n",
            "     | > loss_0: 26.39423  (26.25680)\n",
            "     | > grad_norm_0: 890.54498  (794.23627)\n",
            "     | > D_mse_gan_loss: 0.13432  (0.12646)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00011  (0.00008)\n",
            "     | > loss_1: 0.13432  (0.12646)\n",
            "     | > grad_norm_1: 9.13298  (16.42481)\n",
            "     | > current_lr_0: 1.9307927521823763e-11 \n",
            "     | > current_lr_1: 1.9307927521823763e-11 \n",
            "     | > step_time: 2.42270  (2.44428)\n",
            "     | > loader_time: 0.00080  (0.00158)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 35/364 -- GLOBAL_STEP: 193175\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57618  (0.54745)\n",
            "     | > G_mse_fake_loss: 0.76074  (0.76396)\n",
            "     | > G_feat_match_loss: 0.12644  (0.11477)\n",
            "     | > G_gen_loss: 25.92808  (24.63513)\n",
            "     | > G_adv_loss: 2.02517  (1.91169)\n",
            "     | > loss_0: 27.95324  (26.54682)\n",
            "     | > grad_norm_0: 703.66974  (799.09155)\n",
            "     | > D_mse_gan_loss: 0.08749  (0.12055)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00011  (0.00007)\n",
            "     | > loss_1: 0.08749  (0.12055)\n",
            "     | > grad_norm_1: 33.88972  (20.53290)\n",
            "     | > current_lr_0: 1.8830977547024284e-11 \n",
            "     | > current_lr_1: 1.8830977547024284e-11 \n",
            "     | > step_time: 2.42480  (2.43063)\n",
            "     | > loader_time: 0.00230  (0.00181)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/364 -- GLOBAL_STEP: 193200\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59243  (0.54746)\n",
            "     | > G_mse_fake_loss: 0.74509  (0.76397)\n",
            "     | > G_feat_match_loss: 0.11557  (0.11461)\n",
            "     | > G_gen_loss: 26.65944  (24.63561)\n",
            "     | > G_adv_loss: 1.90083  (1.91005)\n",
            "     | > loss_0: 28.56027  (26.54566)\n",
            "     | > grad_norm_0: 852.97656  (798.24341)\n",
            "     | > D_mse_gan_loss: 0.13080  (0.12098)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00047  (0.00008)\n",
            "     | > loss_1: 0.13080  (0.12098)\n",
            "     | > grad_norm_1: 14.47115  (19.67714)\n",
            "     | > current_lr_0: 1.8365809327579137e-11 \n",
            "     | > current_lr_1: 1.8365809327579137e-11 \n",
            "     | > step_time: 2.42740  (2.42758)\n",
            "     | > loader_time: 0.00220  (0.00193)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 85/364 -- GLOBAL_STEP: 193225\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52798  (0.54762)\n",
            "     | > G_mse_fake_loss: 0.75974  (0.76303)\n",
            "     | > G_feat_match_loss: 0.10788  (0.11462)\n",
            "     | > G_gen_loss: 23.75898  (24.64290)\n",
            "     | > G_adv_loss: 1.83850  (1.90922)\n",
            "     | > loss_0: 25.59747  (26.55212)\n",
            "     | > grad_norm_0: 790.72626  (804.23651)\n",
            "     | > D_mse_gan_loss: 0.13809  (0.12075)\n",
            "     | > D_mse_gan_real_loss: 0.00012  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00009)\n",
            "     | > loss_1: 0.13809  (0.12075)\n",
            "     | > grad_norm_1: 17.17992  (19.46897)\n",
            "     | > current_lr_0: 1.7912131827181435e-11 \n",
            "     | > current_lr_1: 1.7912131827181435e-11 \n",
            "     | > step_time: 2.42370  (2.42689)\n",
            "     | > loader_time: 0.00220  (0.00201)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 110/364 -- GLOBAL_STEP: 193250\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55782  (0.54700)\n",
            "     | > G_mse_fake_loss: 0.73908  (0.76225)\n",
            "     | > G_feat_match_loss: 0.11625  (0.11450)\n",
            "     | > G_gen_loss: 25.10169  (24.61519)\n",
            "     | > G_adv_loss: 1.90154  (1.90728)\n",
            "     | > loss_0: 27.00323  (26.52246)\n",
            "     | > grad_norm_0: 678.85016  (800.33337)\n",
            "     | > D_mse_gan_loss: 0.11774  (0.12077)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00012  (0.00009)\n",
            "     | > loss_1: 0.11774  (0.12077)\n",
            "     | > grad_norm_1: 19.67209  (19.96282)\n",
            "     | > current_lr_0: 1.7469661198786804e-11 \n",
            "     | > current_lr_1: 1.7469661198786804e-11 \n",
            "     | > step_time: 2.42490  (2.42607)\n",
            "     | > loader_time: 0.00220  (0.00204)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 135/364 -- GLOBAL_STEP: 193275\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52776  (0.54661)\n",
            "     | > G_mse_fake_loss: 0.78016  (0.76250)\n",
            "     | > G_feat_match_loss: 0.11022  (0.11429)\n",
            "     | > G_gen_loss: 23.74907  (24.59761)\n",
            "     | > G_adv_loss: 1.88236  (1.90537)\n",
            "     | > loss_0: 25.63142  (26.50297)\n",
            "     | > grad_norm_0: 754.02020  (804.47015)\n",
            "     | > D_mse_gan_loss: 0.13003  (0.12122)\n",
            "     | > D_mse_gan_real_loss: 4.644750333682168e-06  (0.00015576736039764477)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00009)\n",
            "     | > loss_1: 0.13003  (0.12122)\n",
            "     | > grad_norm_1: 31.01758  (19.83172)\n",
            "     | > current_lr_0: 1.703812060702215e-11 \n",
            "     | > current_lr_1: 1.703812060702215e-11 \n",
            "     | > step_time: 2.42990  (2.42603)\n",
            "     | > loader_time: 0.00230  (0.00206)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 160/364 -- GLOBAL_STEP: 193300\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57180  (0.54768)\n",
            "     | > G_mse_fake_loss: 0.74711  (0.76227)\n",
            "     | > G_feat_match_loss: 0.11763  (0.11440)\n",
            "     | > G_gen_loss: 25.73096  (24.64541)\n",
            "     | > G_adv_loss: 1.92344  (1.90625)\n",
            "     | > loss_0: 27.65441  (26.55166)\n",
            "     | > grad_norm_0: 656.47607  (800.32770)\n",
            "     | > D_mse_gan_loss: 0.10838  (0.12119)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00009  (0.00011)\n",
            "     | > loss_1: 0.10838  (0.12119)\n",
            "     | > grad_norm_1: 20.59713  (19.91205)\n",
            "     | > current_lr_0: 1.6617240054981305e-11 \n",
            "     | > current_lr_1: 1.6617240054981305e-11 \n",
            "     | > step_time: 2.42530  (2.42584)\n",
            "     | > loader_time: 0.00210  (0.00207)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 185/364 -- GLOBAL_STEP: 193325\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52429  (0.54803)\n",
            "     | > G_mse_fake_loss: 0.77979  (0.76294)\n",
            "     | > G_feat_match_loss: 0.11388  (0.11448)\n",
            "     | > G_gen_loss: 23.59322  (24.66126)\n",
            "     | > G_adv_loss: 1.91857  (1.90771)\n",
            "     | > loss_0: 25.51180  (26.56897)\n",
            "     | > grad_norm_0: 979.76300  (801.84583)\n",
            "     | > D_mse_gan_loss: 0.12118  (0.12105)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00010)\n",
            "     | > loss_1: 0.12118  (0.12105)\n",
            "     | > grad_norm_1: 15.45721  (19.37783)\n",
            "     | > current_lr_0: 1.620675621529929e-11 \n",
            "     | > current_lr_1: 1.620675621529929e-11 \n",
            "     | > step_time: 2.43610  (2.42572)\n",
            "     | > loader_time: 0.00230  (0.00209)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 210/364 -- GLOBAL_STEP: 193350\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53076  (0.54793)\n",
            "     | > G_mse_fake_loss: 0.76348  (0.76267)\n",
            "     | > G_feat_match_loss: 0.11556  (0.11452)\n",
            "     | > G_gen_loss: 23.88416  (24.65688)\n",
            "     | > G_adv_loss: 1.91905  (1.90785)\n",
            "     | > loss_0: 25.80321  (26.56473)\n",
            "     | > grad_norm_0: 732.94916  (800.43951)\n",
            "     | > D_mse_gan_loss: 0.11996  (0.12099)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00010)\n",
            "     | > loss_1: 0.11996  (0.12099)\n",
            "     | > grad_norm_1: 7.62380  (19.04497)\n",
            "     | > current_lr_0: 1.5806412265399373e-11 \n",
            "     | > current_lr_1: 1.5806412265399373e-11 \n",
            "     | > step_time: 2.42730  (2.42562)\n",
            "     | > loader_time: 0.00190  (0.00210)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 235/364 -- GLOBAL_STEP: 193375\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58631  (0.54858)\n",
            "     | > G_mse_fake_loss: 0.72802  (0.76248)\n",
            "     | > G_feat_match_loss: 0.12030  (0.11466)\n",
            "     | > G_gen_loss: 26.38414  (24.68595)\n",
            "     | > G_adv_loss: 1.93099  (1.90908)\n",
            "     | > loss_0: 28.31512  (26.59504)\n",
            "     | > grad_norm_0: 673.60651  (801.21185)\n",
            "     | > D_mse_gan_loss: 0.10312  (0.12060)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00070  (0.00011)\n",
            "     | > loss_1: 0.10312  (0.12060)\n",
            "     | > grad_norm_1: 39.60473  (18.89659)\n",
            "     | > current_lr_0: 1.5415957726809912e-11 \n",
            "     | > current_lr_1: 1.5415957726809912e-11 \n",
            "     | > step_time: 2.42210  (2.42541)\n",
            "     | > loader_time: 0.00220  (0.00211)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 260/364 -- GLOBAL_STEP: 193400\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57859  (0.54852)\n",
            "     | > G_mse_fake_loss: 0.77267  (0.76297)\n",
            "     | > G_feat_match_loss: 0.12194  (0.11476)\n",
            "     | > G_gen_loss: 26.03639  (24.68347)\n",
            "     | > G_adv_loss: 1.99207  (1.91058)\n",
            "     | > loss_0: 28.02847  (26.59405)\n",
            "     | > grad_norm_0: 818.21191  (800.68042)\n",
            "     | > D_mse_gan_loss: 0.08908  (0.12022)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00011)\n",
            "     | > loss_1: 0.08908  (0.12022)\n",
            "     | > grad_norm_1: 21.80215  (18.87853)\n",
            "     | > current_lr_0: 1.503514830845048e-11 \n",
            "     | > current_lr_1: 1.503514830845048e-11 \n",
            "     | > step_time: 2.42210  (2.42527)\n",
            "     | > loader_time: 0.00220  (0.00211)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 285/364 -- GLOBAL_STEP: 193425\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53294  (0.54821)\n",
            "     | > G_mse_fake_loss: 0.77358  (0.76291)\n",
            "     | > G_feat_match_loss: 0.10900  (0.11465)\n",
            "     | > G_gen_loss: 23.98248  (24.66953)\n",
            "     | > G_adv_loss: 1.86361  (1.90939)\n",
            "     | > loss_0: 25.84608  (26.57891)\n",
            "     | > grad_norm_0: 779.91876  (800.43353)\n",
            "     | > D_mse_gan_loss: 0.14260  (0.12054)\n",
            "     | > D_mse_gan_real_loss: 0.00028  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00011)\n",
            "     | > loss_1: 0.14260  (0.12054)\n",
            "     | > grad_norm_1: 25.61251  (18.85545)\n",
            "     | > current_lr_0: 1.466374575378911e-11 \n",
            "     | > current_lr_1: 1.466374575378911e-11 \n",
            "     | > step_time: 2.42880  (2.42511)\n",
            "     | > loader_time: 0.00230  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 310/364 -- GLOBAL_STEP: 193450\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56463  (0.54870)\n",
            "     | > G_mse_fake_loss: 0.75884  (0.76282)\n",
            "     | > G_feat_match_loss: 0.12017  (0.11473)\n",
            "     | > G_gen_loss: 25.40837  (24.69134)\n",
            "     | > G_adv_loss: 1.96054  (1.91009)\n",
            "     | > loss_0: 27.36891  (26.60142)\n",
            "     | > grad_norm_0: 679.49023  (801.77759)\n",
            "     | > D_mse_gan_loss: 0.10648  (0.12035)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00011)\n",
            "     | > loss_1: 0.10648  (0.12035)\n",
            "     | > grad_norm_1: 19.94353  (18.79825)\n",
            "     | > current_lr_0: 1.4301517691775171e-11 \n",
            "     | > current_lr_1: 1.4301517691775171e-11 \n",
            "     | > step_time: 2.41990  (2.42511)\n",
            "     | > loader_time: 0.00200  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 335/364 -- GLOBAL_STEP: 193475\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54141  (0.54890)\n",
            "     | > G_mse_fake_loss: 0.76854  (0.76278)\n",
            "     | > G_feat_match_loss: 0.11714  (0.11477)\n",
            "     | > G_gen_loss: 24.36334  (24.70061)\n",
            "     | > G_adv_loss: 1.93989  (1.91052)\n",
            "     | > loss_0: 26.30323  (26.61113)\n",
            "     | > grad_norm_0: 706.30322  (799.84106)\n",
            "     | > D_mse_gan_loss: 0.10476  (0.12019)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00011)\n",
            "     | > loss_1: 0.10476  (0.12019)\n",
            "     | > grad_norm_1: 9.52663  (18.88542)\n",
            "     | > current_lr_0: 1.3948237491454515e-11 \n",
            "     | > current_lr_1: 1.3948237491454515e-11 \n",
            "     | > step_time: 2.42140  (2.42510)\n",
            "     | > loader_time: 0.00210  (0.00213)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 360/364 -- GLOBAL_STEP: 193500\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56845  (0.54875)\n",
            "     | > G_mse_fake_loss: 0.74283  (0.76260)\n",
            "     | > G_feat_match_loss: 0.11974  (0.11471)\n",
            "     | > G_gen_loss: 25.58043  (24.69375)\n",
            "     | > G_adv_loss: 1.94020  (1.90974)\n",
            "     | > loss_0: 27.52064  (26.60349)\n",
            "     | > grad_norm_0: 688.34955  (800.89905)\n",
            "     | > D_mse_gan_loss: 0.10201  (0.12031)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00011)\n",
            "     | > loss_1: 0.10201  (0.12031)\n",
            "     | > grad_norm_1: 30.66011  (19.04290)\n",
            "     | > current_lr_0: 1.3603684120175955e-11 \n",
            "     | > current_lr_1: 1.3603684120175955e-11 \n",
            "     | > step_time: 2.42370  (2.42504)\n",
            "     | > loader_time: 0.00200  (0.00213)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00114 \u001b[0m(-0.00028)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 0.58177 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70195 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 26.17972 \u001b[0m(+0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81714 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[91m 27.99685 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[91m 0.13729 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[92m 0.00057 \u001b[0m(-0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 37/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 17:05:57) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/364 -- GLOBAL_STEP: 193525\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51838  (0.56168)\n",
            "     | > G_mse_fake_loss: 0.75031  (0.75553)\n",
            "     | > G_feat_match_loss: 0.10755  (0.11605)\n",
            "     | > G_gen_loss: 23.32695  (25.27573)\n",
            "     | > G_adv_loss: 1.82576  (1.91608)\n",
            "     | > loss_0: 25.15271  (27.19181)\n",
            "     | > grad_norm_0: 771.26764  (785.30927)\n",
            "     | > D_mse_gan_loss: 0.13450  (0.11724)\n",
            "     | > D_mse_gan_real_loss: 0.00010  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00018  (0.00009)\n",
            "     | > loss_1: 0.13450  (0.11724)\n",
            "     | > grad_norm_1: 15.86459  (22.98916)\n",
            "     | > current_lr_0: 1.3267642005300375e-11 \n",
            "     | > current_lr_1: 1.3267642005300375e-11 \n",
            "     | > step_time: 2.42570  (2.43599)\n",
            "     | > loader_time: 0.00200  (0.00158)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 45/364 -- GLOBAL_STEP: 193550\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57850  (0.56235)\n",
            "     | > G_mse_fake_loss: 0.76778  (0.75445)\n",
            "     | > G_feat_match_loss: 0.12522  (0.11623)\n",
            "     | > G_gen_loss: 26.03251  (25.30571)\n",
            "     | > G_adv_loss: 2.01996  (1.91677)\n",
            "     | > loss_0: 28.05247  (27.22248)\n",
            "     | > grad_norm_0: 693.14301  (774.11383)\n",
            "     | > D_mse_gan_loss: 0.09101  (0.11681)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00012)\n",
            "     | > loss_1: 0.09101  (0.11681)\n",
            "     | > grad_norm_1: 30.67217  (19.79315)\n",
            "     | > current_lr_0: 1.293990089932595e-11 \n",
            "     | > current_lr_1: 1.293990089932595e-11 \n",
            "     | > step_time: 2.42500  (2.42919)\n",
            "     | > loader_time: 0.00200  (0.00187)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/364 -- GLOBAL_STEP: 193575\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58053  (0.56050)\n",
            "     | > G_mse_fake_loss: 0.75346  (0.75471)\n",
            "     | > G_feat_match_loss: 0.11975  (0.11619)\n",
            "     | > G_gen_loss: 26.12377  (25.22246)\n",
            "     | > G_adv_loss: 1.95091  (1.91663)\n",
            "     | > loss_0: 28.07468  (27.13909)\n",
            "     | > grad_norm_0: 760.84467  (781.01794)\n",
            "     | > D_mse_gan_loss: 0.10811  (0.11709)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00036  (0.00013)\n",
            "     | > loss_1: 0.10811  (0.11709)\n",
            "     | > grad_norm_1: 21.32255  (20.61798)\n",
            "     | > current_lr_0: 1.2620255748345065e-11 \n",
            "     | > current_lr_1: 1.2620255748345065e-11 \n",
            "     | > step_time: 2.41670  (2.42742)\n",
            "     | > loader_time: 0.00200  (0.00194)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 95/364 -- GLOBAL_STEP: 193600\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52853  (0.56038)\n",
            "     | > G_mse_fake_loss: 0.77082  (0.75518)\n",
            "     | > G_feat_match_loss: 0.11249  (0.11632)\n",
            "     | > G_gen_loss: 23.78397  (25.21706)\n",
            "     | > G_adv_loss: 1.89577  (1.91839)\n",
            "     | > loss_0: 25.67974  (27.13545)\n",
            "     | > grad_norm_0: 920.44635  (785.09467)\n",
            "     | > D_mse_gan_loss: 0.12644  (0.11639)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00013)\n",
            "     | > loss_1: 0.12644  (0.11639)\n",
            "     | > grad_norm_1: 11.32045  (20.65083)\n",
            "     | > current_lr_0: 1.2308506563750674e-11 \n",
            "     | > current_lr_1: 1.2308506563750674e-11 \n",
            "     | > step_time: 2.41560  (2.42673)\n",
            "     | > loader_time: 0.00200  (0.00199)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/364 -- GLOBAL_STEP: 193625\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54678  (0.55961)\n",
            "     | > G_mse_fake_loss: 0.76135  (0.75537)\n",
            "     | > G_feat_match_loss: 0.11300  (0.11622)\n",
            "     | > G_gen_loss: 24.60530  (25.18231)\n",
            "     | > G_adv_loss: 1.89131  (1.91758)\n",
            "     | > loss_0: 26.49661  (27.09989)\n",
            "     | > grad_norm_0: 714.81305  (785.89008)\n",
            "     | > D_mse_gan_loss: 0.12371  (0.11645)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00013)\n",
            "     | > loss_1: 0.12371  (0.11645)\n",
            "     | > grad_norm_1: 10.19203  (20.46426)\n",
            "     | > current_lr_0: 1.2004458297111769e-11 \n",
            "     | > current_lr_1: 1.2004458297111769e-11 \n",
            "     | > step_time: 2.41870  (2.42604)\n",
            "     | > loader_time: 0.00240  (0.00203)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 145/364 -- GLOBAL_STEP: 193650\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59431  (0.55958)\n",
            "     | > G_mse_fake_loss: 0.74734  (0.75630)\n",
            "     | > G_feat_match_loss: 0.12433  (0.11645)\n",
            "     | > G_gen_loss: 26.74403  (25.18114)\n",
            "     | > G_adv_loss: 1.99066  (1.92076)\n",
            "     | > loss_0: 28.73469  (27.10189)\n",
            "     | > grad_norm_0: 653.62842  (780.76697)\n",
            "     | > D_mse_gan_loss: 0.10010  (0.11577)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00014)\n",
            "     | > loss_1: 0.10010  (0.11577)\n",
            "     | > grad_norm_1: 36.88385  (20.71107)\n",
            "     | > current_lr_0: 1.1707920718139742e-11 \n",
            "     | > current_lr_1: 1.1707920718139742e-11 \n",
            "     | > step_time: 2.42220  (2.42585)\n",
            "     | > loader_time: 0.00200  (0.00206)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 170/364 -- GLOBAL_STEP: 193675\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58814  (0.55991)\n",
            "     | > G_mse_fake_loss: 0.73988  (0.75639)\n",
            "     | > G_feat_match_loss: 0.11944  (0.11664)\n",
            "     | > G_gen_loss: 26.46642  (25.19604)\n",
            "     | > G_adv_loss: 1.93429  (1.92275)\n",
            "     | > loss_0: 28.40071  (27.11880)\n",
            "     | > grad_norm_0: 662.19458  (778.76978)\n",
            "     | > D_mse_gan_loss: 0.12101  (0.11545)\n",
            "     | > D_mse_gan_real_loss: 0.00033  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00014)\n",
            "     | > loss_1: 0.12101  (0.11545)\n",
            "     | > grad_norm_1: 25.60463  (20.75491)\n",
            "     | > current_lr_0: 1.141870829566926e-11 \n",
            "     | > current_lr_1: 1.141870829566926e-11 \n",
            "     | > step_time: 2.42330  (2.42574)\n",
            "     | > loader_time: 0.00200  (0.00207)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 195/364 -- GLOBAL_STEP: 193700\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51622  (0.56022)\n",
            "     | > G_mse_fake_loss: 0.76726  (0.75678)\n",
            "     | > G_feat_match_loss: 0.11044  (0.11679)\n",
            "     | > G_gen_loss: 23.22980  (25.20969)\n",
            "     | > G_adv_loss: 1.87164  (1.92471)\n",
            "     | > loss_0: 25.10143  (27.13439)\n",
            "     | > grad_norm_0: 956.90326  (775.91364)\n",
            "     | > D_mse_gan_loss: 0.12618  (0.11517)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00013  (0.00013)\n",
            "     | > loss_1: 0.12618  (0.11517)\n",
            "     | > grad_norm_1: 18.14130  (21.13535)\n",
            "     | > current_lr_0: 1.1136640081579147e-11 \n",
            "     | > current_lr_1: 1.1136640081579147e-11 \n",
            "     | > step_time: 2.42270  (2.42548)\n",
            "     | > loader_time: 0.00230  (0.00209)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 220/364 -- GLOBAL_STEP: 193725\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53781  (0.56028)\n",
            "     | > G_mse_fake_loss: 0.76801  (0.75681)\n",
            "     | > G_feat_match_loss: 0.11444  (0.11677)\n",
            "     | > G_gen_loss: 24.20166  (25.21267)\n",
            "     | > G_adv_loss: 1.91239  (1.92447)\n",
            "     | > loss_0: 26.11404  (27.13714)\n",
            "     | > grad_norm_0: 761.41956  (776.21973)\n",
            "     | > D_mse_gan_loss: 0.12128  (0.11548)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00015)\n",
            "     | > loss_1: 0.12128  (0.11548)\n",
            "     | > grad_norm_1: 5.75073  (21.51631)\n",
            "     | > current_lr_0: 1.0861539597580725e-11 \n",
            "     | > current_lr_1: 1.0861539597580725e-11 \n",
            "     | > step_time: 2.41890  (2.42514)\n",
            "     | > loader_time: 0.00240  (0.00210)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 245/364 -- GLOBAL_STEP: 193750\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59833  (0.56037)\n",
            "     | > G_mse_fake_loss: 0.73878  (0.75653)\n",
            "     | > G_feat_match_loss: 0.12551  (0.11687)\n",
            "     | > G_gen_loss: 26.92472  (25.21666)\n",
            "     | > G_adv_loss: 1.99392  (1.92522)\n",
            "     | > loss_0: 28.91863  (27.14188)\n",
            "     | > grad_norm_0: 661.70624  (773.07611)\n",
            "     | > D_mse_gan_loss: 0.09979  (0.11518)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00015  (0.00016)\n",
            "     | > loss_1: 0.09979  (0.11518)\n",
            "     | > grad_norm_1: 44.11061  (21.74862)\n",
            "     | > current_lr_0: 1.0593234724802727e-11 \n",
            "     | > current_lr_1: 1.0593234724802727e-11 \n",
            "     | > step_time: 2.42800  (2.42516)\n",
            "     | > loader_time: 0.00210  (0.00210)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 270/364 -- GLOBAL_STEP: 193775\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55987  (0.56042)\n",
            "     | > G_mse_fake_loss: 0.76437  (0.75686)\n",
            "     | > G_feat_match_loss: 0.11344  (0.11683)\n",
            "     | > G_gen_loss: 25.19414  (25.21881)\n",
            "     | > G_adv_loss: 1.89876  (1.92521)\n",
            "     | > loss_0: 27.09289  (27.14402)\n",
            "     | > grad_norm_0: 938.34265  (777.13300)\n",
            "     | > D_mse_gan_loss: 0.12288  (0.11527)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00015)\n",
            "     | > loss_1: 0.12288  (0.11527)\n",
            "     | > grad_norm_1: 7.15270  (21.46881)\n",
            "     | > current_lr_0: 1.0331557596103702e-11 \n",
            "     | > current_lr_1: 1.0331557596103702e-11 \n",
            "     | > step_time: 2.42400  (2.42512)\n",
            "     | > loader_time: 0.00240  (0.00211)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 295/364 -- GLOBAL_STEP: 193800\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54730  (0.56035)\n",
            "     | > G_mse_fake_loss: 0.77002  (0.75681)\n",
            "     | > G_feat_match_loss: 0.11244  (0.11680)\n",
            "     | > G_gen_loss: 24.62844  (25.21594)\n",
            "     | > G_adv_loss: 1.89441  (1.92478)\n",
            "     | > loss_0: 26.52285  (27.14072)\n",
            "     | > grad_norm_0: 991.41217  (777.16669)\n",
            "     | > D_mse_gan_loss: 0.12595  (0.11545)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00023  (0.00015)\n",
            "     | > loss_1: 0.12595  (0.11545)\n",
            "     | > grad_norm_1: 13.68169  (21.36912)\n",
            "     | > current_lr_0: 1.0076344491044583e-11 \n",
            "     | > current_lr_1: 1.0076344491044583e-11 \n",
            "     | > step_time: 2.41840  (2.42503)\n",
            "     | > loader_time: 0.00210  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 320/364 -- GLOBAL_STEP: 193825\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54417  (0.56074)\n",
            "     | > G_mse_fake_loss: 0.76708  (0.75665)\n",
            "     | > G_feat_match_loss: 0.11800  (0.11690)\n",
            "     | > G_gen_loss: 24.48781  (25.23327)\n",
            "     | > G_adv_loss: 1.94707  (1.92567)\n",
            "     | > loss_0: 26.43488  (27.15894)\n",
            "     | > grad_norm_0: 746.16064  (775.90973)\n",
            "     | > D_mse_gan_loss: 0.11254  (0.11523)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00013  (0.00015)\n",
            "     | > loss_1: 0.11254  (0.11523)\n",
            "     | > grad_norm_1: 8.30483  (21.26156)\n",
            "     | > current_lr_0: 9.827435733455636e-12 \n",
            "     | > current_lr_1: 9.827435733455636e-12 \n",
            "     | > step_time: 2.42780  (2.42502)\n",
            "     | > loader_time: 0.00210  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 345/364 -- GLOBAL_STEP: 193850\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60153  (0.56051)\n",
            "     | > G_mse_fake_loss: 0.74359  (0.75667)\n",
            "     | > G_feat_match_loss: 0.12646  (0.11685)\n",
            "     | > G_gen_loss: 27.06893  (25.22317)\n",
            "     | > G_adv_loss: 2.00816  (1.92520)\n",
            "     | > loss_0: 29.07709  (27.14837)\n",
            "     | > grad_norm_0: 676.01056  (777.93951)\n",
            "     | > D_mse_gan_loss: 0.10115  (0.11535)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00015)\n",
            "     | > loss_1: 0.10115  (0.11535)\n",
            "     | > grad_norm_1: 47.47997  (21.14855)\n",
            "     | > current_lr_0: 9.584675591533763e-12 \n",
            "     | > current_lr_1: 9.584675591533763e-12 \n",
            "     | > step_time: 2.42470  (2.42499)\n",
            "     | > loader_time: 0.00220  (0.00213)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00108 \u001b[0m(-0.00005)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.58177 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70195 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 26.17971 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81714 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 27.99685 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.27955 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.13729 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00057 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.27955 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 38/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 17:20:55) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/364 -- GLOBAL_STEP: 193875\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52538  (0.53518)\n",
            "     | > G_mse_fake_loss: 0.78031  (0.77359)\n",
            "     | > G_feat_match_loss: 0.10928  (0.11222)\n",
            "     | > G_gen_loss: 23.64219  (24.08328)\n",
            "     | > G_adv_loss: 1.87315  (1.89578)\n",
            "     | > loss_0: 25.51534  (25.97906)\n",
            "     | > grad_norm_0: 803.11957  (823.61224)\n",
            "     | > D_mse_gan_loss: 0.13999  (0.12855)\n",
            "     | > D_mse_gan_real_loss: 0.00017  (0.00006)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00006)\n",
            "     | > loss_1: 0.13999  (0.12855)\n",
            "     | > grad_norm_1: 32.22413  (18.43827)\n",
            "     | > current_lr_0: 9.347912180407623e-12 \n",
            "     | > current_lr_1: 9.347912180407623e-12 \n",
            "     | > step_time: 2.41850  (2.47262)\n",
            "     | > loader_time: 0.00090  (0.00443)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/364 -- GLOBAL_STEP: 193900\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53377  (0.53735)\n",
            "     | > G_mse_fake_loss: 0.75814  (0.76449)\n",
            "     | > G_feat_match_loss: 0.11015  (0.11114)\n",
            "     | > G_gen_loss: 24.01967  (24.18054)\n",
            "     | > G_adv_loss: 1.85967  (1.87586)\n",
            "     | > loss_0: 25.87934  (26.05640)\n",
            "     | > grad_norm_0: 741.24420  (868.22144)\n",
            "     | > D_mse_gan_loss: 0.12557  (0.12954)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00030)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00018)\n",
            "     | > loss_1: 0.12557  (0.12954)\n",
            "     | > grad_norm_1: 11.09654  (16.81234)\n",
            "     | > current_lr_0: 9.116997367109627e-12 \n",
            "     | > current_lr_1: 9.116997367109627e-12 \n",
            "     | > step_time: 2.43160  (2.43231)\n",
            "     | > loader_time: 0.00200  (0.00240)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/364 -- GLOBAL_STEP: 193925\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51482  (0.53858)\n",
            "     | > G_mse_fake_loss: 0.77121  (0.76380)\n",
            "     | > G_feat_match_loss: 0.10977  (0.11137)\n",
            "     | > G_gen_loss: 23.16707  (24.23631)\n",
            "     | > G_adv_loss: 1.86886  (1.87749)\n",
            "     | > loss_0: 25.03593  (26.11380)\n",
            "     | > grad_norm_0: 816.53070  (892.73157)\n",
            "     | > D_mse_gan_loss: 0.13213  (0.12955)\n",
            "     | > D_mse_gan_real_loss: 0.00006  (0.00028)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00020)\n",
            "     | > loss_1: 0.13213  (0.12955)\n",
            "     | > grad_norm_1: 20.18315  (17.78168)\n",
            "     | > current_lr_0: 8.891786677895314e-12 \n",
            "     | > current_lr_1: 8.891786677895314e-12 \n",
            "     | > step_time: 2.41860  (2.42881)\n",
            "     | > loader_time: 0.00240  (0.00230)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/364 -- GLOBAL_STEP: 193950\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58428  (0.54063)\n",
            "     | > G_mse_fake_loss: 0.72862  (0.76325)\n",
            "     | > G_feat_match_loss: 0.10667  (0.11141)\n",
            "     | > G_gen_loss: 26.29278  (24.32819)\n",
            "     | > G_adv_loss: 1.79535  (1.87732)\n",
            "     | > loss_0: 28.08814  (26.20552)\n",
            "     | > grad_norm_0: 1646.21155  (899.25482)\n",
            "     | > D_mse_gan_loss: 0.13985  (0.12923)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00025)\n",
            "     | > D_mse_gan_fake_loss: 0.00390  (0.00024)\n",
            "     | > loss_1: 0.13985  (0.12923)\n",
            "     | > grad_norm_1: 12.93541  (17.74295)\n",
            "     | > current_lr_0: 8.672139207852185e-12 \n",
            "     | > current_lr_1: 8.672139207852185e-12 \n",
            "     | > step_time: 2.42210  (2.42701)\n",
            "     | > loader_time: 0.00200  (0.00227)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/364 -- GLOBAL_STEP: 193975\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51575  (0.53940)\n",
            "     | > G_mse_fake_loss: 0.76748  (0.76346)\n",
            "     | > G_feat_match_loss: 0.10900  (0.11146)\n",
            "     | > G_gen_loss: 23.20871  (24.27286)\n",
            "     | > G_adv_loss: 1.85751  (1.87807)\n",
            "     | > loss_0: 25.06621  (26.15093)\n",
            "     | > grad_norm_0: 904.67432  (892.94983)\n",
            "     | > D_mse_gan_loss: 0.13386  (0.12888)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00024)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00021)\n",
            "     | > loss_1: 0.13386  (0.12888)\n",
            "     | > grad_norm_1: 20.03316  (17.70261)\n",
            "     | > current_lr_0: 8.4579175327414e-12 \n",
            "     | > current_lr_1: 8.4579175327414e-12 \n",
            "     | > step_time: 2.42350  (2.42649)\n",
            "     | > loader_time: 0.00230  (0.00223)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/364 -- GLOBAL_STEP: 194000\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52723  (0.53933)\n",
            "     | > G_mse_fake_loss: 0.76734  (0.76302)\n",
            "     | > G_feat_match_loss: 0.10522  (0.11144)\n",
            "     | > G_gen_loss: 23.72523  (24.26982)\n",
            "     | > G_adv_loss: 1.81952  (1.87742)\n",
            "     | > loss_0: 25.54475  (26.14724)\n",
            "     | > grad_norm_0: 895.10217  (893.55389)\n",
            "     | > D_mse_gan_loss: 0.14697  (0.12894)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00021)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00020)\n",
            "     | > loss_1: 0.14697  (0.12894)\n",
            "     | > grad_norm_1: 34.13883  (18.26050)\n",
            "     | > current_lr_0: 8.248987623017143e-12 \n",
            "     | > current_lr_1: 8.248987623017143e-12 \n",
            "     | > step_time: 2.42400  (2.42617)\n",
            "     | > loader_time: 0.00200  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 155/364 -- GLOBAL_STEP: 194025\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55692  (0.53981)\n",
            "     | > G_mse_fake_loss: 0.76229  (0.76240)\n",
            "     | > G_feat_match_loss: 0.11753  (0.11131)\n",
            "     | > G_gen_loss: 25.06158  (24.29139)\n",
            "     | > G_adv_loss: 1.93754  (1.87555)\n",
            "     | > loss_0: 26.99912  (26.16694)\n",
            "     | > grad_norm_0: 679.00909  (894.35803)\n",
            "     | > D_mse_gan_loss: 0.11729  (0.12932)\n",
            "     | > D_mse_gan_real_loss: 0.00014  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00020)\n",
            "     | > loss_1: 0.11729  (0.12932)\n",
            "     | > grad_norm_1: 13.95721  (18.05901)\n",
            "     | > current_lr_0: 8.04521875996997e-12 \n",
            "     | > current_lr_1: 8.04521875996997e-12 \n",
            "     | > step_time: 2.41590  (2.42567)\n",
            "     | > loader_time: 0.00200  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 180/364 -- GLOBAL_STEP: 194050\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54733  (0.54076)\n",
            "     | > G_mse_fake_loss: 0.77323  (0.76284)\n",
            "     | > G_feat_match_loss: 0.11577  (0.11160)\n",
            "     | > G_gen_loss: 24.62989  (24.33414)\n",
            "     | > G_adv_loss: 1.93091  (1.87887)\n",
            "     | > loss_0: 26.56081  (26.21301)\n",
            "     | > grad_norm_0: 765.67249  (894.21503)\n",
            "     | > D_mse_gan_loss: 0.12371  (0.12865)\n",
            "     | > D_mse_gan_real_loss: 0.00011  (0.00021)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00019)\n",
            "     | > loss_1: 0.12371  (0.12865)\n",
            "     | > grad_norm_1: 11.73371  (17.65087)\n",
            "     | > current_lr_0: 7.846483453941559e-12 \n",
            "     | > current_lr_1: 7.846483453941559e-12 \n",
            "     | > step_time: 2.42980  (2.42536)\n",
            "     | > loader_time: 0.00210  (0.00220)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 205/364 -- GLOBAL_STEP: 194075\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53595  (0.54042)\n",
            "     | > G_mse_fake_loss: 0.76107  (0.76287)\n",
            "     | > G_feat_match_loss: 0.11310  (0.11157)\n",
            "     | > G_gen_loss: 24.11756  (24.31893)\n",
            "     | > G_adv_loss: 1.89206  (1.87856)\n",
            "     | > loss_0: 26.00962  (26.19749)\n",
            "     | > grad_norm_0: 870.97656  (893.47516)\n",
            "     | > D_mse_gan_loss: 0.13405  (0.12858)\n",
            "     | > D_mse_gan_real_loss: 0.00103  (0.00021)\n",
            "     | > D_mse_gan_fake_loss: 0.00016  (0.00018)\n",
            "     | > loss_1: 0.13405  (0.12858)\n",
            "     | > grad_norm_1: 8.68506  (17.42367)\n",
            "     | > current_lr_0: 7.652657364559775e-12 \n",
            "     | > current_lr_1: 7.652657364559775e-12 \n",
            "     | > step_time: 2.41790  (2.42505)\n",
            "     | > loader_time: 0.00220  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/364 -- GLOBAL_STEP: 194100\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54712  (0.54090)\n",
            "     | > G_mse_fake_loss: 0.76735  (0.76331)\n",
            "     | > G_feat_match_loss: 0.11909  (0.11180)\n",
            "     | > G_gen_loss: 24.62041  (24.34028)\n",
            "     | > G_adv_loss: 1.95821  (1.88127)\n",
            "     | > loss_0: 26.57863  (26.22154)\n",
            "     | > grad_norm_0: 758.82544  (892.18915)\n",
            "     | > D_mse_gan_loss: 0.10404  (0.12796)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00019)\n",
            "     | > loss_1: 0.10404  (0.12796)\n",
            "     | > grad_norm_1: 12.19623  (17.07376)\n",
            "     | > current_lr_0: 7.463619222944095e-12 \n",
            "     | > current_lr_1: 7.463619222944095e-12 \n",
            "     | > step_time: 2.42540  (2.42507)\n",
            "     | > loader_time: 0.00200  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 255/364 -- GLOBAL_STEP: 194125\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53384  (0.54099)\n",
            "     | > G_mse_fake_loss: 0.76873  (0.76290)\n",
            "     | > G_feat_match_loss: 0.11178  (0.11179)\n",
            "     | > G_gen_loss: 24.02278  (24.34470)\n",
            "     | > G_adv_loss: 1.88648  (1.88076)\n",
            "     | > loss_0: 25.90926  (26.22546)\n",
            "     | > grad_norm_0: 846.95862  (892.34387)\n",
            "     | > D_mse_gan_loss: 0.13443  (0.12803)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00019  (0.00019)\n",
            "     | > loss_1: 0.13443  (0.12803)\n",
            "     | > grad_norm_1: 18.67399  (16.94132)\n",
            "     | > current_lr_0: 7.279250755832724e-12 \n",
            "     | > current_lr_1: 7.279250755832724e-12 \n",
            "     | > step_time: 2.42680  (2.42495)\n",
            "     | > loader_time: 0.00210  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 280/364 -- GLOBAL_STEP: 194150\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59477  (0.54105)\n",
            "     | > G_mse_fake_loss: 0.73864  (0.76308)\n",
            "     | > G_feat_match_loss: 0.10799  (0.11176)\n",
            "     | > G_gen_loss: 26.76449  (24.34727)\n",
            "     | > G_adv_loss: 1.81849  (1.88065)\n",
            "     | > loss_0: 28.58298  (26.22792)\n",
            "     | > grad_norm_0: 1154.50952  (892.57678)\n",
            "     | > D_mse_gan_loss: 0.13953  (0.12800)\n",
            "     | > D_mse_gan_real_loss: 0.00009  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00019)\n",
            "     | > loss_1: 0.13953  (0.12800)\n",
            "     | > grad_norm_1: 16.00295  (17.21199)\n",
            "     | > current_lr_0: 7.099436611583978e-12 \n",
            "     | > current_lr_1: 7.099436611583978e-12 \n",
            "     | > step_time: 2.42440  (2.42481)\n",
            "     | > loader_time: 0.00220  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 305/364 -- GLOBAL_STEP: 194175\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55325  (0.54135)\n",
            "     | > G_mse_fake_loss: 0.76652  (0.76312)\n",
            "     | > G_feat_match_loss: 0.11306  (0.11185)\n",
            "     | > G_gen_loss: 24.89625  (24.36091)\n",
            "     | > G_adv_loss: 1.89714  (1.88159)\n",
            "     | > loss_0: 26.79339  (26.24250)\n",
            "     | > grad_norm_0: 884.17975  (890.73718)\n",
            "     | > D_mse_gan_loss: 0.12404  (0.12782)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00021)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00018)\n",
            "     | > loss_1: 0.12404  (0.12782)\n",
            "     | > grad_norm_1: 8.45621  (17.10515)\n",
            "     | > current_lr_0: 6.924064288005582e-12 \n",
            "     | > current_lr_1: 6.924064288005582e-12 \n",
            "     | > step_time: 2.42940  (2.42478)\n",
            "     | > loader_time: 0.00220  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 330/364 -- GLOBAL_STEP: 194200\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53806  (0.54130)\n",
            "     | > G_mse_fake_loss: 0.76886  (0.76291)\n",
            "     | > G_feat_match_loss: 0.11043  (0.11184)\n",
            "     | > G_gen_loss: 24.21268  (24.35832)\n",
            "     | > G_adv_loss: 1.87321  (1.88131)\n",
            "     | > loss_0: 26.08590  (26.23963)\n",
            "     | > grad_norm_0: 899.51898  (891.65393)\n",
            "     | > D_mse_gan_loss: 0.13161  (0.12773)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00021)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00018)\n",
            "     | > loss_1: 0.13161  (0.12773)\n",
            "     | > grad_norm_1: 17.57762  (17.08303)\n",
            "     | > current_lr_0: 6.753024061966743e-12 \n",
            "     | > current_lr_1: 6.753024061966743e-12 \n",
            "     | > step_time: 2.42350  (2.42483)\n",
            "     | > loader_time: 0.00200  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 355/364 -- GLOBAL_STEP: 194225\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52728  (0.54131)\n",
            "     | > G_mse_fake_loss: 0.76510  (0.76290)\n",
            "     | > G_feat_match_loss: 0.10981  (0.11186)\n",
            "     | > G_gen_loss: 23.72755  (24.35880)\n",
            "     | > G_adv_loss: 1.86317  (1.88155)\n",
            "     | > loss_0: 25.59072  (26.24035)\n",
            "     | > grad_norm_0: 795.50305  (891.11200)\n",
            "     | > D_mse_gan_loss: 0.13453  (0.12765)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00018)\n",
            "     | > loss_1: 0.13453  (0.12765)\n",
            "     | > grad_norm_1: 19.07899  (17.06942)\n",
            "     | > current_lr_0: 6.586208920748979e-12 \n",
            "     | > current_lr_1: 6.586208920748979e-12 \n",
            "     | > step_time: 2.42040  (2.42479)\n",
            "     | > loader_time: 0.00220  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00118 \u001b[0m(+0.00010)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.58177 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70195 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 26.17971 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81714 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[91m 27.99685 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss: 0.13729 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00057 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 39/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 17:35:53) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/364 -- GLOBAL_STEP: 194250\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57501  (0.55247)\n",
            "     | > G_mse_fake_loss: 0.77234  (0.76385)\n",
            "     | > G_feat_match_loss: 0.12017  (0.11728)\n",
            "     | > G_gen_loss: 25.87545  (24.86093)\n",
            "     | > G_adv_loss: 1.97405  (1.93661)\n",
            "     | > loss_0: 27.84950  (26.79754)\n",
            "     | > grad_norm_0: 763.88629  (787.32495)\n",
            "     | > D_mse_gan_loss: 0.10442  (0.11343)\n",
            "     | > D_mse_gan_real_loss: 0.00017  (0.00004)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00010)\n",
            "     | > loss_1: 0.10442  (0.11343)\n",
            "     | > grad_norm_1: 13.55230  (16.54642)\n",
            "     | > current_lr_0: 6.4235144950927415e-12 \n",
            "     | > current_lr_1: 6.4235144950927415e-12 \n",
            "     | > step_time: 2.42310  (2.43265)\n",
            "     | > loader_time: 0.00240  (0.00152)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 40/364 -- GLOBAL_STEP: 194275\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61609  (0.55794)\n",
            "     | > G_mse_fake_loss: 0.73620  (0.75839)\n",
            "     | > G_feat_match_loss: 0.12617  (0.11711)\n",
            "     | > G_gen_loss: 27.72400  (25.10749)\n",
            "     | > G_adv_loss: 1.99789  (1.92950)\n",
            "     | > loss_0: 29.72189  (27.03699)\n",
            "     | > grad_norm_0: 688.74988  (764.97052)\n",
            "     | > D_mse_gan_loss: 0.09926  (0.11474)\n",
            "     | > D_mse_gan_real_loss: 0.00033  (0.00008)\n",
            "     | > D_mse_gan_fake_loss: 0.00010  (0.00011)\n",
            "     | > loss_1: 0.09926  (0.11474)\n",
            "     | > grad_norm_1: 46.73275  (19.61475)\n",
            "     | > current_lr_0: 6.264838993897924e-12 \n",
            "     | > current_lr_1: 6.264838993897924e-12 \n",
            "     | > step_time: 2.42060  (2.42839)\n",
            "     | > loader_time: 0.00230  (0.00198)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 65/364 -- GLOBAL_STEP: 194300\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52184  (0.55821)\n",
            "     | > G_mse_fake_loss: 0.78610  (0.75828)\n",
            "     | > G_feat_match_loss: 0.10947  (0.11707)\n",
            "     | > G_gen_loss: 23.48269  (25.11923)\n",
            "     | > G_adv_loss: 1.88080  (1.92896)\n",
            "     | > loss_0: 25.36349  (27.04819)\n",
            "     | > grad_norm_0: 808.94165  (762.54596)\n",
            "     | > D_mse_gan_loss: 0.14316  (0.11518)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00006)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00012)\n",
            "     | > loss_1: 0.14316  (0.11518)\n",
            "     | > grad_norm_1: 39.18750  (21.27793)\n",
            "     | > current_lr_0: 6.110083140537427e-12 \n",
            "     | > current_lr_1: 6.110083140537427e-12 \n",
            "     | > step_time: 2.42840  (2.42729)\n",
            "     | > loader_time: 0.00230  (0.00202)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 90/364 -- GLOBAL_STEP: 194325\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52876  (0.55854)\n",
            "     | > G_mse_fake_loss: 0.76247  (0.75754)\n",
            "     | > G_feat_match_loss: 0.11189  (0.11701)\n",
            "     | > G_gen_loss: 23.79440  (25.13419)\n",
            "     | > G_adv_loss: 1.88140  (1.92761)\n",
            "     | > loss_0: 25.67580  (27.06180)\n",
            "     | > grad_norm_0: 948.58875  (763.01227)\n",
            "     | > D_mse_gan_loss: 0.12527  (0.11479)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00011  (0.00014)\n",
            "     | > loss_1: 0.12527  (0.11479)\n",
            "     | > grad_norm_1: 9.33678  (21.26590)\n",
            "     | > current_lr_0: 5.9591501107439325e-12 \n",
            "     | > current_lr_1: 5.9591501107439325e-12 \n",
            "     | > step_time: 2.42700  (2.42662)\n",
            "     | > loader_time: 0.00210  (0.00209)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 115/364 -- GLOBAL_STEP: 194350\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53658  (0.55800)\n",
            "     | > G_mse_fake_loss: 0.74307  (0.75758)\n",
            "     | > G_feat_match_loss: 0.10887  (0.11704)\n",
            "     | > G_gen_loss: 24.14603  (25.10993)\n",
            "     | > G_adv_loss: 1.83175  (1.92803)\n",
            "     | > loss_0: 25.97779  (27.03796)\n",
            "     | > grad_norm_0: 797.03137  (764.69849)\n",
            "     | > D_mse_gan_loss: 0.13825  (0.11412)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00071  (0.00013)\n",
            "     | > loss_1: 0.13825  (0.11412)\n",
            "     | > grad_norm_1: 10.07553  (20.97655)\n",
            "     | > current_lr_0: 5.8119454720309934e-12 \n",
            "     | > current_lr_1: 5.8119454720309934e-12 \n",
            "     | > step_time: 2.42420  (2.42627)\n",
            "     | > loader_time: 0.00200  (0.00209)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 140/364 -- GLOBAL_STEP: 194375\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51045  (0.55770)\n",
            "     | > G_mse_fake_loss: 0.75444  (0.75781)\n",
            "     | > G_feat_match_loss: 0.10952  (0.11689)\n",
            "     | > G_gen_loss: 22.97025  (25.09657)\n",
            "     | > G_adv_loss: 1.84963  (1.92672)\n",
            "     | > loss_0: 24.81988  (27.02328)\n",
            "     | > grad_norm_0: 908.62018  (764.58752)\n",
            "     | > D_mse_gan_loss: 0.13624  (0.11438)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00010)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00012)\n",
            "     | > loss_1: 0.13624  (0.11438)\n",
            "     | > grad_norm_1: 13.82271  (20.66440)\n",
            "     | > current_lr_0: 5.66837712461059e-12 \n",
            "     | > current_lr_1: 5.66837712461059e-12 \n",
            "     | > step_time: 2.42330  (2.42574)\n",
            "     | > loader_time: 0.00200  (0.00211)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 165/364 -- GLOBAL_STEP: 194400\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58096  (0.55841)\n",
            "     | > G_mse_fake_loss: 0.74648  (0.75745)\n",
            "     | > G_feat_match_loss: 0.12204  (0.11690)\n",
            "     | > G_gen_loss: 26.14328  (25.12827)\n",
            "     | > G_adv_loss: 1.96687  (1.92648)\n",
            "     | > loss_0: 28.11014  (27.05475)\n",
            "     | > grad_norm_0: 688.09033  (764.76770)\n",
            "     | > D_mse_gan_loss: 0.10186  (0.11433)\n",
            "     | > D_mse_gan_real_loss: 0.00009  (0.00009)\n",
            "     | > D_mse_gan_fake_loss: 0.00012  (0.00013)\n",
            "     | > loss_1: 0.10186  (0.11433)\n",
            "     | > grad_norm_1: 33.68161  (20.47480)\n",
            "     | > current_lr_0: 5.5283552437701334e-12 \n",
            "     | > current_lr_1: 5.5283552437701334e-12 \n",
            "     | > step_time: 2.42690  (2.42566)\n",
            "     | > loader_time: 0.00240  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 190/364 -- GLOBAL_STEP: 194425\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54777  (0.55837)\n",
            "     | > G_mse_fake_loss: 0.75924  (0.75746)\n",
            "     | > G_feat_match_loss: 0.11827  (0.11691)\n",
            "     | > G_gen_loss: 24.64978  (25.12660)\n",
            "     | > G_adv_loss: 1.94196  (1.92653)\n",
            "     | > loss_0: 26.59173  (27.05313)\n",
            "     | > grad_norm_0: 713.28192  (764.78802)\n",
            "     | > D_mse_gan_loss: 0.10646  (0.11427)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00009)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00013)\n",
            "     | > loss_1: 0.10646  (0.11427)\n",
            "     | > grad_norm_1: 13.53011  (20.32655)\n",
            "     | > current_lr_0: 5.3917922236729e-12 \n",
            "     | > current_lr_1: 5.3917922236729e-12 \n",
            "     | > step_time: 2.42780  (2.42572)\n",
            "     | > loader_time: 0.00210  (0.00213)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 215/364 -- GLOBAL_STEP: 194450\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53575  (0.55864)\n",
            "     | > G_mse_fake_loss: 0.75951  (0.75758)\n",
            "     | > G_feat_match_loss: 0.10806  (0.11689)\n",
            "     | > G_gen_loss: 24.10891  (25.13882)\n",
            "     | > G_adv_loss: 1.84012  (1.92644)\n",
            "     | > loss_0: 25.94903  (27.06526)\n",
            "     | > grad_norm_0: 846.18982  (767.90234)\n",
            "     | > D_mse_gan_loss: 0.13354  (0.11442)\n",
            "     | > D_mse_gan_real_loss: 0.00028  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00014)\n",
            "     | > loss_1: 0.13354  (0.11442)\n",
            "     | > grad_norm_1: 16.07906  (20.65210)\n",
            "     | > current_lr_0: 5.258602622546722e-12 \n",
            "     | > current_lr_1: 5.258602622546722e-12 \n",
            "     | > step_time: 2.42550  (2.42569)\n",
            "     | > loader_time: 0.00190  (0.00213)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 240/364 -- GLOBAL_STEP: 194475\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61101  (0.55901)\n",
            "     | > G_mse_fake_loss: 0.74901  (0.75745)\n",
            "     | > G_feat_match_loss: 0.12612  (0.11701)\n",
            "     | > G_gen_loss: 27.49550  (25.15564)\n",
            "     | > G_adv_loss: 2.01018  (1.92750)\n",
            "     | > loss_0: 29.50569  (27.08314)\n",
            "     | > grad_norm_0: 659.81342  (767.85364)\n",
            "     | > D_mse_gan_loss: 0.09407  (0.11411)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00013)\n",
            "     | > loss_1: 0.09407  (0.11411)\n",
            "     | > grad_norm_1: 41.52705  (20.89938)\n",
            "     | > current_lr_0: 5.128703109226646e-12 \n",
            "     | > current_lr_1: 5.128703109226646e-12 \n",
            "     | > step_time: 2.42250  (2.42551)\n",
            "     | > loader_time: 0.00200  (0.00214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 265/364 -- GLOBAL_STEP: 194500\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52434  (0.55874)\n",
            "     | > G_mse_fake_loss: 0.76896  (0.75742)\n",
            "     | > G_feat_match_loss: 0.10944  (0.11693)\n",
            "     | > G_gen_loss: 23.59531  (25.14336)\n",
            "     | > G_adv_loss: 1.86332  (1.92668)\n",
            "     | > loss_0: 25.45863  (27.07003)\n",
            "     | > grad_norm_0: 877.65363  (769.28802)\n",
            "     | > D_mse_gan_loss: 0.13538  (0.11445)\n",
            "     | > D_mse_gan_real_loss: 0.00116  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00010  (0.00014)\n",
            "     | > loss_1: 0.13538  (0.11445)\n",
            "     | > grad_norm_1: 19.03000  (21.12513)\n",
            "     | > current_lr_0: 5.0020124110181085e-12 \n",
            "     | > current_lr_1: 5.0020124110181085e-12 \n",
            "     | > step_time: 2.42240  (2.42545)\n",
            "     | > loader_time: 0.00220  (0.00214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 290/364 -- GLOBAL_STEP: 194525\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56174  (0.55888)\n",
            "     | > G_mse_fake_loss: 0.76188  (0.75738)\n",
            "     | > G_feat_match_loss: 0.11572  (0.11690)\n",
            "     | > G_gen_loss: 25.27828  (25.14950)\n",
            "     | > G_adv_loss: 1.91912  (1.92634)\n",
            "     | > loss_0: 27.19740  (27.07584)\n",
            "     | > grad_norm_0: 781.63531  (768.19580)\n",
            "     | > D_mse_gan_loss: 0.11472  (0.11455)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00013)\n",
            "     | > loss_1: 0.11472  (0.11455)\n",
            "     | > grad_norm_1: 7.23682  (21.25985)\n",
            "     | > current_lr_0: 4.878451262847998e-12 \n",
            "     | > current_lr_1: 4.878451262847998e-12 \n",
            "     | > step_time: 2.41860  (2.42546)\n",
            "     | > loader_time: 0.00230  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 315/364 -- GLOBAL_STEP: 194550\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53196  (0.55918)\n",
            "     | > G_mse_fake_loss: 0.76666  (0.75736)\n",
            "     | > G_feat_match_loss: 0.11469  (0.11697)\n",
            "     | > G_gen_loss: 23.93831  (25.16295)\n",
            "     | > G_adv_loss: 1.91361  (1.92704)\n",
            "     | > loss_0: 25.85192  (27.09000)\n",
            "     | > grad_norm_0: 876.13812  (767.74817)\n",
            "     | > D_mse_gan_loss: 0.11613  (0.11443)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00013)\n",
            "     | > loss_1: 0.11613  (0.11443)\n",
            "     | > grad_norm_1: 4.50176  (21.40299)\n",
            "     | > current_lr_0: 4.7579423576718275e-12 \n",
            "     | > current_lr_1: 4.7579423576718275e-12 \n",
            "     | > step_time: 2.42250  (2.42550)\n",
            "     | > loader_time: 0.00220  (0.00214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 340/364 -- GLOBAL_STEP: 194575\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52345  (0.55894)\n",
            "     | > G_mse_fake_loss: 0.75427  (0.75730)\n",
            "     | > G_feat_match_loss: 0.11285  (0.11692)\n",
            "     | > G_gen_loss: 23.55537  (25.15242)\n",
            "     | > G_adv_loss: 1.88278  (1.92649)\n",
            "     | > loss_0: 25.43815  (27.07891)\n",
            "     | > grad_norm_0: 749.83649  (767.32007)\n",
            "     | > D_mse_gan_loss: 0.11929  (0.11456)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00018  (0.00013)\n",
            "     | > loss_1: 0.11929  (0.11456)\n",
            "     | > grad_norm_1: 13.54580  (21.42923)\n",
            "     | > current_lr_0: 4.640410298105932e-12 \n",
            "     | > current_lr_1: 4.640410298105932e-12 \n",
            "     | > step_time: 2.42470  (2.42542)\n",
            "     | > loader_time: 0.00200  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00120 \u001b[0m(+0.00001)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.58177 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70195 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 26.17970 \u001b[0m(-0.00001)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81714 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 27.99685 \u001b[0m(-0.00001)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.13729 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00057 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 40/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 17:50:51) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/364 -- GLOBAL_STEP: 194600\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51620  (0.51620)\n",
            "     | > G_mse_fake_loss: 0.77863  (0.77863)\n",
            "     | > G_feat_match_loss: 0.11213  (0.11213)\n",
            "     | > G_gen_loss: 23.22910  (23.22910)\n",
            "     | > G_adv_loss: 1.89991  (1.89991)\n",
            "     | > loss_0: 25.12901  (25.12901)\n",
            "     | > grad_norm_0: 850.92743  (850.92743)\n",
            "     | > D_mse_gan_loss: 0.12669  (0.12669)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00003)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00006)\n",
            "     | > loss_1: 0.12669  (0.12669)\n",
            "     | > grad_norm_1: 17.79617  (17.79617)\n",
            "     | > current_lr_0: 4.525781549254494e-12 \n",
            "     | > current_lr_1: 4.525781549254494e-12 \n",
            "     | > step_time: 3.16490  (3.16490)\n",
            "     | > loader_time: 5.22090  (5.22094)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 25/364 -- GLOBAL_STEP: 194625\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62046  (0.56882)\n",
            "     | > G_mse_fake_loss: 0.76397  (0.75405)\n",
            "     | > G_feat_match_loss: 0.12821  (0.11759)\n",
            "     | > G_gen_loss: 27.92061  (25.59673)\n",
            "     | > G_adv_loss: 2.04607  (1.92996)\n",
            "     | > loss_0: 29.96668  (27.52669)\n",
            "     | > grad_norm_0: 772.39771  (793.08521)\n",
            "     | > D_mse_gan_loss: 0.08414  (0.11295)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00005)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00018)\n",
            "     | > loss_1: 0.08414  (0.11295)\n",
            "     | > grad_norm_1: 42.08413  (31.37009)\n",
            "     | > current_lr_0: 4.413984392701826e-12 \n",
            "     | > current_lr_1: 4.413984392701826e-12 \n",
            "     | > step_time: 2.41860  (2.43398)\n",
            "     | > loader_time: 0.00190  (0.00211)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/364 -- GLOBAL_STEP: 194650\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60070  (0.57023)\n",
            "     | > G_mse_fake_loss: 0.74611  (0.75556)\n",
            "     | > G_feat_match_loss: 0.12523  (0.11761)\n",
            "     | > G_gen_loss: 27.03142  (25.66054)\n",
            "     | > G_adv_loss: 1.99836  (1.93166)\n",
            "     | > loss_0: 29.02979  (27.59220)\n",
            "     | > grad_norm_0: 662.90460  (779.15076)\n",
            "     | > D_mse_gan_loss: 0.10264  (0.11284)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00014)\n",
            "     | > loss_1: 0.10264  (0.11284)\n",
            "     | > grad_norm_1: 40.18429  (27.85953)\n",
            "     | > current_lr_0: 4.304948881641154e-12 \n",
            "     | > current_lr_1: 4.304948881641154e-12 \n",
            "     | > step_time: 2.42500  (2.42905)\n",
            "     | > loader_time: 0.00200  (0.00213)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 75/364 -- GLOBAL_STEP: 194675\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.64267  (0.57029)\n",
            "     | > G_mse_fake_loss: 0.73281  (0.75585)\n",
            "     | > G_feat_match_loss: 0.12499  (0.11771)\n",
            "     | > G_gen_loss: 28.92027  (25.66296)\n",
            "     | > G_adv_loss: 1.98271  (1.93292)\n",
            "     | > loss_0: 30.90298  (27.59588)\n",
            "     | > grad_norm_0: 740.37305  (783.40784)\n",
            "     | > D_mse_gan_loss: 0.09894  (0.11232)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00018  (0.00013)\n",
            "     | > loss_1: 0.09894  (0.11232)\n",
            "     | > grad_norm_1: 49.50977  (26.24593)\n",
            "     | > current_lr_0: 4.198606797111831e-12 \n",
            "     | > current_lr_1: 4.198606797111831e-12 \n",
            "     | > step_time: 2.41890  (2.42759)\n",
            "     | > loader_time: 0.00210  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/364 -- GLOBAL_STEP: 194700\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56023  (0.56897)\n",
            "     | > G_mse_fake_loss: 0.75360  (0.75583)\n",
            "     | > G_feat_match_loss: 0.11626  (0.11751)\n",
            "     | > G_gen_loss: 25.21017  (25.60375)\n",
            "     | > G_adv_loss: 1.91623  (1.93096)\n",
            "     | > loss_0: 27.12640  (27.53471)\n",
            "     | > grad_norm_0: 741.76715  (781.60504)\n",
            "     | > D_mse_gan_loss: 0.12192  (0.11299)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00009  (0.00014)\n",
            "     | > loss_1: 0.12192  (0.11299)\n",
            "     | > grad_norm_1: 11.26326  (25.61012)\n",
            "     | > current_lr_0: 4.094891605317582e-12 \n",
            "     | > current_lr_1: 4.094891605317582e-12 \n",
            "     | > step_time: 2.42930  (2.42691)\n",
            "     | > loader_time: 0.00260  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 125/364 -- GLOBAL_STEP: 194725\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54428  (0.56816)\n",
            "     | > G_mse_fake_loss: 0.76182  (0.75559)\n",
            "     | > G_feat_match_loss: 0.11770  (0.11732)\n",
            "     | > G_gen_loss: 24.49238  (25.56736)\n",
            "     | > G_adv_loss: 1.93886  (1.92877)\n",
            "     | > loss_0: 26.43124  (27.49612)\n",
            "     | > grad_norm_0: 784.73651  (780.76630)\n",
            "     | > D_mse_gan_loss: 0.11073  (0.11341)\n",
            "     | > D_mse_gan_real_loss: 0.00006  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00020)\n",
            "     | > loss_1: 0.11073  (0.11341)\n",
            "     | > grad_norm_1: 11.31601  (25.14629)\n",
            "     | > current_lr_0: 3.993738415999085e-12 \n",
            "     | > current_lr_1: 3.993738415999085e-12 \n",
            "     | > step_time: 2.42450  (2.42632)\n",
            "     | > loader_time: 0.00220  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 150/364 -- GLOBAL_STEP: 194750\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53333  (0.56819)\n",
            "     | > G_mse_fake_loss: 0.77917  (0.75586)\n",
            "     | > G_feat_match_loss: 0.10752  (0.11735)\n",
            "     | > G_gen_loss: 23.99999  (25.56846)\n",
            "     | > G_adv_loss: 1.85436  (1.92932)\n",
            "     | > loss_0: 25.85435  (27.49778)\n",
            "     | > grad_norm_0: 874.89941  (780.07874)\n",
            "     | > D_mse_gan_loss: 0.14680  (0.11334)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00002  (0.00018)\n",
            "     | > loss_1: 0.14680  (0.11334)\n",
            "     | > grad_norm_1: 36.73989  (25.48537)\n",
            "     | > current_lr_0: 3.895083941834859e-12 \n",
            "     | > current_lr_1: 3.895083941834859e-12 \n",
            "     | > step_time: 2.41850  (2.42609)\n",
            "     | > loader_time: 0.00220  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 175/364 -- GLOBAL_STEP: 194775\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61279  (0.56911)\n",
            "     | > G_mse_fake_loss: 0.75378  (0.75516)\n",
            "     | > G_feat_match_loss: 0.12595  (0.11737)\n",
            "     | > G_gen_loss: 27.57572  (25.61004)\n",
            "     | > G_adv_loss: 2.01326  (1.92888)\n",
            "     | > loss_0: 29.58898  (27.53891)\n",
            "     | > grad_norm_0: 867.25421  (781.87830)\n",
            "     | > D_mse_gan_loss: 0.09243  (0.11359)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00017)\n",
            "     | > loss_1: 0.09243  (0.11359)\n",
            "     | > grad_norm_1: 40.59711  (25.93486)\n",
            "     | > current_lr_0: 3.798866458845027e-12 \n",
            "     | > current_lr_1: 3.798866458845027e-12 \n",
            "     | > step_time: 2.42070  (2.42581)\n",
            "     | > loader_time: 0.00210  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 200/364 -- GLOBAL_STEP: 194800\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52711  (0.56928)\n",
            "     | > G_mse_fake_loss: 0.73984  (0.75499)\n",
            "     | > G_feat_match_loss: 0.10506  (0.11739)\n",
            "     | > G_gen_loss: 23.71988  (25.61778)\n",
            "     | > G_adv_loss: 1.79044  (1.92885)\n",
            "     | > loss_0: 25.51032  (27.54663)\n",
            "     | > grad_norm_0: 887.63727  (781.12665)\n",
            "     | > D_mse_gan_loss: 0.14966  (0.11343)\n",
            "     | > D_mse_gan_real_loss: 0.00016  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00019  (0.00018)\n",
            "     | > loss_1: 0.14966  (0.11343)\n",
            "     | > grad_norm_1: 17.70225  (26.28184)\n",
            "     | > current_lr_0: 3.705025767773196e-12 \n",
            "     | > current_lr_1: 3.705025767773196e-12 \n",
            "     | > step_time: 2.42160  (2.42566)\n",
            "     | > loader_time: 0.00250  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 225/364 -- GLOBAL_STEP: 194825\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62890  (0.56932)\n",
            "     | > G_mse_fake_loss: 0.74339  (0.75499)\n",
            "     | > G_feat_match_loss: 0.12835  (0.11755)\n",
            "     | > G_gen_loss: 28.30059  (25.61937)\n",
            "     | > G_adv_loss: 2.02691  (1.93053)\n",
            "     | > loss_0: 30.32749  (27.54990)\n",
            "     | > grad_norm_0: 738.81812  (778.78265)\n",
            "     | > D_mse_gan_loss: 0.09284  (0.11304)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00013  (0.00018)\n",
            "     | > loss_1: 0.09284  (0.11304)\n",
            "     | > grad_norm_1: 51.03116  (26.22262)\n",
            "     | > current_lr_0: 3.6135031564223156e-12 \n",
            "     | > current_lr_1: 3.6135031564223156e-12 \n",
            "     | > step_time: 2.42330  (2.42561)\n",
            "     | > loader_time: 0.00210  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 250/364 -- GLOBAL_STEP: 194850\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.56270  (0.56919)\n",
            "     | > G_mse_fake_loss: 0.75287  (0.75490)\n",
            "     | > G_feat_match_loss: 0.11731  (0.11753)\n",
            "     | > G_gen_loss: 25.32167  (25.61348)\n",
            "     | > G_adv_loss: 1.92602  (1.93017)\n",
            "     | > loss_0: 27.24769  (27.54365)\n",
            "     | > grad_norm_0: 680.88245  (778.66870)\n",
            "     | > D_mse_gan_loss: 0.10944  (0.11316)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00018)\n",
            "     | > loss_1: 0.10944  (0.11316)\n",
            "     | > grad_norm_1: 19.48757  (26.21487)\n",
            "     | > current_lr_0: 3.5242413629208915e-12 \n",
            "     | > current_lr_1: 3.5242413629208915e-12 \n",
            "     | > step_time: 2.42590  (2.42538)\n",
            "     | > loader_time: 0.00230  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 275/364 -- GLOBAL_STEP: 194875\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63323  (0.56905)\n",
            "     | > G_mse_fake_loss: 0.73268  (0.75519)\n",
            "     | > G_feat_match_loss: 0.12579  (0.11754)\n",
            "     | > G_gen_loss: 28.49513  (25.60722)\n",
            "     | > G_adv_loss: 1.99056  (1.93062)\n",
            "     | > loss_0: 30.48568  (27.53784)\n",
            "     | > grad_norm_0: 702.26453  (779.42328)\n",
            "     | > D_mse_gan_loss: 0.09286  (0.11305)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00011  (0.00019)\n",
            "     | > loss_1: 0.09286  (0.11305)\n",
            "     | > grad_norm_1: 51.72477  (26.12858)\n",
            "     | > current_lr_0: 3.4371845398966426e-12 \n",
            "     | > current_lr_1: 3.4371845398966426e-12 \n",
            "     | > step_time: 2.42170  (2.42525)\n",
            "     | > loader_time: 0.00240  (0.00221)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 300/364 -- GLOBAL_STEP: 194900\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53014  (0.56940)\n",
            "     | > G_mse_fake_loss: 0.75083  (0.75470)\n",
            "     | > G_feat_match_loss: 0.10988  (0.11753)\n",
            "     | > G_gen_loss: 23.85619  (25.62296)\n",
            "     | > G_adv_loss: 1.84963  (1.92995)\n",
            "     | > loss_0: 25.70581  (27.55292)\n",
            "     | > grad_norm_0: 757.09497  (778.67145)\n",
            "     | > D_mse_gan_loss: 0.12852  (0.11314)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00020  (0.00018)\n",
            "     | > loss_1: 0.12852  (0.11314)\n",
            "     | > grad_norm_1: 9.82764  (25.83552)\n",
            "     | > current_lr_0: 3.352278219535128e-12 \n",
            "     | > current_lr_1: 3.352278219535128e-12 \n",
            "     | > step_time: 2.43110  (2.42522)\n",
            "     | > loader_time: 0.00220  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 325/364 -- GLOBAL_STEP: 194925\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53951  (0.56936)\n",
            "     | > G_mse_fake_loss: 0.76796  (0.75457)\n",
            "     | > G_feat_match_loss: 0.11397  (0.11755)\n",
            "     | > G_gen_loss: 24.27796  (25.62142)\n",
            "     | > G_adv_loss: 1.90763  (1.93009)\n",
            "     | > loss_0: 26.18559  (27.55151)\n",
            "     | > grad_norm_0: 760.99231  (777.59814)\n",
            "     | > D_mse_gan_loss: 0.11684  (0.11300)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00018)\n",
            "     | > loss_1: 0.11684  (0.11300)\n",
            "     | > grad_norm_1: 5.84864  (25.73909)\n",
            "     | > current_lr_0: 3.2694692795015102e-12 \n",
            "     | > current_lr_1: 3.2694692795015102e-12 \n",
            "     | > step_time: 2.42190  (2.42516)\n",
            "     | > loader_time: 0.00210  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 350/364 -- GLOBAL_STEP: 194950\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53448  (0.56910)\n",
            "     | > G_mse_fake_loss: 0.76804  (0.75475)\n",
            "     | > G_feat_match_loss: 0.11188  (0.11757)\n",
            "     | > G_gen_loss: 24.05151  (25.60938)\n",
            "     | > G_adv_loss: 1.88685  (1.93044)\n",
            "     | > loss_0: 25.93836  (27.53981)\n",
            "     | > grad_norm_0: 1031.03284  (778.69153)\n",
            "     | > D_mse_gan_loss: 0.12953  (0.11293)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00017)\n",
            "     | > loss_1: 0.12953  (0.11293)\n",
            "     | > grad_norm_1: 11.43831  (25.61538)\n",
            "     | > current_lr_0: 3.1887059097041365e-12 \n",
            "     | > current_lr_1: 3.1887059097041365e-12 \n",
            "     | > step_time: 2.42520  (2.42517)\n",
            "     | > loader_time: 0.00190  (0.00222)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00132 \u001b[0m(+0.00012)\n",
            "     | > avg_G_l1_spec_loss:\u001b[91m 0.58177 \u001b[0m(+0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70195 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[91m 26.17971 \u001b[0m(+0.00001)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81714 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[91m 27.99685 \u001b[0m(+0.00001)\n",
            "     | > avg_D_mse_gan_loss:\u001b[91m 0.27955 \u001b[0m(+0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.13729 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00057 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[91m 0.27955 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 41/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 18:05:50) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/364 -- GLOBAL_STEP: 194975\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59998  (0.55664)\n",
            "     | > G_mse_fake_loss: 0.74842  (0.75504)\n",
            "     | > G_feat_match_loss: 0.12500  (0.11461)\n",
            "     | > G_gen_loss: 26.99922  (25.04868)\n",
            "     | > G_adv_loss: 1.99839  (1.90112)\n",
            "     | > loss_0: 28.99761  (26.94981)\n",
            "     | > grad_norm_0: 649.30725  (817.56433)\n",
            "     | > D_mse_gan_loss: 0.09453  (0.12138)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00008  (0.00007)\n",
            "     | > loss_1: 0.09453  (0.12138)\n",
            "     | > grad_norm_1: 39.36945  (29.00788)\n",
            "     | > current_lr_0: 3.10993757987913e-12 \n",
            "     | > current_lr_1: 3.10993757987913e-12 \n",
            "     | > step_time: 2.42840  (2.44046)\n",
            "     | > loader_time: 0.00080  (0.00156)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 35/364 -- GLOBAL_STEP: 195000\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53105  (0.55529)\n",
            "     | > G_mse_fake_loss: 0.78527  (0.76142)\n",
            "     | > G_feat_match_loss: 0.11818  (0.11625)\n",
            "     | > G_gen_loss: 23.89729  (24.98805)\n",
            "     | > G_adv_loss: 1.96705  (1.92390)\n",
            "     | > loss_0: 25.86434  (26.91195)\n",
            "     | > grad_norm_0: 897.85822  (795.74182)\n",
            "     | > D_mse_gan_loss: 0.10586  (0.11640)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00008)\n",
            "     | > loss_1: 0.10586  (0.11640)\n",
            "     | > grad_norm_1: 6.13953  (25.03285)\n",
            "     | > current_lr_0: 3.0331150079757104e-12 \n",
            "     | > current_lr_1: 3.0331150079757104e-12 \n",
            "     | > step_time: 2.42310  (2.43024)\n",
            "     | > loader_time: 0.00210  (0.00186)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 60/364 -- GLOBAL_STEP: 195025\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.58557  (0.55791)\n",
            "     | > G_mse_fake_loss: 0.73997  (0.75958)\n",
            "     | > G_feat_match_loss: 0.11951  (0.11613)\n",
            "     | > G_gen_loss: 26.35058  (25.10581)\n",
            "     | > G_adv_loss: 1.93502  (1.92085)\n",
            "     | > loss_0: 28.28560  (27.02666)\n",
            "     | > grad_norm_0: 690.45746  (791.11786)\n",
            "     | > D_mse_gan_loss: 0.10735  (0.11772)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00008)\n",
            "     | > loss_1: 0.10735  (0.11772)\n",
            "     | > grad_norm_1: 29.44171  (24.80375)\n",
            "     | > current_lr_0: 2.9581901293224836e-12 \n",
            "     | > current_lr_1: 2.9581901293224836e-12 \n",
            "     | > step_time: 2.42560  (2.42762)\n",
            "     | > loader_time: 0.00210  (0.00199)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 85/364 -- GLOBAL_STEP: 195050\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52774  (0.55713)\n",
            "     | > G_mse_fake_loss: 0.76196  (0.75950)\n",
            "     | > G_feat_match_loss: 0.11122  (0.11619)\n",
            "     | > G_gen_loss: 23.74838  (25.07078)\n",
            "     | > G_adv_loss: 1.87417  (1.92141)\n",
            "     | > loss_0: 25.62255  (26.99219)\n",
            "     | > grad_norm_0: 838.20496  (792.44177)\n",
            "     | > D_mse_gan_loss: 0.12727  (0.11744)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00013)\n",
            "     | > loss_1: 0.12727  (0.11744)\n",
            "     | > grad_norm_1: 11.38490  (25.53854)\n",
            "     | > current_lr_0: 2.8851160665553792e-12 \n",
            "     | > current_lr_1: 2.8851160665553792e-12 \n",
            "     | > step_time: 2.42760  (2.42712)\n",
            "     | > loader_time: 0.00250  (0.00204)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 110/364 -- GLOBAL_STEP: 195075\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62690  (0.55753)\n",
            "     | > G_mse_fake_loss: 0.75557  (0.75920)\n",
            "     | > G_feat_match_loss: 0.12849  (0.11632)\n",
            "     | > G_gen_loss: 28.21066  (25.08889)\n",
            "     | > G_adv_loss: 2.04046  (1.92241)\n",
            "     | > loss_0: 30.25112  (27.01131)\n",
            "     | > grad_norm_0: 729.34723  (789.95557)\n",
            "     | > D_mse_gan_loss: 0.09588  (0.11668)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00012)\n",
            "     | > loss_1: 0.09588  (0.11668)\n",
            "     | > grad_norm_1: 42.00742  (25.63944)\n",
            "     | > current_lr_0: 2.8138471002884503e-12 \n",
            "     | > current_lr_1: 2.8138471002884503e-12 \n",
            "     | > step_time: 2.42160  (2.42650)\n",
            "     | > loader_time: 0.00220  (0.00207)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 135/364 -- GLOBAL_STEP: 195100\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52882  (0.55754)\n",
            "     | > G_mse_fake_loss: 0.76554  (0.75837)\n",
            "     | > G_feat_match_loss: 0.10716  (0.11603)\n",
            "     | > G_gen_loss: 23.79687  (25.08914)\n",
            "     | > G_adv_loss: 1.83716  (1.91871)\n",
            "     | > loss_0: 25.63403  (27.00785)\n",
            "     | > grad_norm_0: 844.36145  (789.51654)\n",
            "     | > D_mse_gan_loss: 0.13892  (0.11752)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00014)\n",
            "     | > loss_1: 0.13892  (0.11752)\n",
            "     | > grad_norm_1: 24.91877  (24.90001)\n",
            "     | > current_lr_0: 2.744338640509159e-12 \n",
            "     | > current_lr_1: 2.744338640509159e-12 \n",
            "     | > step_time: 2.41920  (2.42589)\n",
            "     | > loader_time: 0.00230  (0.00209)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 160/364 -- GLOBAL_STEP: 195125\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55519  (0.55713)\n",
            "     | > G_mse_fake_loss: 0.74706  (0.75865)\n",
            "     | > G_feat_match_loss: 0.11462  (0.11600)\n",
            "     | > G_gen_loss: 24.98339  (25.07107)\n",
            "     | > G_adv_loss: 1.89322  (1.91865)\n",
            "     | > loss_0: 26.87661  (26.98972)\n",
            "     | > grad_norm_0: 748.88684  (791.31274)\n",
            "     | > D_mse_gan_loss: 0.11734  (0.11737)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00013)\n",
            "     | > loss_1: 0.11734  (0.11737)\n",
            "     | > grad_norm_1: 11.11585  (23.79667)\n",
            "     | > current_lr_0: 2.6765471986802716e-12 \n",
            "     | > current_lr_1: 2.6765471986802716e-12 \n",
            "     | > step_time: 2.41740  (2.42555)\n",
            "     | > loader_time: 0.00220  (0.00211)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 185/364 -- GLOBAL_STEP: 195150\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53952  (0.55781)\n",
            "     | > G_mse_fake_loss: 0.76711  (0.75883)\n",
            "     | > G_feat_match_loss: 0.11186  (0.11611)\n",
            "     | > G_gen_loss: 24.27850  (25.10152)\n",
            "     | > G_adv_loss: 1.88569  (1.91990)\n",
            "     | > loss_0: 26.16418  (27.02142)\n",
            "     | > grad_norm_0: 926.39539  (791.75177)\n",
            "     | > D_mse_gan_loss: 0.12631  (0.11704)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00012)\n",
            "     | > loss_1: 0.12631  (0.11704)\n",
            "     | > grad_norm_1: 11.38763  (23.29080)\n",
            "     | > current_lr_0: 2.610430360530902e-12 \n",
            "     | > current_lr_1: 2.610430360530902e-12 \n",
            "     | > step_time: 2.43530  (2.42552)\n",
            "     | > loader_time: 0.00200  (0.00212)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 210/364 -- GLOBAL_STEP: 195175\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60315  (0.55819)\n",
            "     | > G_mse_fake_loss: 0.72104  (0.75863)\n",
            "     | > G_feat_match_loss: 0.12389  (0.11619)\n",
            "     | > G_gen_loss: 27.14168  (25.11860)\n",
            "     | > G_adv_loss: 1.95998  (1.92056)\n",
            "     | > loss_0: 29.10166  (27.03916)\n",
            "     | > grad_norm_0: 653.92914  (791.30493)\n",
            "     | > D_mse_gan_loss: 0.10032  (0.11676)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00010)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00013)\n",
            "     | > loss_1: 0.10032  (0.11676)\n",
            "     | > grad_norm_1: 52.07033  (23.33336)\n",
            "     | > current_lr_0: 2.545946759519672e-12 \n",
            "     | > current_lr_1: 2.545946759519672e-12 \n",
            "     | > step_time: 2.42100  (2.42541)\n",
            "     | > loader_time: 0.00250  (0.00213)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 235/364 -- GLOBAL_STEP: 195200\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54343  (0.55833)\n",
            "     | > G_mse_fake_loss: 0.74754  (0.75924)\n",
            "     | > G_feat_match_loss: 0.10829  (0.11635)\n",
            "     | > G_gen_loss: 24.45447  (25.12484)\n",
            "     | > G_adv_loss: 1.83043  (1.92269)\n",
            "     | > loss_0: 26.28489  (27.04752)\n",
            "     | > grad_norm_0: 768.50531  (789.23456)\n",
            "     | > D_mse_gan_loss: 0.13656  (0.11638)\n",
            "     | > D_mse_gan_real_loss: 0.00026  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00037  (0.00012)\n",
            "     | > loss_1: 0.13656  (0.11638)\n",
            "     | > grad_norm_1: 12.04362  (22.95860)\n",
            "     | > current_lr_0: 2.4830560509533982e-12 \n",
            "     | > current_lr_1: 2.4830560509533982e-12 \n",
            "     | > step_time: 2.42200  (2.42546)\n",
            "     | > loader_time: 0.00210  (0.00213)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 260/364 -- GLOBAL_STEP: 195225\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57703  (0.55850)\n",
            "     | > G_mse_fake_loss: 0.74084  (0.75892)\n",
            "     | > G_feat_match_loss: 0.12149  (0.11634)\n",
            "     | > G_gen_loss: 25.96622  (25.13256)\n",
            "     | > G_adv_loss: 1.95574  (1.92232)\n",
            "     | > loss_0: 27.92197  (27.05487)\n",
            "     | > grad_norm_0: 706.32745  (788.49768)\n",
            "     | > D_mse_gan_loss: 0.10184  (0.11638)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00010  (0.00012)\n",
            "     | > loss_1: 0.10184  (0.11638)\n",
            "     | > grad_norm_1: 38.11430  (23.00160)\n",
            "     | > current_lr_0: 2.421718886745104e-12 \n",
            "     | > current_lr_1: 2.421718886745104e-12 \n",
            "     | > step_time: 2.42920  (2.42547)\n",
            "     | > loader_time: 0.00260  (0.00214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 285/364 -- GLOBAL_STEP: 195250\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52277  (0.55820)\n",
            "     | > G_mse_fake_loss: 0.75730  (0.75889)\n",
            "     | > G_feat_match_loss: 0.10819  (0.11631)\n",
            "     | > G_gen_loss: 23.52467  (25.11881)\n",
            "     | > G_adv_loss: 1.83918  (1.92198)\n",
            "     | > loss_0: 25.36385  (27.04079)\n",
            "     | > grad_norm_0: 784.96826  (787.40649)\n",
            "     | > D_mse_gan_loss: 0.14727  (0.11638)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00088  (0.00012)\n",
            "     | > loss_1: 0.14727  (0.11638)\n",
            "     | > grad_norm_1: 21.56619  (22.81530)\n",
            "     | > current_lr_0: 2.361896890795566e-12 \n",
            "     | > current_lr_1: 2.361896890795566e-12 \n",
            "     | > step_time: 2.42580  (2.42544)\n",
            "     | > loader_time: 0.00230  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 310/364 -- GLOBAL_STEP: 195275\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61450  (0.55887)\n",
            "     | > G_mse_fake_loss: 0.73028  (0.75872)\n",
            "     | > G_feat_match_loss: 0.12226  (0.11644)\n",
            "     | > G_gen_loss: 27.65262  (25.14910)\n",
            "     | > G_adv_loss: 1.95284  (1.92316)\n",
            "     | > loss_0: 29.60545  (27.07226)\n",
            "     | > grad_norm_0: 656.44238  (787.67914)\n",
            "     | > D_mse_gan_loss: 0.09822  (0.11608)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00011)\n",
            "     | > D_mse_gan_fake_loss: 0.00016  (0.00013)\n",
            "     | > loss_1: 0.09822  (0.11608)\n",
            "     | > grad_norm_1: 45.68577  (23.00333)\n",
            "     | > current_lr_0: 2.303552634982992e-12 \n",
            "     | > current_lr_1: 2.303552634982992e-12 \n",
            "     | > step_time: 2.42700  (2.42535)\n",
            "     | > loader_time: 0.00230  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 335/364 -- GLOBAL_STEP: 195300\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55384  (0.55925)\n",
            "     | > G_mse_fake_loss: 0.75716  (0.75857)\n",
            "     | > G_feat_match_loss: 0.11658  (0.11647)\n",
            "     | > G_gen_loss: 24.92288  (25.16614)\n",
            "     | > G_adv_loss: 1.92294  (1.92323)\n",
            "     | > loss_0: 26.84582  (27.08937)\n",
            "     | > grad_norm_0: 787.75275  (789.90149)\n",
            "     | > D_mse_gan_loss: 0.11532  (0.11618)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00013)\n",
            "     | > loss_1: 0.11532  (0.11618)\n",
            "     | > grad_norm_1: 12.61653  (23.26253)\n",
            "     | > current_lr_0: 2.246649615745811e-12 \n",
            "     | > current_lr_1: 2.246649615745811e-12 \n",
            "     | > step_time: 2.42130  (2.42536)\n",
            "     | > loader_time: 0.00210  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 360/364 -- GLOBAL_STEP: 195325\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54946  (0.55892)\n",
            "     | > G_mse_fake_loss: 0.75278  (0.75848)\n",
            "     | > G_feat_match_loss: 0.11585  (0.11644)\n",
            "     | > G_gen_loss: 24.72589  (25.15160)\n",
            "     | > G_adv_loss: 1.91123  (1.92288)\n",
            "     | > loss_0: 26.63712  (27.07448)\n",
            "     | > grad_norm_0: 747.21307  (789.77130)\n",
            "     | > D_mse_gan_loss: 0.11287  (0.11617)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00013)\n",
            "     | > loss_1: 0.11287  (0.11617)\n",
            "     | > grad_norm_1: 12.28777  (23.32092)\n",
            "     | > current_lr_0: 2.191152231243923e-12 \n",
            "     | > current_lr_1: 2.191152231243923e-12 \n",
            "     | > step_time: 2.41830  (2.42524)\n",
            "     | > loader_time: 0.00200  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.00132 \u001b[0m(+0.00000)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.58177 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[91m 0.70195 \u001b[0m(+0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 26.17971 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81714 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 27.99685 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.13729 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00057 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 42/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 18:20:48) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/364 -- GLOBAL_STEP: 195350\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51601  (0.55140)\n",
            "     | > G_mse_fake_loss: 0.76885  (0.76226)\n",
            "     | > G_feat_match_loss: 0.10743  (0.11423)\n",
            "     | > G_gen_loss: 23.22066  (24.81278)\n",
            "     | > G_adv_loss: 1.84312  (1.90460)\n",
            "     | > loss_0: 25.06378  (26.71737)\n",
            "     | > grad_norm_0: 812.38837  (825.60614)\n",
            "     | > D_mse_gan_loss: 0.14018  (0.12188)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00014  (0.00022)\n",
            "     | > loss_1: 0.14018  (0.12188)\n",
            "     | > grad_norm_1: 27.49865  (21.83417)\n",
            "     | > current_lr_0: 2.137025759084113e-12 \n",
            "     | > current_lr_1: 2.137025759084113e-12 \n",
            "     | > step_time: 2.42450  (2.43794)\n",
            "     | > loader_time: 0.00220  (0.00176)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 45/364 -- GLOBAL_STEP: 195375\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55964  (0.55393)\n",
            "     | > G_mse_fake_loss: 0.75620  (0.76051)\n",
            "     | > G_feat_match_loss: 0.11506  (0.11411)\n",
            "     | > G_gen_loss: 25.18370  (24.92691)\n",
            "     | > G_adv_loss: 1.90684  (1.90159)\n",
            "     | > loss_0: 27.09054  (26.82850)\n",
            "     | > grad_norm_0: 786.86682  (846.42261)\n",
            "     | > D_mse_gan_loss: 0.11278  (0.12181)\n",
            "     | > D_mse_gan_real_loss: 0.00041  (0.00012)\n",
            "     | > D_mse_gan_fake_loss: 0.00009  (0.00016)\n",
            "     | > loss_1: 0.11278  (0.12181)\n",
            "     | > grad_norm_1: 9.73827  (20.38669)\n",
            "     | > current_lr_0: 2.084236334595702e-12 \n",
            "     | > current_lr_1: 2.084236334595702e-12 \n",
            "     | > step_time: 2.43140  (2.43089)\n",
            "     | > loader_time: 0.00210  (0.00199)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/364 -- GLOBAL_STEP: 195400\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51791  (0.55356)\n",
            "     | > G_mse_fake_loss: 0.77945  (0.76005)\n",
            "     | > G_feat_match_loss: 0.11208  (0.11413)\n",
            "     | > G_gen_loss: 23.30606  (24.91031)\n",
            "     | > G_adv_loss: 1.90028  (1.90130)\n",
            "     | > loss_0: 25.20634  (26.81161)\n",
            "     | > grad_norm_0: 828.36877  (845.05896)\n",
            "     | > D_mse_gan_loss: 0.12366  (0.12183)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00161  (0.00016)\n",
            "     | > loss_1: 0.12366  (0.12183)\n",
            "     | > grad_norm_1: 17.75710  (18.97323)\n",
            "     | > current_lr_0: 2.032750929642841e-12 \n",
            "     | > current_lr_1: 2.032750929642841e-12 \n",
            "     | > step_time: 2.41980  (2.42870)\n",
            "     | > loader_time: 0.00220  (0.00205)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 95/364 -- GLOBAL_STEP: 195425\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.62806  (0.55391)\n",
            "     | > G_mse_fake_loss: 0.70882  (0.76056)\n",
            "     | > G_feat_match_loss: 0.11491  (0.11410)\n",
            "     | > G_gen_loss: 28.26283  (24.92578)\n",
            "     | > G_adv_loss: 1.85793  (1.90151)\n",
            "     | > loss_0: 30.12076  (26.82729)\n",
            "     | > grad_norm_0: 796.41138  (847.16132)\n",
            "     | > D_mse_gan_loss: 0.12418  (0.12197)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00021)\n",
            "     | > D_mse_gan_fake_loss: 0.00225  (0.00016)\n",
            "     | > loss_1: 0.12418  (0.12197)\n",
            "     | > grad_norm_1: 38.89624  (20.04850)\n",
            "     | > current_lr_0: 1.9825373319601826e-12 \n",
            "     | > current_lr_1: 1.9825373319601826e-12 \n",
            "     | > step_time: 2.41360  (2.42762)\n",
            "     | > loader_time: 0.00240  (0.00210)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/364 -- GLOBAL_STEP: 195450\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52327  (0.55311)\n",
            "     | > G_mse_fake_loss: 0.78413  (0.76011)\n",
            "     | > G_feat_match_loss: 0.11252  (0.11390)\n",
            "     | > G_gen_loss: 23.54733  (24.89005)\n",
            "     | > G_adv_loss: 1.90935  (1.89912)\n",
            "     | > loss_0: 25.45668  (26.78918)\n",
            "     | > grad_norm_0: 774.09222  (846.83319)\n",
            "     | > D_mse_gan_loss: 0.12595  (0.12246)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00019)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00016)\n",
            "     | > loss_1: 0.12595  (0.12246)\n",
            "     | > grad_norm_1: 20.98172  (20.29849)\n",
            "     | > current_lr_0: 1.9335641249990186e-12 \n",
            "     | > current_lr_1: 1.9335641249990186e-12 \n",
            "     | > step_time: 2.42530  (2.42699)\n",
            "     | > loader_time: 0.00220  (0.00213)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 145/364 -- GLOBAL_STEP: 195475\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54711  (0.55291)\n",
            "     | > G_mse_fake_loss: 0.76853  (0.76008)\n",
            "     | > G_feat_match_loss: 0.12049  (0.11401)\n",
            "     | > G_gen_loss: 24.61996  (24.88086)\n",
            "     | > G_adv_loss: 1.97346  (1.90015)\n",
            "     | > loss_0: 26.59342  (26.78102)\n",
            "     | > grad_norm_0: 738.13702  (845.95367)\n",
            "     | > D_mse_gan_loss: 0.10458  (0.12210)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00016)\n",
            "     | > loss_1: 0.10458  (0.12210)\n",
            "     | > grad_norm_1: 15.86161  (20.53152)\n",
            "     | > current_lr_0: 1.8858006682712527e-12 \n",
            "     | > current_lr_1: 1.8858006682712527e-12 \n",
            "     | > step_time: 2.43750  (2.42654)\n",
            "     | > loader_time: 0.00270  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 170/364 -- GLOBAL_STEP: 195500\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.54170  (0.55320)\n",
            "     | > G_mse_fake_loss: 0.74770  (0.76011)\n",
            "     | > G_feat_match_loss: 0.10672  (0.11396)\n",
            "     | > G_gen_loss: 24.37642  (24.89413)\n",
            "     | > G_adv_loss: 1.81490  (1.89972)\n",
            "     | > loss_0: 26.19132  (26.79385)\n",
            "     | > grad_norm_0: 833.44946  (848.54523)\n",
            "     | > D_mse_gan_loss: 0.14656  (0.12240)\n",
            "     | > D_mse_gan_real_loss: 0.00013  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00011  (0.00015)\n",
            "     | > loss_1: 0.14656  (0.12240)\n",
            "     | > grad_norm_1: 19.90138  (20.74790)\n",
            "     | > current_lr_0: 1.8392170781789345e-12 \n",
            "     | > current_lr_1: 1.8392170781789345e-12 \n",
            "     | > step_time: 2.42930  (2.42626)\n",
            "     | > loader_time: 0.00230  (0.00217)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 195/364 -- GLOBAL_STEP: 195525\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57180  (0.55397)\n",
            "     | > G_mse_fake_loss: 0.74205  (0.75957)\n",
            "     | > G_feat_match_loss: 0.12145  (0.11413)\n",
            "     | > G_gen_loss: 25.73116  (24.92851)\n",
            "     | > G_adv_loss: 1.95658  (1.90088)\n",
            "     | > loss_0: 27.68774  (26.82939)\n",
            "     | > grad_norm_0: 741.58435  (842.12305)\n",
            "     | > D_mse_gan_loss: 0.09757  (0.12197)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00012  (0.00015)\n",
            "     | > loss_1: 0.09757  (0.12197)\n",
            "     | > grad_norm_1: 37.59353  (20.95396)\n",
            "     | > current_lr_0: 1.793784209317339e-12 \n",
            "     | > current_lr_1: 1.793784209317339e-12 \n",
            "     | > step_time: 2.42080  (2.42604)\n",
            "     | > loader_time: 0.00230  (0.00218)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 220/364 -- GLOBAL_STEP: 195550\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53399  (0.55387)\n",
            "     | > G_mse_fake_loss: 0.76507  (0.75975)\n",
            "     | > G_feat_match_loss: 0.11459  (0.11414)\n",
            "     | > G_gen_loss: 24.02944  (24.92411)\n",
            "     | > G_adv_loss: 1.91099  (1.90119)\n",
            "     | > loss_0: 25.94043  (26.82531)\n",
            "     | > grad_norm_0: 757.32312  (845.36200)\n",
            "     | > D_mse_gan_loss: 0.12292  (0.12197)\n",
            "     | > D_mse_gan_real_loss: 0.00004  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00030  (0.00015)\n",
            "     | > loss_1: 0.12292  (0.12197)\n",
            "     | > grad_norm_1: 7.45044  (20.70422)\n",
            "     | > current_lr_0: 1.749473636239902e-12 \n",
            "     | > current_lr_1: 1.749473636239902e-12 \n",
            "     | > step_time: 2.42210  (2.42592)\n",
            "     | > loader_time: 0.00230  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 245/364 -- GLOBAL_STEP: 195575\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57655  (0.55403)\n",
            "     | > G_mse_fake_loss: 0.72572  (0.75977)\n",
            "     | > G_feat_match_loss: 0.11934  (0.11429)\n",
            "     | > G_gen_loss: 25.94497  (24.93151)\n",
            "     | > G_adv_loss: 1.91907  (1.90262)\n",
            "     | > loss_0: 27.86404  (26.83412)\n",
            "     | > grad_norm_0: 718.92810  (842.52454)\n",
            "     | > D_mse_gan_loss: 0.11146  (0.12159)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00016)\n",
            "     | > D_mse_gan_fake_loss: 0.00038  (0.00016)\n",
            "     | > loss_1: 0.11146  (0.12159)\n",
            "     | > grad_norm_1: 36.00162  (20.93518)\n",
            "     | > current_lr_0: 1.7062576356736142e-12 \n",
            "     | > current_lr_1: 1.7062576356736142e-12 \n",
            "     | > step_time: 2.41720  (2.42588)\n",
            "     | > loader_time: 0.00230  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 270/364 -- GLOBAL_STEP: 195600\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.50985  (0.55398)\n",
            "     | > G_mse_fake_loss: 0.77462  (0.75998)\n",
            "     | > G_feat_match_loss: 0.10408  (0.11423)\n",
            "     | > G_gen_loss: 22.94345  (24.92902)\n",
            "     | > G_adv_loss: 1.81544  (1.90231)\n",
            "     | > loss_0: 24.75889  (26.83134)\n",
            "     | > grad_norm_0: 1032.46240  (843.13690)\n",
            "     | > D_mse_gan_loss: 0.14955  (0.12178)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00016)\n",
            "     | > loss_1: 0.14955  (0.12178)\n",
            "     | > grad_norm_1: 41.62757  (20.71479)\n",
            "     | > current_lr_0: 1.6641091691737208e-12 \n",
            "     | > current_lr_1: 1.6641091691737208e-12 \n",
            "     | > step_time: 2.42800  (2.42586)\n",
            "     | > loader_time: 0.00230  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 295/364 -- GLOBAL_STEP: 195625\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.61843  (0.55467)\n",
            "     | > G_mse_fake_loss: 0.76591  (0.75968)\n",
            "     | > G_feat_match_loss: 0.12422  (0.11432)\n",
            "     | > G_gen_loss: 27.82923  (24.96025)\n",
            "     | > G_adv_loss: 2.00809  (1.90283)\n",
            "     | > loss_0: 29.83733  (26.86309)\n",
            "     | > grad_norm_0: 819.24976  (844.06122)\n",
            "     | > D_mse_gan_loss: 0.08766  (0.12145)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00016)\n",
            "     | > loss_1: 0.08766  (0.12145)\n",
            "     | > grad_norm_1: 30.96526  (20.67372)\n",
            "     | > current_lr_0: 1.6230018662069016e-12 \n",
            "     | > current_lr_1: 1.6230018662069016e-12 \n",
            "     | > step_time: 2.41960  (2.42582)\n",
            "     | > loader_time: 0.00210  (0.00219)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 320/364 -- GLOBAL_STEP: 195650\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53729  (0.55532)\n",
            "     | > G_mse_fake_loss: 0.77081  (0.75932)\n",
            "     | > G_feat_match_loss: 0.10779  (0.11442)\n",
            "     | > G_gen_loss: 24.17803  (24.98922)\n",
            "     | > G_adv_loss: 1.84871  (1.90348)\n",
            "     | > loss_0: 26.02674  (26.89270)\n",
            "     | > grad_norm_0: 1072.18860  (842.11798)\n",
            "     | > D_mse_gan_loss: 0.13906  (0.12132)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00014)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00016)\n",
            "     | > loss_1: 0.13906  (0.12132)\n",
            "     | > grad_norm_1: 26.78511  (20.68883)\n",
            "     | > current_lr_0: 1.5829100076523284e-12 \n",
            "     | > current_lr_1: 1.5829100076523284e-12 \n",
            "     | > step_time: 2.42130  (2.42573)\n",
            "     | > loader_time: 0.00230  (0.00220)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 345/364 -- GLOBAL_STEP: 195675\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.55602  (0.55523)\n",
            "     | > G_mse_fake_loss: 0.76656  (0.75951)\n",
            "     | > G_feat_match_loss: 0.12492  (0.11438)\n",
            "     | > G_gen_loss: 25.02104  (24.98534)\n",
            "     | > G_adv_loss: 2.01572  (1.90333)\n",
            "     | > loss_0: 27.03676  (26.88867)\n",
            "     | > grad_norm_0: 686.38654  (841.56915)\n",
            "     | > D_mse_gan_loss: 0.09633  (0.12139)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00015)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00016)\n",
            "     | > loss_1: 0.09633  (0.12139)\n",
            "     | > grad_norm_1: 26.18928  (20.75668)\n",
            "     | > current_lr_0: 1.5438085097102892e-12 \n",
            "     | > current_lr_1: 1.5438085097102892e-12 \n",
            "     | > step_time: 2.42330  (2.42563)\n",
            "     | > loader_time: 0.00230  (0.00220)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00098 \u001b[0m(-0.00035)\n",
            "     | > avg_G_l1_spec_loss:\u001b[92m 0.58177 \u001b[0m(-0.00000)\n",
            "     | > avg_G_mse_fake_loss:\u001b[92m 0.70195 \u001b[0m(-0.00000)\n",
            "     | > avg_G_feat_match_loss:\u001b[91m 0.11152 \u001b[0m(+0.00000)\n",
            "     | > avg_G_gen_loss:\u001b[92m 26.17970 \u001b[0m(-0.00000)\n",
            "     | > avg_G_adv_loss:\u001b[91m 1.81714 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_0:\u001b[92m 27.99685 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_loss:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_real_loss:\u001b[92m 0.13729 \u001b[0m(-0.00000)\n",
            "     | > avg_D_mse_gan_fake_loss:\u001b[91m 0.00057 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_1:\u001b[92m 0.27955 \u001b[0m(-0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 43/10000\u001b[0m\n",
            " --> /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-12-22 18:35:46) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/364 -- GLOBAL_STEP: 195700\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.51560  (0.56813)\n",
            "     | > G_mse_fake_loss: 0.78183  (0.75397)\n",
            "     | > G_feat_match_loss: 0.10822  (0.11246)\n",
            "     | > G_gen_loss: 23.20199  (25.56603)\n",
            "     | > G_adv_loss: 1.86408  (1.87853)\n",
            "     | > loss_0: 25.06607  (27.44456)\n",
            "     | > grad_norm_0: 734.25684  (967.25775)\n",
            "     | > D_mse_gan_loss: 0.14485  (0.12330)\n",
            "     | > D_mse_gan_real_loss: 0.00050  (0.00013)\n",
            "     | > D_mse_gan_fake_loss: 0.00004  (0.00016)\n",
            "     | > loss_1: 0.14485  (0.12330)\n",
            "     | > grad_norm_1: 35.56353  (21.44401)\n",
            "     | > current_lr_0: 1.5056729082083009e-12 \n",
            "     | > current_lr_1: 1.5056729082083009e-12 \n",
            "     | > step_time: 2.42050  (2.47062)\n",
            "     | > loader_time: 0.00080  (0.00152)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 30/364 -- GLOBAL_STEP: 195725\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52305  (0.56896)\n",
            "     | > G_mse_fake_loss: 0.75698  (0.75247)\n",
            "     | > G_feat_match_loss: 0.10850  (0.11413)\n",
            "     | > G_gen_loss: 23.53715  (25.60327)\n",
            "     | > G_adv_loss: 1.84194  (1.89381)\n",
            "     | > loss_0: 25.37909  (27.49707)\n",
            "     | > grad_norm_0: 805.44196  (953.02179)\n",
            "     | > D_mse_gan_loss: 0.13795  (0.12010)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00025)\n",
            "     | > D_mse_gan_fake_loss: 0.00009  (0.00012)\n",
            "     | > loss_1: 0.13795  (0.12010)\n",
            "     | > grad_norm_1: 19.11443  (22.25558)\n",
            "     | > current_lr_0: 1.4684793432948988e-12 \n",
            "     | > current_lr_1: 1.4684793432948988e-12 \n",
            "     | > step_time: 2.42190  (2.43293)\n",
            "     | > loader_time: 0.00210  (0.00195)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 55/364 -- GLOBAL_STEP: 195750\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.52590  (0.56853)\n",
            "     | > G_mse_fake_loss: 0.76117  (0.75011)\n",
            "     | > G_feat_match_loss: 0.11332  (0.11412)\n",
            "     | > G_gen_loss: 23.66538  (25.58386)\n",
            "     | > G_adv_loss: 1.89437  (1.89132)\n",
            "     | > loss_0: 25.55975  (27.47518)\n",
            "     | > grad_norm_0: 873.71478  (947.55847)\n",
            "     | > D_mse_gan_loss: 0.12717  (0.12059)\n",
            "     | > D_mse_gan_real_loss: 0.00001  (0.00024)\n",
            "     | > D_mse_gan_fake_loss: 0.00005  (0.00019)\n",
            "     | > loss_1: 0.12717  (0.12059)\n",
            "     | > grad_norm_1: 7.23942  (22.54488)\n",
            "     | > current_lr_0: 1.4322045445115285e-12 \n",
            "     | > current_lr_1: 1.4322045445115285e-12 \n",
            "     | > step_time: 2.41600  (2.42923)\n",
            "     | > loader_time: 0.00200  (0.00202)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 80/364 -- GLOBAL_STEP: 195775\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.65064  (0.56888)\n",
            "     | > G_mse_fake_loss: 0.71435  (0.75078)\n",
            "     | > G_feat_match_loss: 0.12221  (0.11432)\n",
            "     | > G_gen_loss: 29.27864  (25.59957)\n",
            "     | > G_adv_loss: 1.93649  (1.89401)\n",
            "     | > loss_0: 31.21512  (27.49358)\n",
            "     | > grad_norm_0: 804.68158  (940.04315)\n",
            "     | > D_mse_gan_loss: 0.11522  (0.12002)\n",
            "     | > D_mse_gan_real_loss: 0.00003  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00015  (0.00017)\n",
            "     | > loss_1: 0.11522  (0.12002)\n",
            "     | > grad_norm_1: 46.24423  (22.60622)\n",
            "     | > current_lr_0: 1.396825816233189e-12 \n",
            "     | > current_lr_1: 1.396825816233189e-12 \n",
            "     | > step_time: 2.41950  (2.42749)\n",
            "     | > loader_time: 0.00210  (0.00209)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 105/364 -- GLOBAL_STEP: 195800\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53301  (0.56773)\n",
            "     | > G_mse_fake_loss: 0.75507  (0.75147)\n",
            "     | > G_feat_match_loss: 0.11212  (0.11424)\n",
            "     | > G_gen_loss: 23.98535  (25.54795)\n",
            "     | > G_adv_loss: 1.87626  (1.89389)\n",
            "     | > loss_0: 25.86161  (27.44185)\n",
            "     | > grad_norm_0: 736.18475  (930.41028)\n",
            "     | > D_mse_gan_loss: 0.12672  (0.11997)\n",
            "     | > D_mse_gan_real_loss: 0.00005  (0.00021)\n",
            "     | > D_mse_gan_fake_loss: 0.00066  (0.00019)\n",
            "     | > loss_1: 0.12672  (0.11997)\n",
            "     | > grad_norm_1: 10.31138  (22.20177)\n",
            "     | > current_lr_0: 1.3623210234687311e-12 \n",
            "     | > current_lr_1: 1.3623210234687311e-12 \n",
            "     | > step_time: 2.42350  (2.42658)\n",
            "     | > loader_time: 0.00200  (0.00211)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 130/364 -- GLOBAL_STEP: 195825\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.60970  (0.56825)\n",
            "     | > G_mse_fake_loss: 0.74118  (0.75174)\n",
            "     | > G_feat_match_loss: 0.11665  (0.11431)\n",
            "     | > G_gen_loss: 27.43661  (25.57138)\n",
            "     | > G_adv_loss: 1.90772  (1.89486)\n",
            "     | > loss_0: 29.34433  (27.46624)\n",
            "     | > grad_norm_0: 1135.04504  (937.59235)\n",
            "     | > D_mse_gan_loss: 0.11330  (0.11975)\n",
            "     | > D_mse_gan_real_loss: 0.00010  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00075  (0.00021)\n",
            "     | > loss_1: 0.11330  (0.11975)\n",
            "     | > grad_norm_1: 20.10397  (22.53456)\n",
            "     | > current_lr_0: 1.328668578011921e-12 \n",
            "     | > current_lr_1: 1.328668578011921e-12 \n",
            "     | > step_time: 2.42190  (2.42602)\n",
            "     | > loader_time: 0.00200  (0.00214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 155/364 -- GLOBAL_STEP: 195850\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.57683  (0.56927)\n",
            "     | > G_mse_fake_loss: 0.73637  (0.75115)\n",
            "     | > G_feat_match_loss: 0.12027  (0.11431)\n",
            "     | > G_gen_loss: 25.95732  (25.61699)\n",
            "     | > G_adv_loss: 1.93907  (1.89425)\n",
            "     | > loss_0: 27.89639  (27.51124)\n",
            "     | > grad_norm_0: 625.49200  (937.38245)\n",
            "     | > D_mse_gan_loss: 0.11571  (0.11987)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00020)\n",
            "     | > D_mse_gan_fake_loss: 0.00006  (0.00020)\n",
            "     | > loss_1: 0.11571  (0.11987)\n",
            "     | > grad_norm_1: 32.60339  (22.87293)\n",
            "     | > current_lr_0: 1.2958474249345967e-12 \n",
            "     | > current_lr_1: 1.2958474249345967e-12 \n",
            "     | > step_time: 2.42800  (2.42596)\n",
            "     | > loader_time: 0.00230  (0.00215)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 180/364 -- GLOBAL_STEP: 195875\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.59100  (0.56970)\n",
            "     | > G_mse_fake_loss: 0.74032  (0.75145)\n",
            "     | > G_feat_match_loss: 0.10611  (0.11448)\n",
            "     | > G_gen_loss: 26.59496  (25.63632)\n",
            "     | > G_adv_loss: 1.80145  (1.89628)\n",
            "     | > loss_0: 28.39641  (27.53260)\n",
            "     | > grad_norm_0: 1426.42847  (941.06073)\n",
            "     | > D_mse_gan_loss: 0.13321  (0.11949)\n",
            "     | > D_mse_gan_real_loss: 0.00006  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00003  (0.00019)\n",
            "     | > loss_1: 0.13321  (0.11949)\n",
            "     | > grad_norm_1: 25.31072  (22.92708)\n",
            "     | > current_lr_0: 1.2638370294134853e-12 \n",
            "     | > current_lr_1: 1.2638370294134853e-12 \n",
            "     | > step_time: 2.42680  (2.42590)\n",
            "     | > loader_time: 0.00200  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 205/364 -- GLOBAL_STEP: 195900\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53816  (0.56923)\n",
            "     | > G_mse_fake_loss: 0.75916  (0.75162)\n",
            "     | > G_feat_match_loss: 0.11335  (0.11451)\n",
            "     | > G_gen_loss: 24.21724  (25.61521)\n",
            "     | > G_adv_loss: 1.89266  (1.89676)\n",
            "     | > loss_0: 26.10989  (27.51197)\n",
            "     | > grad_norm_0: 913.63599  (938.21039)\n",
            "     | > D_mse_gan_loss: 0.13594  (0.11934)\n",
            "     | > D_mse_gan_real_loss: 0.00136  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00012  (0.00018)\n",
            "     | > loss_1: 0.13594  (0.11934)\n",
            "     | > grad_norm_1: 10.01451  (22.94907)\n",
            "     | > current_lr_0: 1.2326173638824185e-12 \n",
            "     | > current_lr_1: 1.2326173638824185e-12 \n",
            "     | > step_time: 2.41960  (2.42570)\n",
            "     | > loader_time: 0.00220  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 230/364 -- GLOBAL_STEP: 195925\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53953  (0.56940)\n",
            "     | > G_mse_fake_loss: 0.75444  (0.75194)\n",
            "     | > G_feat_match_loss: 0.11458  (0.11464)\n",
            "     | > G_gen_loss: 24.27867  (25.62296)\n",
            "     | > G_adv_loss: 1.90029  (1.89834)\n",
            "     | > loss_0: 26.17896  (27.52130)\n",
            "     | > grad_norm_0: 876.93353  (933.49402)\n",
            "     | > D_mse_gan_loss: 0.11962  (0.11903)\n",
            "     | > D_mse_gan_real_loss: 0.00007  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00018)\n",
            "     | > loss_1: 0.11962  (0.11903)\n",
            "     | > grad_norm_1: 5.18918  (22.65597)\n",
            "     | > current_lr_0: 1.2021688955019246e-12 \n",
            "     | > current_lr_1: 1.2021688955019246e-12 \n",
            "     | > step_time: 2.42570  (2.42554)\n",
            "     | > loader_time: 0.00220  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 255/364 -- GLOBAL_STEP: 195950\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.53889  (0.56947)\n",
            "     | > G_mse_fake_loss: 0.78078  (0.75171)\n",
            "     | > G_feat_match_loss: 0.11948  (0.11465)\n",
            "     | > G_gen_loss: 24.25026  (25.62627)\n",
            "     | > G_adv_loss: 1.97563  (1.89821)\n",
            "     | > loss_0: 26.22589  (27.52447)\n",
            "     | > grad_norm_0: 773.16730  (933.41016)\n",
            "     | > D_mse_gan_loss: 0.10945  (0.11909)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00018)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00019)\n",
            "     | > loss_1: 0.10945  (0.11909)\n",
            "     | > grad_norm_1: 5.44526  (22.56207)\n",
            "     | > current_lr_0: 1.1724725739383448e-12 \n",
            "     | > current_lr_1: 1.1724725739383448e-12 \n",
            "     | > step_time: 2.42380  (2.42552)\n",
            "     | > loader_time: 0.00190  (0.00216)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 280/364 -- GLOBAL_STEP: 195975\u001b[0m\n",
            "     | > G_l1_spec_loss: 0.63537  (0.56950)\n",
            "     | > G_mse_fake_loss: 0.72144  (0.75153)\n",
            "     | > G_feat_match_loss: 0.12253  (0.11463)\n",
            "     | > G_gen_loss: 28.59165  (25.62772)\n",
            "     | > G_adv_loss: 1.94669  (1.89782)\n",
            "     | > loss_0: 30.53834  (27.52554)\n",
            "     | > grad_norm_0: 732.99481  (929.59528)\n",
            "     | > D_mse_gan_loss: 0.11398  (0.11914)\n",
            "     | > D_mse_gan_real_loss: 0.00002  (0.00017)\n",
            "     | > D_mse_gan_fake_loss: 0.00007  (0.00019)\n",
            "     | > loss_1: 0.11398  (0.11914)\n",
            "     | > grad_norm_1: 45.14587  (22.96139)\n",
            "     | > current_lr_0: 1.1435098194448391e-12 \n",
            "     | > current_lr_1: 1.1435098194448391e-12 \n",
            "     | > step_time: 2.42500  (2.42551)\n",
            "     | > loader_time: 0.00230  (0.00216)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Emergent/train/hifigan/Copyhifigan.py \\\n",
        "    --continue_path /content/drive/MyDrive/Emergent/train/hifigan/coqui_tts-December-02-2021_07+40PM-33aa27e2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "24DB New Test Successful Colab .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
