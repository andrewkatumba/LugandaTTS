{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Successful Colab trial",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siYsG7OCqLCu",
        "outputId": "cd928a9e-3ff4-469e-b1e3-8578bb896c70"
      },
      "source": [
        "!git clone https://github.com/coqui-ai/TTS/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TTS'...\n",
            "remote: Enumerating objects: 19006, done.\u001b[K\n",
            "remote: Counting objects: 100% (736/736), done.\u001b[K\n",
            "remote: Compressing objects: 100% (325/325), done.\u001b[K\n",
            "remote: Total 19006 (delta 440), reused 633 (delta 393), pack-reused 18270\u001b[K\n",
            "Receiving objects: 100% (19006/19006), 126.71 MiB | 30.86 MiB/s, done.\n",
            "Resolving deltas: 100% (13555/13555), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkdP8yGHcoQg",
        "outputId": "c81cc1ad-014e-4c16-db1c-21bc407e7f67"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pPFQZ1XUyfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05da80b2-649a-4c48-98d8-0e633bf5539d"
      },
      "source": [
        "! sudo apt-get install espeak"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak espeak-data libespeak1 libportaudio2 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 1,219 kB of archives.\n",
            "After this operation, 3,031 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-data amd64 1.48.04+dfsg-5 [934 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libespeak1 amd64 1.48.04+dfsg-5 [145 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak amd64 1.48.04+dfsg-5 [61.6 kB]\n",
            "Fetched 1,219 kB in 1s (859 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-6) ...\n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "Preparing to unpack .../espeak-data_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../libespeak1_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package espeak.\n",
            "Preparing to unpack .../espeak_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak (1.48.04+dfsg-5) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-6) ...\n",
            "Setting up libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up espeak (1.48.04+dfsg-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4VyGXgPqP_y",
        "outputId": "13c50734-bd39-4f8d-cfa4-04f07d279f95"
      },
      "source": [
        "%cd TTS"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-V4eaibqVAk",
        "outputId": "80c1ed06-2886-486a-8d5a-e83a9b544535"
      },
      "source": [
        "!make system-deps"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sudo apt-get install -y libsndfile1-dev\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsndfile1-dev is already the newest version (1.0.28-4ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWr2EJOHqgQi",
        "outputId": "91cc8581-760c-4af7-a7cc-ab2311d65788"
      },
      "source": [
        "!make install"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pip install -e .[all]\n",
            "Obtaining file:///content/TTS\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (3.13)\n",
            "Collecting mecab-python3==1.0.3\n",
            "  Downloading mecab_python3-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (487 kB)\n",
            "\u001b[K     |████████████████████████████████| 487 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (0.42.1)\n",
            "Collecting umap-learn==0.5.1\n",
            "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (0.10.3.post1)\n",
            "Collecting pysbd\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting pypinyin\n",
            "  Downloading pypinyin-0.42.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 41.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (1.9.0+cu102)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (1.1.5)\n",
            "Collecting coqpit\n",
            "  Downloading coqpit-0.0.10.tar.gz (16 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (1.4.1)\n",
            "Collecting unidic-lite==1.0.8\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4 MB 48 kB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (0.29.23)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (2.1.0)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 61.0 MB/s \n",
            "\u001b[?25hCollecting librosa==0.8.0\n",
            "  Downloading librosa-0.8.0.tar.gz (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 60.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (3.6.4)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (1.1.4)\n",
            "Collecting gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=1.2.0\n",
            "  Downloading gruut-1.2.3.tar.gz (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (4.41.1)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.2.0-py3-none-any.whl (283 kB)\n",
            "\u001b[K     |████████████████████████████████| 283 kB 50.1 MB/s \n",
            "\u001b[?25hCollecting numba==0.53\n",
            "  Downloading numba-0.53.0-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (1.19.5)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 55.5 MB/s \n",
            "\u001b[?25hCollecting bokeh==1.4.0\n",
            "  Downloading bokeh-1.4.0.tar.gz (32.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.4 MB 71 kB/s \n",
            "\u001b[?25hCollecting pylint==2.8.3\n",
            "  Downloading pylint-2.8.3-py3-none-any.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting black\n",
            "  Downloading black-21.7b0-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 75.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow==2.5.0 in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (2.5.0)\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from TTS==0.1.3) (3.7.1)\n",
            "Collecting isort\n",
            "  Downloading isort-5.9.3-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 66.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from bokeh==1.4.0->TTS==0.1.3) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh==1.4.0->TTS==0.1.3) (2.8.1)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.7/dist-packages (from bokeh==1.4.0->TTS==0.1.3) (2.11.3)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.7/dist-packages (from bokeh==1.4.0->TTS==0.1.3) (7.1.2)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh==1.4.0->TTS==0.1.3) (21.0)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.7/dist-packages (from bokeh==1.4.0->TTS==0.1.3) (5.1.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.1.3) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.1.3) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.1.3) (1.0.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.1.3) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.1.3) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->TTS==0.1.3) (1.4.0)\n",
            "Collecting llvmlite<0.37,>=0.36.0rc1\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 76 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.53->TTS==0.1.3) (57.2.0)\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint==2.8.3->TTS==0.1.3) (0.10.2)\n",
            "Collecting mccabe<0.7,>=0.6\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting astroid==2.5.6\n",
            "  Downloading astroid-2.5.6-py3-none-any.whl (219 kB)\n",
            "\u001b[K     |████████████████████████████████| 219 kB 58.3 MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0\n",
            "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 69.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt<1.13,>=1.11 in /usr/local/lib/python3.7/dist-packages (from astroid==2.5.6->pylint==2.8.3->TTS==0.1.3) (1.12.1)\n",
            "Collecting lazy-object-proxy>=1.4.0\n",
            "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (3.3.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (1.34.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (0.36.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (3.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (0.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (1.12)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (3.17.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (1.6.3)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->TTS==0.1.3) (2.5.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.4.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 55.9 MB/s \n",
            "\u001b[?25hCollecting Babel~=2.8.0\n",
            "  Downloading Babel-2.8.1-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 22.3 MB/s \n",
            "\u001b[?25hCollecting gruut-ipa~=0.9.0\n",
            "  Downloading gruut-ipa-0.9.3.tar.gz (34 kB)\n",
            "Collecting jsonlines~=1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting num2words==0.5.10\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.3 MB/s \n",
            "\u001b[?25hCollecting python-crfsuite~=0.9.7\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 61.8 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_it~=1.2.0\n",
            "  Downloading gruut_lang_it-1.2.tar.gz (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 45.3 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_ru~=1.2.0\n",
            "  Downloading gruut_lang_ru-1.2.tar.gz (16.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.9 MB 158 kB/s \n",
            "\u001b[?25hCollecting gruut_lang_cs~=1.2.0\n",
            "  Downloading gruut_lang_cs-1.2.tar.gz (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 49.0 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_nl~=1.2.0\n",
            "  Downloading gruut_lang_nl-1.2.tar.gz (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 53.5 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_es~=1.2.0\n",
            "  Downloading gruut_lang_es-1.2.tar.gz (15.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.2 MB 116 kB/s \n",
            "\u001b[?25hCollecting gruut_lang_fr~=1.2.0\n",
            "  Downloading gruut_lang_fr-1.2.1.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 176 kB/s \n",
            "\u001b[?25hCollecting gruut_lang_sv~=1.2.0\n",
            "  Downloading gruut_lang_sv-1.2.tar.gz (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 40.8 MB/s \n",
            "\u001b[?25hCollecting gruut_lang_de~=1.2.0\n",
            "  Downloading gruut_lang_de-1.2.tar.gz (9.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 110 kB/s \n",
            "\u001b[?25hCollecting gruut_lang_pt~=1.2.0\n",
            "  Downloading gruut_lang_pt-1.2.tar.gz (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words==0.5.10->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=1.2.0->TTS==0.1.3) (0.6.2)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from Babel~=2.8.0->gruut[cs,de,es,fr,it,nl,pt,ru,sv]~=1.2.0->TTS==0.1.3) (2018.9)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0->TTS==0.1.3) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.7->bokeh==1.4.0->TTS==0.1.3) (2.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh==1.4.0->TTS==0.1.3) (2.4.7)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS==0.1.3) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS==0.1.3) (2.23.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->TTS==0.1.3) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->TTS==0.1.3) (2.20)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (1.32.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS==0.1.3) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS==0.1.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS==0.1.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa==0.8.0->TTS==0.1.3) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (3.1.1)\n",
            "Collecting tomli<2.0.0,>=0.2.6\n",
            "  Downloading tomli-1.2.0-py3-none-any.whl (11 kB)\n",
            "Collecting pathspec<1,>=0.8.1\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting regex>=2020.1.8\n",
            "  Downloading regex-2021.8.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black->TTS==0.1.3) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->TTS==0.1.3) (1.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->TTS==0.1.3) (3.5.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS==0.1.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->TTS==0.1.3) (0.10.0)\n",
            "Building wheels for collected packages: bokeh, librosa, umap-learn, unidic-lite, gruut, gruut-ipa, gruut-lang-cs, gruut-lang-de, gruut-lang-es, gruut-lang-fr, gruut-lang-it, gruut-lang-nl, gruut-lang-pt, gruut-lang-ru, gruut-lang-sv, pynndescent, coqpit\n",
            "  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bokeh: filename=bokeh-1.4.0-py3-none-any.whl size=23689209 sha256=64d3c47fb0b2318d10a8c6214e61941a32bdc5d1a9c1ecda75995d0abb9a4499\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/8c/d1/6b8e1f57e542671673cb3d2faee1a9eccb36be2c08a3915498\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.8.0-py3-none-any.whl size=201395 sha256=378e602989c7519f9da3a64b032efc6dad67a93a8df9fbdccf227479d7171bb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/1e/aa/d91797ae7e1ce11853ee100bee9d1781ae9d750e7458c95afb\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76566 sha256=cbf39ff80f51333c097a6b6d21a4c4749350d85430cc89c00014b3143a8d8f91\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e7/bb/347dc0e510803d7116a13d592b10cc68262da56a8eec4dd72f\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658837 sha256=5aaa9909eb062e2ab7f8daaa0ad15f08f87351a3cb10c1b51c568974c3d94b18\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/69/b1/112140b599f2b13f609d485a99e357ba68df194d2079c5b1a2\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-1.2.3-py3-none-any.whl size=11091277 sha256=91a730d173fbc0a72b045577d3d48470f7722bba1595ee9ba91bec0d7ab2c077\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/b4/52/94a0c0762e55a284000bc16c089a4239c600749a5b9f0f31ad\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.9.3-py3-none-any.whl size=39000 sha256=afd4197f9c6c605c7d0394cead1fae15341a4cbc85cce7e22698bfeb45676690\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/54/14/3e4f28f11774f67536576662cb932b3cac8428e26be86b3410\n",
            "  Building wheel for gruut-lang-cs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-cs: filename=gruut_lang_cs-1.2-py3-none-any.whl size=5796594 sha256=d58720e0058da0a45000a8c7fac66469288066e55ebf86b8b3231167c77f16a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/c6/7a/44b8bab3aedb5b889ef425c2e516019c7e9e9869125d3c2dc6\n",
            "  Building wheel for gruut-lang-de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-de: filename=gruut_lang_de-1.2-py3-none-any.whl size=9574679 sha256=6766fa267d2bc47190905ed57ea5e73fb3a581b4d6874e099f72ded942e48c6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/1f/2c/a23ce1aac2c09cf617df745642d280e0be6f894862f63c05b4\n",
            "  Building wheel for gruut-lang-es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-es: filename=gruut_lang_es-1.2-py3-none-any.whl size=15403813 sha256=d3b03cffb4d3a5a3bc898c7758b7067f93dbebedced1a33b983e77fe4f58794d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/57/11/d4a2445ff6dd3a228db36548f95c1f763d3defbff2b7fcdb85\n",
            "  Building wheel for gruut-lang-fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-fr: filename=gruut_lang_fr-1.2.1-py3-none-any.whl size=12103214 sha256=618bb674411326348358fec0d7dd8590ebf2bc251c9a3902cb8429b78c9cd460\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/62/e7/e103446f6db651133ac6871bfe0a6a582f00a74a1f0c44ca2f\n",
            "  Building wheel for gruut-lang-it (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-it: filename=gruut_lang_it-1.2-py3-none-any.whl size=1944815 sha256=8b5751a55c9ef1c84de94c3cf7d8c65b163920d658f4dfcdd4e2566276864d18\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/f4/2d/0dc17ee06d958a37a6a873d7ba15bb90f710e307941dc8928f\n",
            "  Building wheel for gruut-lang-nl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-nl: filename=gruut_lang_nl-1.2-py3-none-any.whl size=7399312 sha256=016d700f8a031ac6fda9937389fe27702a53f6b683cb203fcf75fee2c5db8a52\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/c0/d8/125b811bf3df62785bffbfdccafe9d414ebc4770a35080669b\n",
            "  Building wheel for gruut-lang-pt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-pt: filename=gruut_lang_pt-1.2-py3-none-any.whl size=3412232 sha256=1af01cffa984a4af3cf96c49b6c7e0712e903a1d513b4168b022c2365c80191b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/c6/e2/458765db8d0ded8b33a1fc7551d28e5ae06b450ff5af0a585c\n",
            "  Building wheel for gruut-lang-ru (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-ru: filename=gruut_lang_ru-1.2-py3-none-any.whl size=16955788 sha256=5ceddad209d4c48bdd8c10b9029e3054dec30be57bb2ba74d4bb17ee8a7047a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/36/50/5375078bb647157ed425620b69fc87ba9830161886c600d175\n",
            "  Building wheel for gruut-lang-sv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-lang-sv: filename=gruut_lang_sv-1.2-py3-none-any.whl size=2104833 sha256=9f65cc12c0f87c3a89267b29e5ab764bcbc9b7ffef8a4acf76248c2be24b4778\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/2c/0b/a087fc907ed77d1dde8df72ab70fffde05ea8bcf06b33b1acb\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.4-py3-none-any.whl size=52372 sha256=3abd5c7041002ce74ba5b8722957036b16187a22d803da8baf17a5cb5123121a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/5b/62/3401692ddad12324249c774c4b15ccb046946021e2b581c043\n",
            "  Building wheel for coqpit (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for coqpit: filename=coqpit-0.0.10-py3-none-any.whl size=12892 sha256=cb9276eeb255e5fc6aaa8c8911f5ead794a8e241c5cf40a2e63168e84ea9d358\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/7b/f5/df89ad4facce9dcabaf226bd31ded222d8bd861df69c3c31f5\n",
            "Successfully built bokeh librosa umap-learn unidic-lite gruut gruut-ipa gruut-lang-cs gruut-lang-de gruut-lang-es gruut-lang-fr gruut-lang-it gruut-lang-nl gruut-lang-pt gruut-lang-ru gruut-lang-sv pynndescent coqpit\n",
            "Installing collected packages: llvmlite, python-crfsuite, numba, num2words, jsonlines, gruut-ipa, Babel, typed-ast, pynndescent, lazy-object-proxy, gruut-lang-sv, gruut-lang-ru, gruut-lang-pt, gruut-lang-nl, gruut-lang-it, gruut-lang-fr, gruut-lang-es, gruut-lang-de, gruut-lang-cs, gruut, unidic-lite, umap-learn, tomli, tensorboardX, regex, pysbd, pypinyin, pathspec, mypy-extensions, mecab-python3, mccabe, librosa, isort, coqpit, astroid, anyascii, TTS, pylint, nose, bokeh, black\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: Babel\n",
            "    Found existing installation: Babel 2.9.1\n",
            "    Uninstalling Babel-2.9.1:\n",
            "      Successfully uninstalled Babel-2.9.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.1\n",
            "    Uninstalling librosa-0.8.1:\n",
            "      Successfully uninstalled librosa-0.8.1\n",
            "  Running setup.py develop for TTS\n",
            "  Attempting uninstall: bokeh\n",
            "    Found existing installation: bokeh 2.3.3\n",
            "    Uninstalling bokeh-2.3.3:\n",
            "      Successfully uninstalled bokeh-2.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.11.3 requires bokeh<2.4.0,>=2.3.0, but you have bokeh 1.4.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Babel-2.8.1 TTS-0.1.3 anyascii-0.2.0 astroid-2.5.6 black-21.7b0 bokeh-1.4.0 coqpit-0.0.10 gruut-1.2.3 gruut-ipa-0.9.3 gruut-lang-cs-1.2 gruut-lang-de-1.2 gruut-lang-es-1.2 gruut-lang-fr-1.2.1 gruut-lang-it-1.2 gruut-lang-nl-1.2 gruut-lang-pt-1.2 gruut-lang-ru-1.2 gruut-lang-sv-1.2 isort-5.9.3 jsonlines-1.2.0 lazy-object-proxy-1.6.0 librosa-0.8.0 llvmlite-0.36.0 mccabe-0.6.1 mecab-python3-1.0.3 mypy-extensions-0.4.3 nose-1.3.7 num2words-0.5.10 numba-0.53.0 pathspec-0.9.0 pylint-2.8.3 pynndescent-0.5.4 pypinyin-0.42.0 pysbd-0.3.4 python-crfsuite-0.9.7 regex-2021.8.3 tensorboardX-2.4 tomli-1.2.0 typed-ast-1.4.3 umap-learn-0.5.1 unidic-lite-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SePa1pAy3DvZ",
        "outputId": "bb21d832-80be-45bc-f717-1f79da80c793"
      },
      "source": [
        "!tts --list_models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Name format: type/language/dataset/model\n",
            " >: tts_models/en/ek1/tacotron2\n",
            " >: tts_models/en/ljspeech/tacotron2-DDC\n",
            " >: tts_models/en/ljspeech/glow-tts\n",
            " >: tts_models/en/ljspeech/tacotron2-DCA\n",
            " >: tts_models/en/ljspeech/speedy-speech-wn\n",
            " >: tts_models/en/vctk/sc-glow-tts\n",
            " >: tts_models/en/sam/tacotron-DDC\n",
            " >: tts_models/es/mai/tacotron2-DDC\n",
            " >: tts_models/fr/mai/tacotron2-DDC\n",
            " >: tts_models/zh-CN/baker/tacotron2-DDC-GST\n",
            " >: tts_models/nl/mai/tacotron2-DDC\n",
            " >: tts_models/ru/ruslan/tacotron2-DDC\n",
            " >: tts_models/de/thorsten/tacotron2-DCA\n",
            " >: tts_models/ja/kokoro/tacotron2-DDC\n",
            " >: vocoder_models/universal/libri-tts/wavegrad\n",
            " >: vocoder_models/universal/libri-tts/fullband-melgan\n",
            " >: vocoder_models/en/ek1/wavegrad\n",
            " >: vocoder_models/en/ljspeech/multiband-melgan\n",
            " >: vocoder_models/en/ljspeech/hifigan_v2\n",
            " >: vocoder_models/en/vctk/hifigan_v2\n",
            " >: vocoder_models/en/sam/hifigan_v2\n",
            " >: vocoder_models/nl/mai/parallel-wavegan\n",
            " >: vocoder_models/de/thorsten/wavegrad\n",
            " >: vocoder_models/de/thorsten/fullband-melgan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp487xJ14Nyd",
        "outputId": "a009ecae-d520-471e-dede-74c5eaeb1b6e"
      },
      "source": [
        "!tts --text \"Enabling a good quality output from our set needs the best also like to know how you recommend I approach this problem of cross-language voice cloning I'm tackling. This is a long term project for me and not a one-time hack, so I'd like to know the best approach for long-term productionizing. I'm basically trying to dub speeches/lectures from English to other languages.\" \\\n",
        "      --model_name tts_models/en/ljspeech/glow-tts \\\n",
        "      --out_path ../output.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " > Downloading model to /root/.local/share/tts/tts_models--en--ljspeech--glow-tts\n",
            " > Downloading model to /root/.local/share/tts/vocoder_models--en--ljspeech--multiband-melgan\n",
            " > Using model: glow_tts\n",
            " > Vocoder Model: multiband_melgan\n",
            " > Generator Model: multiband_melgan_generator\n",
            " > Discriminator Model: melgan_multiscale_discriminator\n",
            " > Text: Enabling a good quality output from our set needs the best also like to know how you recommend I approach this problem of cross-language voice cloning I'm tackling. This is a long term project for me and not a one-time hack, so I'd like to know the best approach for long-term productionizing. I'm basically trying to dub speeches/lectures from English to other languages.\n",
            " > Text splitted to sentences.\n",
            "[\"Enabling a good quality output from our set needs the best also like to know how you recommend I approach this problem of cross-language voice cloning I'm tackling.\", \"This is a long term project for me and not a one-time hack, so I'd like to know the best approach for long-term productionizing.\", \"I'm basically trying to dub speeches/lectures from English to other languages.\"]\n",
            " > Phonemes: ɛ|n|eɪ|b|l|ɪ|ŋ| ɐ| ɡ|ʊ|d| k|w|ɔ|l|ɪ|ɾ|i| aʊ|t|p|ʊ|t| f|ɹ|ʌ|m| aʊ|ɚ| s|ɛ|t| n|iː|d|z| ð|ə| b|ɛ|s|t| ɔː|l|s|oʊ| l|aɪ|k| t|uː| n|oʊ| h|aʊ| j|uː| ɹ|ɛ|k|ə|m|ɛ|n|d| aɪ| ɐ|p|ɹ|oʊ|tʃ| ð|ɪ|s| p|ɹ|ɑː|b|l|ə|m| ʌ|v| k|ɹ|ɔ|s| l|æ|ŋ|ɡ|w|ɪ|dʒ| v|ɔɪ|s| k|l|oʊ|n|ɪ|ŋ| aɪ|m| t|æ|k|l|ɪ|ŋ| .\n",
            " > Phonemes: ð|ɪ|s| ɪ|z| ɐ| l|ɔ|ŋ| t|ɜː|m| p|ɹ|ɑː|dʒ|ɛ|k|t| f|ɔːɹ| m|iː| æ|n|d| n|ɑː|t| ɐ| w|ʌ|n| t|aɪ|m| h|æ|k| ,| s|oʊ| aɪ|d| l|aɪ|k| t|uː| n|oʊ| ð|ə| b|ɛ|s|t| ɐ|p|ɹ|oʊ|tʃ| f|ɔːɹ| l|ɔ|ŋ| t|ɜː|m| p|ɹ|ɑː|d|ʌ|k|ʃ|ə|n|aɪ|z|ɪ|ŋ| .\n",
            " > Phonemes: aɪ|m| b|eɪ|s|ɪ|k|l|i| t|ɹ|aɪ|ɪ|ŋ| t|uː| d|ʌ|b| s|p|iː|tʃ|ə|s|l|ɛ|k|tʃ|ɚ|z| f|ɹ|ʌ|m| ɪ|ŋ|ɡ|l|ɪ|ʃ| t|uː| ʌ|ð|ɚ| l|æ|ŋ|ɡ|w|ɪ|dʒ|ᵻ|z| .\n",
            " > Processing time: 9.701854228973389\n",
            " > Real-time factor: 0.3468588439905557\n",
            " > Saving output to ../output.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPJ-nyjLeeKs"
      },
      "source": [
        "!wget https://github.com/andrewkatumba/LugandaTTS/blob/main/plugins/config.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PIG94VPrRJd",
        "outputId": "d6297e2d-8025-4e34-d26b-684caeeff145"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN7uSS-DrZ3q",
        "outputId": "c5d9e5e0-a98c-4898-995c-9547eff6dc11"
      },
      "source": [
        "!wget http://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-04 14:00:48--  http://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "Resolving data.keithito.com (data.keithito.com)... 174.138.79.61\n",
            "Connecting to data.keithito.com (data.keithito.com)|174.138.79.61|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2748572632 (2.6G) [application/octet-stream]\n",
            "Saving to: ‘LJSpeech-1.1.tar.bz2’\n",
            "\n",
            "LJSpeech-1.1.tar.bz 100%[===================>]   2.56G  43.7MB/s    in 62s     \n",
            "\n",
            "2021-08-04 14:01:50 (42.5 MB/s) - ‘LJSpeech-1.1.tar.bz2’ saved [2748572632/2748572632]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppDRzsS6rjgh",
        "outputId": "363bf8ba-d49b-4410-aa87-bf3623bec511"
      },
      "source": [
        "!tar xvf LJSpeech-1.1.tar.bz2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LJSpeech-1.1/\n",
            "LJSpeech-1.1/metadata.csv\n",
            "LJSpeech-1.1/wavs/\n",
            "LJSpeech-1.1/wavs/LJ007-0048.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0060.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0133.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0220.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0274.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0043.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0129.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0265.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0418.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0116.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0143.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0083.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0004.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0181.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0056.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0053.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0004.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0180.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0237.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0136.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0208.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0026.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0009.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0056.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0130.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0015.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0158.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0029.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0293.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0084.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0236.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0008.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0132.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0250.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0230.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0012.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0271.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0173.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0199.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0014.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0193.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0168.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0020.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0042.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0150.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0189.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0102.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0273.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0044.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0234.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0032.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0021.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0037.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0211.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0169.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0148.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0259.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0175.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0210.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0119.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0080.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0289.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0183.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0184.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0149.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0299.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0143.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0116.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0057.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0117.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0167.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0112.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0118.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0125.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0263.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0057.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0080.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0133.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0015.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0058.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0298.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0216.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0023.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0289.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0282.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0128.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0025.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0020.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0092.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0143.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0201.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0013.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0096.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0103.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0238.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0302.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0198.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0018.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0273.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0201.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0147.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0235.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0198.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0297.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0161.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0037.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0137.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0113.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0176.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0192.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0057.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0155.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0213.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0020.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0148.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0204.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0267.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0200.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0270.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0278.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0099.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0202.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0163.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0098.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0396.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0047.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0016.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0065.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0175.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0331.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0089.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0168.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0047.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0013.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0191.wav\n",
            "LJSpeech-1.1/wavs/LJ023-0109.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0257.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0302.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0109.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0038.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0011.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0284.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0155.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0330.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0028.wav\n",
            "LJSpeech-1.1/wavs/LJ023-0103.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0038.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0091.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0229.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0289.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0048.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0009.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0095.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0232.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0164.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0233.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0007.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0266.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0519.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0040.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0204.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0286.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0116.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0225.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0020.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0272.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0079.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0178.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0112.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0315.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0002.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0077.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0016.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0080.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0297.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0104.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0273.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0123.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0203.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0065.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0305.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0138.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0230.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0163.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0049.wav\n",
            "LJSpeech-1.1/wavs/LJ023-0128.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0068.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0079.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0028.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0332.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0147.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0061.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0128.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0070.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0190.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0198.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0253.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0054.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0070.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0011.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0137.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0195.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0129.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0162.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0186.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0065.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0076.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0047.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0308.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0011.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0240.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0134.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0302.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0290.wav\n",
            "LJSpeech-1.1/wavs/LJ020-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0035.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0216.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0238.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0293.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0037.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0275.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0216.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0061.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0064.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0010.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0092.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0340.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0027.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0116.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0067.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0252.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0259.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0087.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0190.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0019.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0117.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0076.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0069.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0138.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0080.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0182.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0184.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0478.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0078.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0253.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0162.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0037.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0242.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0169.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0268.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0028.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0032.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0125.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0185.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0426.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0275.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0028.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0011.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0153.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0380.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0067.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0050.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0115.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0081.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0178.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0193.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0014.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0141.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0007.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0254.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0054.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0049.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0168.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0103.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0003.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0114.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0232.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0106.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0193.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0201.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0090.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0148.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0098.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0122.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0185.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0384.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0264.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0312.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0303.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0100.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0138.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0196.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0096.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0421.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0142.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0386.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0243.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0071.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0087.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0251.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0031.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0074.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0030.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0282.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0084.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0081.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0188.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0261.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0260.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0399.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0166.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0284.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0113.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0276.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0080.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0068.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0232.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0097.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0089.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0052.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0285.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0091.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0179.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0015.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0221.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0030.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0001.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0112.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0042.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0345.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0219.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0060.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0149.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0184.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0159.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0227.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0015.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0001.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0233.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0110.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0029.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0182.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0132.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0015.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0441.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0001.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0035.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0205.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0153.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0002.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0313.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0106.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0177.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0146.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0105.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0138.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0104.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0250.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0183.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0124.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0226.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0021.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0078.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0134.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0059.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0149.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0024.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0080.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0309.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0218.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0075.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0264.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0047.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0186.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0120.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0087.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0028.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0144.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0001.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0016.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0145.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0017.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0055.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0224.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0191.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0285.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0221.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0032.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0126.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0104.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0093.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0116.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0118.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0227.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0104.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0241.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0013.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0039.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0010.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0175.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0249.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0140.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0245.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0385.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0245.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0050.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0008.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0155.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0091.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0223.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0209.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0025.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0114.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0209.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0028.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0174.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0266.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0119.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0202.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0077.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0064.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0049.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0222.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0220.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0057.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0221.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0234.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0091.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0300.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0234.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0161.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0007.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0166.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0140.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0097.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0095.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0194.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0187.wav\n",
            "LJSpeech-1.1/wavs/LJ020-0083.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0189.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0086.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0128.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0077.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0263.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0122.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0025.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0097.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0005.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0390.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0032.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0210.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0309.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0216.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0162.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0153.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0233.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0117.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0200.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0271.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0180.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0152.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0175.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0195.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0084.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0115.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0212.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0241.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0125.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0092.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0248.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0067.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0156.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0202.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0222.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0064.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0296.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0043.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0165.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0099.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0158.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0094.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0160.wav\n",
            "LJSpeech-1.1/wavs/LJ020-0003.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0220.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0079.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0203.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0173.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0156.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0061.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0112.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0284.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0123.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0071.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0138.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0134.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0260.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0292.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0155.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0063.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0120.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0085.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0025.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0218.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0287.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0121.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0102.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0071.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0026.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0158.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0038.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0073.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0098.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0277.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0137.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0218.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0250.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0026.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0054.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0034.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0003.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0203.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0007.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0180.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0166.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0204.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0277.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0259.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0235.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0222.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0204.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0096.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0070.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0070.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0093.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0042.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0196.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0089.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0235.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0045.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0234.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0146.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0290.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0022.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0098.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0278.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0023.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0119.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0100.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0099.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0024.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0181.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0100.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0275.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0224.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0080.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0122.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0228.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0007.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0217.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0081.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0299.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0192.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0123.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0084.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0223.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0137.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0168.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0044.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0058.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0206.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0114.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0154.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0370.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0265.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0186.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0215.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0102.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0084.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0059.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0115.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0396.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0156.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0226.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0092.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0217.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0013.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0161.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0131.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0194.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0048.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0115.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0136.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0112.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0185.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0040.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0033.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0264.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0139.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0141.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0163.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0041.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0063.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0091.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0023.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0307.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0145.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0070.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0189.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0091.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0283.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0030.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0021.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0100.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0006.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0119.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0224.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0093.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0146.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0092.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0020.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0030.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0245.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0346.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0031.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0093.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0085.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0004.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0052.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0274.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0070.wav\n",
            "LJSpeech-1.1/wavs/LJ020-0049.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0002.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0019.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0161.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0195.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0096.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0030.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0163.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0175.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0229.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0120.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0189.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0131.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0118.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0274.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0157.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0123.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0170.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0443.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0236.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0260.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0070.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0154.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0215.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0200.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0012.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0210.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0037.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0078.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0020.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0172.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0247.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0006.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0206.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0061.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0077.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0307.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0170.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0070.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0164.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0078.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0242.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0162.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0074.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0288.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0173.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0292.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0192.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0102.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0184.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0284.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0203.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0171.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0058.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0064.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0196.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0087.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0202.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0006.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0051.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0047.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0062.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0268.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0169.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0314.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0033.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0246.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0049.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0033.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0331.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0242.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0057.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0281.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0012.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0235.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0149.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0108.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0299.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0154.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0113.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0210.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0228.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0193.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0228.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0027.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0361.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0050.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0244.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0268.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0121.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0023.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0033.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0291.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0204.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0133.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0142.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0333.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0075.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0062.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0024.wav\n",
            "LJSpeech-1.1/wavs/LJ023-0020.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0296.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0053.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0158.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0023.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0002.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0194.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0118.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0198.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0190.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0084.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0022.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0249.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0025.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0070.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0215.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0220.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0109.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0153.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0125.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0042.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0198.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0127.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0250.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0034.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0394.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0160.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0121.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0160.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0055.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0212.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0151.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0161.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0130.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0016.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0008.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0049.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0068.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0158.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0014.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0225.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0097.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0014.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0054.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0060.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0099.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0248.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0056.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0062.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0311.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0078.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0089.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0184.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0303.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0111.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0392.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0105.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0067.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0140.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0219.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0251.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0016.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0175.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0005.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0158.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0116.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0183.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0049.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0230.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0254.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0269.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0117.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0203.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0026.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0160.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0338.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0194.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0055.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0054.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0005.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0172.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0028.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0169.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0140.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0161.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0103.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0191.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0189.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0171.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0089.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0150.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0229.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0072.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0239.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0213.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0136.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0093.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0134.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0206.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0026.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0189.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0005.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0156.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0240.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0096.wav\n",
            "LJSpeech-1.1/wavs/LJ023-0129.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0001.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0202.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0022.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0025.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0225.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0213.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0240.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0044.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0165.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0083.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0044.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0082.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0325.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0079.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0212.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0022.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0250.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0089.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0038.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0256.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0186.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0023.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0211.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0063.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0058.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0249.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0136.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0162.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0022.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0193.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0207.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0010.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0081.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0051.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0241.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0350.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0029.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0022.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0188.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0182.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0135.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0195.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0070.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0124.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0268.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0288.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0072.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0082.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0108.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0250.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0231.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0332.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0221.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0261.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0277.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0141.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0192.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0121.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0023.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0207.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0232.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0009.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0219.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0257.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0233.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0148.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0155.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0235.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0282.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0024.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0187.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0012.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0180.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0004.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0181.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0283.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0039.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0092.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0012.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0222.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0045.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0191.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0135.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0364.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0231.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0056.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0247.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0280.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0028.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0176.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0402.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0154.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0005.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0259.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0049.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0210.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0037.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0158.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0165.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0252.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0071.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0142.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0222.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0099.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0267.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0245.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0322.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0025.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0185.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0188.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0152.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0134.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0128.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0205.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0293.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0051.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0132.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0087.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0018.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0021.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0268.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0241.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0084.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0102.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0117.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0142.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0217.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0009.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0150.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0065.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0099.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0206.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0108.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0116.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0083.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0169.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0262.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0009.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0187.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0376.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0002.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0192.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0003.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0332.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0161.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0240.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0216.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0162.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0269.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0292.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0239.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0176.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0006.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0095.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0117.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0016.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0125.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0089.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0255.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0328.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0089.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0113.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0249.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0132.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0112.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0253.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0006.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0080.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0043.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0224.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0145.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0298.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0122.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0250.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0168.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0125.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0188.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0140.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0114.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0038.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0096.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0199.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0335.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0078.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0277.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0002.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0246.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0128.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0015.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0195.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0180.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0074.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0234.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0051.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0245.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0049.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0139.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0218.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0165.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0259.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0175.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0013.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0113.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0158.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0071.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0176.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0018.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0143.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0164.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0051.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0017.wav\n",
            "LJSpeech-1.1/wavs/LJ010-0009.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0106.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0268.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0034.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0003.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0093.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0363.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0103.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0246.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0254.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0044.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0292.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0158.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0136.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0051.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0002.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0250.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0012.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0289.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0047.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0251.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0166.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0023.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0461.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0175.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0051.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0055.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0102.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0178.wav\n",
            "LJSpeech-1.1/wavs/LJ040-0192.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0097.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0116.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0189.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0199.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0007.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0242.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0231.wav\n",
            "LJSpeech-1.1/wavs/LJ030-0041.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0171.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0123.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0042.wav\n",
            "LJSpeech-1.1/wavs/LJ006-0190.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0134.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0046.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0252.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0071.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0001.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0137.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0029.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0008.wav\n",
            "LJSpeech-1.1/wavs/LJ037-0225.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0160.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0099.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0124.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0375.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0033.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0215.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0052.wav\n",
            "LJSpeech-1.1/wavs/LJ044-0071.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0194.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0077.wav\n",
            "LJSpeech-1.1/wavs/LJ043-0047.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0232.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0326.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0003.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0121.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0164.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0131.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0050.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0251.wav\n",
            "LJSpeech-1.1/wavs/LJ033-0009.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0165.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0026.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0117.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0025.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0225.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0244.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0045.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0091.wav\n",
            "LJSpeech-1.1/wavs/LJ001-0030.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0125.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0088.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0200.wav\n",
            "LJSpeech-1.1/wavs/LJ045-0196.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0060.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0135.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0051.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0059.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0222.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0106.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0083.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0063.wav\n",
            "LJSpeech-1.1/wavs/LJ050-0241.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0014.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0368.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0025.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0056.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0120.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0178.wav\n",
            "LJSpeech-1.1/wavs/LJ021-0115.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0151.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0043.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0255.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0118.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0296.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0007.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0280.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ035-0103.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0137.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0065.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0109.wav\n",
            "LJSpeech-1.1/wavs/LJ036-0115.wav\n",
            "LJSpeech-1.1/wavs/LJ016-0130.wav\n",
            "LJSpeech-1.1/wavs/LJ018-0357.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0146.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0249.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0148.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0276.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0162.wav\n",
            "LJSpeech-1.1/wavs/LJ004-0107.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0066.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0171.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0254.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0136.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0119.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0244.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0085.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0263.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0152.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0150.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0060.wav\n",
            "LJSpeech-1.1/wavs/LJ024-0080.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0019.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0031.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0139.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0188.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0077.wav\n",
            "LJSpeech-1.1/wavs/LJ029-0114.wav\n",
            "LJSpeech-1.1/wavs/LJ047-0109.wav\n",
            "LJSpeech-1.1/wavs/LJ048-0225.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0186.wav\n",
            "LJSpeech-1.1/wavs/LJ038-0263.wav\n",
            "LJSpeech-1.1/wavs/LJ042-0227.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0004.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0306.wav\n",
            "LJSpeech-1.1/wavs/LJ022-0034.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0218.wav\n",
            "LJSpeech-1.1/wavs/LJ002-0089.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0023.wav\n",
            "LJSpeech-1.1/wavs/LJ017-0032.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0089.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0226.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0076.wav\n",
            "LJSpeech-1.1/wavs/LJ049-0203.wav\n",
            "LJSpeech-1.1/wavs/LJ025-0008.wav\n",
            "LJSpeech-1.1/wavs/LJ003-0218.wav\n",
            "LJSpeech-1.1/wavs/LJ039-0181.wav\n",
            "LJSpeech-1.1/wavs/LJ011-0088.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0304.wav\n",
            "LJSpeech-1.1/wavs/LJ013-0078.wav\n",
            "LJSpeech-1.1/wavs/LJ019-0171.wav\n",
            "LJSpeech-1.1/wavs/LJ005-0299.wav\n",
            "LJSpeech-1.1/wavs/LJ031-0092.wav\n",
            "LJSpeech-1.1/wavs/LJ009-0152.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0271.wav\n",
            "LJSpeech-1.1/wavs/LJ034-0200.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0267.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0281.wav\n",
            "LJSpeech-1.1/wavs/LJ027-0036.wav\n",
            "LJSpeech-1.1/wavs/LJ007-0065.wav\n",
            "LJSpeech-1.1/wavs/LJ014-0098.wav\n",
            "LJSpeech-1.1/wavs/LJ012-0179.wav\n",
            "LJSpeech-1.1/wavs/LJ046-0058.wav\n",
            "LJSpeech-1.1/wavs/LJ020-0048.wav\n",
            "LJSpeech-1.1/wavs/LJ008-0246.wav\n",
            "LJSpeech-1.1/wavs/LJ028-0222.wav\n",
            "LJSpeech-1.1/wavs/LJ032-0023.wav\n",
            "LJSpeech-1.1/wavs/LJ041-0122.wav\n",
            "LJSpeech-1.1/wavs/LJ026-0116.wav\n",
            "LJSpeech-1.1/wavs/LJ015-0095.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yTMYaJFtoRw",
        "outputId": "769ebb88-7c9e-4b90-9dab-e8ca1225543a"
      },
      "source": [
        "%cd TTS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqQRwZ_Ewwlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48049673-a2e0-4a9a-b0b5-4fc615b0e3ed"
      },
      "source": [
        "!tensorboard --logdir=<./TTS/bin/>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 0: `tensorboard --logdir=<./TTS/bin/>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO7RbrfNxVSL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p3Tbx8cWEFA"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fd6Lx0njnHm"
      },
      "source": [
        "import subprocess\n",
        "import json\n",
        "import os\n",
        "def godir(condir):\n",
        "  if os.getcwd() != condir:\n",
        "    os.chdir(condir)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC5J0L2akepi",
        "outputId": "9734a11f-2ccb-4122-92c7-f01f07ac3c6b"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-02 11:27:50--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 3.82.123.10, 54.243.169.86, 35.153.244.194, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|3.82.123.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  19.1MB/s    in 0.7s    \n",
            "\n",
            "2021-08-02 11:27:51 (19.1 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k2LCmfIknKl"
      },
      "source": [
        "LOG_DIR = './recipes/ljspeech/glow_tts/'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E8vUEvqk0SS"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kaujvuhdk4y-",
        "outputId": "88957cdb-6c46-45df-a465-a5e09b9c0b17"
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://4f598d13c1fd.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMNZbHC1w_NQ"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMqSzuhFzBvi"
      },
      "source": [
        "!rm -rf ./logs/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6AO4ppQxCuS"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLAFJZFFBLV4",
        "outputId": "9233e4a1-e027-4bf8-81b0-a778ba1c7b81"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 28 22:30:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-i0AcnwxX7b",
        "outputId": "c5fd84a4-3141-4afa-b5d5-dfa405fd13ba"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=\"0\" python TTS/bin/train_tts.py --config_path config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "     | > log_mle: 1.19710 \n",
            "     | > loss_dur: 0.45177 \n",
            "     | > align_error: 0.32629 \n",
            "     | > current_lr: 1.8999999999999998e-05\n",
            "     | > grad_norm: 3.3624069690704346\n",
            "     | > step_time: 0.8785\n",
            "     | > loader_time: 0.0072\n",
            "\n",
            "\u001b[1m   --> STEP: 100/405 -- GLOBAL_STEP: 100\u001b[0m\n",
            "     | > loss: 1.41319 \n",
            "     | > log_mle: 1.06554 \n",
            "     | > loss_dur: 0.34765 \n",
            "     | > align_error: 0.27804 \n",
            "     | > current_lr: 2.525e-05\n",
            "     | > grad_norm: 2.528381109237671\n",
            "     | > step_time: 0.9586\n",
            "     | > loader_time: 0.003\n",
            "\n",
            "\u001b[1m   --> STEP: 125/405 -- GLOBAL_STEP: 125\u001b[0m\n",
            "     | > loss: 1.34986 \n",
            "     | > log_mle: 1.00831 \n",
            "     | > loss_dur: 0.34155 \n",
            "     | > align_error: 0.23351 \n",
            "     | > current_lr: 3.15e-05\n",
            "     | > grad_norm: 1.7375026941299438\n",
            "     | > step_time: 1.0051\n",
            "     | > loader_time: 0.0078\n",
            "\n",
            "\u001b[1m   --> STEP: 150/405 -- GLOBAL_STEP: 150\u001b[0m\n",
            "     | > loss: 1.44205 \n",
            "     | > log_mle: 0.94182 \n",
            "     | > loss_dur: 0.50024 \n",
            "     | > align_error: 0.19709 \n",
            "     | > current_lr: 3.775e-05\n",
            "     | > grad_norm: 1.4176961183547974\n",
            "     | > step_time: 1.118\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 175/405 -- GLOBAL_STEP: 175\u001b[0m\n",
            "     | > loss: 1.42604 \n",
            "     | > log_mle: 0.91224 \n",
            "     | > loss_dur: 0.51379 \n",
            "     | > align_error: 0.33297 \n",
            "     | > current_lr: 4.4e-05\n",
            "     | > grad_norm: 1.2796385288238525\n",
            "     | > step_time: 1.1417\n",
            "     | > loader_time: 0.0036\n",
            "\n",
            "\u001b[1m   --> STEP: 200/405 -- GLOBAL_STEP: 200\u001b[0m\n",
            "     | > loss: 1.35142 \n",
            "     | > log_mle: 0.90143 \n",
            "     | > loss_dur: 0.44999 \n",
            "     | > align_error: 0.13362 \n",
            "     | > current_lr: 5.025e-05\n",
            "     | > grad_norm: 2.422745704650879\n",
            "     | > step_time: 1.0741\n",
            "     | > loader_time: 0.0048\n",
            "\n",
            "\u001b[1m   --> STEP: 225/405 -- GLOBAL_STEP: 225\u001b[0m\n",
            "     | > loss: 1.35242 \n",
            "     | > log_mle: 0.89860 \n",
            "     | > loss_dur: 0.45382 \n",
            "     | > align_error: 0.16473 \n",
            "     | > current_lr: 5.6500000000000005e-05\n",
            "     | > grad_norm: 1.0684218406677246\n",
            "     | > step_time: 1.2143\n",
            "     | > loader_time: 0.0053\n",
            "\n",
            "\u001b[1m   --> STEP: 250/405 -- GLOBAL_STEP: 250\u001b[0m\n",
            "     | > loss: 1.29939 \n",
            "     | > log_mle: 0.89483 \n",
            "     | > loss_dur: 0.40456 \n",
            "     | > align_error: 0.11230 \n",
            "     | > current_lr: 6.275e-05\n",
            "     | > grad_norm: 0.8334086537361145\n",
            "     | > step_time: 1.183\n",
            "     | > loader_time: 0.004\n",
            "\n",
            "\u001b[1m   --> STEP: 275/405 -- GLOBAL_STEP: 275\u001b[0m\n",
            "     | > loss: 1.31761 \n",
            "     | > log_mle: 0.89274 \n",
            "     | > loss_dur: 0.42487 \n",
            "     | > align_error: 0.09590 \n",
            "     | > current_lr: 6.9e-05\n",
            "     | > grad_norm: 1.7803562879562378\n",
            "     | > step_time: 1.2938\n",
            "     | > loader_time: 0.0083\n",
            "\n",
            "\u001b[1m   --> STEP: 300/405 -- GLOBAL_STEP: 300\u001b[0m\n",
            "     | > loss: 1.30047 \n",
            "     | > log_mle: 0.88916 \n",
            "     | > loss_dur: 0.41131 \n",
            "     | > align_error: 0.05611 \n",
            "     | > current_lr: 7.525e-05\n",
            "     | > grad_norm: 0.8375706672668457\n",
            "     | > step_time: 1.3338\n",
            "     | > loader_time: 0.0064\n",
            "\n",
            "\u001b[1m   --> STEP: 325/405 -- GLOBAL_STEP: 325\u001b[0m\n",
            "     | > loss: 1.28583 \n",
            "     | > log_mle: 0.88663 \n",
            "     | > loss_dur: 0.39920 \n",
            "     | > align_error: 0.11958 \n",
            "     | > current_lr: 8.15e-05\n",
            "     | > grad_norm: 1.650907039642334\n",
            "     | > step_time: 1.4018\n",
            "     | > loader_time: 0.0056\n",
            "\n",
            "\u001b[1m   --> STEP: 350/405 -- GLOBAL_STEP: 350\u001b[0m\n",
            "     | > loss: 1.24362 \n",
            "     | > log_mle: 0.88238 \n",
            "     | > loss_dur: 0.36124 \n",
            "     | > align_error: 0.12797 \n",
            "     | > current_lr: 8.774999999999999e-05\n",
            "     | > grad_norm: 1.0257344245910645\n",
            "     | > step_time: 1.3426\n",
            "     | > loader_time: 0.0059\n",
            "\n",
            "\u001b[1m   --> STEP: 375/405 -- GLOBAL_STEP: 375\u001b[0m\n",
            "     | > loss: 1.22969 \n",
            "     | > log_mle: 0.88158 \n",
            "     | > loss_dur: 0.34811 \n",
            "     | > align_error: 0.08696 \n",
            "     | > current_lr: 9.400000000000001e-05\n",
            "     | > grad_norm: 1.1953661441802979\n",
            "     | > step_time: 1.5289\n",
            "     | > loader_time: 0.0108\n",
            "\n",
            "\u001b[1m   --> STEP: 400/405 -- GLOBAL_STEP: 400\u001b[0m\n",
            "     | > loss: 1.18732 \n",
            "     | > log_mle: 0.88093 \n",
            "     | > loss_dur: 0.30639 \n",
            "     | > align_error: 0.01939 \n",
            "     | > current_lr: 0.00010025\n",
            "     | > grad_norm: 1.7366780042648315\n",
            "     | > step_time: 1.0708\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 1.16163  (1.16163)\n",
            "     | > log_mle: 0.82904  (0.82904)\n",
            "     | > loss_dur: 0.33260  (0.33260)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.20489  (1.20489)\n",
            "     | > log_mle: 0.85785  (0.85785)\n",
            "     | > loss_dur: 0.34703  (0.34703)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 1.19891  (1.20190)\n",
            "     | > log_mle: 0.85873  (0.85829)\n",
            "     | > loss_dur: 0.34018  (0.34361)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.22273  (1.20884)\n",
            "     | > log_mle: 0.86822  (0.86160)\n",
            "     | > loss_dur: 0.35451  (0.34724)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.21368  (1.21005)\n",
            "     | > log_mle: 0.87724  (0.86551)\n",
            "     | > loss_dur: 0.33645  (0.34454)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 1.21103  (1.21025)\n",
            "     | > log_mle: 0.87729  (0.86787)\n",
            "     | > loss_dur: 0.33374  (0.34238)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.21438  (1.21094)\n",
            "     | > log_mle: 0.88031  (0.86994)\n",
            "     | > loss_dur: 0.33406  (0.34100)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.20675  (1.21034)\n",
            "     | > log_mle: 0.88216  (0.87169)\n",
            "     | > loss_dur: 0.32459  (0.33865)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 1.17608  (1.20606)\n",
            "     | > log_mle: 0.87778  (0.87245)\n",
            "     | > loss_dur: 0.29830  (0.33361)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 4.35240 \u001b[0m(+0.00000)\n",
            "     | > avg_loss: 1.20606 \u001b[0m(+0.00000)\n",
            "     | > avg_log_mle: 0.87245 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_dur: 0.33361 \u001b[0m(+0.00000)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_406.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 1/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 01:18:01) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 19/405 -- GLOBAL_STEP: 425\u001b[0m\n",
            "     | > loss: 1.17789 \n",
            "     | > log_mle: 0.83128 \n",
            "     | > loss_dur: 0.34662 \n",
            "     | > align_error: 0.37793 \n",
            "     | > current_lr: 0.0001065\n",
            "     | > grad_norm: 3.0380747318267822\n",
            "     | > step_time: 0.5205\n",
            "     | > loader_time: 0.0017\n",
            "\n",
            "\u001b[1m   --> STEP: 44/405 -- GLOBAL_STEP: 450\u001b[0m\n",
            "     | > loss: 1.19630 \n",
            "     | > log_mle: 0.86138 \n",
            "     | > loss_dur: 0.33492 \n",
            "     | > align_error: 0.52682 \n",
            "     | > current_lr: 0.00011275\n",
            "     | > grad_norm: 0.929108738899231\n",
            "     | > step_time: 0.9546\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            "\u001b[1m   --> STEP: 69/405 -- GLOBAL_STEP: 475\u001b[0m\n",
            "     | > loss: 1.14796 \n",
            "     | > log_mle: 0.83664 \n",
            "     | > loss_dur: 0.31132 \n",
            "     | > align_error: 0.18038 \n",
            "     | > current_lr: 0.00011899999999999999\n",
            "     | > grad_norm: 2.1786744594573975\n",
            "     | > step_time: 0.8359\n",
            "     | > loader_time: 0.0089\n",
            "\n",
            "\u001b[1m   --> STEP: 94/405 -- GLOBAL_STEP: 500\u001b[0m\n",
            "     | > loss: 1.15406 \n",
            "     | > log_mle: 0.84223 \n",
            "     | > loss_dur: 0.31184 \n",
            "     | > align_error: 0.15770 \n",
            "     | > current_lr: 0.00012524999999999998\n",
            "     | > grad_norm: 0.5744799971580505\n",
            "     | > step_time: 0.7853\n",
            "     | > loader_time: 0.0024\n",
            "\n",
            "\u001b[1m   --> STEP: 119/405 -- GLOBAL_STEP: 525\u001b[0m\n",
            "     | > loss: 1.14631 \n",
            "     | > log_mle: 0.84774 \n",
            "     | > loss_dur: 0.29858 \n",
            "     | > align_error: 0.22633 \n",
            "     | > current_lr: 0.0001315\n",
            "     | > grad_norm: 2.563915491104126\n",
            "     | > step_time: 0.8824\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            "\u001b[1m   --> STEP: 144/405 -- GLOBAL_STEP: 550\u001b[0m\n",
            "     | > loss: 1.15374 \n",
            "     | > log_mle: 0.85358 \n",
            "     | > loss_dur: 0.30017 \n",
            "     | > align_error: 0.12153 \n",
            "     | > current_lr: 0.00013775000000000001\n",
            "     | > grad_norm: 0.7477476000785828\n",
            "     | > step_time: 0.9771\n",
            "     | > loader_time: 0.003\n",
            "\n",
            "\u001b[1m   --> STEP: 169/405 -- GLOBAL_STEP: 575\u001b[0m\n",
            "     | > loss: 1.18022 \n",
            "     | > log_mle: 0.86172 \n",
            "     | > loss_dur: 0.31850 \n",
            "     | > align_error: 0.21798 \n",
            "     | > current_lr: 0.000144\n",
            "     | > grad_norm: 0.6983417272567749\n",
            "     | > step_time: 1.1363\n",
            "     | > loader_time: 0.0097\n",
            "\n",
            "\u001b[1m   --> STEP: 194/405 -- GLOBAL_STEP: 600\u001b[0m\n",
            "     | > loss: 1.17495 \n",
            "     | > log_mle: 0.85921 \n",
            "     | > loss_dur: 0.31574 \n",
            "     | > align_error: 0.18473 \n",
            "     | > current_lr: 0.00015025\n",
            "     | > grad_norm: 0.28814777731895447\n",
            "     | > step_time: 1.2062\n",
            "     | > loader_time: 0.0029\n",
            "\n",
            "\u001b[1m   --> STEP: 219/405 -- GLOBAL_STEP: 625\u001b[0m\n",
            "     | > loss: 1.18156 \n",
            "     | > log_mle: 0.86311 \n",
            "     | > loss_dur: 0.31844 \n",
            "     | > align_error: 0.15900 \n",
            "     | > current_lr: 0.0001565\n",
            "     | > grad_norm: 0.3855096399784088\n",
            "     | > step_time: 1.2889\n",
            "     | > loader_time: 0.0046\n",
            "\n",
            "\u001b[1m   --> STEP: 244/405 -- GLOBAL_STEP: 650\u001b[0m\n",
            "     | > loss: 1.15732 \n",
            "     | > log_mle: 0.85538 \n",
            "     | > loss_dur: 0.30194 \n",
            "     | > align_error: 0.16541 \n",
            "     | > current_lr: 0.00016275\n",
            "     | > grad_norm: 0.4006569981575012\n",
            "     | > step_time: 1.2083\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            "\u001b[1m   --> STEP: 269/405 -- GLOBAL_STEP: 675\u001b[0m\n",
            "     | > loss: 1.16165 \n",
            "     | > log_mle: 0.86054 \n",
            "     | > loss_dur: 0.30110 \n",
            "     | > align_error: 0.11212 \n",
            "     | > current_lr: 0.00016900000000000002\n",
            "     | > grad_norm: 0.6753407120704651\n",
            "     | > step_time: 1.3551\n",
            "     | > loader_time: 0.0105\n",
            "\n",
            "\u001b[1m   --> STEP: 294/405 -- GLOBAL_STEP: 700\u001b[0m\n",
            "     | > loss: 1.17259 \n",
            "     | > log_mle: 0.86058 \n",
            "     | > loss_dur: 0.31201 \n",
            "     | > align_error: 0.10892 \n",
            "     | > current_lr: 0.00017525\n",
            "     | > grad_norm: 1.4724076986312866\n",
            "     | > step_time: 1.3597\n",
            "     | > loader_time: 0.0036\n",
            "\n",
            "\u001b[1m   --> STEP: 319/405 -- GLOBAL_STEP: 725\u001b[0m\n",
            "     | > loss: 1.16222 \n",
            "     | > log_mle: 0.85982 \n",
            "     | > loss_dur: 0.30240 \n",
            "     | > align_error: 0.08045 \n",
            "     | > current_lr: 0.0001815\n",
            "     | > grad_norm: 0.7480699419975281\n",
            "     | > step_time: 1.2481\n",
            "     | > loader_time: 0.018\n",
            "\n",
            "\u001b[1m   --> STEP: 344/405 -- GLOBAL_STEP: 750\u001b[0m\n",
            "     | > loss: 1.15871 \n",
            "     | > log_mle: 0.85799 \n",
            "     | > loss_dur: 0.30072 \n",
            "     | > align_error: 0.13436 \n",
            "     | > current_lr: 0.00018775\n",
            "     | > grad_norm: 0.7489616274833679\n",
            "     | > step_time: 1.356\n",
            "     | > loader_time: 0.0195\n",
            "\n",
            "\u001b[1m   --> STEP: 369/405 -- GLOBAL_STEP: 775\u001b[0m\n",
            "     | > loss: 1.16673 \n",
            "     | > log_mle: 0.86070 \n",
            "     | > loss_dur: 0.30603 \n",
            "     | > align_error: 0.02849 \n",
            "     | > current_lr: 0.000194\n",
            "     | > grad_norm: 1.7546882629394531\n",
            "     | > step_time: 1.4768\n",
            "     | > loader_time: 0.0162\n",
            "\n",
            "\u001b[1m   --> STEP: 394/405 -- GLOBAL_STEP: 800\u001b[0m\n",
            "     | > loss: 1.13327 \n",
            "     | > log_mle: 0.85618 \n",
            "     | > loss_dur: 0.27709 \n",
            "     | > align_error: 0.05252 \n",
            "     | > current_lr: 0.00020025\n",
            "     | > grad_norm: 0.4538891017436981\n",
            "     | > step_time: 1.4321\n",
            "     | > loader_time: 0.0113\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 1.05641  (1.05641)\n",
            "     | > log_mle: 0.77115  (0.77115)\n",
            "     | > loss_dur: 0.28526  (0.28526)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.12311  (1.12311)\n",
            "     | > log_mle: 0.82007  (0.82007)\n",
            "     | > loss_dur: 0.30304  (0.30304)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 1.11449  (1.11880)\n",
            "     | > log_mle: 0.82153  (0.82080)\n",
            "     | > loss_dur: 0.29296  (0.29800)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.14239  (1.12667)\n",
            "     | > log_mle: 0.83749  (0.82636)\n",
            "     | > loss_dur: 0.30490  (0.30030)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.14398  (1.13099)\n",
            "     | > log_mle: 0.85127  (0.83259)\n",
            "     | > loss_dur: 0.29271  (0.29840)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 1.13256  (1.13131)\n",
            "     | > log_mle: 0.85046  (0.83617)\n",
            "     | > loss_dur: 0.28210  (0.29514)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.14984  (1.13440)\n",
            "     | > log_mle: 0.85682  (0.83961)\n",
            "     | > loss_dur: 0.29303  (0.29479)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.13892  (1.13504)\n",
            "     | > log_mle: 0.85876  (0.84234)\n",
            "     | > loss_dur: 0.28016  (0.29270)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 1.11438  (1.13246)\n",
            "     | > log_mle: 0.85380  (0.84378)\n",
            "     | > loss_dur: 0.26058  (0.28868)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 2.76396 \u001b[0m(-1.58844)\n",
            "     | > avg_loss:\u001b[92m 1.13246 \u001b[0m(-0.07360)\n",
            "     | > avg_log_mle:\u001b[92m 0.84378 \u001b[0m(-0.02867)\n",
            "     | > avg_loss_dur:\u001b[92m 0.28868 \u001b[0m(-0.04492)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_812.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 2/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 01:25:58) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 13/405 -- GLOBAL_STEP: 825\u001b[0m\n",
            "     | > loss: 1.13740 \n",
            "     | > log_mle: 0.80918 \n",
            "     | > loss_dur: 0.32822 \n",
            "     | > align_error: 0.35436 \n",
            "     | > current_lr: 0.0002065\n",
            "     | > grad_norm: 2.2431018352508545\n",
            "     | > step_time: 0.776\n",
            "     | > loader_time: 0.0019\n",
            "\n",
            "\u001b[1m   --> STEP: 38/405 -- GLOBAL_STEP: 850\u001b[0m\n",
            "     | > loss: 1.14088 \n",
            "     | > log_mle: 0.83628 \n",
            "     | > loss_dur: 0.30460 \n",
            "     | > align_error: 0.34453 \n",
            "     | > current_lr: 0.00021275000000000002\n",
            "     | > grad_norm: 0.5187902450561523\n",
            "     | > step_time: 0.8989\n",
            "     | > loader_time: 0.003\n",
            "\n",
            "\u001b[1m   --> STEP: 63/405 -- GLOBAL_STEP: 875\u001b[0m\n",
            "     | > loss: 1.13820 \n",
            "     | > log_mle: 0.82987 \n",
            "     | > loss_dur: 0.30833 \n",
            "     | > align_error: 0.33577 \n",
            "     | > current_lr: 0.00021899999999999998\n",
            "     | > grad_norm: 0.912859320640564\n",
            "     | > step_time: 0.8805\n",
            "     | > loader_time: 0.0057\n",
            "\n",
            "\u001b[1m   --> STEP: 88/405 -- GLOBAL_STEP: 900\u001b[0m\n",
            "     | > loss: 1.11804 \n",
            "     | > log_mle: 0.81632 \n",
            "     | > loss_dur: 0.30173 \n",
            "     | > align_error: 0.16533 \n",
            "     | > current_lr: 0.00022525\n",
            "     | > grad_norm: 1.1954102516174316\n",
            "     | > step_time: 0.7739\n",
            "     | > loader_time: 0.0023\n",
            "\n",
            "\u001b[1m   --> STEP: 113/405 -- GLOBAL_STEP: 925\u001b[0m\n",
            "     | > loss: 1.11708 \n",
            "     | > log_mle: 0.82057 \n",
            "     | > loss_dur: 0.29651 \n",
            "     | > align_error: 0.22360 \n",
            "     | > current_lr: 0.00023150000000000002\n",
            "     | > grad_norm: 0.8320538997650146\n",
            "     | > step_time: 0.8621\n",
            "     | > loader_time: 0.0028\n",
            "\n",
            "\u001b[1m   --> STEP: 138/405 -- GLOBAL_STEP: 950\u001b[0m\n",
            "     | > loss: 1.13353 \n",
            "     | > log_mle: 0.83916 \n",
            "     | > loss_dur: 0.29437 \n",
            "     | > align_error: 0.26471 \n",
            "     | > current_lr: 0.00023775\n",
            "     | > grad_norm: 0.44061529636383057\n",
            "     | > step_time: 1.0835\n",
            "     | > loader_time: 0.005\n",
            "\n",
            "\u001b[1m   --> STEP: 163/405 -- GLOBAL_STEP: 975\u001b[0m\n",
            "     | > loss: 1.12381 \n",
            "     | > log_mle: 0.82968 \n",
            "     | > loss_dur: 0.29413 \n",
            "     | > align_error: 0.36341 \n",
            "     | > current_lr: 0.000244\n",
            "     | > grad_norm: 0.9133480191230774\n",
            "     | > step_time: 1.0693\n",
            "     | > loader_time: 0.0032\n",
            "\n",
            "\u001b[1m   --> STEP: 188/405 -- GLOBAL_STEP: 1000\u001b[0m\n",
            "     | > loss: 1.11763 \n",
            "     | > log_mle: 0.83132 \n",
            "     | > loss_dur: 0.28631 \n",
            "     | > align_error: 0.17975 \n",
            "     | > current_lr: 0.00025025\n",
            "     | > grad_norm: 0.513360321521759\n",
            "     | > step_time: 1.2152\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            "\u001b[1m   --> STEP: 213/405 -- GLOBAL_STEP: 1025\u001b[0m\n",
            "     | > loss: 1.13284 \n",
            "     | > log_mle: 0.83866 \n",
            "     | > loss_dur: 0.29419 \n",
            "     | > align_error: 0.17336 \n",
            "     | > current_lr: 0.0002565\n",
            "     | > grad_norm: 0.3702177107334137\n",
            "     | > step_time: 1.1947\n",
            "     | > loader_time: 0.0076\n",
            "\n",
            "\u001b[1m   --> STEP: 238/405 -- GLOBAL_STEP: 1050\u001b[0m\n",
            "     | > loss: 1.14011 \n",
            "     | > log_mle: 0.84181 \n",
            "     | > loss_dur: 0.29831 \n",
            "     | > align_error: 0.23112 \n",
            "     | > current_lr: 0.00026275\n",
            "     | > grad_norm: 1.2234976291656494\n",
            "     | > step_time: 1.3227\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            "\u001b[1m   --> STEP: 263/405 -- GLOBAL_STEP: 1075\u001b[0m\n",
            "     | > loss: 1.12424 \n",
            "     | > log_mle: 0.83350 \n",
            "     | > loss_dur: 0.29074 \n",
            "     | > align_error: 0.15399 \n",
            "     | > current_lr: 0.00026900000000000003\n",
            "     | > grad_norm: 0.9356362819671631\n",
            "     | > step_time: 1.1048\n",
            "     | > loader_time: 0.0122\n",
            "\n",
            "\u001b[1m   --> STEP: 288/405 -- GLOBAL_STEP: 1100\u001b[0m\n",
            "     | > loss: 1.15155 \n",
            "     | > log_mle: 0.84367 \n",
            "     | > loss_dur: 0.30788 \n",
            "     | > align_error: 0.10915 \n",
            "     | > current_lr: 0.00027525\n",
            "     | > grad_norm: 0.6389747858047485\n",
            "     | > step_time: 1.4206\n",
            "     | > loader_time: 0.0107\n",
            "\n",
            "\u001b[1m   --> STEP: 313/405 -- GLOBAL_STEP: 1125\u001b[0m\n",
            "     | > loss: 1.12898 \n",
            "     | > log_mle: 0.84173 \n",
            "     | > loss_dur: 0.28725 \n",
            "     | > align_error: 0.15707 \n",
            "     | > current_lr: 0.00028149999999999996\n",
            "     | > grad_norm: 0.6558767557144165\n",
            "     | > step_time: 1.4356\n",
            "     | > loader_time: 0.0104\n",
            "\n",
            "\u001b[1m   --> STEP: 338/405 -- GLOBAL_STEP: 1150\u001b[0m\n",
            "     | > loss: 1.13551 \n",
            "     | > log_mle: 0.83947 \n",
            "     | > loss_dur: 0.29604 \n",
            "     | > align_error: 0.14676 \n",
            "     | > current_lr: 0.00028775\n",
            "     | > grad_norm: 0.7072677612304688\n",
            "     | > step_time: 1.5405\n",
            "     | > loader_time: 0.0114\n",
            "\n",
            "\u001b[1m   --> STEP: 363/405 -- GLOBAL_STEP: 1175\u001b[0m\n",
            "     | > loss: 1.12400 \n",
            "     | > log_mle: 0.83922 \n",
            "     | > loss_dur: 0.28478 \n",
            "     | > align_error: 0.09731 \n",
            "     | > current_lr: 0.000294\n",
            "     | > grad_norm: 0.5726197957992554\n",
            "     | > step_time: 1.5211\n",
            "     | > loader_time: 0.0063\n",
            "\n",
            "\u001b[1m   --> STEP: 388/405 -- GLOBAL_STEP: 1200\u001b[0m\n",
            "     | > loss: 1.13113 \n",
            "     | > log_mle: 0.84023 \n",
            "     | > loss_dur: 0.29091 \n",
            "     | > align_error: 0.02644 \n",
            "     | > current_lr: 0.00030025\n",
            "     | > grad_norm: 0.6877223253250122\n",
            "     | > step_time: 1.4739\n",
            "     | > loader_time: 0.0054\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 1.00973  (1.00973)\n",
            "     | > log_mle: 0.72691  (0.72691)\n",
            "     | > loss_dur: 0.28281  (0.28281)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.08208  (1.08208)\n",
            "     | > log_mle: 0.79026  (0.79026)\n",
            "     | > loss_dur: 0.29182  (0.29182)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 1.07736  (1.07972)\n",
            "     | > log_mle: 0.79300  (0.79163)\n",
            "     | > loss_dur: 0.28437  (0.28809)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.11806  (1.09250)\n",
            "     | > log_mle: 0.81333  (0.79886)\n",
            "     | > loss_dur: 0.30473  (0.29364)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.11177  (1.09732)\n",
            "     | > log_mle: 0.83075  (0.80683)\n",
            "     | > loss_dur: 0.28102  (0.29048)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 1.10589  (1.09903)\n",
            "     | > log_mle: 0.82974  (0.81142)\n",
            "     | > loss_dur: 0.27615  (0.28762)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.12363  (1.10313)\n",
            "     | > log_mle: 0.83821  (0.81588)\n",
            "     | > loss_dur: 0.28541  (0.28725)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.11809  (1.10527)\n",
            "     | > log_mle: 0.84062  (0.81942)\n",
            "     | > loss_dur: 0.27747  (0.28585)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 1.09979  (1.10458)\n",
            "     | > log_mle: 0.83565  (0.82144)\n",
            "     | > loss_dur: 0.26414  (0.28314)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 2.91960 \u001b[0m(+0.15563)\n",
            "     | > avg_loss:\u001b[92m 1.10458 \u001b[0m(-0.02788)\n",
            "     | > avg_log_mle:\u001b[92m 0.82144 \u001b[0m(-0.02233)\n",
            "     | > avg_loss_dur:\u001b[92m 0.28314 \u001b[0m(-0.00555)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_1218.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 3/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 01:33:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 7/405 -- GLOBAL_STEP: 1225\u001b[0m\n",
            "     | > loss: 1.16867 \n",
            "     | > log_mle: 0.81943 \n",
            "     | > loss_dur: 0.34924 \n",
            "     | > align_error: 0.43875 \n",
            "     | > current_lr: 0.0003065\n",
            "     | > grad_norm: 1.9992254972457886\n",
            "     | > step_time: 0.6006\n",
            "     | > loader_time: 0.0014\n",
            "\n",
            "\u001b[1m   --> STEP: 32/405 -- GLOBAL_STEP: 1250\u001b[0m\n",
            "     | > loss: 1.07041 \n",
            "     | > log_mle: 0.76664 \n",
            "     | > loss_dur: 0.30377 \n",
            "     | > align_error: 0.29393 \n",
            "     | > current_lr: 0.00031275000000000004\n",
            "     | > grad_norm: 0.6717415452003479\n",
            "     | > step_time: 0.6484\n",
            "     | > loader_time: 0.002\n",
            "\n",
            "\u001b[1m   --> STEP: 57/405 -- GLOBAL_STEP: 1275\u001b[0m\n",
            "     | > loss: 1.08083 \n",
            "     | > log_mle: 0.78174 \n",
            "     | > loss_dur: 0.29909 \n",
            "     | > align_error: 0.20583 \n",
            "     | > current_lr: 0.000319\n",
            "     | > grad_norm: 1.6303431987762451\n",
            "     | > step_time: 0.7962\n",
            "     | > loader_time: 0.0023\n",
            "\n",
            "\u001b[1m   --> STEP: 82/405 -- GLOBAL_STEP: 1300\u001b[0m\n",
            "     | > loss: 1.12235 \n",
            "     | > log_mle: 0.82602 \n",
            "     | > loss_dur: 0.29633 \n",
            "     | > align_error: 0.32692 \n",
            "     | > current_lr: 0.00032524999999999996\n",
            "     | > grad_norm: 0.37821370363235474\n",
            "     | > step_time: 1.0667\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 107/405 -- GLOBAL_STEP: 1325\u001b[0m\n",
            "     | > loss: 1.11448 \n",
            "     | > log_mle: 0.81596 \n",
            "     | > loss_dur: 0.29852 \n",
            "     | > align_error: 0.20780 \n",
            "     | > current_lr: 0.0003315\n",
            "     | > grad_norm: 0.4589383602142334\n",
            "     | > step_time: 1.0187\n",
            "     | > loader_time: 0.0097\n",
            "\n",
            "\u001b[1m   --> STEP: 132/405 -- GLOBAL_STEP: 1350\u001b[0m\n",
            "     | > loss: 1.11252 \n",
            "     | > log_mle: 0.81066 \n",
            "     | > loss_dur: 0.30186 \n",
            "     | > align_error: 0.20210 \n",
            "     | > current_lr: 0.00033775\n",
            "     | > grad_norm: 0.4481629431247711\n",
            "     | > step_time: 1.0212\n",
            "     | > loader_time: 0.0085\n",
            "\n",
            "\u001b[1m   --> STEP: 157/405 -- GLOBAL_STEP: 1375\u001b[0m\n",
            "     | > loss: 1.10507 \n",
            "     | > log_mle: 0.80737 \n",
            "     | > loss_dur: 0.29770 \n",
            "     | > align_error: 0.22405 \n",
            "     | > current_lr: 0.000344\n",
            "     | > grad_norm: 0.5192773938179016\n",
            "     | > step_time: 1.0699\n",
            "     | > loader_time: 0.0025\n",
            "\n",
            "\u001b[1m   --> STEP: 182/405 -- GLOBAL_STEP: 1400\u001b[0m\n",
            "     | > loss: 1.09900 \n",
            "     | > log_mle: 0.80364 \n",
            "     | > loss_dur: 0.29537 \n",
            "     | > align_error: 0.13560 \n",
            "     | > current_lr: 0.00035025000000000003\n",
            "     | > grad_norm: 0.4352273643016815\n",
            "     | > step_time: 1.0878\n",
            "     | > loader_time: 0.0029\n",
            "\n",
            "\u001b[1m   --> STEP: 207/405 -- GLOBAL_STEP: 1425\u001b[0m\n",
            "     | > loss: 1.11391 \n",
            "     | > log_mle: 0.82441 \n",
            "     | > loss_dur: 0.28951 \n",
            "     | > align_error: 0.10156 \n",
            "     | > current_lr: 0.0003565\n",
            "     | > grad_norm: 0.3956961929798126\n",
            "     | > step_time: 1.0882\n",
            "     | > loader_time: 0.0032\n",
            "\n",
            "\u001b[1m   --> STEP: 232/405 -- GLOBAL_STEP: 1450\u001b[0m\n",
            "     | > loss: 1.09919 \n",
            "     | > log_mle: 0.81652 \n",
            "     | > loss_dur: 0.28268 \n",
            "     | > align_error: 0.16899 \n",
            "     | > current_lr: 0.00036275\n",
            "     | > grad_norm: 0.7461303472518921\n",
            "     | > step_time: 1.2475\n",
            "     | > loader_time: 0.0062\n",
            "\n",
            "\u001b[1m   --> STEP: 257/405 -- GLOBAL_STEP: 1475\u001b[0m\n",
            "     | > loss: 1.10998 \n",
            "     | > log_mle: 0.82438 \n",
            "     | > loss_dur: 0.28560 \n",
            "     | > align_error: 0.18860 \n",
            "     | > current_lr: 0.00036899999999999997\n",
            "     | > grad_norm: 0.8865057229995728\n",
            "     | > step_time: 1.2463\n",
            "     | > loader_time: 0.006\n",
            "\n",
            "\u001b[1m   --> STEP: 282/405 -- GLOBAL_STEP: 1500\u001b[0m\n",
            "     | > loss: 1.11962 \n",
            "     | > log_mle: 0.82763 \n",
            "     | > loss_dur: 0.29199 \n",
            "     | > align_error: 0.15647 \n",
            "     | > current_lr: 0.00037525\n",
            "     | > grad_norm: 0.6126053929328918\n",
            "     | > step_time: 1.2608\n",
            "     | > loader_time: 0.0036\n",
            "\n",
            "\u001b[1m   --> STEP: 307/405 -- GLOBAL_STEP: 1525\u001b[0m\n",
            "     | > loss: 1.12074 \n",
            "     | > log_mle: 0.82231 \n",
            "     | > loss_dur: 0.29843 \n",
            "     | > align_error: 0.16577 \n",
            "     | > current_lr: 0.0003815\n",
            "     | > grad_norm: 0.5763819813728333\n",
            "     | > step_time: 1.3249\n",
            "     | > loader_time: 0.0033\n",
            "\n",
            "\u001b[1m   --> STEP: 332/405 -- GLOBAL_STEP: 1550\u001b[0m\n",
            "     | > loss: 1.10211 \n",
            "     | > log_mle: 0.82589 \n",
            "     | > loss_dur: 0.27622 \n",
            "     | > align_error: 0.04167 \n",
            "     | > current_lr: 0.00038775\n",
            "     | > grad_norm: 0.7065145373344421\n",
            "     | > step_time: 1.5128\n",
            "     | > loader_time: 0.0075\n",
            "\n",
            "\u001b[1m   --> STEP: 357/405 -- GLOBAL_STEP: 1575\u001b[0m\n",
            "     | > loss: 1.10495 \n",
            "     | > log_mle: 0.82673 \n",
            "     | > loss_dur: 0.27822 \n",
            "     | > align_error: 0.09355 \n",
            "     | > current_lr: 0.000394\n",
            "     | > grad_norm: 0.475706547498703\n",
            "     | > step_time: 1.3455\n",
            "     | > loader_time: 0.0035\n",
            "\n",
            "\u001b[1m   --> STEP: 382/405 -- GLOBAL_STEP: 1600\u001b[0m\n",
            "     | > loss: 1.11177 \n",
            "     | > log_mle: 0.82640 \n",
            "     | > loss_dur: 0.28536 \n",
            "     | > align_error: 0.11280 \n",
            "     | > current_lr: 0.00040025\n",
            "     | > grad_norm: 0.6183760762214661\n",
            "     | > step_time: 1.541\n",
            "     | > loader_time: 0.0175\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.98266  (0.98266)\n",
            "     | > log_mle: 0.69603  (0.69603)\n",
            "     | > loss_dur: 0.28663  (0.28663)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.06676  (1.06676)\n",
            "     | > log_mle: 0.76988  (0.76988)\n",
            "     | > loss_dur: 0.29687  (0.29687)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 1.05767  (1.06221)\n",
            "     | > log_mle: 0.77383  (0.77186)\n",
            "     | > loss_dur: 0.28384  (0.29036)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.10027  (1.07490)\n",
            "     | > log_mle: 0.79712  (0.78028)\n",
            "     | > loss_dur: 0.30315  (0.29462)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.11324  (1.08449)\n",
            "     | > log_mle: 0.81730  (0.78953)\n",
            "     | > loss_dur: 0.29595  (0.29495)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 1.10660  (1.08891)\n",
            "     | > log_mle: 0.81589  (0.79480)\n",
            "     | > loss_dur: 0.29071  (0.29410)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.11254  (1.09285)\n",
            "     | > log_mle: 0.82597  (0.80000)\n",
            "     | > loss_dur: 0.28657  (0.29285)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.11177  (1.09555)\n",
            "     | > log_mle: 0.82843  (0.80406)\n",
            "     | > loss_dur: 0.28333  (0.29149)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 1.10789  (1.09709)\n",
            "     | > log_mle: 0.82360  (0.80650)\n",
            "     | > loss_dur: 0.28428  (0.29059)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 2.70013 \u001b[0m(-0.21946)\n",
            "     | > avg_loss:\u001b[92m 1.09709 \u001b[0m(-0.00749)\n",
            "     | > avg_log_mle:\u001b[92m 0.80650 \u001b[0m(-0.01494)\n",
            "     | > avg_loss_dur:\u001b[91m 0.29059 \u001b[0m(+0.00745)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_1624.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 4/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 01:41:55) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 1/405 -- GLOBAL_STEP: 1625\u001b[0m\n",
            "     | > loss: 0.97587 \n",
            "     | > log_mle: 0.66110 \n",
            "     | > loss_dur: 0.31477 \n",
            "     | > align_error: 0.52699 \n",
            "     | > current_lr: 0.0004065\n",
            "     | > grad_norm: 6.204343318939209\n",
            "     | > step_time: 0.7736\n",
            "     | > loader_time: 0.0065\n",
            "\n",
            "\u001b[1m   --> STEP: 26/405 -- GLOBAL_STEP: 1650\u001b[0m\n",
            "     | > loss: 1.10874 \n",
            "     | > log_mle: 0.76908 \n",
            "     | > loss_dur: 0.33966 \n",
            "     | > align_error: 0.32369 \n",
            "     | > current_lr: 0.00041275\n",
            "     | > grad_norm: 1.425633430480957\n",
            "     | > step_time: 0.7036\n",
            "     | > loader_time: 0.0018\n",
            "\n",
            "\u001b[1m   --> STEP: 51/405 -- GLOBAL_STEP: 1675\u001b[0m\n",
            "     | > loss: 1.07322 \n",
            "     | > log_mle: 0.78184 \n",
            "     | > loss_dur: 0.29138 \n",
            "     | > align_error: 0.39583 \n",
            "     | > current_lr: 0.000419\n",
            "     | > grad_norm: 0.5282214283943176\n",
            "     | > step_time: 0.8196\n",
            "     | > loader_time: 0.0021\n",
            "\n",
            "\u001b[1m   --> STEP: 76/405 -- GLOBAL_STEP: 1700\u001b[0m\n",
            "     | > loss: 1.09090 \n",
            "     | > log_mle: 0.79466 \n",
            "     | > loss_dur: 0.29625 \n",
            "     | > align_error: 0.20424 \n",
            "     | > current_lr: 0.00042525\n",
            "     | > grad_norm: 0.4411936402320862\n",
            "     | > step_time: 0.9137\n",
            "     | > loader_time: 0.0025\n",
            "\n",
            "\u001b[1m   --> STEP: 101/405 -- GLOBAL_STEP: 1725\u001b[0m\n",
            "     | > loss: 1.07620 \n",
            "     | > log_mle: 0.79627 \n",
            "     | > loss_dur: 0.27992 \n",
            "     | > align_error: 0.32484 \n",
            "     | > current_lr: 0.00043149999999999997\n",
            "     | > grad_norm: 0.47280755639076233\n",
            "     | > step_time: 1.0133\n",
            "     | > loader_time: 0.0047\n",
            "\n",
            "\u001b[1m   --> STEP: 126/405 -- GLOBAL_STEP: 1750\u001b[0m\n",
            "     | > loss: 1.10406 \n",
            "     | > log_mle: 0.80792 \n",
            "     | > loss_dur: 0.29614 \n",
            "     | > align_error: 0.18317 \n",
            "     | > current_lr: 0.00043775\n",
            "     | > grad_norm: 0.38617703318595886\n",
            "     | > step_time: 1.0607\n",
            "     | > loader_time: 0.0037\n",
            "\n",
            "\u001b[1m   --> STEP: 151/405 -- GLOBAL_STEP: 1775\u001b[0m\n",
            "     | > loss: 1.10444 \n",
            "     | > log_mle: 0.80958 \n",
            "     | > loss_dur: 0.29486 \n",
            "     | > align_error: 0.18722 \n",
            "     | > current_lr: 0.000444\n",
            "     | > grad_norm: 0.4555140435695648\n",
            "     | > step_time: 1.079\n",
            "     | > loader_time: 0.0097\n",
            "\n",
            "\u001b[1m   --> STEP: 176/405 -- GLOBAL_STEP: 1800\u001b[0m\n",
            "     | > loss: 1.08927 \n",
            "     | > log_mle: 0.79246 \n",
            "     | > loss_dur: 0.29681 \n",
            "     | > align_error: 0.08070 \n",
            "     | > current_lr: 0.00045025\n",
            "     | > grad_norm: 0.48274266719818115\n",
            "     | > step_time: 1.0963\n",
            "     | > loader_time: 0.0058\n",
            "\n",
            "\u001b[1m   --> STEP: 201/405 -- GLOBAL_STEP: 1825\u001b[0m\n",
            "     | > loss: 1.08286 \n",
            "     | > log_mle: 0.80083 \n",
            "     | > loss_dur: 0.28202 \n",
            "     | > align_error: 0.18372 \n",
            "     | > current_lr: 0.00045650000000000004\n",
            "     | > grad_norm: 0.5901728272438049\n",
            "     | > step_time: 1.056\n",
            "     | > loader_time: 0.0035\n",
            "\n",
            "\u001b[1m   --> STEP: 226/405 -- GLOBAL_STEP: 1850\u001b[0m\n",
            "     | > loss: 1.08926 \n",
            "     | > log_mle: 0.80979 \n",
            "     | > loss_dur: 0.27946 \n",
            "     | > align_error: 0.14100 \n",
            "     | > current_lr: 0.00046275\n",
            "     | > grad_norm: 0.7406749129295349\n",
            "     | > step_time: 1.1718\n",
            "     | > loader_time: 0.0062\n",
            "\n",
            "\u001b[1m   --> STEP: 251/405 -- GLOBAL_STEP: 1875\u001b[0m\n",
            "     | > loss: 1.10281 \n",
            "     | > log_mle: 0.81391 \n",
            "     | > loss_dur: 0.28890 \n",
            "     | > align_error: 0.14098 \n",
            "     | > current_lr: 0.00046899999999999996\n",
            "     | > grad_norm: 0.36179304122924805\n",
            "     | > step_time: 1.3471\n",
            "     | > loader_time: 0.0083\n",
            "\n",
            "\u001b[1m   --> STEP: 276/405 -- GLOBAL_STEP: 1900\u001b[0m\n",
            "     | > loss: 1.09775 \n",
            "     | > log_mle: 0.81488 \n",
            "     | > loss_dur: 0.28286 \n",
            "     | > align_error: 0.15647 \n",
            "     | > current_lr: 0.00047525\n",
            "     | > grad_norm: 0.6699479222297668\n",
            "     | > step_time: 1.2397\n",
            "     | > loader_time: 0.0118\n",
            "\n",
            "\u001b[1m   --> STEP: 301/405 -- GLOBAL_STEP: 1925\u001b[0m\n",
            "     | > loss: 1.10247 \n",
            "     | > log_mle: 0.81790 \n",
            "     | > loss_dur: 0.28457 \n",
            "     | > align_error: 0.17565 \n",
            "     | > current_lr: 0.0004815\n",
            "     | > grad_norm: 0.5866745114326477\n",
            "     | > step_time: 1.3097\n",
            "     | > loader_time: 0.0065\n",
            "\n",
            "\u001b[1m   --> STEP: 326/405 -- GLOBAL_STEP: 1950\u001b[0m\n",
            "     | > loss: 1.10389 \n",
            "     | > log_mle: 0.81954 \n",
            "     | > loss_dur: 0.28436 \n",
            "     | > align_error: 0.14651 \n",
            "     | > current_lr: 0.00048775\n",
            "     | > grad_norm: 0.33065155148506165\n",
            "     | > step_time: 1.3319\n",
            "     | > loader_time: 0.003\n",
            "\n",
            "\u001b[1m   --> STEP: 351/405 -- GLOBAL_STEP: 1975\u001b[0m\n",
            "     | > loss: 1.09649 \n",
            "     | > log_mle: 0.81950 \n",
            "     | > loss_dur: 0.27698 \n",
            "     | > align_error: 0.10151 \n",
            "     | > current_lr: 0.000494\n",
            "     | > grad_norm: 0.5923544764518738\n",
            "     | > step_time: 1.3166\n",
            "     | > loader_time: 0.004\n",
            "\n",
            "\u001b[1m   --> STEP: 376/405 -- GLOBAL_STEP: 2000\u001b[0m\n",
            "     | > loss: 1.08841 \n",
            "     | > log_mle: 0.81958 \n",
            "     | > loss_dur: 0.26883 \n",
            "     | > align_error: 0.08404 \n",
            "     | > current_lr: 0.00050025\n",
            "     | > grad_norm: 0.35573136806488037\n",
            "     | > step_time: 1.4302\n",
            "     | > loader_time: 0.0095\n",
            "\n",
            "\u001b[1m   --> STEP: 401/405 -- GLOBAL_STEP: 2025\u001b[0m\n",
            "     | > loss: 1.07726 \n",
            "     | > log_mle: 0.82026 \n",
            "     | > loss_dur: 0.25700 \n",
            "     | > align_error: 0.08926 \n",
            "     | > current_lr: 0.0005065\n",
            "     | > grad_norm: 0.2577302157878876\n",
            "     | > step_time: 1.0922\n",
            "     | > loader_time: 0.0029\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.94443  (0.94443)\n",
            "     | > log_mle: 0.67448  (0.67448)\n",
            "     | > loss_dur: 0.26995  (0.26995)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.04593  (1.04593)\n",
            "     | > log_mle: 0.75531  (0.75531)\n",
            "     | > loss_dur: 0.29062  (0.29062)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 1.02933  (1.03763)\n",
            "     | > log_mle: 0.76023  (0.75777)\n",
            "     | > loss_dur: 0.26911  (0.27986)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.07441  (1.04989)\n",
            "     | > log_mle: 0.78527  (0.76694)\n",
            "     | > loss_dur: 0.28914  (0.28295)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.08833  (1.05950)\n",
            "     | > log_mle: 0.80747  (0.77707)\n",
            "     | > loss_dur: 0.28086  (0.28243)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 1.07143  (1.06189)\n",
            "     | > log_mle: 0.80601  (0.78286)\n",
            "     | > loss_dur: 0.26542  (0.27903)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.09098  (1.06674)\n",
            "     | > log_mle: 0.81717  (0.78858)\n",
            "     | > loss_dur: 0.27380  (0.27816)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.08913  (1.06993)\n",
            "     | > log_mle: 0.81979  (0.79304)\n",
            "     | > loss_dur: 0.26934  (0.27690)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 1.07803  (1.07095)\n",
            "     | > log_mle: 0.81527  (0.79582)\n",
            "     | > loss_dur: 0.26276  (0.27513)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 2.90544 \u001b[0m(+0.20531)\n",
            "     | > avg_loss:\u001b[92m 1.07095 \u001b[0m(-0.02615)\n",
            "     | > avg_log_mle:\u001b[92m 0.79582 \u001b[0m(-0.01069)\n",
            "     | > avg_loss_dur:\u001b[92m 0.27513 \u001b[0m(-0.01546)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_2030.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 5/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 01:49:52) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/405 -- GLOBAL_STEP: 2050\u001b[0m\n",
            "     | > loss: 1.05875 \n",
            "     | > log_mle: 0.72535 \n",
            "     | > loss_dur: 0.33340 \n",
            "     | > align_error: 0.27159 \n",
            "     | > current_lr: 0.0005127500000000001\n",
            "     | > grad_norm: 1.620715856552124\n",
            "     | > step_time: 0.5855\n",
            "     | > loader_time: 0.0017\n",
            "\n",
            "\u001b[1m   --> STEP: 45/405 -- GLOBAL_STEP: 2075\u001b[0m\n",
            "     | > loss: 1.04901 \n",
            "     | > log_mle: 0.76001 \n",
            "     | > loss_dur: 0.28900 \n",
            "     | > align_error: 0.28125 \n",
            "     | > current_lr: 0.000519\n",
            "     | > grad_norm: 0.27693817019462585\n",
            "     | > step_time: 0.7343\n",
            "     | > loader_time: 0.0021\n",
            "\n",
            "\u001b[1m   --> STEP: 70/405 -- GLOBAL_STEP: 2100\u001b[0m\n",
            "     | > loss: 1.10418 \n",
            "     | > log_mle: 0.79995 \n",
            "     | > loss_dur: 0.30423 \n",
            "     | > align_error: 0.30410 \n",
            "     | > current_lr: 0.00052525\n",
            "     | > grad_norm: 0.789069652557373\n",
            "     | > step_time: 0.9068\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            "\u001b[1m   --> STEP: 95/405 -- GLOBAL_STEP: 2125\u001b[0m\n",
            "     | > loss: 1.06099 \n",
            "     | > log_mle: 0.77944 \n",
            "     | > loss_dur: 0.28154 \n",
            "     | > align_error: 0.19243 \n",
            "     | > current_lr: 0.0005315\n",
            "     | > grad_norm: 0.6954687833786011\n",
            "     | > step_time: 0.8526\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            "\u001b[1m   --> STEP: 120/405 -- GLOBAL_STEP: 2150\u001b[0m\n",
            "     | > loss: 1.04604 \n",
            "     | > log_mle: 0.76383 \n",
            "     | > loss_dur: 0.28221 \n",
            "     | > align_error: 0.03539 \n",
            "     | > current_lr: 0.0005377499999999999\n",
            "     | > grad_norm: 0.8455383777618408\n",
            "     | > step_time: 0.9139\n",
            "     | > loader_time: 0.0072\n",
            "\n",
            "\u001b[1m   --> STEP: 145/405 -- GLOBAL_STEP: 2175\u001b[0m\n",
            "     | > loss: 1.08394 \n",
            "     | > log_mle: 0.80232 \n",
            "     | > loss_dur: 0.28162 \n",
            "     | > align_error: 0.30758 \n",
            "     | > current_lr: 0.000544\n",
            "     | > grad_norm: 0.32482048869132996\n",
            "     | > step_time: 1.1308\n",
            "     | > loader_time: 0.005\n",
            "\n",
            "\u001b[1m   --> STEP: 170/405 -- GLOBAL_STEP: 2200\u001b[0m\n",
            "     | > loss: 1.08624 \n",
            "     | > log_mle: 0.80133 \n",
            "     | > loss_dur: 0.28491 \n",
            "     | > align_error: 0.17334 \n",
            "     | > current_lr: 0.00055025\n",
            "     | > grad_norm: 0.3031485378742218\n",
            "     | > step_time: 1.0276\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            "\u001b[1m   --> STEP: 195/405 -- GLOBAL_STEP: 2225\u001b[0m\n",
            "     | > loss: 1.09590 \n",
            "     | > log_mle: 0.80123 \n",
            "     | > loss_dur: 0.29466 \n",
            "     | > align_error: 0.28556 \n",
            "     | > current_lr: 0.0005565\n",
            "     | > grad_norm: 0.8619526624679565\n",
            "     | > step_time: 1.1461\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 220/405 -- GLOBAL_STEP: 2250\u001b[0m\n",
            "     | > loss: 1.09799 \n",
            "     | > log_mle: 0.81020 \n",
            "     | > loss_dur: 0.28780 \n",
            "     | > align_error: 0.19643 \n",
            "     | > current_lr: 0.00056275\n",
            "     | > grad_norm: 0.43126529455184937\n",
            "     | > step_time: 1.1693\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 245/405 -- GLOBAL_STEP: 2275\u001b[0m\n",
            "     | > loss: 1.09121 \n",
            "     | > log_mle: 0.80738 \n",
            "     | > loss_dur: 0.28383 \n",
            "     | > align_error: 0.15483 \n",
            "     | > current_lr: 0.0005690000000000001\n",
            "     | > grad_norm: 0.40912196040153503\n",
            "     | > step_time: 1.2574\n",
            "     | > loader_time: 0.0044\n",
            "\n",
            "\u001b[1m   --> STEP: 270/405 -- GLOBAL_STEP: 2300\u001b[0m\n",
            "     | > loss: 1.08277 \n",
            "     | > log_mle: 0.80604 \n",
            "     | > loss_dur: 0.27673 \n",
            "     | > align_error: 0.12870 \n",
            "     | > current_lr: 0.00057525\n",
            "     | > grad_norm: 0.25474122166633606\n",
            "     | > step_time: 1.2637\n",
            "     | > loader_time: 0.0082\n",
            "\n",
            "\u001b[1m   --> STEP: 295/405 -- GLOBAL_STEP: 2325\u001b[0m\n",
            "     | > loss: 1.10390 \n",
            "     | > log_mle: 0.81280 \n",
            "     | > loss_dur: 0.29110 \n",
            "     | > align_error: 0.13433 \n",
            "     | > current_lr: 0.0005815\n",
            "     | > grad_norm: 0.6397300958633423\n",
            "     | > step_time: 1.3145\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 320/405 -- GLOBAL_STEP: 2350\u001b[0m\n",
            "     | > loss: 1.09814 \n",
            "     | > log_mle: 0.81402 \n",
            "     | > loss_dur: 0.28412 \n",
            "     | > align_error: 0.09067 \n",
            "     | > current_lr: 0.0005877500000000001\n",
            "     | > grad_norm: 0.3289407193660736\n",
            "     | > step_time: 1.2816\n",
            "     | > loader_time: 0.0096\n",
            "\n",
            "\u001b[1m   --> STEP: 345/405 -- GLOBAL_STEP: 2375\u001b[0m\n",
            "     | > loss: 1.09109 \n",
            "     | > log_mle: 0.81144 \n",
            "     | > loss_dur: 0.27965 \n",
            "     | > align_error: 0.10074 \n",
            "     | > current_lr: 0.000594\n",
            "     | > grad_norm: 0.24689599871635437\n",
            "     | > step_time: 1.2714\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 370/405 -- GLOBAL_STEP: 2400\u001b[0m\n",
            "     | > loss: 1.09783 \n",
            "     | > log_mle: 0.81593 \n",
            "     | > loss_dur: 0.28190 \n",
            "     | > align_error: 0.09648 \n",
            "     | > current_lr: 0.00060025\n",
            "     | > grad_norm: 0.2711293399333954\n",
            "     | > step_time: 1.4967\n",
            "     | > loader_time: 0.0147\n",
            "\n",
            "\u001b[1m   --> STEP: 395/405 -- GLOBAL_STEP: 2425\u001b[0m\n",
            "     | > loss: 1.08203 \n",
            "     | > log_mle: 0.81592 \n",
            "     | > loss_dur: 0.26611 \n",
            "     | > align_error: 0.03528 \n",
            "     | > current_lr: 0.0006064999999999999\n",
            "     | > grad_norm: 0.5861044526100159\n",
            "     | > step_time: 1.4351\n",
            "     | > loader_time: 0.0132\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.94973  (0.94973)\n",
            "     | > log_mle: 0.66444  (0.66444)\n",
            "     | > loss_dur: 0.28529  (0.28529)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.03664  (1.03664)\n",
            "     | > log_mle: 0.74871  (0.74871)\n",
            "     | > loss_dur: 0.28793  (0.28793)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 1.03725  (1.03694)\n",
            "     | > log_mle: 0.75395  (0.75133)\n",
            "     | > loss_dur: 0.28330  (0.28562)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.07059  (1.04816)\n",
            "     | > log_mle: 0.77990  (0.76085)\n",
            "     | > loss_dur: 0.29069  (0.28731)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.08582  (1.05757)\n",
            "     | > log_mle: 0.80302  (0.77140)\n",
            "     | > loss_dur: 0.28279  (0.28618)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 1.07862  (1.06178)\n",
            "     | > log_mle: 0.80145  (0.77741)\n",
            "     | > loss_dur: 0.27717  (0.28438)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.09430  (1.06720)\n",
            "     | > log_mle: 0.81312  (0.78336)\n",
            "     | > loss_dur: 0.28118  (0.28384)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.09409  (1.07104)\n",
            "     | > log_mle: 0.81578  (0.78799)\n",
            "     | > loss_dur: 0.27831  (0.28305)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 1.08946  (1.07335)\n",
            "     | > log_mle: 0.81130  (0.79090)\n",
            "     | > loss_dur: 0.27816  (0.28244)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 2.58715 \u001b[0m(-0.31829)\n",
            "     | > avg_loss:\u001b[91m 1.07335 \u001b[0m(+0.00240)\n",
            "     | > avg_log_mle:\u001b[92m 0.79090 \u001b[0m(-0.00491)\n",
            "     | > avg_loss_dur:\u001b[91m 0.28244 \u001b[0m(+0.00731)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 6/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 01:57:45) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 14/405 -- GLOBAL_STEP: 2450\u001b[0m\n",
            "     | > loss: 1.16705 \n",
            "     | > log_mle: 0.78888 \n",
            "     | > loss_dur: 0.37818 \n",
            "     | > align_error: 0.34594 \n",
            "     | > current_lr: 0.00061275\n",
            "     | > grad_norm: 0.9828085899353027\n",
            "     | > step_time: 0.6727\n",
            "     | > loader_time: 0.0053\n",
            "\n",
            "\u001b[1m   --> STEP: 39/405 -- GLOBAL_STEP: 2475\u001b[0m\n",
            "     | > loss: 1.02937 \n",
            "     | > log_mle: 0.75099 \n",
            "     | > loss_dur: 0.27837 \n",
            "     | > align_error: 0.36211 \n",
            "     | > current_lr: 0.000619\n",
            "     | > grad_norm: 0.3378223180770874\n",
            "     | > step_time: 0.6951\n",
            "     | > loader_time: 0.0019\n",
            "\n",
            "\u001b[1m   --> STEP: 64/405 -- GLOBAL_STEP: 2500\u001b[0m\n",
            "     | > loss: 1.04785 \n",
            "     | > log_mle: 0.76161 \n",
            "     | > loss_dur: 0.28623 \n",
            "     | > align_error: 0.31146 \n",
            "     | > current_lr: 0.0006252499999999999\n",
            "     | > grad_norm: 0.31849515438079834\n",
            "     | > step_time: 0.8001\n",
            "     | > loader_time: 0.0023\n",
            "\n",
            "\u001b[1m   --> STEP: 89/405 -- GLOBAL_STEP: 2525\u001b[0m\n",
            "     | > loss: 1.07009 \n",
            "     | > log_mle: 0.77453 \n",
            "     | > loss_dur: 0.29556 \n",
            "     | > align_error: 0.17493 \n",
            "     | > current_lr: 0.0006315\n",
            "     | > grad_norm: 0.5922514796257019\n",
            "     | > step_time: 0.9379\n",
            "     | > loader_time: 0.0062\n",
            "\n",
            "\u001b[1m   --> STEP: 114/405 -- GLOBAL_STEP: 2550\u001b[0m\n",
            "     | > loss: 1.05279 \n",
            "     | > log_mle: 0.76358 \n",
            "     | > loss_dur: 0.28921 \n",
            "     | > align_error: 0.16489 \n",
            "     | > current_lr: 0.00063775\n",
            "     | > grad_norm: 0.32034504413604736\n",
            "     | > step_time: 0.7856\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            "\u001b[1m   --> STEP: 139/405 -- GLOBAL_STEP: 2575\u001b[0m\n",
            "     | > loss: 1.04924 \n",
            "     | > log_mle: 0.77348 \n",
            "     | > loss_dur: 0.27577 \n",
            "     | > align_error: 0.20110 \n",
            "     | > current_lr: 0.000644\n",
            "     | > grad_norm: 0.43201133608818054\n",
            "     | > step_time: 0.9006\n",
            "     | > loader_time: 0.0028\n",
            "\n",
            "\u001b[1m   --> STEP: 164/405 -- GLOBAL_STEP: 2600\u001b[0m\n",
            "     | > loss: 1.10892 \n",
            "     | > log_mle: 0.80943 \n",
            "     | > loss_dur: 0.29949 \n",
            "     | > align_error: 0.17754 \n",
            "     | > current_lr: 0.00065025\n",
            "     | > grad_norm: 0.7325046062469482\n",
            "     | > step_time: 1.0565\n",
            "     | > loader_time: 0.003\n",
            "\n",
            "\u001b[1m   --> STEP: 189/405 -- GLOBAL_STEP: 2625\u001b[0m\n",
            "     | > loss: 1.09049 \n",
            "     | > log_mle: 0.81149 \n",
            "     | > loss_dur: 0.27900 \n",
            "     | > align_error: 0.13533 \n",
            "     | > current_lr: 0.0006565000000000001\n",
            "     | > grad_norm: 0.20794671773910522\n",
            "     | > step_time: 1.2686\n",
            "     | > loader_time: 0.0083\n",
            "\n",
            "\u001b[1m   --> STEP: 214/405 -- GLOBAL_STEP: 2650\u001b[0m\n",
            "     | > loss: 1.07890 \n",
            "     | > log_mle: 0.79380 \n",
            "     | > loss_dur: 0.28510 \n",
            "     | > align_error: 0.08020 \n",
            "     | > current_lr: 0.00066275\n",
            "     | > grad_norm: 0.527640163898468\n",
            "     | > step_time: 1.0666\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 239/405 -- GLOBAL_STEP: 2675\u001b[0m\n",
            "     | > loss: 1.09255 \n",
            "     | > log_mle: 0.80731 \n",
            "     | > loss_dur: 0.28524 \n",
            "     | > align_error: 0.17546 \n",
            "     | > current_lr: 0.000669\n",
            "     | > grad_norm: 0.2785429358482361\n",
            "     | > step_time: 1.3393\n",
            "     | > loader_time: 0.0123\n",
            "\n",
            "\u001b[1m   --> STEP: 264/405 -- GLOBAL_STEP: 2700\u001b[0m\n",
            "     | > loss: 1.10927 \n",
            "     | > log_mle: 0.81222 \n",
            "     | > loss_dur: 0.29705 \n",
            "     | > align_error: 0.10793 \n",
            "     | > current_lr: 0.00067525\n",
            "     | > grad_norm: 0.32652607560157776\n",
            "     | > step_time: 1.2532\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            "\u001b[1m   --> STEP: 289/405 -- GLOBAL_STEP: 2725\u001b[0m\n",
            "     | > loss: 1.09964 \n",
            "     | > log_mle: 0.81306 \n",
            "     | > loss_dur: 0.28657 \n",
            "     | > align_error: 0.15082 \n",
            "     | > current_lr: 0.0006814999999999999\n",
            "     | > grad_norm: 0.2851434350013733\n",
            "     | > step_time: 1.3069\n",
            "     | > loader_time: 0.0035\n",
            "\n",
            "\u001b[1m   --> STEP: 314/405 -- GLOBAL_STEP: 2750\u001b[0m\n",
            "     | > loss: 1.06800 \n",
            "     | > log_mle: 0.80166 \n",
            "     | > loss_dur: 0.26634 \n",
            "     | > align_error: 0.11487 \n",
            "     | > current_lr: 0.00068775\n",
            "     | > grad_norm: 0.3272051215171814\n",
            "     | > step_time: 1.3169\n",
            "     | > loader_time: 0.0072\n",
            "\n",
            "\u001b[1m   --> STEP: 339/405 -- GLOBAL_STEP: 2775\u001b[0m\n",
            "     | > loss: 1.09066 \n",
            "     | > log_mle: 0.81338 \n",
            "     | > loss_dur: 0.27728 \n",
            "     | > align_error: 0.12984 \n",
            "     | > current_lr: 0.000694\n",
            "     | > grad_norm: 0.18667156994342804\n",
            "     | > step_time: 1.3329\n",
            "     | > loader_time: 0.0129\n",
            "\n",
            "\u001b[1m   --> STEP: 364/405 -- GLOBAL_STEP: 2800\u001b[0m\n",
            "     | > loss: 1.08979 \n",
            "     | > log_mle: 0.81087 \n",
            "     | > loss_dur: 0.27892 \n",
            "     | > align_error: 0.06188 \n",
            "     | > current_lr: 0.00070025\n",
            "     | > grad_norm: 0.2992886006832123\n",
            "     | > step_time: 1.2842\n",
            "     | > loader_time: 0.0088\n",
            "\n",
            "\u001b[1m   --> STEP: 389/405 -- GLOBAL_STEP: 2825\u001b[0m\n",
            "     | > loss: 1.08066 \n",
            "     | > log_mle: 0.81256 \n",
            "     | > loss_dur: 0.26810 \n",
            "     | > align_error: 0.03715 \n",
            "     | > current_lr: 0.0007065\n",
            "     | > grad_norm: 0.5187549591064453\n",
            "     | > step_time: 1.6096\n",
            "     | > loader_time: 0.0128\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.92787  (0.92787)\n",
            "     | > log_mle: 0.65550  (0.65550)\n",
            "     | > loss_dur: 0.27237  (0.27237)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.02304  (1.02304)\n",
            "     | > log_mle: 0.74289  (0.74289)\n",
            "     | > loss_dur: 0.28015  (0.28015)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 1.01971  (1.02137)\n",
            "     | > log_mle: 0.74839  (0.74564)\n",
            "     | > loss_dur: 0.27132  (0.27574)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.06432  (1.03569)\n",
            "     | > log_mle: 0.77513  (0.75547)\n",
            "     | > loss_dur: 0.28919  (0.28022)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.07849  (1.04639)\n",
            "     | > log_mle: 0.79909  (0.76637)\n",
            "     | > loss_dur: 0.27939  (0.28001)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 1.07441  (1.05199)\n",
            "     | > log_mle: 0.79738  (0.77257)\n",
            "     | > loss_dur: 0.27703  (0.27942)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.08826  (1.05804)\n",
            "     | > log_mle: 0.80955  (0.77874)\n",
            "     | > loss_dur: 0.27871  (0.27930)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.08314  (1.06162)\n",
            "     | > log_mle: 0.81233  (0.78354)\n",
            "     | > loss_dur: 0.27082  (0.27809)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 1.07395  (1.06316)\n",
            "     | > log_mle: 0.80768  (0.78655)\n",
            "     | > loss_dur: 0.26627  (0.27661)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 2.79536 \u001b[0m(+0.20821)\n",
            "     | > avg_loss:\u001b[92m 1.06316 \u001b[0m(-0.01018)\n",
            "     | > avg_log_mle:\u001b[92m 0.78655 \u001b[0m(-0.00435)\n",
            "     | > avg_loss_dur:\u001b[92m 0.27661 \u001b[0m(-0.00583)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_2842.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 7/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 02:05:40) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 8/405 -- GLOBAL_STEP: 2850\u001b[0m\n",
            "     | > loss: 1.49636 \n",
            "     | > log_mle: 1.20727 \n",
            "     | > loss_dur: 0.28908 \n",
            "     | > align_error: 0.41827 \n",
            "     | > current_lr: 0.00071275\n",
            "     | > grad_norm: 6.910617828369141\n",
            "     | > step_time: 0.6079\n",
            "     | > loader_time: 0.0016\n",
            "\n",
            "\u001b[1m   --> STEP: 33/405 -- GLOBAL_STEP: 2875\u001b[0m\n",
            "     | > loss: 1.14188 \n",
            "     | > log_mle: 0.86338 \n",
            "     | > loss_dur: 0.27850 \n",
            "     | > align_error: 0.40399 \n",
            "     | > current_lr: 0.000719\n",
            "     | > grad_norm: 0.641204833984375\n",
            "     | > step_time: 0.9004\n",
            "     | > loader_time: 0.0047\n",
            "\n",
            "\u001b[1m   --> STEP: 58/405 -- GLOBAL_STEP: 2900\u001b[0m\n",
            "     | > loss: 1.09700 \n",
            "     | > log_mle: 0.79276 \n",
            "     | > loss_dur: 0.30423 \n",
            "     | > align_error: 0.29469 \n",
            "     | > current_lr: 0.00072525\n",
            "     | > grad_norm: 0.4143599569797516\n",
            "     | > step_time: 0.7889\n",
            "     | > loader_time: 0.0024\n",
            "\n",
            "\u001b[1m   --> STEP: 83/405 -- GLOBAL_STEP: 2925\u001b[0m\n",
            "     | > loss: 1.06860 \n",
            "     | > log_mle: 0.80329 \n",
            "     | > loss_dur: 0.26531 \n",
            "     | > align_error: 0.20277 \n",
            "     | > current_lr: 0.0007315\n",
            "     | > grad_norm: 0.12777554988861084\n",
            "     | > step_time: 0.9095\n",
            "     | > loader_time: 0.0029\n",
            "\n",
            "\u001b[1m   --> STEP: 108/405 -- GLOBAL_STEP: 2950\u001b[0m\n",
            "     | > loss: 1.09183 \n",
            "     | > log_mle: 0.80280 \n",
            "     | > loss_dur: 0.28903 \n",
            "     | > align_error: 0.40788 \n",
            "     | > current_lr: 0.00073775\n",
            "     | > grad_norm: 0.1960994452238083\n",
            "     | > step_time: 0.995\n",
            "     | > loader_time: 0.0054\n",
            "\n",
            "\u001b[1m   --> STEP: 133/405 -- GLOBAL_STEP: 2975\u001b[0m\n",
            "     | > loss: 1.06069 \n",
            "     | > log_mle: 0.77535 \n",
            "     | > loss_dur: 0.28534 \n",
            "     | > align_error: 0.21007 \n",
            "     | > current_lr: 0.0007440000000000001\n",
            "     | > grad_norm: 0.26158326864242554\n",
            "     | > step_time: 0.89\n",
            "     | > loader_time: 0.0026\n",
            "\n",
            "\u001b[1m   --> STEP: 158/405 -- GLOBAL_STEP: 3000\u001b[0m\n",
            "     | > loss: 1.08543 \n",
            "     | > log_mle: 0.79604 \n",
            "     | > loss_dur: 0.28939 \n",
            "     | > align_error: 0.28706 \n",
            "     | > current_lr: 0.0007502499999999999\n",
            "     | > grad_norm: 0.4391930103302002\n",
            "     | > step_time: 1.0245\n",
            "     | > loader_time: 0.0028\n",
            "\n",
            "\u001b[1m   --> STEP: 183/405 -- GLOBAL_STEP: 3025\u001b[0m\n",
            "     | > loss: 1.05824 \n",
            "     | > log_mle: 0.78372 \n",
            "     | > loss_dur: 0.27452 \n",
            "     | > align_error: 0.10532 \n",
            "     | > current_lr: 0.0007565\n",
            "     | > grad_norm: 0.21043483912944794\n",
            "     | > step_time: 0.9503\n",
            "     | > loader_time: 0.0028\n",
            "\n",
            "\u001b[1m   --> STEP: 208/405 -- GLOBAL_STEP: 3050\u001b[0m\n",
            "     | > loss: 1.08896 \n",
            "     | > log_mle: 0.80825 \n",
            "     | > loss_dur: 0.28071 \n",
            "     | > align_error: 0.11993 \n",
            "     | > current_lr: 0.00076275\n",
            "     | > grad_norm: 0.1593175083398819\n",
            "     | > step_time: 1.128\n",
            "     | > loader_time: 0.0035\n",
            "\n",
            "\u001b[1m   --> STEP: 233/405 -- GLOBAL_STEP: 3075\u001b[0m\n",
            "     | > loss: 1.08970 \n",
            "     | > log_mle: 0.80665 \n",
            "     | > loss_dur: 0.28305 \n",
            "     | > align_error: 0.11845 \n",
            "     | > current_lr: 0.0007689999999999999\n",
            "     | > grad_norm: 0.15592612326145172\n",
            "     | > step_time: 1.1555\n",
            "     | > loader_time: 0.0082\n",
            "\n",
            "\u001b[1m   --> STEP: 258/405 -- GLOBAL_STEP: 3100\u001b[0m\n",
            "     | > loss: 1.08565 \n",
            "     | > log_mle: 0.81064 \n",
            "     | > loss_dur: 0.27501 \n",
            "     | > align_error: 0.14051 \n",
            "     | > current_lr: 0.00077525\n",
            "     | > grad_norm: 0.4052453935146332\n",
            "     | > step_time: 1.2327\n",
            "     | > loader_time: 0.0037\n",
            "\n",
            "\u001b[1m   --> STEP: 283/405 -- GLOBAL_STEP: 3125\u001b[0m\n",
            "     | > loss: 1.09541 \n",
            "     | > log_mle: 0.81001 \n",
            "     | > loss_dur: 0.28540 \n",
            "     | > align_error: 0.01537 \n",
            "     | > current_lr: 0.0007815\n",
            "     | > grad_norm: 0.2847530245780945\n",
            "     | > step_time: 1.271\n",
            "     | > loader_time: 0.0036\n",
            "\n",
            "\u001b[1m   --> STEP: 308/405 -- GLOBAL_STEP: 3150\u001b[0m\n",
            "     | > loss: 1.10156 \n",
            "     | > log_mle: 0.81398 \n",
            "     | > loss_dur: 0.28758 \n",
            "     | > align_error: 0.12306 \n",
            "     | > current_lr: 0.00078775\n",
            "     | > grad_norm: 0.19178828597068787\n",
            "     | > step_time: 1.3379\n",
            "     | > loader_time: 0.0033\n",
            "\n",
            "\u001b[1m   --> STEP: 333/405 -- GLOBAL_STEP: 3175\u001b[0m\n",
            "     | > loss: 1.08513 \n",
            "     | > log_mle: 0.81124 \n",
            "     | > loss_dur: 0.27388 \n",
            "     | > align_error: 0.04721 \n",
            "     | > current_lr: 0.000794\n",
            "     | > grad_norm: 0.11680928617715836\n",
            "     | > step_time: 1.3664\n",
            "     | > loader_time: 0.0128\n",
            "\n",
            "\u001b[1m   --> STEP: 358/405 -- GLOBAL_STEP: 3200\u001b[0m\n",
            "     | > loss: 1.09570 \n",
            "     | > log_mle: 0.81585 \n",
            "     | > loss_dur: 0.27985 \n",
            "     | > align_error: 0.02710 \n",
            "     | > current_lr: 0.0008002500000000001\n",
            "     | > grad_norm: 0.44121623039245605\n",
            "     | > step_time: 1.3448\n",
            "     | > loader_time: 0.0134\n",
            "\n",
            "\u001b[1m   --> STEP: 383/405 -- GLOBAL_STEP: 3225\u001b[0m\n",
            "     | > loss: 1.07881 \n",
            "     | > log_mle: 0.81544 \n",
            "     | > loss_dur: 0.26336 \n",
            "     | > align_error: 0.00625 \n",
            "     | > current_lr: 0.0008065\n",
            "     | > grad_norm: 0.19245319068431854\n",
            "     | > step_time: 1.4774\n",
            "     | > loader_time: 0.0061\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.93652  (0.93652)\n",
            "     | > log_mle: 0.65949  (0.65949)\n",
            "     | > loss_dur: 0.27703  (0.27703)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.03844  (1.03844)\n",
            "     | > log_mle: 0.74525  (0.74525)\n",
            "     | > loss_dur: 0.29319  (0.29319)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 1.02431  (1.03137)\n",
            "     | > log_mle: 0.75072  (0.74798)\n",
            "     | > loss_dur: 0.27359  (0.28339)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.06230  (1.04168)\n",
            "     | > log_mle: 0.77685  (0.75760)\n",
            "     | > loss_dur: 0.28545  (0.28408)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.07613  (1.05029)\n",
            "     | > log_mle: 0.80071  (0.76838)\n",
            "     | > loss_dur: 0.27542  (0.28191)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 1.07299  (1.05483)\n",
            "     | > log_mle: 0.79905  (0.77451)\n",
            "     | > loss_dur: 0.27394  (0.28032)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.08699  (1.06019)\n",
            "     | > log_mle: 0.81097  (0.78059)\n",
            "     | > loss_dur: 0.27602  (0.27960)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.07932  (1.06292)\n",
            "     | > log_mle: 0.81377  (0.78533)\n",
            "     | > loss_dur: 0.26555  (0.27759)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 1.07384  (1.06429)\n",
            "     | > log_mle: 0.80887  (0.78827)\n",
            "     | > loss_dur: 0.26497  (0.27602)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 2.68439 \u001b[0m(-0.11097)\n",
            "     | > avg_loss:\u001b[91m 1.06429 \u001b[0m(+0.00113)\n",
            "     | > avg_log_mle:\u001b[91m 0.78827 \u001b[0m(+0.00172)\n",
            "     | > avg_loss_dur:\u001b[92m 0.27602 \u001b[0m(-0.00059)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 8/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 02:13:32) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/405 -- GLOBAL_STEP: 3250\u001b[0m\n",
            "     | > loss: 1.16234 \n",
            "     | > log_mle: 0.79755 \n",
            "     | > loss_dur: 0.36479 \n",
            "     | > align_error: 0.44531 \n",
            "     | > current_lr: 0.00081275\n",
            "     | > grad_norm: 8.798216819763184\n",
            "     | > step_time: 0.4987\n",
            "     | > loader_time: 0.0012\n",
            "\n",
            "\u001b[1m   --> STEP: 27/405 -- GLOBAL_STEP: 3275\u001b[0m\n",
            "     | > loss: 1.12364 \n",
            "     | > log_mle: 0.79257 \n",
            "     | > loss_dur: 0.33107 \n",
            "     | > align_error: 0.31201 \n",
            "     | > current_lr: 0.0008190000000000001\n",
            "     | > grad_norm: 0.930683434009552\n",
            "     | > step_time: 0.6786\n",
            "     | > loader_time: 0.0068\n",
            "\n",
            "\u001b[1m   --> STEP: 52/405 -- GLOBAL_STEP: 3300\u001b[0m\n",
            "     | > loss: 1.04155 \n",
            "     | > log_mle: 0.74199 \n",
            "     | > loss_dur: 0.29956 \n",
            "     | > align_error: 0.01754 \n",
            "     | > current_lr: 0.0008252499999999999\n",
            "     | > grad_norm: 0.3280735909938812\n",
            "     | > step_time: 0.6686\n",
            "     | > loader_time: 0.0018\n",
            "\n",
            "\u001b[1m   --> STEP: 77/405 -- GLOBAL_STEP: 3325\u001b[0m\n",
            "     | > loss: 1.04689 \n",
            "     | > log_mle: 0.75348 \n",
            "     | > loss_dur: 0.29341 \n",
            "     | > align_error: 0.04167 \n",
            "     | > current_lr: 0.0008315\n",
            "     | > grad_norm: 0.19660013914108276\n",
            "     | > step_time: 0.7108\n",
            "     | > loader_time: 0.0021\n",
            "\n",
            "\u001b[1m   --> STEP: 102/405 -- GLOBAL_STEP: 3350\u001b[0m\n",
            "     | > loss: 1.04927 \n",
            "     | > log_mle: 0.75953 \n",
            "     | > loss_dur: 0.28975 \n",
            "     | > align_error: 0.16415 \n",
            "     | > current_lr: 0.00083775\n",
            "     | > grad_norm: 0.3028799295425415\n",
            "     | > step_time: 0.9031\n",
            "     | > loader_time: 0.0072\n",
            "\n",
            "\u001b[1m   --> STEP: 127/405 -- GLOBAL_STEP: 3375\u001b[0m\n",
            "     | > loss: 1.09882 \n",
            "     | > log_mle: 0.80133 \n",
            "     | > loss_dur: 0.29749 \n",
            "     | > align_error: 0.42450 \n",
            "     | > current_lr: 0.000844\n",
            "     | > grad_norm: 0.21209174394607544\n",
            "     | > step_time: 1.0972\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            "\u001b[1m   --> STEP: 152/405 -- GLOBAL_STEP: 3400\u001b[0m\n",
            "     | > loss: 1.09282 \n",
            "     | > log_mle: 0.80696 \n",
            "     | > loss_dur: 0.28586 \n",
            "     | > align_error: 0.34925 \n",
            "     | > current_lr: 0.00085025\n",
            "     | > grad_norm: 0.4566195011138916\n",
            "     | > step_time: 1.1351\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            "\u001b[1m   --> STEP: 177/405 -- GLOBAL_STEP: 3425\u001b[0m\n",
            "     | > loss: 1.09977 \n",
            "     | > log_mle: 0.80376 \n",
            "     | > loss_dur: 0.29602 \n",
            "     | > align_error: 0.23886 \n",
            "     | > current_lr: 0.0008565\n",
            "     | > grad_norm: 0.28557807207107544\n",
            "     | > step_time: 1.0835\n",
            "     | > loader_time: 0.0032\n",
            "\n",
            "\u001b[1m   --> STEP: 202/405 -- GLOBAL_STEP: 3450\u001b[0m\n",
            "     | > loss: 1.09370 \n",
            "     | > log_mle: 0.80030 \n",
            "     | > loss_dur: 0.29340 \n",
            "     | > align_error: 0.20483 \n",
            "     | > current_lr: 0.00086275\n",
            "     | > grad_norm: 0.3513554334640503\n",
            "     | > step_time: 1.1739\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            "\u001b[1m   --> STEP: 227/405 -- GLOBAL_STEP: 3475\u001b[0m\n",
            "     | > loss: 1.11439 \n",
            "     | > log_mle: 0.81068 \n",
            "     | > loss_dur: 0.30371 \n",
            "     | > align_error: 0.17661 \n",
            "     | > current_lr: 0.000869\n",
            "     | > grad_norm: 0.20322385430335999\n",
            "     | > step_time: 1.2107\n",
            "     | > loader_time: 0.0035\n",
            "\n",
            "\u001b[1m   --> STEP: 252/405 -- GLOBAL_STEP: 3500\u001b[0m\n",
            "     | > loss: 1.10282 \n",
            "     | > log_mle: 0.80690 \n",
            "     | > loss_dur: 0.29592 \n",
            "     | > align_error: 0.14654 \n",
            "     | > current_lr: 0.00087525\n",
            "     | > grad_norm: 0.25740379095077515\n",
            "     | > step_time: 1.2967\n",
            "     | > loader_time: 0.0102\n",
            "\n",
            "\u001b[1m   --> STEP: 277/405 -- GLOBAL_STEP: 3525\u001b[0m\n",
            "     | > loss: 1.10188 \n",
            "     | > log_mle: 0.81333 \n",
            "     | > loss_dur: 0.28854 \n",
            "     | > align_error: 0.11241 \n",
            "     | > current_lr: 0.0008815\n",
            "     | > grad_norm: 0.29112306237220764\n",
            "     | > step_time: 1.2799\n",
            "     | > loader_time: 0.009\n",
            "\n",
            "\u001b[1m   --> STEP: 302/405 -- GLOBAL_STEP: 3550\u001b[0m\n",
            "     | > loss: 1.10047 \n",
            "     | > log_mle: 0.81388 \n",
            "     | > loss_dur: 0.28659 \n",
            "     | > align_error: 0.10893 \n",
            "     | > current_lr: 0.0008877500000000001\n",
            "     | > grad_norm: 0.23884321749210358\n",
            "     | > step_time: 1.3542\n",
            "     | > loader_time: 0.0036\n",
            "\n",
            "\u001b[1m   --> STEP: 327/405 -- GLOBAL_STEP: 3575\u001b[0m\n",
            "     | > loss: 1.10872 \n",
            "     | > log_mle: 0.81437 \n",
            "     | > loss_dur: 0.29436 \n",
            "     | > align_error: 0.01762 \n",
            "     | > current_lr: 0.000894\n",
            "     | > grad_norm: 0.3112625479698181\n",
            "     | > step_time: 1.2401\n",
            "     | > loader_time: 0.0054\n",
            "\n",
            "\u001b[1m   --> STEP: 352/405 -- GLOBAL_STEP: 3600\u001b[0m\n",
            "     | > loss: 1.09763 \n",
            "     | > log_mle: 0.81352 \n",
            "     | > loss_dur: 0.28410 \n",
            "     | > align_error: 0.05954 \n",
            "     | > current_lr: 0.0009002499999999999\n",
            "     | > grad_norm: 0.17024734616279602\n",
            "     | > step_time: 1.4155\n",
            "     | > loader_time: 0.0141\n",
            "\n",
            "\u001b[1m   --> STEP: 377/405 -- GLOBAL_STEP: 3625\u001b[0m\n",
            "     | > loss: 1.08935 \n",
            "     | > log_mle: 0.81344 \n",
            "     | > loss_dur: 0.27590 \n",
            "     | > align_error: 0.02229 \n",
            "     | > current_lr: 0.0009065\n",
            "     | > grad_norm: 0.2776155173778534\n",
            "     | > step_time: 1.4232\n",
            "     | > loader_time: 0.0035\n",
            "\n",
            "\u001b[1m   --> STEP: 402/405 -- GLOBAL_STEP: 3650\u001b[0m\n",
            "     | > loss: 1.07392 \n",
            "     | > log_mle: 0.81476 \n",
            "     | > loss_dur: 0.25916 \n",
            "     | > align_error: 0.01801 \n",
            "     | > current_lr: 0.0009127499999999999\n",
            "     | > grad_norm: 0.14856518805027008\n",
            "     | > step_time: 1.1039\n",
            "     | > loader_time: 0.003\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.96638  (0.96638)\n",
            "     | > log_mle: 0.65795  (0.65795)\n",
            "     | > loss_dur: 0.30843  (0.30843)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.03695  (1.03695)\n",
            "     | > log_mle: 0.74453  (0.74453)\n",
            "     | > loss_dur: 0.29241  (0.29241)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 1.03058  (1.03376)\n",
            "     | > log_mle: 0.75005  (0.74729)\n",
            "     | > loss_dur: 0.28053  (0.28647)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.05409  (1.04054)\n",
            "     | > log_mle: 0.77634  (0.75698)\n",
            "     | > loss_dur: 0.27775  (0.28356)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.05218  (1.04345)\n",
            "     | > log_mle: 0.80030  (0.76781)\n",
            "     | > loss_dur: 0.25187  (0.27564)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 1.04874  (1.04451)\n",
            "     | > log_mle: 0.79856  (0.77396)\n",
            "     | > loss_dur: 0.25017  (0.27055)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.05529  (1.04630)\n",
            "     | > log_mle: 0.81058  (0.78006)\n",
            "     | > loss_dur: 0.24471  (0.26624)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.05944  (1.04818)\n",
            "     | > log_mle: 0.81341  (0.78483)\n",
            "     | > loss_dur: 0.24603  (0.26335)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 1.03941  (1.04708)\n",
            "     | > log_mle: 0.80854  (0.78779)\n",
            "     | > loss_dur: 0.23087  (0.25929)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 2.85428 \u001b[0m(+0.16989)\n",
            "     | > avg_loss:\u001b[92m 1.04708 \u001b[0m(-0.01721)\n",
            "     | > avg_log_mle:\u001b[92m 0.78779 \u001b[0m(-0.00048)\n",
            "     | > avg_loss_dur:\u001b[92m 0.25929 \u001b[0m(-0.01672)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_3654.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 9/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 02:21:28) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 21/405 -- GLOBAL_STEP: 3675\u001b[0m\n",
            "     | > loss: 1.08756 \n",
            "     | > log_mle: 0.75791 \n",
            "     | > loss_dur: 0.32965 \n",
            "     | > align_error: 0.33014 \n",
            "     | > current_lr: 0.000919\n",
            "     | > grad_norm: 0.39201462268829346\n",
            "     | > step_time: 0.755\n",
            "     | > loader_time: 0.009\n",
            "\n",
            "\u001b[1m   --> STEP: 46/405 -- GLOBAL_STEP: 3700\u001b[0m\n",
            "     | > loss: 1.07974 \n",
            "     | > log_mle: 0.77806 \n",
            "     | > loss_dur: 0.30168 \n",
            "     | > align_error: 0.39271 \n",
            "     | > current_lr: 0.00092525\n",
            "     | > grad_norm: 0.14908434450626373\n",
            "     | > step_time: 0.7528\n",
            "     | > loader_time: 0.0025\n",
            "\n",
            "\u001b[1m   --> STEP: 71/405 -- GLOBAL_STEP: 3725\u001b[0m\n",
            "     | > loss: 1.07040 \n",
            "     | > log_mle: 0.78126 \n",
            "     | > loss_dur: 0.28914 \n",
            "     | > align_error: 0.35316 \n",
            "     | > current_lr: 0.0009315\n",
            "     | > grad_norm: 0.1324271261692047\n",
            "     | > step_time: 0.8681\n",
            "     | > loader_time: 0.0077\n",
            "\n",
            "\u001b[1m   --> STEP: 96/405 -- GLOBAL_STEP: 3750\u001b[0m\n",
            "     | > loss: 1.05412 \n",
            "     | > log_mle: 0.74919 \n",
            "     | > loss_dur: 0.30492 \n",
            "     | > align_error: 0.17917 \n",
            "     | > current_lr: 0.00093775\n",
            "     | > grad_norm: 0.33173736929893494\n",
            "     | > step_time: 0.9185\n",
            "     | > loader_time: 0.0025\n",
            "\n",
            "\u001b[1m   --> STEP: 121/405 -- GLOBAL_STEP: 3775\u001b[0m\n",
            "     | > loss: 1.07263 \n",
            "     | > log_mle: 0.78894 \n",
            "     | > loss_dur: 0.28369 \n",
            "     | > align_error: 0.16048 \n",
            "     | > current_lr: 0.000944\n",
            "     | > grad_norm: 0.29116129875183105\n",
            "     | > step_time: 1.0077\n",
            "     | > loader_time: 0.0029\n",
            "\n",
            "\u001b[1m   --> STEP: 146/405 -- GLOBAL_STEP: 3800\u001b[0m\n",
            "     | > loss: 1.05909 \n",
            "     | > log_mle: 0.77069 \n",
            "     | > loss_dur: 0.28839 \n",
            "     | > align_error: 0.10108 \n",
            "     | > current_lr: 0.00095025\n",
            "     | > grad_norm: 0.19989897310733795\n",
            "     | > step_time: 0.956\n",
            "     | > loader_time: 0.0083\n",
            "\n",
            "\u001b[1m   --> STEP: 171/405 -- GLOBAL_STEP: 3825\u001b[0m\n",
            "     | > loss: 1.07929 \n",
            "     | > log_mle: 0.78850 \n",
            "     | > loss_dur: 0.29079 \n",
            "     | > align_error: 0.22581 \n",
            "     | > current_lr: 0.0009565\n",
            "     | > grad_norm: 0.2527483105659485\n",
            "     | > step_time: 1.0707\n",
            "     | > loader_time: 0.0059\n",
            "\n",
            "\u001b[1m   --> STEP: 196/405 -- GLOBAL_STEP: 3850\u001b[0m\n",
            "     | > loss: 1.09370 \n",
            "     | > log_mle: 0.81097 \n",
            "     | > loss_dur: 0.28273 \n",
            "     | > align_error: 0.20164 \n",
            "     | > current_lr: 0.0009627500000000001\n",
            "     | > grad_norm: 0.27370578050613403\n",
            "     | > step_time: 1.2573\n",
            "     | > loader_time: 0.0083\n",
            "\n",
            "\u001b[1m   --> STEP: 221/405 -- GLOBAL_STEP: 3875\u001b[0m\n",
            "     | > loss: 1.08765 \n",
            "     | > log_mle: 0.80600 \n",
            "     | > loss_dur: 0.28165 \n",
            "     | > align_error: 0.17587 \n",
            "     | > current_lr: 0.000969\n",
            "     | > grad_norm: 0.14973780512809753\n",
            "     | > step_time: 1.2607\n",
            "     | > loader_time: 0.0073\n",
            "\n",
            "\u001b[1m   --> STEP: 246/405 -- GLOBAL_STEP: 3900\u001b[0m\n",
            "     | > loss: 1.10482 \n",
            "     | > log_mle: 0.81172 \n",
            "     | > loss_dur: 0.29310 \n",
            "     | > align_error: 0.28511 \n",
            "     | > current_lr: 0.00097525\n",
            "     | > grad_norm: 0.16367004811763763\n",
            "     | > step_time: 1.341\n",
            "     | > loader_time: 0.0084\n",
            "\n",
            "\u001b[1m   --> STEP: 271/405 -- GLOBAL_STEP: 3925\u001b[0m\n",
            "     | > loss: 1.08545 \n",
            "     | > log_mle: 0.80176 \n",
            "     | > loss_dur: 0.28369 \n",
            "     | > align_error: 0.12894 \n",
            "     | > current_lr: 0.0009815\n",
            "     | > grad_norm: 0.1786523163318634\n",
            "     | > step_time: 1.3656\n",
            "     | > loader_time: 0.0081\n",
            "\n",
            "\u001b[1m   --> STEP: 296/405 -- GLOBAL_STEP: 3950\u001b[0m\n",
            "     | > loss: 1.09873 \n",
            "     | > log_mle: 0.80988 \n",
            "     | > loss_dur: 0.28884 \n",
            "     | > align_error: 0.06723 \n",
            "     | > current_lr: 0.00098775\n",
            "     | > grad_norm: 0.20115335285663605\n",
            "     | > step_time: 1.3613\n",
            "     | > loader_time: 0.0092\n",
            "\n",
            "\u001b[1m   --> STEP: 321/405 -- GLOBAL_STEP: 3975\u001b[0m\n",
            "     | > loss: 1.09225 \n",
            "     | > log_mle: 0.81130 \n",
            "     | > loss_dur: 0.28096 \n",
            "     | > align_error: 0.14894 \n",
            "     | > current_lr: 0.000994\n",
            "     | > grad_norm: 0.25460532307624817\n",
            "     | > step_time: 1.2705\n",
            "     | > loader_time: 0.0062\n",
            "\n",
            "\u001b[1m   --> STEP: 346/405 -- GLOBAL_STEP: 4000\u001b[0m\n",
            "     | > loss: 1.10483 \n",
            "     | > log_mle: 0.81099 \n",
            "     | > loss_dur: 0.29384 \n",
            "     | > align_error: 0.08389 \n",
            "     | > current_lr: 0.0009998750234326182\n",
            "     | > grad_norm: 0.32062649726867676\n",
            "     | > step_time: 1.4758\n",
            "     | > loader_time: 0.0032\n",
            "\n",
            "\u001b[1m   --> STEP: 371/405 -- GLOBAL_STEP: 4025\u001b[0m\n",
            "     | > loss: 1.09109 \n",
            "     | > log_mle: 0.81185 \n",
            "     | > loss_dur: 0.27925 \n",
            "     | > align_error: 0.03729 \n",
            "     | > current_lr: 0.000996765758414952\n",
            "     | > grad_norm: 0.20714232325553894\n",
            "     | > step_time: 1.4089\n",
            "     | > loader_time: 0.0073\n",
            "\n",
            "\u001b[1m   --> STEP: 396/405 -- GLOBAL_STEP: 4050\u001b[0m\n",
            "     | > loss: 1.07155 \n",
            "     | > log_mle: 0.81163 \n",
            "     | > loss_dur: 0.25992 \n",
            "     | > align_error: 0.04631 \n",
            "     | > current_lr: 0.0009936853203715925\n",
            "     | > grad_norm: 0.11036215722560883\n",
            "     | > step_time: 1.4935\n",
            "     | > loader_time: 0.0134\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.93261  (0.93261)\n",
            "     | > log_mle: 0.65075  (0.65075)\n",
            "     | > loss_dur: 0.28186  (0.28186)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.00375  (1.00375)\n",
            "     | > log_mle: 0.73956  (0.73956)\n",
            "     | > loss_dur: 0.26419  (0.26419)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.97470  (0.98923)\n",
            "     | > log_mle: 0.74520  (0.74238)\n",
            "     | > loss_dur: 0.22950  (0.24685)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.01514  (0.99786)\n",
            "     | > log_mle: 0.77224  (0.75233)\n",
            "     | > loss_dur: 0.24290  (0.24553)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.01461  (1.00205)\n",
            "     | > log_mle: 0.79702  (0.76351)\n",
            "     | > loss_dur: 0.21759  (0.23854)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.99985  (1.00161)\n",
            "     | > log_mle: 0.79520  (0.76984)\n",
            "     | > loss_dur: 0.20465  (0.23177)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.00819  (1.00271)\n",
            "     | > log_mle: 0.80747  (0.77611)\n",
            "     | > loss_dur: 0.20072  (0.22659)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.01072  (1.00385)\n",
            "     | > log_mle: 0.81054  (0.78103)\n",
            "     | > loss_dur: 0.20018  (0.22282)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 0.99865  (1.00320)\n",
            "     | > log_mle: 0.80559  (0.78410)\n",
            "     | > loss_dur: 0.19306  (0.21910)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 2.57372 \u001b[0m(-0.28056)\n",
            "     | > avg_loss:\u001b[92m 1.00320 \u001b[0m(-0.04388)\n",
            "     | > avg_log_mle:\u001b[92m 0.78410 \u001b[0m(-0.00369)\n",
            "     | > avg_loss_dur:\u001b[92m 0.21910 \u001b[0m(-0.04019)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_4060.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 10/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 02:29:20) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/405 -- GLOBAL_STEP: 4075\u001b[0m\n",
            "     | > loss: 1.03871 \n",
            "     | > log_mle: 0.73905 \n",
            "     | > loss_dur: 0.29966 \n",
            "     | > align_error: 0.39931 \n",
            "     | > current_lr: 0.0009906332665982247\n",
            "     | > grad_norm: 0.4756525158882141\n",
            "     | > step_time: 0.6271\n",
            "     | > loader_time: 0.0017\n",
            "\n",
            "\u001b[1m   --> STEP: 40/405 -- GLOBAL_STEP: 4100\u001b[0m\n",
            "     | > loss: 1.00617 \n",
            "     | > log_mle: 0.71980 \n",
            "     | > loss_dur: 0.28637 \n",
            "     | > align_error: 0.28125 \n",
            "     | > current_lr: 0.0009876091638506721\n",
            "     | > grad_norm: 0.27505266666412354\n",
            "     | > step_time: 0.6972\n",
            "     | > loader_time: 0.0052\n",
            "\n",
            "\u001b[1m   --> STEP: 65/405 -- GLOBAL_STEP: 4125\u001b[0m\n",
            "     | > loss: 1.08497 \n",
            "     | > log_mle: 0.78628 \n",
            "     | > loss_dur: 0.29869 \n",
            "     | > align_error: 0.33191 \n",
            "     | > current_lr: 0.0009846125880865614\n",
            "     | > grad_norm: 0.21448995172977448\n",
            "     | > step_time: 0.8512\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            "\u001b[1m   --> STEP: 90/405 -- GLOBAL_STEP: 4150\u001b[0m\n",
            "     | > loss: 1.03242 \n",
            "     | > log_mle: 0.75356 \n",
            "     | > loss_dur: 0.27886 \n",
            "     | > align_error: 0.22547 \n",
            "     | > current_lr: 0.0009816431242155583\n",
            "     | > grad_norm: 0.1537347286939621\n",
            "     | > step_time: 0.7896\n",
            "     | > loader_time: 0.0083\n",
            "\n",
            "\u001b[1m   --> STEP: 115/405 -- GLOBAL_STEP: 4175\u001b[0m\n",
            "     | > loss: 1.08050 \n",
            "     | > log_mle: 0.79180 \n",
            "     | > loss_dur: 0.28871 \n",
            "     | > align_error: 0.19669 \n",
            "     | > current_lr: 0.0009787003658578391\n",
            "     | > grad_norm: 0.33958733081817627\n",
            "     | > step_time: 1.1369\n",
            "     | > loader_time: 0.0078\n",
            "\n",
            "\u001b[1m   --> STEP: 140/405 -- GLOBAL_STEP: 4200\u001b[0m\n",
            "     | > loss: 1.10772 \n",
            "     | > log_mle: 0.80281 \n",
            "     | > loss_dur: 0.30491 \n",
            "     | > align_error: 0.34306 \n",
            "     | > current_lr: 0.0009757839151104798\n",
            "     | > grad_norm: 0.2539978325366974\n",
            "     | > step_time: 1.1055\n",
            "     | > loader_time: 0.0032\n",
            "\n",
            "\u001b[1m   --> STEP: 165/405 -- GLOBAL_STEP: 4225\u001b[0m\n",
            "     | > loss: 1.06267 \n",
            "     | > log_mle: 0.78740 \n",
            "     | > loss_dur: 0.27527 \n",
            "     | > align_error: 0.20536 \n",
            "     | > current_lr: 0.0009728933823214567\n",
            "     | > grad_norm: 0.2562463879585266\n",
            "     | > step_time: 1.027\n",
            "     | > loader_time: 0.0065\n",
            "\n",
            "\u001b[1m   --> STEP: 190/405 -- GLOBAL_STEP: 4250\u001b[0m\n",
            "     | > loss: 1.08404 \n",
            "     | > log_mle: 0.79414 \n",
            "     | > loss_dur: 0.28991 \n",
            "     | > align_error: 0.10170 \n",
            "     | > current_lr: 0.0009700283858709683\n",
            "     | > grad_norm: 0.2536346912384033\n",
            "     | > step_time: 1.1551\n",
            "     | > loader_time: 0.0101\n",
            "\n",
            "\u001b[1m   --> STEP: 215/405 -- GLOBAL_STEP: 4275\u001b[0m\n",
            "     | > loss: 1.09314 \n",
            "     | > log_mle: 0.79653 \n",
            "     | > loss_dur: 0.29661 \n",
            "     | > align_error: 0.12526 \n",
            "     | > current_lr: 0.0009671885519598004\n",
            "     | > grad_norm: 0.430366575717926\n",
            "     | > step_time: 1.1211\n",
            "     | > loader_time: 0.0135\n",
            "\n",
            "\u001b[1m   --> STEP: 240/405 -- GLOBAL_STEP: 4300\u001b[0m\n",
            "     | > loss: 1.09345 \n",
            "     | > log_mle: 0.80431 \n",
            "     | > loss_dur: 0.28914 \n",
            "     | > align_error: 0.16293 \n",
            "     | > current_lr: 0.0009643735144044643\n",
            "     | > grad_norm: 0.26884812116622925\n",
            "     | > step_time: 1.2422\n",
            "     | > loader_time: 0.003\n",
            "\n",
            "\u001b[1m   --> STEP: 265/405 -- GLOBAL_STEP: 4325\u001b[0m\n",
            "     | > loss: 1.07878 \n",
            "     | > log_mle: 0.79642 \n",
            "     | > loss_dur: 0.28236 \n",
            "     | > align_error: 0.11355 \n",
            "     | > current_lr: 0.0009615829144388593\n",
            "     | > grad_norm: 0.3616609573364258\n",
            "     | > step_time: 1.2563\n",
            "     | > loader_time: 0.0062\n",
            "\n",
            "\u001b[1m   --> STEP: 290/405 -- GLOBAL_STEP: 4350\u001b[0m\n",
            "     | > loss: 1.08996 \n",
            "     | > log_mle: 0.80628 \n",
            "     | > loss_dur: 0.28368 \n",
            "     | > align_error: 0.05862 \n",
            "     | > current_lr: 0.0009588164005222092\n",
            "     | > grad_norm: 0.16965502500534058\n",
            "     | > step_time: 1.2856\n",
            "     | > loader_time: 0.0095\n",
            "\n",
            "\u001b[1m   --> STEP: 315/405 -- GLOBAL_STEP: 4375\u001b[0m\n",
            "     | > loss: 1.08488 \n",
            "     | > log_mle: 0.80710 \n",
            "     | > loss_dur: 0.27778 \n",
            "     | > align_error: 0.19468 \n",
            "     | > current_lr: 0.0009560736281530443\n",
            "     | > grad_norm: 0.1424393206834793\n",
            "     | > step_time: 1.4981\n",
            "     | > loader_time: 0.013\n",
            "\n",
            "\u001b[1m   --> STEP: 340/405 -- GLOBAL_STEP: 4400\u001b[0m\n",
            "     | > loss: 1.08654 \n",
            "     | > log_mle: 0.80897 \n",
            "     | > loss_dur: 0.27757 \n",
            "     | > align_error: 0.07898 \n",
            "     | > current_lr: 0.0009533542596890027\n",
            "     | > grad_norm: 0.2943817973136902\n",
            "     | > step_time: 1.3334\n",
            "     | > loader_time: 0.0038\n",
            "\n",
            "\u001b[1m   --> STEP: 365/405 -- GLOBAL_STEP: 4425\u001b[0m\n",
            "     | > loss: 1.08388 \n",
            "     | > log_mle: 0.80962 \n",
            "     | > loss_dur: 0.27426 \n",
            "     | > align_error: 0.09869 \n",
            "     | > current_lr: 0.0009506579641722373\n",
            "     | > grad_norm: 0.32541894912719727\n",
            "     | > step_time: 1.4067\n",
            "     | > loader_time: 0.008\n",
            "\n",
            "\u001b[1m   --> STEP: 390/405 -- GLOBAL_STEP: 4450\u001b[0m\n",
            "     | > loss: 1.07240 \n",
            "     | > log_mle: 0.80741 \n",
            "     | > loss_dur: 0.26499 \n",
            "     | > align_error: 0.07784 \n",
            "     | > current_lr: 0.0009479844171602249\n",
            "     | > grad_norm: 0.14645302295684814\n",
            "     | > step_time: 1.5549\n",
            "     | > loader_time: 0.0155\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.92770  (0.92770)\n",
            "     | > log_mle: 0.64623  (0.64623)\n",
            "     | > loss_dur: 0.28146  (0.28146)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 1.00468  (1.00468)\n",
            "     | > log_mle: 0.73687  (0.73687)\n",
            "     | > loss_dur: 0.26782  (0.26782)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.97238  (0.98853)\n",
            "     | > log_mle: 0.74259  (0.73973)\n",
            "     | > loss_dur: 0.22978  (0.24880)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 1.00917  (0.99541)\n",
            "     | > log_mle: 0.77014  (0.74987)\n",
            "     | > loss_dur: 0.23903  (0.24554)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.01991  (1.00153)\n",
            "     | > log_mle: 0.79506  (0.76116)\n",
            "     | > loss_dur: 0.22484  (0.24037)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.99921  (1.00107)\n",
            "     | > log_mle: 0.79324  (0.76758)\n",
            "     | > loss_dur: 0.20597  (0.23349)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.00450  (1.00164)\n",
            "     | > log_mle: 0.80584  (0.77396)\n",
            "     | > loss_dur: 0.19865  (0.22768)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 0.99342  (1.00046)\n",
            "     | > log_mle: 0.80891  (0.77895)\n",
            "     | > loss_dur: 0.18451  (0.22152)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 0.97278  (0.99700)\n",
            "     | > log_mle: 0.80403  (0.78208)\n",
            "     | > loss_dur: 0.16874  (0.21492)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 2.90215 \u001b[0m(+0.32843)\n",
            "     | > avg_loss:\u001b[92m 0.99700 \u001b[0m(-0.00620)\n",
            "     | > avg_log_mle:\u001b[92m 0.78208 \u001b[0m(-0.00202)\n",
            "     | > avg_loss_dur:\u001b[92m 0.21492 \u001b[0m(-0.00418)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_4466.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 11/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 02:37:12) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 9/405 -- GLOBAL_STEP: 4475\u001b[0m\n",
            "     | > loss: 1.13707 \n",
            "     | > log_mle: 0.76093 \n",
            "     | > loss_dur: 0.37614 \n",
            "     | > align_error: 0.39294 \n",
            "     | > current_lr: 0.0009453333005617795\n",
            "     | > grad_norm: 0.874969482421875\n",
            "     | > step_time: 0.5832\n",
            "     | > loader_time: 0.0014\n",
            "\n",
            "\u001b[1m   --> STEP: 34/405 -- GLOBAL_STEP: 4500\u001b[0m\n",
            "     | > loss: 1.09284 \n",
            "     | > log_mle: 0.78436 \n",
            "     | > loss_dur: 0.30848 \n",
            "     | > align_error: 0.50751 \n",
            "     | > current_lr: 0.0009427043024780817\n",
            "     | > grad_norm: 0.2817106544971466\n",
            "     | > step_time: 0.7567\n",
            "     | > loader_time: 0.0025\n",
            "\n",
            "\u001b[1m   --> STEP: 59/405 -- GLOBAL_STEP: 4525\u001b[0m\n",
            "     | > loss: 1.02143 \n",
            "     | > log_mle: 0.72275 \n",
            "     | > loss_dur: 0.29868 \n",
            "     | > align_error: 0.03227 \n",
            "     | > current_lr: 0.0009400971170485458\n",
            "     | > grad_norm: 0.2268393337726593\n",
            "     | > step_time: 0.629\n",
            "     | > loader_time: 0.0021\n",
            "\n",
            "\u001b[1m   --> STEP: 84/405 -- GLOBAL_STEP: 4550\u001b[0m\n",
            "     | > loss: 1.02799 \n",
            "     | > log_mle: 0.75083 \n",
            "     | > loss_dur: 0.27717 \n",
            "     | > align_error: 0.20402 \n",
            "     | > current_lr: 0.0009375114443013487\n",
            "     | > grad_norm: 0.10315526276826859\n",
            "     | > step_time: 0.7289\n",
            "     | > loader_time: 0.0051\n",
            "\n",
            "\u001b[1m   --> STEP: 109/405 -- GLOBAL_STEP: 4575\u001b[0m\n",
            "     | > loss: 1.02894 \n",
            "     | > log_mle: 0.76178 \n",
            "     | > loss_dur: 0.26716 \n",
            "     | > align_error: 0.10138 \n",
            "     | > current_lr: 0.000934946990008457\n",
            "     | > grad_norm: 0.6793675422668457\n",
            "     | > step_time: 0.9138\n",
            "     | > loader_time: 0.0056\n",
            "\n",
            "\u001b[1m   --> STEP: 134/405 -- GLOBAL_STEP: 4600\u001b[0m\n",
            "     | > loss: 1.07284 \n",
            "     | > log_mle: 0.77959 \n",
            "     | > loss_dur: 0.29325 \n",
            "     | > align_error: 0.20415 \n",
            "     | > current_lr: 0.0009324034655449929\n",
            "     | > grad_norm: 0.18398120999336243\n",
            "     | > step_time: 0.9748\n",
            "     | > loader_time: 0.0094\n",
            "\n",
            "\u001b[1m   --> STEP: 159/405 -- GLOBAL_STEP: 4625\u001b[0m\n",
            "     | > loss: 1.03090 \n",
            "     | > log_mle: 0.76186 \n",
            "     | > loss_dur: 0.26904 \n",
            "     | > align_error: 0.10582 \n",
            "     | > current_lr: 0.0009298805877527845\n",
            "     | > grad_norm: 0.20754371583461761\n",
            "     | > step_time: 0.9577\n",
            "     | > loader_time: 0.0026\n",
            "\n",
            "\u001b[1m   --> STEP: 184/405 -- GLOBAL_STEP: 4650\u001b[0m\n",
            "     | > loss: 1.08027 \n",
            "     | > log_mle: 0.80240 \n",
            "     | > loss_dur: 0.27786 \n",
            "     | > align_error: 0.19134 \n",
            "     | > current_lr: 0.0009273780788079567\n",
            "     | > grad_norm: 0.28682154417037964\n",
            "     | > step_time: 1.2647\n",
            "     | > loader_time: 0.0101\n",
            "\n",
            "\u001b[1m   --> STEP: 209/405 -- GLOBAL_STEP: 4675\u001b[0m\n",
            "     | > loss: 1.08301 \n",
            "     | > log_mle: 0.79545 \n",
            "     | > loss_dur: 0.28757 \n",
            "     | > align_error: 0.12447 \n",
            "     | > current_lr: 0.0009248956660924215\n",
            "     | > grad_norm: 0.3853992223739624\n",
            "     | > step_time: 1.1712\n",
            "     | > loader_time: 0.0088\n",
            "\n",
            "\u001b[1m   --> STEP: 234/405 -- GLOBAL_STEP: 4700\u001b[0m\n",
            "     | > loss: 1.09025 \n",
            "     | > log_mle: 0.80484 \n",
            "     | > loss_dur: 0.28540 \n",
            "     | > align_error: 0.19316 \n",
            "     | > current_lr: 0.0009224330820691299\n",
            "     | > grad_norm: 0.2714347839355469\n",
            "     | > step_time: 1.2248\n",
            "     | > loader_time: 0.0065\n",
            "\n",
            "\u001b[1m   --> STEP: 259/405 -- GLOBAL_STEP: 4725\u001b[0m\n",
            "     | > loss: 1.09458 \n",
            "     | > log_mle: 0.80310 \n",
            "     | > loss_dur: 0.29147 \n",
            "     | > align_error: 0.09178 \n",
            "     | > current_lr: 0.0009199900641609602\n",
            "     | > grad_norm: 0.21775679290294647\n",
            "     | > step_time: 1.2549\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 284/405 -- GLOBAL_STEP: 4750\u001b[0m\n",
            "     | > loss: 1.07978 \n",
            "     | > log_mle: 0.80894 \n",
            "     | > loss_dur: 0.27084 \n",
            "     | > align_error: 0.14450 \n",
            "     | > current_lr: 0.0009175663546331154\n",
            "     | > grad_norm: 0.14694543182849884\n",
            "     | > step_time: 1.4091\n",
            "     | > loader_time: 0.01\n",
            "\n",
            "\u001b[1m   --> STEP: 309/405 -- GLOBAL_STEP: 4775\u001b[0m\n",
            "     | > loss: 1.08450 \n",
            "     | > log_mle: 0.80583 \n",
            "     | > loss_dur: 0.27868 \n",
            "     | > align_error: 0.09840 \n",
            "     | > current_lr: 0.0009151617004789102\n",
            "     | > grad_norm: 0.2806079685688019\n",
            "     | > step_time: 1.233\n",
            "     | > loader_time: 0.0035\n",
            "\n",
            "\u001b[1m   --> STEP: 334/405 -- GLOBAL_STEP: 4800\u001b[0m\n",
            "     | > loss: 1.08187 \n",
            "     | > log_mle: 0.80605 \n",
            "     | > loss_dur: 0.27582 \n",
            "     | > align_error: 0.10067 \n",
            "     | > current_lr: 0.000912775853308834\n",
            "     | > grad_norm: 0.23357822000980377\n",
            "     | > step_time: 1.274\n",
            "     | > loader_time: 0.0081\n",
            "\n",
            "\u001b[1m   --> STEP: 359/405 -- GLOBAL_STEP: 4825\u001b[0m\n",
            "     | > loss: 1.07756 \n",
            "     | > log_mle: 0.80910 \n",
            "     | > loss_dur: 0.26846 \n",
            "     | > align_error: 0.09456 \n",
            "     | > current_lr: 0.0009104085692427788\n",
            "     | > grad_norm: 0.2602938115596771\n",
            "     | > step_time: 1.4376\n",
            "     | > loader_time: 0.0133\n",
            "\n",
            "\u001b[1m   --> STEP: 384/405 -- GLOBAL_STEP: 4850\u001b[0m\n",
            "     | > loss: 1.08041 \n",
            "     | > log_mle: 0.81028 \n",
            "     | > loss_dur: 0.27014 \n",
            "     | > align_error: 0.10891 \n",
            "     | > current_lr: 0.000908059608805326\n",
            "     | > grad_norm: 0.15088419616222382\n",
            "     | > step_time: 1.4078\n",
            "     | > loader_time: 0.0162\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.90305  (0.90305)\n",
            "     | > log_mle: 0.64565  (0.64565)\n",
            "     | > loss_dur: 0.25740  (0.25740)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.98694  (0.98694)\n",
            "     | > log_mle: 0.73643  (0.73643)\n",
            "     | > loss_dur: 0.25051  (0.25051)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.95954  (0.97324)\n",
            "     | > log_mle: 0.74211  (0.73927)\n",
            "     | > loss_dur: 0.21743  (0.23397)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 0.99020  (0.97889)\n",
            "     | > log_mle: 0.76939  (0.74931)\n",
            "     | > loss_dur: 0.22082  (0.22958)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 0.99435  (0.98276)\n",
            "     | > log_mle: 0.79474  (0.76067)\n",
            "     | > loss_dur: 0.19961  (0.22209)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.98201  (0.98261)\n",
            "     | > log_mle: 0.79291  (0.76712)\n",
            "     | > loss_dur: 0.18910  (0.21549)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 0.99607  (0.98485)\n",
            "     | > log_mle: 0.80536  (0.77349)\n",
            "     | > loss_dur: 0.19071  (0.21136)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 0.98313  (0.98461)\n",
            "     | > log_mle: 0.80860  (0.77851)\n",
            "     | > loss_dur: 0.17453  (0.20610)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 0.95081  (0.98038)\n",
            "     | > log_mle: 0.80360  (0.78164)\n",
            "     | > loss_dur: 0.14721  (0.19874)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 2.81015 \u001b[0m(-0.09200)\n",
            "     | > avg_loss:\u001b[92m 0.98038 \u001b[0m(-0.01662)\n",
            "     | > avg_log_mle:\u001b[92m 0.78164 \u001b[0m(-0.00044)\n",
            "     | > avg_loss_dur:\u001b[92m 0.19874 \u001b[0m(-0.01618)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_4872.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 12/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 02:45:07) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 3/405 -- GLOBAL_STEP: 4875\u001b[0m\n",
            "     | > loss: 0.99098 \n",
            "     | > log_mle: 0.63679 \n",
            "     | > loss_dur: 0.35419 \n",
            "     | > align_error: 0.47074 \n",
            "     | > current_lr: 0.0009057287368239884\n",
            "     | > grad_norm: 3.3240060806274414\n",
            "     | > step_time: 0.4572\n",
            "     | > loader_time: 0.0014\n",
            "\n",
            "\u001b[1m   --> STEP: 28/405 -- GLOBAL_STEP: 4900\u001b[0m\n",
            "     | > loss: 1.04820 \n",
            "     | > log_mle: 0.72330 \n",
            "     | > loss_dur: 0.32490 \n",
            "     | > align_error: 0.38167 \n",
            "     | > current_lr: 0.0009034157223303129\n",
            "     | > grad_norm: 0.667564332485199\n",
            "     | > step_time: 0.6499\n",
            "     | > loader_time: 0.002\n",
            "\n",
            "\u001b[1m   --> STEP: 53/405 -- GLOBAL_STEP: 4925\u001b[0m\n",
            "     | > loss: 1.02291 \n",
            "     | > log_mle: 0.72719 \n",
            "     | > loss_dur: 0.29572 \n",
            "     | > align_error: 0.25481 \n",
            "     | > current_lr: 0.0009011203384637462\n",
            "     | > grad_norm: 0.47053784132003784\n",
            "     | > step_time: 0.7746\n",
            "     | > loader_time: 0.0023\n",
            "\n",
            "\u001b[1m   --> STEP: 78/405 -- GLOBAL_STEP: 4950\u001b[0m\n",
            "     | > loss: 1.08292 \n",
            "     | > log_mle: 0.80560 \n",
            "     | > loss_dur: 0.27732 \n",
            "     | > align_error: 0.56269 \n",
            "     | > current_lr: 0.0008988423623781728\n",
            "     | > grad_norm: 0.15925955772399902\n",
            "     | > step_time: 1.1874\n",
            "     | > loader_time: 0.0032\n",
            "\n",
            "\u001b[1m   --> STEP: 103/405 -- GLOBAL_STEP: 4975\u001b[0m\n",
            "     | > loss: 1.08929 \n",
            "     | > log_mle: 0.80047 \n",
            "     | > loss_dur: 0.28881 \n",
            "     | > align_error: 0.32826 \n",
            "     | > current_lr: 0.0008965815751510409\n",
            "     | > grad_norm: 0.30028119683265686\n",
            "     | > step_time: 1.0861\n",
            "     | > loader_time: 0.0064\n",
            "\n",
            "\u001b[1m   --> STEP: 128/405 -- GLOBAL_STEP: 5000\u001b[0m\n",
            "     | > loss: 1.09380 \n",
            "     | > log_mle: 0.80594 \n",
            "     | > loss_dur: 0.28786 \n",
            "     | > align_error: 0.34641 \n",
            "     | > current_lr: 0.0008943377616949879\n",
            "     | > grad_norm: 0.13725683093070984\n",
            "     | > step_time: 1.1525\n",
            "     | > loader_time: 0.01\n",
            "\n",
            "\u001b[1m   --> STEP: 153/405 -- GLOBAL_STEP: 5025\u001b[0m\n",
            "     | > loss: 1.07175 \n",
            "     | > log_mle: 0.77947 \n",
            "     | > loss_dur: 0.29228 \n",
            "     | > align_error: 0.35079 \n",
            "     | > current_lr: 0.0008921107106718893\n",
            "     | > grad_norm: 0.21396270394325256\n",
            "     | > step_time: 1.0382\n",
            "     | > loader_time: 0.0116\n",
            "\n",
            "\u001b[1m   --> STEP: 178/405 -- GLOBAL_STEP: 5050\u001b[0m\n",
            "     | > loss: 1.08807 \n",
            "     | > log_mle: 0.80145 \n",
            "     | > loss_dur: 0.28662 \n",
            "     | > align_error: 0.22965 \n",
            "     | > current_lr: 0.0008899002144092463\n",
            "     | > grad_norm: 0.12393342703580856\n",
            "     | > step_time: 1.1553\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 203/405 -- GLOBAL_STEP: 5075\u001b[0m\n",
            "     | > loss: 1.06737 \n",
            "     | > log_mle: 0.78861 \n",
            "     | > loss_dur: 0.27876 \n",
            "     | > align_error: 0.08125 \n",
            "     | > current_lr: 0.0008877060688188456\n",
            "     | > grad_norm: 0.2067415565252304\n",
            "     | > step_time: 1.1513\n",
            "     | > loader_time: 0.0032\n",
            "\n",
            "\u001b[1m   --> STEP: 228/405 -- GLOBAL_STEP: 5100\u001b[0m\n",
            "     | > loss: 1.07355 \n",
            "     | > log_mle: 0.79491 \n",
            "     | > loss_dur: 0.27864 \n",
            "     | > align_error: 0.14715 \n",
            "     | > current_lr: 0.0008855280733176115\n",
            "     | > grad_norm: 0.1575203537940979\n",
            "     | > step_time: 1.2813\n",
            "     | > loader_time: 0.013\n",
            "\n",
            "\u001b[1m   --> STEP: 253/405 -- GLOBAL_STEP: 5125\u001b[0m\n",
            "     | > loss: 1.06347 \n",
            "     | > log_mle: 0.79511 \n",
            "     | > loss_dur: 0.26836 \n",
            "     | > align_error: 0.18949 \n",
            "     | > current_lr: 0.0008833660307505842\n",
            "     | > grad_norm: 0.15909698605537415\n",
            "     | > step_time: 1.252\n",
            "     | > loader_time: 0.0029\n",
            "\n",
            "\u001b[1m   --> STEP: 278/405 -- GLOBAL_STEP: 5150\u001b[0m\n",
            "     | > loss: 1.07720 \n",
            "     | > log_mle: 0.80144 \n",
            "     | > loss_dur: 0.27577 \n",
            "     | > align_error: 0.10588 \n",
            "     | > current_lr: 0.0008812197473159575\n",
            "     | > grad_norm: 0.19576740264892578\n",
            "     | > step_time: 1.3041\n",
            "     | > loader_time: 0.0066\n",
            "\n",
            "\u001b[1m   --> STEP: 303/405 -- GLOBAL_STEP: 5175\u001b[0m\n",
            "     | > loss: 1.08341 \n",
            "     | > log_mle: 0.80446 \n",
            "     | > loss_dur: 0.27895 \n",
            "     | > align_error: 0.08006 \n",
            "     | > current_lr: 0.0008790890324921097\n",
            "     | > grad_norm: 0.3228875696659088\n",
            "     | > step_time: 1.2072\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 328/405 -- GLOBAL_STEP: 5200\u001b[0m\n",
            "     | > loss: 1.07945 \n",
            "     | > log_mle: 0.80221 \n",
            "     | > loss_dur: 0.27724 \n",
            "     | > align_error: 0.04311 \n",
            "     | > current_lr: 0.0008769736989665654\n",
            "     | > grad_norm: 0.16957789659500122\n",
            "     | > step_time: 1.3566\n",
            "     | > loader_time: 0.0081\n",
            "\n",
            "\u001b[1m   --> STEP: 353/405 -- GLOBAL_STEP: 5225\u001b[0m\n",
            "     | > loss: 1.08371 \n",
            "     | > log_mle: 0.80753 \n",
            "     | > loss_dur: 0.27618 \n",
            "     | > align_error: 0.12598 \n",
            "     | > current_lr: 0.0008748735625668301\n",
            "     | > grad_norm: 0.3113616108894348\n",
            "     | > step_time: 1.3295\n",
            "     | > loader_time: 0.0062\n",
            "\n",
            "\u001b[1m   --> STEP: 378/405 -- GLOBAL_STEP: 5250\u001b[0m\n",
            "     | > loss: 1.08486 \n",
            "     | > log_mle: 0.80660 \n",
            "     | > loss_dur: 0.27826 \n",
            "     | > align_error: 0.02021 \n",
            "     | > current_lr: 0.0008727884421930365\n",
            "     | > grad_norm: 0.19045960903167725\n",
            "     | > step_time: 1.4553\n",
            "     | > loader_time: 0.0137\n",
            "\n",
            "\u001b[1m   --> STEP: 403/405 -- GLOBAL_STEP: 5275\u001b[0m\n",
            "     | > loss: 1.05931 \n",
            "     | > log_mle: 0.80674 \n",
            "     | > loss_dur: 0.25258 \n",
            "     | > align_error: 0.01363 \n",
            "     | > current_lr: 0.0008707181597523489\n",
            "     | > grad_norm: 0.12166573852300644\n",
            "     | > step_time: 1.0771\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.89935  (0.89935)\n",
            "     | > log_mle: 0.64029  (0.64029)\n",
            "     | > loss_dur: 0.25906  (0.25906)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.96399  (0.96399)\n",
            "     | > log_mle: 0.73267  (0.73267)\n",
            "     | > loss_dur: 0.23132  (0.23132)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.94919  (0.95659)\n",
            "     | > log_mle: 0.73838  (0.73552)\n",
            "     | > loss_dur: 0.21081  (0.22107)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 0.97613  (0.96311)\n",
            "     | > log_mle: 0.76634  (0.74579)\n",
            "     | > loss_dur: 0.20980  (0.21731)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 0.97734  (0.96666)\n",
            "     | > log_mle: 0.79218  (0.75739)\n",
            "     | > loss_dur: 0.18515  (0.20927)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.95981  (0.96529)\n",
            "     | > log_mle: 0.79031  (0.76398)\n",
            "     | > loss_dur: 0.16949  (0.20132)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 0.96849  (0.96582)\n",
            "     | > log_mle: 0.80306  (0.77049)\n",
            "     | > loss_dur: 0.16543  (0.19534)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 0.95693  (0.96455)\n",
            "     | > log_mle: 0.80651  (0.77564)\n",
            "     | > loss_dur: 0.15042  (0.18892)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 0.92318  (0.95938)\n",
            "     | > log_mle: 0.80122  (0.77883)\n",
            "     | > loss_dur: 0.12195  (0.18055)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 2.67631 \u001b[0m(-0.13383)\n",
            "     | > avg_loss:\u001b[92m 0.95938 \u001b[0m(-0.02100)\n",
            "     | > avg_log_mle:\u001b[92m 0.77883 \u001b[0m(-0.00281)\n",
            "     | > avg_loss_dur:\u001b[92m 0.18055 \u001b[0m(-0.01819)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_5278.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 13/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 02:53:00) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/405 -- GLOBAL_STEP: 5300\u001b[0m\n",
            "     | > loss: 1.04133 \n",
            "     | > log_mle: 0.73160 \n",
            "     | > loss_dur: 0.30973 \n",
            "     | > align_error: 0.40534 \n",
            "     | > current_lr: 0.0008686625400950715\n",
            "     | > grad_norm: 0.34305158257484436\n",
            "     | > step_time: 0.6346\n",
            "     | > loader_time: 0.0021\n",
            "\n",
            "\u001b[1m   --> STEP: 47/405 -- GLOBAL_STEP: 5325\u001b[0m\n",
            "     | > loss: 1.05103 \n",
            "     | > log_mle: 0.76728 \n",
            "     | > loss_dur: 0.28375 \n",
            "     | > align_error: 0.47706 \n",
            "     | > current_lr: 0.0008666214109524074\n",
            "     | > grad_norm: 0.3098669648170471\n",
            "     | > step_time: 0.8582\n",
            "     | > loader_time: 0.0024\n",
            "\n",
            "\u001b[1m   --> STEP: 72/405 -- GLOBAL_STEP: 5350\u001b[0m\n",
            "     | > loss: 1.05778 \n",
            "     | > log_mle: 0.76172 \n",
            "     | > loss_dur: 0.29606 \n",
            "     | > align_error: 0.28192 \n",
            "     | > current_lr: 0.0008645946028758194\n",
            "     | > grad_norm: 0.18037524819374084\n",
            "     | > step_time: 0.8459\n",
            "     | > loader_time: 0.0025\n",
            "\n",
            "\u001b[1m   --> STEP: 97/405 -- GLOBAL_STEP: 5375\u001b[0m\n",
            "     | > loss: 1.07571 \n",
            "     | > log_mle: 0.78663 \n",
            "     | > loss_dur: 0.28907 \n",
            "     | > align_error: 0.30093 \n",
            "     | > current_lr: 0.0008625819491779428\n",
            "     | > grad_norm: 0.2322908490896225\n",
            "     | > step_time: 1.0055\n",
            "     | > loader_time: 0.0032\n",
            "\n",
            "\u001b[1m   --> STEP: 122/405 -- GLOBAL_STEP: 5400\u001b[0m\n",
            "     | > loss: 1.09362 \n",
            "     | > log_mle: 0.80099 \n",
            "     | > loss_dur: 0.29263 \n",
            "     | > align_error: 0.38479 \n",
            "     | > current_lr: 0.0008605832858750051\n",
            "     | > grad_norm: 0.14170236885547638\n",
            "     | > step_time: 1.1953\n",
            "     | > loader_time: 0.0033\n",
            "\n",
            "\u001b[1m   --> STEP: 147/405 -- GLOBAL_STEP: 5425\u001b[0m\n",
            "     | > loss: 1.04552 \n",
            "     | > log_mle: 0.76522 \n",
            "     | > loss_dur: 0.28031 \n",
            "     | > align_error: 0.11469 \n",
            "     | > current_lr: 0.0008585984516307064\n",
            "     | > grad_norm: 0.7693517208099365\n",
            "     | > step_time: 0.921\n",
            "     | > loader_time: 0.0029\n",
            "\n",
            "\u001b[1m   --> STEP: 172/405 -- GLOBAL_STEP: 5450\u001b[0m\n",
            "     | > loss: 1.04312 \n",
            "     | > log_mle: 0.76847 \n",
            "     | > loss_dur: 0.27464 \n",
            "     | > align_error: 0.14106 \n",
            "     | > current_lr: 0.000856627287701516\n",
            "     | > grad_norm: 0.32944756746292114\n",
            "     | > step_time: 1.0091\n",
            "     | > loader_time: 0.0058\n",
            "\n",
            "\u001b[1m   --> STEP: 197/405 -- GLOBAL_STEP: 5475\u001b[0m\n",
            "     | > loss: 1.07571 \n",
            "     | > log_mle: 0.79322 \n",
            "     | > loss_dur: 0.28249 \n",
            "     | > align_error: 0.18347 \n",
            "     | > current_lr: 0.0008546696378833457\n",
            "     | > grad_norm: 0.20102478563785553\n",
            "     | > step_time: 1.1831\n",
            "     | > loader_time: 0.0088\n",
            "\n",
            "\u001b[1m   --> STEP: 222/405 -- GLOBAL_STEP: 5500\u001b[0m\n",
            "     | > loss: 1.08186 \n",
            "     | > log_mle: 0.79564 \n",
            "     | > loss_dur: 0.28623 \n",
            "     | > align_error: 0.17418 \n",
            "     | > current_lr: 0.0008527253484595563\n",
            "     | > grad_norm: 0.17491275072097778\n",
            "     | > step_time: 1.1427\n",
            "     | > loader_time: 0.0059\n",
            "\n",
            "\u001b[1m   --> STEP: 247/405 -- GLOBAL_STEP: 5525\u001b[0m\n",
            "     | > loss: 1.08970 \n",
            "     | > log_mle: 0.80469 \n",
            "     | > loss_dur: 0.28501 \n",
            "     | > align_error: 0.30890 \n",
            "     | > current_lr: 0.0008507942681502598\n",
            "     | > grad_norm: 0.1512923687696457\n",
            "     | > step_time: 1.4176\n",
            "     | > loader_time: 0.0036\n",
            "\n",
            "\u001b[1m   --> STEP: 272/405 -- GLOBAL_STEP: 5550\u001b[0m\n",
            "     | > loss: 1.08940 \n",
            "     | > log_mle: 0.80587 \n",
            "     | > loss_dur: 0.28353 \n",
            "     | > align_error: 0.14411 \n",
            "     | > current_lr: 0.0008488762480628792\n",
            "     | > grad_norm: 0.13908061385154724\n",
            "     | > step_time: 1.2969\n",
            "     | > loader_time: 0.0033\n",
            "\n",
            "\u001b[1m   --> STEP: 297/405 -- GLOBAL_STEP: 5575\u001b[0m\n",
            "     | > loss: 1.08776 \n",
            "     | > log_mle: 0.80282 \n",
            "     | > loss_dur: 0.28494 \n",
            "     | > align_error: 0.16322 \n",
            "     | > current_lr: 0.0008469711416439278\n",
            "     | > grad_norm: 0.18497741222381592\n",
            "     | > step_time: 1.2676\n",
            "     | > loader_time: 0.0088\n",
            "\n",
            "\u001b[1m   --> STEP: 322/405 -- GLOBAL_STEP: 5600\u001b[0m\n",
            "     | > loss: 1.08601 \n",
            "     | > log_mle: 0.80365 \n",
            "     | > loss_dur: 0.28236 \n",
            "     | > align_error: 0.11087 \n",
            "     | > current_lr: 0.0008450788046319747\n",
            "     | > grad_norm: 0.3583438992500305\n",
            "     | > step_time: 1.3172\n",
            "     | > loader_time: 0.0173\n",
            "\n",
            "\u001b[1m   --> STEP: 347/405 -- GLOBAL_STEP: 5625\u001b[0m\n",
            "     | > loss: 1.08443 \n",
            "     | > log_mle: 0.80469 \n",
            "     | > loss_dur: 0.27974 \n",
            "     | > align_error: 0.05100 \n",
            "     | > current_lr: 0.0008431990950117609\n",
            "     | > grad_norm: 0.2692568898200989\n",
            "     | > step_time: 1.4801\n",
            "     | > loader_time: 0.0065\n",
            "\n",
            "\u001b[1m   --> STEP: 372/405 -- GLOBAL_STEP: 5650\u001b[0m\n",
            "     | > loss: 1.07492 \n",
            "     | > log_mle: 0.80453 \n",
            "     | > loss_dur: 0.27039 \n",
            "     | > align_error: 0.00000 \n",
            "     | > current_lr: 0.0008413318729694319\n",
            "     | > grad_norm: 0.17661966383457184\n",
            "     | > step_time: 1.4302\n",
            "     | > loader_time: 0.0099\n",
            "\n",
            "\u001b[1m   --> STEP: 397/405 -- GLOBAL_STEP: 5675\u001b[0m\n",
            "     | > loss: 1.06309 \n",
            "     | > log_mle: 0.80468 \n",
            "     | > loss_dur: 0.25840 \n",
            "     | > align_error: 0.08750 \n",
            "     | > current_lr: 0.0008394770008488577\n",
            "     | > grad_norm: 0.12962119281291962\n",
            "     | > step_time: 1.5325\n",
            "     | > loader_time: 0.0124\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.89682  (0.89682)\n",
            "     | > log_mle: 0.63622  (0.63622)\n",
            "     | > loss_dur: 0.26060  (0.26060)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.96730  (0.96730)\n",
            "     | > log_mle: 0.72988  (0.72988)\n",
            "     | > loss_dur: 0.23742  (0.23742)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.95584  (0.96157)\n",
            "     | > log_mle: 0.73563  (0.73276)\n",
            "     | > loss_dur: 0.22021  (0.22881)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 0.98840  (0.97051)\n",
            "     | > log_mle: 0.76402  (0.74318)\n",
            "     | > loss_dur: 0.22438  (0.22734)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 0.98978  (0.97533)\n",
            "     | > log_mle: 0.79021  (0.75494)\n",
            "     | > loss_dur: 0.19957  (0.22039)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.97812  (0.97589)\n",
            "     | > log_mle: 0.78835  (0.76162)\n",
            "     | > loss_dur: 0.18977  (0.21427)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 0.98041  (0.97664)\n",
            "     | > log_mle: 0.80132  (0.76824)\n",
            "     | > loss_dur: 0.17909  (0.20841)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 0.97641  (0.97661)\n",
            "     | > log_mle: 0.80479  (0.77346)\n",
            "     | > loss_dur: 0.17162  (0.20315)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 0.96605  (0.97529)\n",
            "     | > log_mle: 0.79950  (0.77671)\n",
            "     | > loss_dur: 0.16655  (0.19858)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 2.84348 \u001b[0m(+0.16717)\n",
            "     | > avg_loss:\u001b[91m 0.97529 \u001b[0m(+0.01591)\n",
            "     | > avg_log_mle:\u001b[92m 0.77671 \u001b[0m(-0.00212)\n",
            "     | > avg_loss_dur:\u001b[91m 0.19858 \u001b[0m(+0.01803)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 14/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 03:00:55) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/405 -- GLOBAL_STEP: 5700\u001b[0m\n",
            "     | > loss: 1.15367 \n",
            "     | > log_mle: 0.72794 \n",
            "     | > loss_dur: 0.42574 \n",
            "     | > align_error: 0.37083 \n",
            "     | > current_lr: 0.0008376343431090065\n",
            "     | > grad_norm: 1.0973113775253296\n",
            "     | > step_time: 0.5718\n",
            "     | > loader_time: 0.0017\n",
            "\n",
            "\u001b[1m   --> STEP: 41/405 -- GLOBAL_STEP: 5725\u001b[0m\n",
            "     | > loss: 1.03232 \n",
            "     | > log_mle: 0.74493 \n",
            "     | > loss_dur: 0.28739 \n",
            "     | > align_error: 0.33555 \n",
            "     | > current_lr: 0.0008358037662823432\n",
            "     | > grad_norm: 0.1613866090774536\n",
            "     | > step_time: 0.7582\n",
            "     | > loader_time: 0.0022\n",
            "\n",
            "\u001b[1m   --> STEP: 66/405 -- GLOBAL_STEP: 5750\u001b[0m\n",
            "     | > loss: 1.04832 \n",
            "     | > log_mle: 0.75943 \n",
            "     | > loss_dur: 0.28889 \n",
            "     | > align_error: 0.33632 \n",
            "     | > current_lr: 0.0008339851389342239\n",
            "     | > grad_norm: 0.14143967628479004\n",
            "     | > step_time: 0.883\n",
            "     | > loader_time: 0.0022\n",
            "\n",
            "\u001b[1m   --> STEP: 91/405 -- GLOBAL_STEP: 5775\u001b[0m\n",
            "     | > loss: 1.05309 \n",
            "     | > log_mle: 0.76834 \n",
            "     | > loss_dur: 0.28475 \n",
            "     | > align_error: 0.22110 \n",
            "     | > current_lr: 0.0008321783316232576\n",
            "     | > grad_norm: 0.505680501461029\n",
            "     | > step_time: 1.0241\n",
            "     | > loader_time: 0.0026\n",
            "\n",
            "\u001b[1m   --> STEP: 116/405 -- GLOBAL_STEP: 5800\u001b[0m\n",
            "     | > loss: 1.07486 \n",
            "     | > log_mle: 0.78727 \n",
            "     | > loss_dur: 0.28759 \n",
            "     | > align_error: 0.20913 \n",
            "     | > current_lr: 0.0008303832168626093\n",
            "     | > grad_norm: 0.1078484058380127\n",
            "     | > step_time: 0.9419\n",
            "     | > loader_time: 0.008\n",
            "\n",
            "\u001b[1m   --> STEP: 141/405 -- GLOBAL_STEP: 5825\u001b[0m\n",
            "     | > loss: 1.05367 \n",
            "     | > log_mle: 0.76727 \n",
            "     | > loss_dur: 0.28639 \n",
            "     | > align_error: 0.07964 \n",
            "     | > current_lr: 0.0008285996690822162\n",
            "     | > grad_norm: 0.16270507872104645\n",
            "     | > step_time: 0.9835\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            "\u001b[1m   --> STEP: 166/405 -- GLOBAL_STEP: 5850\u001b[0m\n",
            "     | > loss: 1.05454 \n",
            "     | > log_mle: 0.77247 \n",
            "     | > loss_dur: 0.28207 \n",
            "     | > align_error: 0.13657 \n",
            "     | > current_lr: 0.0008268275645918939\n",
            "     | > grad_norm: 0.16977162659168243\n",
            "     | > step_time: 1.0142\n",
            "     | > loader_time: 0.0075\n",
            "\n",
            "\u001b[1m   --> STEP: 191/405 -- GLOBAL_STEP: 5875\u001b[0m\n",
            "     | > loss: 1.07824 \n",
            "     | > log_mle: 0.79119 \n",
            "     | > loss_dur: 0.28705 \n",
            "     | > align_error: 0.23149 \n",
            "     | > current_lr: 0.0008250667815453063\n",
            "     | > grad_norm: 0.33035311102867126\n",
            "     | > step_time: 1.1205\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 216/405 -- GLOBAL_STEP: 5900\u001b[0m\n",
            "     | > loss: 1.09786 \n",
            "     | > log_mle: 0.80306 \n",
            "     | > loss_dur: 0.29480 \n",
            "     | > align_error: 0.03008 \n",
            "     | > current_lr: 0.000823317199904776\n",
            "     | > grad_norm: 0.7683956623077393\n",
            "     | > step_time: 1.2421\n",
            "     | > loader_time: 0.003\n",
            "\n",
            "\u001b[1m   --> STEP: 241/405 -- GLOBAL_STEP: 5925\u001b[0m\n",
            "     | > loss: 1.07778 \n",
            "     | > log_mle: 0.79134 \n",
            "     | > loss_dur: 0.28643 \n",
            "     | > align_error: 0.18863 \n",
            "     | > current_lr: 0.0008215787014069117\n",
            "     | > grad_norm: 0.13704584538936615\n",
            "     | > step_time: 1.1854\n",
            "     | > loader_time: 0.0028\n",
            "\n",
            "\u001b[1m   --> STEP: 266/405 -- GLOBAL_STEP: 5950\u001b[0m\n",
            "     | > loss: 1.08114 \n",
            "     | > log_mle: 0.79705 \n",
            "     | > loss_dur: 0.28409 \n",
            "     | > align_error: 0.15829 \n",
            "     | > current_lr: 0.0008198511695290318\n",
            "     | > grad_norm: 0.3339757025241852\n",
            "     | > step_time: 1.2894\n",
            "     | > loader_time: 0.009\n",
            "\n",
            "\u001b[1m   --> STEP: 291/405 -- GLOBAL_STEP: 5975\u001b[0m\n",
            "     | > loss: 1.08909 \n",
            "     | > log_mle: 0.80362 \n",
            "     | > loss_dur: 0.28547 \n",
            "     | > align_error: 0.15582 \n",
            "     | > current_lr: 0.0008181344894563599\n",
            "     | > grad_norm: 0.3113330006599426\n",
            "     | > step_time: 1.1879\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            "\u001b[1m   --> STEP: 316/405 -- GLOBAL_STEP: 6000\u001b[0m\n",
            "     | > loss: 1.08660 \n",
            "     | > log_mle: 0.80659 \n",
            "     | > loss_dur: 0.28001 \n",
            "     | > align_error: 0.11687 \n",
            "     | > current_lr: 0.0008164285480499737\n",
            "     | > grad_norm: 0.14601100981235504\n",
            "     | > step_time: 1.3343\n",
            "     | > loader_time: 0.0092\n",
            "\n",
            "\u001b[1m   --> STEP: 341/405 -- GLOBAL_STEP: 6025\u001b[0m\n",
            "     | > loss: 1.08660 \n",
            "     | > log_mle: 0.80150 \n",
            "     | > loss_dur: 0.28510 \n",
            "     | > align_error: 0.15936 \n",
            "     | > current_lr: 0.0008147332338154853\n",
            "     | > grad_norm: 0.13382481038570404\n",
            "     | > step_time: 1.4134\n",
            "     | > loader_time: 0.0065\n",
            "\n",
            "\u001b[1m   --> STEP: 366/405 -- GLOBAL_STEP: 6050\u001b[0m\n",
            "     | > loss: 1.08319 \n",
            "     | > log_mle: 0.80430 \n",
            "     | > loss_dur: 0.27889 \n",
            "     | > align_error: 0.05042 \n",
            "     | > current_lr: 0.0008130484368724354\n",
            "     | > grad_norm: 0.11118149012327194\n",
            "     | > step_time: 1.3874\n",
            "     | > loader_time: 0.0089\n",
            "\n",
            "\u001b[1m   --> STEP: 391/405 -- GLOBAL_STEP: 6075\u001b[0m\n",
            "     | > loss: 1.06831 \n",
            "     | > log_mle: 0.80531 \n",
            "     | > loss_dur: 0.26300 \n",
            "     | > align_error: 0.00203 \n",
            "     | > current_lr: 0.0008113740489243782\n",
            "     | > grad_norm: 0.19423280656337738\n",
            "     | > step_time: 1.4337\n",
            "     | > loader_time: 0.0069\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.89089  (0.89089)\n",
            "     | > log_mle: 0.63617  (0.63617)\n",
            "     | > loss_dur: 0.25473  (0.25473)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.94882  (0.94882)\n",
            "     | > log_mle: 0.72972  (0.72972)\n",
            "     | > loss_dur: 0.21910  (0.21910)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.93402  (0.94142)\n",
            "     | > log_mle: 0.73556  (0.73264)\n",
            "     | > loss_dur: 0.19847  (0.20878)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 0.96538  (0.94941)\n",
            "     | > log_mle: 0.76375  (0.74301)\n",
            "     | > loss_dur: 0.20163  (0.20640)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 0.96431  (0.95314)\n",
            "     | > log_mle: 0.79014  (0.75479)\n",
            "     | > loss_dur: 0.17417  (0.19834)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.94905  (0.95232)\n",
            "     | > log_mle: 0.78827  (0.76149)\n",
            "     | > loss_dur: 0.16078  (0.19083)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 0.95828  (0.95331)\n",
            "     | > log_mle: 0.80126  (0.76812)\n",
            "     | > loss_dur: 0.15703  (0.18520)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 0.94659  (0.95235)\n",
            "     | > log_mle: 0.80480  (0.77336)\n",
            "     | > loss_dur: 0.14180  (0.17900)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 0.94307  (0.95119)\n",
            "     | > log_mle: 0.79947  (0.77662)\n",
            "     | > loss_dur: 0.14361  (0.17457)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 2.67684 \u001b[0m(-0.16664)\n",
            "     | > avg_loss:\u001b[92m 0.95119 \u001b[0m(-0.02410)\n",
            "     | > avg_log_mle:\u001b[92m 0.77662 \u001b[0m(-0.00009)\n",
            "     | > avg_loss_dur:\u001b[92m 0.17457 \u001b[0m(-0.02400)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_6090.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 15/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 03:08:51) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/405 -- GLOBAL_STEP: 6100\u001b[0m\n",
            "     | > loss: 0.99752 \n",
            "     | > log_mle: 0.67630 \n",
            "     | > loss_dur: 0.32121 \n",
            "     | > align_error: 0.45127 \n",
            "     | > current_lr: 0.0008097099632296442\n",
            "     | > grad_norm: 0.5410387516021729\n",
            "     | > step_time: 0.7108\n",
            "     | > loader_time: 0.0022\n",
            "\n",
            "\u001b[1m   --> STEP: 35/405 -- GLOBAL_STEP: 6125\u001b[0m\n",
            "     | > loss: 1.03006 \n",
            "     | > log_mle: 0.73819 \n",
            "     | > loss_dur: 0.29187 \n",
            "     | > align_error: 0.51548 \n",
            "     | > current_lr: 0.0008080560745727575\n",
            "     | > grad_norm: 0.23943546414375305\n",
            "     | > step_time: 0.7127\n",
            "     | > loader_time: 0.0023\n",
            "\n",
            "\u001b[1m   --> STEP: 60/405 -- GLOBAL_STEP: 6150\u001b[0m\n",
            "     | > loss: 0.99843 \n",
            "     | > log_mle: 0.71735 \n",
            "     | > loss_dur: 0.28109 \n",
            "     | > align_error: 0.27426 \n",
            "     | > current_lr: 0.0008064122792364944\n",
            "     | > grad_norm: 0.34291374683380127\n",
            "     | > step_time: 0.6815\n",
            "     | > loader_time: 0.0022\n",
            "\n",
            "\u001b[1m   --> STEP: 85/405 -- GLOBAL_STEP: 6175\u001b[0m\n",
            "     | > loss: 1.06580 \n",
            "     | > log_mle: 0.77185 \n",
            "     | > loss_dur: 0.29396 \n",
            "     | > align_error: 0.42684 \n",
            "     | > current_lr: 0.000804778474974563\n",
            "     | > grad_norm: 0.1852409988641739\n",
            "     | > step_time: 1.0098\n",
            "     | > loader_time: 0.003\n",
            "\n",
            "\u001b[1m   --> STEP: 110/405 -- GLOBAL_STEP: 6200\u001b[0m\n",
            "     | > loss: 1.05105 \n",
            "     | > log_mle: 0.76886 \n",
            "     | > loss_dur: 0.28218 \n",
            "     | > align_error: 0.19643 \n",
            "     | > current_lr: 0.0008031545609848915\n",
            "     | > grad_norm: 0.4303596317768097\n",
            "     | > step_time: 1.069\n",
            "     | > loader_time: 0.0026\n",
            "\n",
            "\u001b[1m   --> STEP: 135/405 -- GLOBAL_STEP: 6225\u001b[0m\n",
            "     | > loss: 1.05409 \n",
            "     | > log_mle: 0.77161 \n",
            "     | > loss_dur: 0.28247 \n",
            "     | > align_error: 0.14047 \n",
            "     | > current_lr: 0.0008015404378835044\n",
            "     | > grad_norm: 0.4093690514564514\n",
            "     | > step_time: 1.0372\n",
            "     | > loader_time: 0.0031\n",
            "\n",
            "\u001b[1m   --> STEP: 160/405 -- GLOBAL_STEP: 6250\u001b[0m\n",
            "     | > loss: 1.08420 \n",
            "     | > log_mle: 0.78879 \n",
            "     | > loss_dur: 0.29541 \n",
            "     | > align_error: 0.15653 \n",
            "     | > current_lr: 0.0007999360076789761\n",
            "     | > grad_norm: 0.17582933604717255\n",
            "     | > step_time: 1.0895\n",
            "     | > loader_time: 0.008\n",
            "\n",
            "\u001b[1m   --> STEP: 185/405 -- GLOBAL_STEP: 6275\u001b[0m\n",
            "     | > loss: 1.07991 \n",
            "     | > log_mle: 0.79402 \n",
            "     | > loss_dur: 0.28589 \n",
            "     | > align_error: 0.16224 \n",
            "     | > current_lr: 0.0007983411737474437\n",
            "     | > grad_norm: 0.13004615902900696\n",
            "     | > step_time: 1.1789\n",
            "     | > loader_time: 0.0092\n",
            "\n",
            "\u001b[1m   --> STEP: 210/405 -- GLOBAL_STEP: 6300\u001b[0m\n",
            "     | > loss: 1.08242 \n",
            "     | > log_mle: 0.79253 \n",
            "     | > loss_dur: 0.28989 \n",
            "     | > align_error: 0.19823 \n",
            "     | > current_lr: 0.0007967558408081654\n",
            "     | > grad_norm: 0.11949567496776581\n",
            "     | > step_time: 1.1313\n",
            "     | > loader_time: 0.0098\n",
            "\n",
            "\u001b[1m   --> STEP: 235/405 -- GLOBAL_STEP: 6325\u001b[0m\n",
            "     | > loss: 1.09177 \n",
            "     | > log_mle: 0.80349 \n",
            "     | > loss_dur: 0.28828 \n",
            "     | > align_error: 0.22070 \n",
            "     | > current_lr: 0.0007951799148996114\n",
            "     | > grad_norm: 0.16579678654670715\n",
            "     | > step_time: 1.3041\n",
            "     | > loader_time: 0.004\n",
            "\n",
            "\u001b[1m   --> STEP: 260/405 -- GLOBAL_STEP: 6350\u001b[0m\n",
            "     | > loss: 1.07419 \n",
            "     | > log_mle: 0.79081 \n",
            "     | > loss_dur: 0.28338 \n",
            "     | > align_error: 0.15785 \n",
            "     | > current_lr: 0.0007936133033560728\n",
            "     | > grad_norm: 0.260915070772171\n",
            "     | > step_time: 1.2406\n",
            "     | > loader_time: 0.0033\n",
            "\n",
            "\u001b[1m   --> STEP: 285/405 -- GLOBAL_STEP: 6375\u001b[0m\n",
            "     | > loss: 1.07439 \n",
            "     | > log_mle: 0.79147 \n",
            "     | > loss_dur: 0.28293 \n",
            "     | > align_error: 0.11075 \n",
            "     | > current_lr: 0.0007920559147847744\n",
            "     | > grad_norm: 0.48495590686798096\n",
            "     | > step_time: 1.3106\n",
            "     | > loader_time: 0.0059\n",
            "\n",
            "\u001b[1m   --> STEP: 310/405 -- GLOBAL_STEP: 6400\u001b[0m\n",
            "     | > loss: 1.08718 \n",
            "     | > log_mle: 0.80073 \n",
            "     | > loss_dur: 0.28645 \n",
            "     | > align_error: 0.14702 \n",
            "     | > current_lr: 0.0007905076590434814\n",
            "     | > grad_norm: 0.2971780002117157\n",
            "     | > step_time: 1.3238\n",
            "     | > loader_time: 0.0123\n",
            "\n",
            "\u001b[1m   --> STEP: 335/405 -- GLOBAL_STEP: 6425\u001b[0m\n",
            "     | > loss: 1.08366 \n",
            "     | > log_mle: 0.80198 \n",
            "     | > loss_dur: 0.28168 \n",
            "     | > align_error: 0.13774 \n",
            "     | > current_lr: 0.0007889684472185848\n",
            "     | > grad_norm: 0.366110622882843\n",
            "     | > step_time: 1.4009\n",
            "     | > loader_time: 0.0078\n",
            "\n",
            "\u001b[1m   --> STEP: 360/405 -- GLOBAL_STEP: 6450\u001b[0m\n",
            "     | > loss: 1.08385 \n",
            "     | > log_mle: 0.80355 \n",
            "     | > loss_dur: 0.28030 \n",
            "     | > align_error: 0.03917 \n",
            "     | > current_lr: 0.0007874381916036532\n",
            "     | > grad_norm: 0.22771568596363068\n",
            "     | > step_time: 1.3772\n",
            "     | > loader_time: 0.0197\n",
            "\n",
            "\u001b[1m   --> STEP: 385/405 -- GLOBAL_STEP: 6475\u001b[0m\n",
            "     | > loss: 1.07665 \n",
            "     | > log_mle: 0.80418 \n",
            "     | > loss_dur: 0.27247 \n",
            "     | > align_error: 0.04080 \n",
            "     | > current_lr: 0.0007859168056784427\n",
            "     | > grad_norm: 0.4321666657924652\n",
            "     | > step_time: 1.4249\n",
            "     | > loader_time: 0.0083\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.88579  (0.88579)\n",
            "     | > log_mle: 0.63191  (0.63191)\n",
            "     | > loss_dur: 0.25388  (0.25388)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.97560  (0.97560)\n",
            "     | > log_mle: 0.72683  (0.72683)\n",
            "     | > loss_dur: 0.24877  (0.24877)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.95014  (0.96287)\n",
            "     | > log_mle: 0.73273  (0.72978)\n",
            "     | > loss_dur: 0.21741  (0.23309)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 0.98436  (0.97003)\n",
            "     | > log_mle: 0.76135  (0.74030)\n",
            "     | > loss_dur: 0.22301  (0.22973)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 0.97879  (0.97222)\n",
            "     | > log_mle: 0.78828  (0.75230)\n",
            "     | > loss_dur: 0.19051  (0.21993)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.96817  (0.97141)\n",
            "     | > log_mle: 0.78624  (0.75909)\n",
            "     | > loss_dur: 0.18193  (0.21233)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 0.97791  (0.97250)\n",
            "     | > log_mle: 0.79948  (0.76582)\n",
            "     | > loss_dur: 0.17843  (0.20668)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 0.96881  (0.97197)\n",
            "     | > log_mle: 0.80312  (0.77115)\n",
            "     | > loss_dur: 0.16570  (0.20082)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 0.95257  (0.96954)\n",
            "     | > log_mle: 0.79775  (0.77447)\n",
            "     | > loss_dur: 0.15482  (0.19507)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 2.90875 \u001b[0m(+0.23191)\n",
            "     | > avg_loss:\u001b[91m 0.96954 \u001b[0m(+0.01835)\n",
            "     | > avg_log_mle:\u001b[92m 0.77447 \u001b[0m(-0.00215)\n",
            "     | > avg_loss_dur:\u001b[91m 0.19507 \u001b[0m(+0.02050)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 16/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 03:16:49) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 4/405 -- GLOBAL_STEP: 6500\u001b[0m\n",
            "     | > loss: 0.93273 \n",
            "     | > log_mle: 0.63449 \n",
            "     | > loss_dur: 0.29824 \n",
            "     | > align_error: 0.52963 \n",
            "     | > current_lr: 0.0007844042040883502\n",
            "     | > grad_norm: 1.364518404006958\n",
            "     | > step_time: 0.6006\n",
            "     | > loader_time: 0.0014\n",
            "\n",
            "\u001b[1m   --> STEP: 29/405 -- GLOBAL_STEP: 6525\u001b[0m\n",
            "     | > loss: 0.95804 \n",
            "     | > log_mle: 0.66520 \n",
            "     | > loss_dur: 0.29284 \n",
            "     | > align_error: 0.25154 \n",
            "     | > current_lr: 0.0007829003026242987\n",
            "     | > grad_norm: 0.46029624342918396\n",
            "     | > step_time: 0.6317\n",
            "     | > loader_time: 0.0046\n",
            "\n",
            "\u001b[1m   --> STEP: 54/405 -- GLOBAL_STEP: 6550\u001b[0m\n",
            "     | > loss: 1.03851 \n",
            "     | > log_mle: 0.75180 \n",
            "     | > loss_dur: 0.28670 \n",
            "     | > align_error: 0.46791 \n",
            "     | > current_lr: 0.0007814050182030473\n",
            "     | > grad_norm: 0.3810298442840576\n",
            "     | > step_time: 0.84\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            "\u001b[1m   --> STEP: 79/405 -- GLOBAL_STEP: 6575\u001b[0m\n",
            "     | > loss: 1.05393 \n",
            "     | > log_mle: 0.77234 \n",
            "     | > loss_dur: 0.28159 \n",
            "     | > align_error: 0.44400 \n",
            "     | > current_lr: 0.0007799182688479127\n",
            "     | > grad_norm: 0.20183159410953522\n",
            "     | > step_time: 0.9021\n",
            "     | > loader_time: 0.0029\n",
            "\n",
            "\u001b[1m   --> STEP: 104/405 -- GLOBAL_STEP: 6600\u001b[0m\n",
            "     | > loss: 1.01934 \n",
            "     | > log_mle: 0.72967 \n",
            "     | > loss_dur: 0.28967 \n",
            "     | > align_error: 0.19868 \n",
            "     | > current_lr: 0.000778439973669891\n",
            "     | > grad_norm: 0.31136611104011536\n",
            "     | > step_time: 0.7445\n",
            "     | > loader_time: 0.0022\n",
            "\n",
            "\u001b[1m   --> STEP: 129/405 -- GLOBAL_STEP: 6625\u001b[0m\n",
            "     | > loss: 1.05311 \n",
            "     | > log_mle: 0.75915 \n",
            "     | > loss_dur: 0.29396 \n",
            "     | > align_error: 0.16225 \n",
            "     | > current_lr: 0.0007769700528491746\n",
            "     | > grad_norm: 0.3854982554912567\n",
            "     | > step_time: 0.9593\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            "\u001b[1m   --> STEP: 154/405 -- GLOBAL_STEP: 6650\u001b[0m\n",
            "     | > loss: 1.06912 \n",
            "     | > log_mle: 0.77443 \n",
            "     | > loss_dur: 0.29469 \n",
            "     | > align_error: 0.15854 \n",
            "     | > current_lr: 0.0007755084276170485\n",
            "     | > grad_norm: 0.53708815574646\n",
            "     | > step_time: 1.0919\n",
            "     | > loader_time: 0.0028\n",
            "\n",
            "\u001b[1m   --> STEP: 179/405 -- GLOBAL_STEP: 6675\u001b[0m\n",
            "     | > loss: 1.06381 \n",
            "     | > log_mle: 0.77568 \n",
            "     | > loss_dur: 0.28813 \n",
            "     | > align_error: 0.19271 \n",
            "     | > current_lr: 0.0007740550202381622\n",
            "     | > grad_norm: 0.19248828291893005\n",
            "     | > step_time: 0.9991\n",
            "     | > loader_time: 0.0061\n",
            "\n",
            "\u001b[1m   --> STEP: 204/405 -- GLOBAL_STEP: 6700\u001b[0m\n",
            "     | > loss: 1.09122 \n",
            "     | > log_mle: 0.79258 \n",
            "     | > loss_dur: 0.29864 \n",
            "     | > align_error: 0.18824 \n",
            "     | > current_lr: 0.0007726097539931645\n",
            "     | > grad_norm: 0.3325706720352173\n",
            "     | > step_time: 1.2713\n",
            "     | > loader_time: 0.0089\n",
            "\n",
            "\u001b[1m   --> STEP: 229/405 -- GLOBAL_STEP: 6725\u001b[0m\n",
            "     | > loss: 1.07939 \n",
            "     | > log_mle: 0.79544 \n",
            "     | > loss_dur: 0.28395 \n",
            "     | > align_error: 0.19845 \n",
            "     | > current_lr: 0.000771172553161694\n",
            "     | > grad_norm: 0.5073626637458801\n",
            "     | > step_time: 1.2928\n",
            "     | > loader_time: 0.0096\n",
            "\n",
            "\u001b[1m   --> STEP: 254/405 -- GLOBAL_STEP: 6750\u001b[0m\n",
            "     | > loss: 1.08870 \n",
            "     | > log_mle: 0.79445 \n",
            "     | > loss_dur: 0.29425 \n",
            "     | > align_error: 0.14051 \n",
            "     | > current_lr: 0.0007697433430057154\n",
            "     | > grad_norm: 0.23917342722415924\n",
            "     | > step_time: 1.4752\n",
            "     | > loader_time: 0.0079\n",
            "\n",
            "\u001b[1m   --> STEP: 279/405 -- GLOBAL_STEP: 6775\u001b[0m\n",
            "     | > loss: 1.08840 \n",
            "     | > log_mle: 0.80213 \n",
            "     | > loss_dur: 0.28627 \n",
            "     | > align_error: 0.12203 \n",
            "     | > current_lr: 0.0007683220497531969\n",
            "     | > grad_norm: 0.45058679580688477\n",
            "     | > step_time: 1.3929\n",
            "     | > loader_time: 0.0074\n",
            "\n",
            "\u001b[1m   --> STEP: 304/405 -- GLOBAL_STEP: 6800\u001b[0m\n",
            "     | > loss: 1.08208 \n",
            "     | > log_mle: 0.80123 \n",
            "     | > loss_dur: 0.28085 \n",
            "     | > align_error: 0.15436 \n",
            "     | > current_lr: 0.000766908600582114\n",
            "     | > grad_norm: 0.23482918739318848\n",
            "     | > step_time: 1.481\n",
            "     | > loader_time: 0.0088\n",
            "\n",
            "\u001b[1m   --> STEP: 329/405 -- GLOBAL_STEP: 6825\u001b[0m\n",
            "     | > loss: 1.08152 \n",
            "     | > log_mle: 0.80154 \n",
            "     | > loss_dur: 0.27998 \n",
            "     | > align_error: 0.10515 \n",
            "     | > current_lr: 0.0007655029236047799\n",
            "     | > grad_norm: 0.30060261487960815\n",
            "     | > step_time: 1.4691\n",
            "     | > loader_time: 0.0076\n",
            "\n",
            "\u001b[1m   --> STEP: 354/405 -- GLOBAL_STEP: 6850\u001b[0m\n",
            "     | > loss: 1.08163 \n",
            "     | > log_mle: 0.80373 \n",
            "     | > loss_dur: 0.27790 \n",
            "     | > align_error: 0.14101 \n",
            "     | > current_lr: 0.0007641049478524871\n",
            "     | > grad_norm: 0.13152121007442474\n",
            "     | > step_time: 1.6163\n",
            "     | > loader_time: 0.0142\n",
            "\n",
            "\u001b[1m   --> STEP: 379/405 -- GLOBAL_STEP: 6875\u001b[0m\n",
            "     | > loss: 1.07721 \n",
            "     | > log_mle: 0.80055 \n",
            "     | > loss_dur: 0.27666 \n",
            "     | > align_error: 0.02256 \n",
            "     | > current_lr: 0.0007627146032604592\n",
            "     | > grad_norm: 0.21534085273742676\n",
            "     | > step_time: 1.4562\n",
            "     | > loader_time: 0.0238\n",
            "\n",
            "\u001b[1m   --> STEP: 404/405 -- GLOBAL_STEP: 6900\u001b[0m\n",
            "     | > loss: 1.04548 \n",
            "     | > log_mle: 0.80197 \n",
            "     | > loss_dur: 0.24351 \n",
            "     | > align_error: 0.02988 \n",
            "     | > current_lr: 0.0007613318206531009\n",
            "     | > grad_norm: 0.22976510226726532\n",
            "     | > step_time: 1.107\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.85654  (0.85654)\n",
            "     | > log_mle: 0.62876  (0.62876)\n",
            "     | > loss_dur: 0.22778  (0.22778)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.94976  (0.94976)\n",
            "     | > log_mle: 0.72474  (0.72474)\n",
            "     | > loss_dur: 0.22502  (0.22502)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.93953  (0.94465)\n",
            "     | > log_mle: 0.73076  (0.72775)\n",
            "     | > loss_dur: 0.20877  (0.21690)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 0.96496  (0.95142)\n",
            "     | > log_mle: 0.75970  (0.73840)\n",
            "     | > loss_dur: 0.20526  (0.21302)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 0.95700  (0.95281)\n",
            "     | > log_mle: 0.78684  (0.75051)\n",
            "     | > loss_dur: 0.17016  (0.20230)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.96100  (0.95445)\n",
            "     | > log_mle: 0.78483  (0.75737)\n",
            "     | > loss_dur: 0.17618  (0.19708)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 0.96385  (0.95602)\n",
            "     | > log_mle: 0.79830  (0.76419)\n",
            "     | > loss_dur: 0.16555  (0.19182)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 0.95059  (0.95524)\n",
            "     | > log_mle: 0.80187  (0.76958)\n",
            "     | > loss_dur: 0.14873  (0.18567)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 0.93171  (0.95230)\n",
            "     | > log_mle: 0.79649  (0.77294)\n",
            "     | > loss_dur: 0.13522  (0.17936)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 2.87011 \u001b[0m(-0.03863)\n",
            "     | > avg_loss:\u001b[92m 0.95230 \u001b[0m(-0.01724)\n",
            "     | > avg_log_mle:\u001b[92m 0.77294 \u001b[0m(-0.00153)\n",
            "     | > avg_loss_dur:\u001b[92m 0.17936 \u001b[0m(-0.01571)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 17/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 03:25:13) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 23/405 -- GLOBAL_STEP: 6925\u001b[0m\n",
            "     | > loss: 0.95865 \n",
            "     | > log_mle: 0.66817 \n",
            "     | > loss_dur: 0.29048 \n",
            "     | > align_error: 0.34470 \n",
            "     | > current_lr: 0.000759956531729542\n",
            "     | > grad_norm: 0.3409181833267212\n",
            "     | > step_time: 0.539\n",
            "     | > loader_time: 0.002\n",
            "\n",
            "\u001b[1m   --> STEP: 48/405 -- GLOBAL_STEP: 6950\u001b[0m\n",
            "     | > loss: 1.01781 \n",
            "     | > log_mle: 0.72422 \n",
            "     | > loss_dur: 0.29360 \n",
            "     | > align_error: 0.27029 \n",
            "     | > current_lr: 0.0007585886690494679\n",
            "     | > grad_norm: 0.2499200999736786\n",
            "     | > step_time: 0.6643\n",
            "     | > loader_time: 0.0021\n",
            "\n",
            "\u001b[1m   --> STEP: 73/405 -- GLOBAL_STEP: 6975\u001b[0m\n",
            "     | > loss: 0.97577 \n",
            "     | > log_mle: 0.69972 \n",
            "     | > loss_dur: 0.27604 \n",
            "     | > align_error: 0.14145 \n",
            "     | > current_lr: 0.0007572281660192283\n",
            "     | > grad_norm: 0.3392658531665802\n",
            "     | > step_time: 0.759\n",
            "     | > loader_time: 0.0023\n",
            "\n",
            "\u001b[1m   --> STEP: 98/405 -- GLOBAL_STEP: 7000\u001b[0m\n",
            "     | > loss: 1.04927 \n",
            "     | > log_mle: 0.76512 \n",
            "     | > loss_dur: 0.28415 \n",
            "     | > align_error: 0.26409 \n",
            "     | > current_lr: 0.0007558749568782208\n",
            "     | > grad_norm: 0.37681978940963745\n",
            "     | > step_time: 1.0389\n",
            "     | > loader_time: 0.0077\n",
            "\n",
            "\u001b[1m   --> STEP: 123/405 -- GLOBAL_STEP: 7025\u001b[0m\n",
            "     | > loss: 1.02914 \n",
            "     | > log_mle: 0.74929 \n",
            "     | > loss_dur: 0.27985 \n",
            "     | > align_error: 0.22024 \n",
            "     | > current_lr: 0.0007545289766855392\n",
            "     | > grad_norm: 0.4762396216392517\n",
            "     | > step_time: 0.9722\n",
            "     | > loader_time: 0.0022\n",
            "\n",
            "\u001b[1m   --> STEP: 148/405 -- GLOBAL_STEP: 7050\u001b[0m\n",
            "     | > loss: 1.05187 \n",
            "     | > log_mle: 0.76568 \n",
            "     | > loss_dur: 0.28619 \n",
            "     | > align_error: 0.12375 \n",
            "     | > current_lr: 0.0007531901613068846\n",
            "     | > grad_norm: 2.2040607929229736\n",
            "     | > step_time: 1.0147\n",
            "     | > loader_time: 0.0024\n",
            "\n",
            "\u001b[1m   --> STEP: 173/405 -- GLOBAL_STEP: 7075\u001b[0m\n",
            "     | > loss: 1.05354 \n",
            "     | > log_mle: 0.77383 \n",
            "     | > loss_dur: 0.27971 \n",
            "     | > align_error: 0.06745 \n",
            "     | > current_lr: 0.000751858447401729\n",
            "     | > grad_norm: 0.2495637983083725\n",
            "     | > step_time: 1.1827\n",
            "     | > loader_time: 0.0088\n",
            "\n",
            "\u001b[1m   --> STEP: 198/405 -- GLOBAL_STEP: 7100\u001b[0m\n",
            "     | > loss: 1.07612 \n",
            "     | > log_mle: 0.78952 \n",
            "     | > loss_dur: 0.28660 \n",
            "     | > align_error: 0.21705 \n",
            "     | > current_lr: 0.0007505337724107293\n",
            "     | > grad_norm: 0.22728049755096436\n",
            "     | > step_time: 1.2365\n",
            "     | > loader_time: 0.0093\n",
            "\n",
            "\u001b[1m   --> STEP: 223/405 -- GLOBAL_STEP: 7125\u001b[0m\n",
            "     | > loss: 1.06345 \n",
            "     | > log_mle: 0.77738 \n",
            "     | > loss_dur: 0.28607 \n",
            "     | > align_error: 0.12319 \n",
            "     | > current_lr: 0.0007492160745433828\n",
            "     | > grad_norm: 0.3841668665409088\n",
            "     | > step_time: 1.2297\n",
            "     | > loader_time: 0.0065\n",
            "\n",
            "\u001b[1m   --> STEP: 248/405 -- GLOBAL_STEP: 7150\u001b[0m\n",
            "     | > loss: 1.07487 \n",
            "     | > log_mle: 0.78973 \n",
            "     | > loss_dur: 0.28514 \n",
            "     | > align_error: 0.13372 \n",
            "     | > current_lr: 0.0007479052927659223\n",
            "     | > grad_norm: 0.29204797744750977\n",
            "     | > step_time: 1.2553\n",
            "     | > loader_time: 0.0034\n",
            "\n",
            "\u001b[1m   --> STEP: 273/405 -- GLOBAL_STEP: 7175\u001b[0m\n",
            "     | > loss: 1.08182 \n",
            "     | > log_mle: 0.79504 \n",
            "     | > loss_dur: 0.28678 \n",
            "     | > align_error: 0.10937 \n",
            "     | > current_lr: 0.0007466013667894412\n",
            "     | > grad_norm: 0.19751036167144775\n",
            "     | > step_time: 1.4063\n",
            "     | > loader_time: 0.0054\n",
            "\n",
            "\u001b[1m   --> STEP: 298/405 -- GLOBAL_STEP: 7200\u001b[0m\n",
            "     | > loss: 1.08193 \n",
            "     | > log_mle: 0.79686 \n",
            "     | > loss_dur: 0.28507 \n",
            "     | > align_error: 0.14753 \n",
            "     | > current_lr: 0.0007453042370582469\n",
            "     | > grad_norm: 0.35338613390922546\n",
            "     | > step_time: 1.3536\n",
            "     | > loader_time: 0.0053\n",
            "\n",
            "\u001b[1m   --> STEP: 323/405 -- GLOBAL_STEP: 7225\u001b[0m\n",
            "     | > loss: 1.09593 \n",
            "     | > log_mle: 0.80045 \n",
            "     | > loss_dur: 0.29548 \n",
            "     | > align_error: 0.06362 \n",
            "     | > current_lr: 0.0007440138447384355\n",
            "     | > grad_norm: 0.19019633531570435\n",
            "     | > step_time: 1.5551\n",
            "     | > loader_time: 0.0064\n",
            "\n",
            "\u001b[1m   --> STEP: 348/405 -- GLOBAL_STEP: 7250\u001b[0m\n",
            "     | > loss: 1.07832 \n",
            "     | > log_mle: 0.80081 \n",
            "     | > loss_dur: 0.27751 \n",
            "     | > align_error: 0.04720 \n",
            "     | > current_lr: 0.000742730131706684\n",
            "     | > grad_norm: 0.32560259103775024\n",
            "     | > step_time: 1.3951\n",
            "     | > loader_time: 0.0085\n",
            "\n",
            "\u001b[1m   --> STEP: 373/405 -- GLOBAL_STEP: 7275\u001b[0m\n",
            "     | > loss: 1.07601 \n",
            "     | > log_mle: 0.80227 \n",
            "     | > loss_dur: 0.27374 \n",
            "     | > align_error: 0.03125 \n",
            "     | > current_lr: 0.0007414530405392534\n",
            "     | > grad_norm: 0.4627898931503296\n",
            "     | > step_time: 1.6268\n",
            "     | > loader_time: 0.0103\n",
            "\n",
            "\u001b[1m   --> STEP: 398/405 -- GLOBAL_STEP: 7300\u001b[0m\n",
            "     | > loss: 1.06467 \n",
            "     | > log_mle: 0.80203 \n",
            "     | > loss_dur: 0.26265 \n",
            "     | > align_error: 0.02992 \n",
            "     | > current_lr: 0.0007401825145011997\n",
            "     | > grad_norm: 0.2695859670639038\n",
            "     | > step_time: 1.1655\n",
            "     | > loader_time: 0.0033\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.85027  (0.85027)\n",
            "     | > log_mle: 0.62772  (0.62772)\n",
            "     | > loss_dur: 0.22255  (0.22255)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.93157  (0.93157)\n",
            "     | > log_mle: 0.72450  (0.72450)\n",
            "     | > loss_dur: 0.20707  (0.20707)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.93108  (0.93132)\n",
            "     | > log_mle: 0.73050  (0.72750)\n",
            "     | > loss_dur: 0.20058  (0.20383)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 0.95120  (0.93795)\n",
            "     | > log_mle: 0.75978  (0.73826)\n",
            "     | > loss_dur: 0.19142  (0.19969)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 0.94696  (0.94020)\n",
            "     | > log_mle: 0.78664  (0.75035)\n",
            "     | > loss_dur: 0.16032  (0.18985)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.94949  (0.94206)\n",
            "     | > log_mle: 0.78454  (0.75719)\n",
            "     | > loss_dur: 0.16495  (0.18487)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 0.95302  (0.94389)\n",
            "     | > log_mle: 0.79824  (0.76403)\n",
            "     | > loss_dur: 0.15477  (0.17985)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 0.95090  (0.94489)\n",
            "     | > log_mle: 0.80164  (0.76941)\n",
            "     | > loss_dur: 0.14925  (0.17548)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 0.92523  (0.94243)\n",
            "     | > log_mle: 0.79653  (0.77280)\n",
            "     | > loss_dur: 0.12870  (0.16963)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 2.64069 \u001b[0m(-0.22943)\n",
            "     | > avg_loss:\u001b[92m 0.94243 \u001b[0m(-0.00987)\n",
            "     | > avg_log_mle:\u001b[92m 0.77280 \u001b[0m(-0.00014)\n",
            "     | > avg_loss_dur:\u001b[92m 0.16963 \u001b[0m(-0.00973)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_7308.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 18/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 03:33:33) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 17/405 -- GLOBAL_STEP: 7325\u001b[0m\n",
            "     | > loss: 0.98283 \n",
            "     | > log_mle: 0.68594 \n",
            "     | > loss_dur: 0.29689 \n",
            "     | > align_error: 0.26202 \n",
            "     | > current_lr: 0.0007389184975357877\n",
            "     | > grad_norm: 0.32116276025772095\n",
            "     | > step_time: 0.6156\n",
            "     | > loader_time: 0.0021\n",
            "\n",
            "\u001b[1m   --> STEP: 42/405 -- GLOBAL_STEP: 7350\u001b[0m\n",
            "     | > loss: 1.02360 \n",
            "     | > log_mle: 0.73820 \n",
            "     | > loss_dur: 0.28539 \n",
            "     | > align_error: 0.31090 \n",
            "     | > current_lr: 0.0007376609342541029\n",
            "     | > grad_norm: 0.4536314904689789\n",
            "     | > step_time: 0.793\n",
            "     | > loader_time: 0.0037\n",
            "\n",
            "\u001b[1m   --> STEP: 67/405 -- GLOBAL_STEP: 7375\u001b[0m\n",
            "     | > loss: 0.97828 \n",
            "     | > log_mle: 0.69394 \n",
            "     | > loss_dur: 0.28433 \n",
            "     | > align_error: 0.19511 \n",
            "     | > current_lr: 0.000736409769924857\n",
            "     | > grad_norm: 0.38025689125061035\n",
            "     | > step_time: 0.7205\n",
            "     | > loader_time: 0.0019\n",
            "\n",
            "\u001b[1m   --> STEP: 92/405 -- GLOBAL_STEP: 7400\u001b[0m\n",
            "     | > loss: 1.00504 \n",
            "     | > log_mle: 0.72062 \n",
            "     | > loss_dur: 0.28442 \n",
            "     | > align_error: 0.15699 \n",
            "     | > current_lr: 0.0007351649504643828\n",
            "     | > grad_norm: 0.3303934335708618\n",
            "     | > step_time: 0.8789\n",
            "     | > loader_time: 0.0021\n",
            "\n",
            "\u001b[1m   --> STEP: 117/405 -- GLOBAL_STEP: 7425\u001b[0m\n",
            "     | > loss: 1.05958 \n",
            "     | > log_mle: 0.76628 \n",
            "     | > loss_dur: 0.29330 \n",
            "     | > align_error: 0.25086 \n",
            "     | > current_lr: 0.0007339264224268152\n",
            "     | > grad_norm: 0.5631762146949768\n",
            "     | > step_time: 1.0154\n",
            "     | > loader_time: 0.0026\n",
            "\n",
            "\u001b[1m   --> STEP: 142/405 -- GLOBAL_STEP: 7450\u001b[0m\n",
            "     | > loss: 1.08721 \n",
            "     | > log_mle: 0.80191 \n",
            "     | > loss_dur: 0.28531 \n",
            "     | > align_error: 0.45332 \n",
            "     | > current_lr: 0.0007326941329944531\n",
            "     | > grad_norm: 0.25157564878463745\n",
            "     | > step_time: 1.2558\n",
            "     | > loader_time: 0.0076\n",
            "\n",
            "\u001b[1m   --> STEP: 167/405 -- GLOBAL_STEP: 7475\u001b[0m\n",
            "     | > loss: 1.07112 \n",
            "     | > log_mle: 0.77472 \n",
            "     | > loss_dur: 0.29639 \n",
            "     | > align_error: 0.22262 \n",
            "     | > current_lr: 0.0007314680299682983\n",
            "     | > grad_norm: 0.5956523418426514\n",
            "     | > step_time: 1.1445\n",
            "     | > loader_time: 0.0033\n",
            "\n",
            "\u001b[1m   --> STEP: 192/405 -- GLOBAL_STEP: 7500\u001b[0m\n",
            "     | > loss: 1.07900 \n",
            "     | > log_mle: 0.79658 \n",
            "     | > loss_dur: 0.28242 \n",
            "     | > align_error: 0.27316 \n",
            "     | > current_lr: 0.0007302480617587695\n",
            "     | > grad_norm: 0.2554600238800049\n",
            "     | > step_time: 1.2492\n",
            "     | > loader_time: 0.0113\n",
            "\n",
            "\u001b[1m   --> STEP: 217/405 -- GLOBAL_STEP: 7525\u001b[0m\n",
            "     | > loss: 1.06681 \n",
            "     | > log_mle: 0.78510 \n",
            "     | > loss_dur: 0.28171 \n",
            "     | > align_error: 0.24308 \n",
            "     | > current_lr: 0.0007290341773765834\n",
            "     | > grad_norm: 0.284627228975296\n",
            "     | > step_time: 1.186\n",
            "     | > loader_time: 0.0062\n",
            "\n",
            "\u001b[1m   --> STEP: 242/405 -- GLOBAL_STEP: 7550\u001b[0m\n",
            "     | > loss: 1.07845 \n",
            "     | > log_mle: 0.79668 \n",
            "     | > loss_dur: 0.28177 \n",
            "     | > align_error: 0.14183 \n",
            "     | > current_lr: 0.0007278263264238052\n",
            "     | > grad_norm: 0.8495243191719055\n",
            "     | > step_time: 1.3528\n",
            "     | > loader_time: 0.0064\n",
            "\n",
            "\u001b[1m   --> STEP: 267/405 -- GLOBAL_STEP: 7575\u001b[0m\n",
            "     | > loss: 1.07208 \n",
            "     | > log_mle: 0.78870 \n",
            "     | > loss_dur: 0.28338 \n",
            "     | > align_error: 0.11021 \n",
            "     | > current_lr: 0.000726624459085059\n",
            "     | > grad_norm: 0.523591160774231\n",
            "     | > step_time: 1.3199\n",
            "     | > loader_time: 0.0051\n",
            "\n",
            "\u001b[1m   --> STEP: 292/405 -- GLOBAL_STEP: 7600\u001b[0m\n",
            "     | > loss: 1.08103 \n",
            "     | > log_mle: 0.79803 \n",
            "     | > loss_dur: 0.28300 \n",
            "     | > align_error: 0.12165 \n",
            "     | > current_lr: 0.0007254285261188993\n",
            "     | > grad_norm: 0.2548803985118866\n",
            "     | > step_time: 1.4562\n",
            "     | > loader_time: 0.008\n",
            "\n",
            "\u001b[1m   --> STEP: 317/405 -- GLOBAL_STEP: 7625\u001b[0m\n",
            "     | > loss: 1.07813 \n",
            "     | > log_mle: 0.79974 \n",
            "     | > loss_dur: 0.27839 \n",
            "     | > align_error: 0.10938 \n",
            "     | > current_lr: 0.0007242384788493363\n",
            "     | > grad_norm: 0.17998680472373962\n",
            "     | > step_time: 1.3983\n",
            "     | > loader_time: 0.0129\n",
            "\n",
            "\u001b[1m   --> STEP: 342/405 -- GLOBAL_STEP: 7650\u001b[0m\n",
            "     | > loss: 1.08378 \n",
            "     | > log_mle: 0.79832 \n",
            "     | > loss_dur: 0.28546 \n",
            "     | > align_error: 0.10141 \n",
            "     | > current_lr: 0.0007230542691575155\n",
            "     | > grad_norm: 0.31019607186317444\n",
            "     | > step_time: 1.6373\n",
            "     | > loader_time: 0.0067\n",
            "\n",
            "\u001b[1m   --> STEP: 367/405 -- GLOBAL_STEP: 7675\u001b[0m\n",
            "     | > loss: 1.07263 \n",
            "     | > log_mle: 0.79993 \n",
            "     | > loss_dur: 0.27270 \n",
            "     | > align_error: 0.03168 \n",
            "     | > current_lr: 0.0007218758494735453\n",
            "     | > grad_norm: 0.7102156281471252\n",
            "     | > step_time: 1.4116\n",
            "     | > loader_time: 0.0089\n",
            "\n",
            "\u001b[1m   --> STEP: 392/405 -- GLOBAL_STEP: 7700\u001b[0m\n",
            "     | > loss: 1.06203 \n",
            "     | > log_mle: 0.80039 \n",
            "     | > loss_dur: 0.26164 \n",
            "     | > align_error: 0.07988 \n",
            "     | > current_lr: 0.0007207031727684709\n",
            "     | > grad_norm: 0.5570501089096069\n",
            "     | > step_time: 1.6151\n",
            "     | > loader_time: 0.0081\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.84203  (0.84203)\n",
            "     | > log_mle: 0.62339  (0.62339)\n",
            "     | > loss_dur: 0.21864  (0.21864)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.93688  (0.93688)\n",
            "     | > log_mle: 0.72090  (0.72090)\n",
            "     | > loss_dur: 0.21599  (0.21599)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.91875  (0.92782)\n",
            "     | > log_mle: 0.72693  (0.72392)\n",
            "     | > loss_dur: 0.19181  (0.20390)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 0.93844  (0.93136)\n",
            "     | > log_mle: 0.75637  (0.73473)\n",
            "     | > loss_dur: 0.18207  (0.19662)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 0.93634  (0.93260)\n",
            "     | > log_mle: 0.78429  (0.74712)\n",
            "     | > loss_dur: 0.15205  (0.18548)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.93158  (0.93240)\n",
            "     | > log_mle: 0.78213  (0.75413)\n",
            "     | > loss_dur: 0.14944  (0.17827)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 0.93031  (0.93205)\n",
            "     | > log_mle: 0.79587  (0.76108)\n",
            "     | > loss_dur: 0.13444  (0.17097)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 0.92600  (0.93119)\n",
            "     | > log_mle: 0.79968  (0.76660)\n",
            "     | > loss_dur: 0.12633  (0.16459)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 0.90940  (0.92846)\n",
            "     | > log_mle: 0.79408  (0.77003)\n",
            "     | > loss_dur: 0.11531  (0.15843)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 2.65156 \u001b[0m(+0.01087)\n",
            "     | > avg_loss:\u001b[92m 0.92846 \u001b[0m(-0.01397)\n",
            "     | > avg_log_mle:\u001b[92m 0.77003 \u001b[0m(-0.00276)\n",
            "     | > avg_loss_dur:\u001b[92m 0.15843 \u001b[0m(-0.01120)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : ../drive/MyDrive/train/-August-04-2021_01+09AM-0fc9f387/best_model_7714.pth.tar\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 19/20\u001b[0m\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 12969\n",
            " | > Max length sequence: 187\n",
            " | > Min length sequence: 5\n",
            " | > Avg length sequence: 98.3403500655409\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > TRAINING (2021-08-04 03:41:45) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 11/405 -- GLOBAL_STEP: 7725\u001b[0m\n",
            "     | > loss: 0.97253 \n",
            "     | > log_mle: 0.66088 \n",
            "     | > loss_dur: 0.31164 \n",
            "     | > align_error: 0.41218 \n",
            "     | > current_lr: 0.0007195361925463913\n",
            "     | > grad_norm: 1.9443758726119995\n",
            "     | > step_time: 0.662\n",
            "     | > loader_time: 0.0018\n",
            "\n",
            "\u001b[1m   --> STEP: 36/405 -- GLOBAL_STEP: 7750\u001b[0m\n",
            "     | > loss: 0.98334 \n",
            "     | > log_mle: 0.71149 \n",
            "     | > loss_dur: 0.27185 \n",
            "     | > align_error: 0.40037 \n",
            "     | > current_lr: 0.0007183748628367158\n",
            "     | > grad_norm: 0.17058011889457703\n",
            "     | > step_time: 0.756\n",
            "     | > loader_time: 0.0019\n",
            "\n",
            "\u001b[1m   --> STEP: 61/405 -- GLOBAL_STEP: 7775\u001b[0m\n",
            "     | > loss: 0.94605 \n",
            "     | > log_mle: 0.67899 \n",
            "     | > loss_dur: 0.26706 \n",
            "     | > align_error: 0.20739 \n",
            "     | > current_lr: 0.0007172191381865586\n",
            "     | > grad_norm: 0.5502458214759827\n",
            "     | > step_time: 0.7726\n",
            "     | > loader_time: 0.0018\n",
            "\n",
            "\u001b[1m   --> STEP: 86/405 -- GLOBAL_STEP: 7800\u001b[0m\n",
            "     | > loss: 1.02641 \n",
            "     | > log_mle: 0.74446 \n",
            "     | > loss_dur: 0.28195 \n",
            "     | > align_error: 0.34346 \n",
            "     | > current_lr: 0.0007160689736532665\n",
            "     | > grad_norm: 0.1779375523328781\n",
            "     | > step_time: 0.8876\n",
            "     | > loader_time: 0.0025\n",
            "\n",
            "\u001b[1m   --> STEP: 111/405 -- GLOBAL_STEP: 7825\u001b[0m\n",
            "     | > loss: 1.05110 \n",
            "     | > log_mle: 0.76765 \n",
            "     | > loss_dur: 0.28345 \n",
            "     | > align_error: 0.31331 \n",
            "     | > current_lr: 0.0007149243247970778\n",
            "     | > grad_norm: 0.12188054621219635\n",
            "     | > step_time: 1.0572\n",
            "     | > loader_time: 0.0089\n",
            "\n",
            "\u001b[1m   --> STEP: 136/405 -- GLOBAL_STEP: 7850\u001b[0m\n",
            "     | > loss: 1.05547 \n",
            "     | > log_mle: 0.76600 \n",
            "     | > loss_dur: 0.28947 \n",
            "     | > align_error: 0.21364 \n",
            "     | > current_lr: 0.0007137851476739114\n",
            "     | > grad_norm: 0.2220931053161621\n",
            "     | > step_time: 1.0864\n",
            "     | > loader_time: 0.0027\n",
            "\n",
            "\u001b[1m   --> STEP: 161/405 -- GLOBAL_STEP: 7875\u001b[0m\n",
            "     | > loss: 1.04581 \n",
            "     | > log_mle: 0.76104 \n",
            "     | > loss_dur: 0.28478 \n",
            "     | > align_error: 0.31273 \n",
            "     | > current_lr: 0.0007126513988282802\n",
            "     | > grad_norm: 0.4424932897090912\n",
            "     | > step_time: 1.0119\n",
            "     | > loader_time: 0.0029\n",
            "\n",
            "\u001b[1m   --> STEP: 186/405 -- GLOBAL_STEP: 7900\u001b[0m\n",
            "     | > loss: 1.04494 \n",
            "     | > log_mle: 0.76079 \n",
            "     | > loss_dur: 0.28415 \n",
            "     | > align_error: 0.04779 \n",
            "     | > current_lr: 0.0007115230352863289\n",
            "     | > grad_norm: 0.18095867335796356\n",
            "     | > step_time: 1.0744\n",
            "     | > loader_time: 0.0029\n",
            "\n",
            "\u001b[1m   --> STEP: 211/405 -- GLOBAL_STEP: 7925\u001b[0m\n",
            "     | > loss: 1.06083 \n",
            "     | > log_mle: 0.77620 \n",
            "     | > loss_dur: 0.28462 \n",
            "     | > align_error: 0.14036 \n",
            "     | > current_lr: 0.0007104000145489925\n",
            "     | > grad_norm: 0.3325411081314087\n",
            "     | > step_time: 1.1193\n",
            "     | > loader_time: 0.003\n",
            "\n",
            "\u001b[1m   --> STEP: 236/405 -- GLOBAL_STEP: 7950\u001b[0m\n",
            "     | > loss: 1.08748 \n",
            "     | > log_mle: 0.79503 \n",
            "     | > loss_dur: 0.29245 \n",
            "     | > align_error: 0.16430 \n",
            "     | > current_lr: 0.0007092822945852725\n",
            "     | > grad_norm: 0.27366846799850464\n",
            "     | > step_time: 1.2638\n",
            "     | > loader_time: 0.0036\n",
            "\n",
            "\u001b[1m   --> STEP: 261/405 -- GLOBAL_STEP: 7975\u001b[0m\n",
            "     | > loss: 1.08820 \n",
            "     | > log_mle: 0.79982 \n",
            "     | > loss_dur: 0.28838 \n",
            "     | > align_error: 0.15108 \n",
            "     | > current_lr: 0.000708169833825631\n",
            "     | > grad_norm: 0.15381677448749542\n",
            "     | > step_time: 1.3185\n",
            "     | > loader_time: 0.0132\n",
            "\n",
            "\u001b[1m   --> STEP: 286/405 -- GLOBAL_STEP: 8000\u001b[0m\n",
            "     | > loss: 1.07118 \n",
            "     | > log_mle: 0.78562 \n",
            "     | > loss_dur: 0.28556 \n",
            "     | > align_error: 0.13264 \n",
            "     | > current_lr: 0.0007070625911554956\n",
            "     | > grad_norm: 0.34248122572898865\n",
            "     | > step_time: 1.3597\n",
            "     | > loader_time: 0.0102\n",
            "\n",
            "\u001b[1m   --> STEP: 311/405 -- GLOBAL_STEP: 8025\u001b[0m\n",
            "     | > loss: 1.08645 \n",
            "     | > log_mle: 0.79769 \n",
            "     | > loss_dur: 0.28876 \n",
            "     | > align_error: 0.11276 \n",
            "     | > current_lr: 0.0007059605259088782\n",
            "     | > grad_norm: 0.48142296075820923\n",
            "     | > step_time: 1.4167\n",
            "     | > loader_time: 0.007\n",
            "\n",
            "\u001b[1m   --> STEP: 336/405 -- GLOBAL_STEP: 8050\u001b[0m\n",
            "     | > loss: 1.08194 \n",
            "     | > log_mle: 0.80043 \n",
            "     | > loss_dur: 0.28151 \n",
            "     | > align_error: 0.09186 \n",
            "     | > current_lr: 0.0007048635978621017\n",
            "     | > grad_norm: 0.3098524212837219\n",
            "     | > step_time: 1.5571\n",
            "     | > loader_time: 0.0126\n",
            "\n",
            "\u001b[1m   --> STEP: 361/405 -- GLOBAL_STEP: 8075\u001b[0m\n",
            "     | > loss: 1.07923 \n",
            "     | > log_mle: 0.79885 \n",
            "     | > loss_dur: 0.28038 \n",
            "     | > align_error: 0.05152 \n",
            "     | > current_lr: 0.0007037717672276334\n",
            "     | > grad_norm: 0.4756665825843811\n",
            "     | > step_time: 1.4901\n",
            "     | > loader_time: 0.008\n",
            "\n",
            "\u001b[1m   --> STEP: 386/405 -- GLOBAL_STEP: 8100\u001b[0m\n",
            "     | > loss: 1.06799 \n",
            "     | > log_mle: 0.80034 \n",
            "     | > loss_dur: 0.26766 \n",
            "     | > align_error: 0.03662 \n",
            "     | > current_lr: 0.0007026849946480239\n",
            "     | > grad_norm: 0.17773453891277313\n",
            "     | > step_time: 1.5167\n",
            "     | > loader_time: 0.0104\n",
            "\n",
            " > DataLoader initialization\n",
            " | > Use phonemes: False\n",
            " | > Number of instances : 131\n",
            " | > Max length sequence: 173\n",
            " | > Min length sequence: 19\n",
            " | > Avg length sequence: 98.95419847328245\n",
            " | > Num. instances discarded by max-min (max=500, min=3) seq limits: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss: 0.85420  (0.85420)\n",
            "     | > log_mle: 0.62219  (0.62219)\n",
            "     | > loss_dur: 0.23201  (0.23201)\n",
            "     | > align_error: 0.25108  (0.25108)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss: 0.97063  (0.97063)\n",
            "     | > log_mle: 0.72022  (0.72022)\n",
            "     | > loss_dur: 0.25041  (0.25041)\n",
            "     | > align_error: 0.19284  (0.19284)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss: 0.96385  (0.96724)\n",
            "     | > log_mle: 0.72636  (0.72329)\n",
            "     | > loss_dur: 0.23750  (0.24395)\n",
            "     | > align_error: 0.05515  (0.12399)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss: 0.99325  (0.97591)\n",
            "     | > log_mle: 0.75595  (0.73418)\n",
            "     | > loss_dur: 0.23730  (0.24174)\n",
            "     | > align_error: 0.08045  (0.10948)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss: 1.00743  (0.98379)\n",
            "     | > log_mle: 0.78379  (0.74658)\n",
            "     | > loss_dur: 0.22364  (0.23721)\n",
            "     | > align_error: 0.14874  (0.11929)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss: 0.99894  (0.98682)\n",
            "     | > log_mle: 0.78161  (0.75359)\n",
            "     | > loss_dur: 0.21733  (0.23324)\n",
            "     | > align_error: 0.14732  (0.12490)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss: 1.01953  (0.99227)\n",
            "     | > log_mle: 0.79547  (0.76057)\n",
            "     | > loss_dur: 0.22406  (0.23171)\n",
            "     | > align_error: 0.05520  (0.11328)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss: 1.02036  (0.99629)\n",
            "     | > log_mle: 0.79920  (0.76609)\n",
            "     | > loss_dur: 0.22116  (0.23020)\n",
            "     | > align_error: 0.04943  (0.10416)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss: 1.00408  (0.99726)\n",
            "     | > log_mle: 0.79373  (0.76954)\n",
            "     | > loss_dur: 0.21035  (0.22772)\n",
            "     | > align_error: 0.06551  (0.09933)\n",
            "\n",
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 2.90581 \u001b[0m(+0.25426)\n",
            "     | > avg_loss:\u001b[91m 0.99726 \u001b[0m(+0.06880)\n",
            "     | > avg_log_mle:\u001b[92m 0.76954 \u001b[0m(-0.00049)\n",
            "     | > avg_loss_dur:\u001b[91m 0.22772 \u001b[0m(+0.06929)\n",
            "     | > avg_align_error: 0.09933 \u001b[0m(+0.00000)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}